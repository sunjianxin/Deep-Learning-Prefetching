{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, show_statistic, min_max_scaling, normalize_one, normalize_all, construct_train_valid_tensor, construct_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14975\n",
      "1975\n"
     ]
    }
   ],
   "source": [
    "# load each of the 5 dataset and do min_max_scaling on each of them\n",
    "train_set_x_1, valid_set_x_1, train_set_y_1, valid_set_y_1, train_set_z_1, valid_set_z_1 = load_data_2('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, valid_set_x_2, train_set_y_2, valid_set_y_2, train_set_z_2, valid_set_z_2 = load_data_2('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, valid_set_x_3, train_set_y_3, valid_set_y_3, train_set_z_3, valid_set_z_3 = load_data_2('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, valid_set_x_4, train_set_y_4, valid_set_y_4, train_set_z_4, valid_set_z_4 = load_data_2('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, valid_set_x_5, train_set_y_5, valid_set_y_5, train_set_z_5, valid_set_z_5 = load_data_2('data_preprocessing/test_5_training_xyz.txt', 400)\n",
    "\n",
    "# do min-max-scaling for each data set\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_1)\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_2)\n",
    "min_max_scaling(train_set_x_3)\n",
    "min_max_scaling(train_set_x_4)\n",
    "min_max_scaling(train_set_x_5)\n",
    "\n",
    "min_max_scaling(valid_set_x_1)\n",
    "min_max_scaling(valid_set_x_2)\n",
    "min_max_scaling(valid_set_x_3)\n",
    "min_max_scaling(valid_set_x_4)\n",
    "min_max_scaling(valid_set_x_5)\n",
    "\n",
    "min_max_scaling(train_set_y_1)\n",
    "min_max_scaling(train_set_y_2)\n",
    "min_max_scaling(train_set_y_3)\n",
    "min_max_scaling(train_set_y_4)\n",
    "min_max_scaling(train_set_y_5)\n",
    "\n",
    "min_max_scaling(valid_set_y_1)\n",
    "min_max_scaling(valid_set_y_2)\n",
    "min_max_scaling(valid_set_y_3)\n",
    "min_max_scaling(valid_set_y_4)\n",
    "min_max_scaling(valid_set_y_5)\n",
    "\n",
    "min_max_scaling(train_set_z_1)\n",
    "min_max_scaling(train_set_z_2)\n",
    "min_max_scaling(train_set_z_3)\n",
    "min_max_scaling(train_set_z_4)\n",
    "min_max_scaling(train_set_z_5)\n",
    "\n",
    "min_max_scaling(valid_set_z_1)\n",
    "min_max_scaling(valid_set_z_2)\n",
    "min_max_scaling(valid_set_z_3)\n",
    "min_max_scaling(valid_set_z_4)\n",
    "min_max_scaling(valid_set_z_5)\n",
    "\n",
    "'''\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "x_mean, x_std = normalize_all(train_set_x_1, train_set_x_2, train_set_x_3, train_set_x_4, train_set_x_5, valid_set_x_1, valid_set_x_2, valid_set_x_3, valid_set_x_4, valid_set_x_5)\n",
    "y_mean, y_std = normalize_all(train_set_y_1, train_set_y_2, train_set_y_3, train_set_y_4, train_set_y_5, valid_set_y_1, valid_set_y_2, valid_set_y_3, valid_set_y_4, valid_set_y_5)\n",
    "z_mean, z_std = normalize_all(train_set_z_1, train_set_z_2, train_set_z_3, train_set_z_4, train_set_z_5, valid_set_z_1, valid_set_z_2, valid_set_z_3, valid_set_z_4, valid_set_z_5)\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "\n",
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)\n",
    "'''\n",
    "train_dataset_1, train_label_1, valid_dataset_1, valid_label_1 = construct_train_valid_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           valid_set_x_1,\n",
    "                                                                                           valid_set_y_1,\n",
    "                                                                                           valid_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, valid_dataset_2, valid_label_2 = construct_train_valid_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           valid_set_x_2,\n",
    "                                                                                           valid_set_y_2,\n",
    "                                                                                           valid_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, valid_dataset_3, valid_label_3 = construct_train_valid_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           valid_set_x_3,\n",
    "                                                                                           valid_set_y_3,\n",
    "                                                                                           valid_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, valid_dataset_4, valid_label_4 = construct_train_valid_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           valid_set_x_4,\n",
    "                                                                                           valid_set_y_4,\n",
    "                                                                                           valid_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, valid_dataset_5, valid_label_5 = construct_train_valid_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           valid_set_x_5,\n",
    "                                                                                           valid_set_y_5,\n",
    "                                                                                           valid_set_z_5,\n",
    "                                                                                           window_size)\n",
    "\n",
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "valid_dataset = np.concatenate((valid_dataset_1,\n",
    "                               valid_dataset_2,\n",
    "                               valid_dataset_3,\n",
    "                               valid_dataset_4,\n",
    "                               valid_dataset_5), axis=0)\n",
    "valid_label = np.concatenate((valid_label_1,\n",
    "                             valid_label_2,\n",
    "                             valid_label_3,\n",
    "                             valid_label_4,\n",
    "                             valid_label_5), axis=0)\n",
    "\n",
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(valid_dataset, valid_label)\n",
    "print(len(valid_set))\n",
    "\n",
    "# batch_size = 30\n",
    "# batch_size = 50\n",
    "batch_size = 650\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# keep the valid data trajectory order\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle valid data for using continous trajectory later on\n",
    "# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "min: -1.5190000000000001\n",
      "max: 1.41144\n",
      "mean: -0.058533702725000004\n",
      "std: 0.7861266362964188\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "395\n",
      "395\n",
      "395\n",
      "395\n",
      "395\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4999879488895008\n",
      "std: 0.3247696467306799\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.49462499118347864\n",
      "std: 0.2888055402177855\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5101133475610831\n",
      "std: 0.25434808389556507\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4851962413697185\n",
      "std: 0.2642834718504606\n"
     ]
    }
   ],
   "source": [
    "test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "# do min-max-scaling for each test data set\n",
    "show_statistic(test_set_x_1)\n",
    "min_max_scaling(test_set_x_1)\n",
    "show_statistic(test_set_x_1)\n",
    "min_max_scaling(test_set_x_2)\n",
    "min_max_scaling(test_set_x_3)\n",
    "min_max_scaling(test_set_x_4)\n",
    "min_max_scaling(test_set_x_5)\n",
    "\n",
    "min_max_scaling(test_set_y_1)\n",
    "min_max_scaling(test_set_y_2)\n",
    "min_max_scaling(test_set_y_3)\n",
    "min_max_scaling(test_set_y_4)\n",
    "min_max_scaling(test_set_y_5)\n",
    "\n",
    "min_max_scaling(test_set_z_1)\n",
    "min_max_scaling(test_set_z_2)\n",
    "min_max_scaling(test_set_z_3)\n",
    "min_max_scaling(test_set_z_4)\n",
    "min_max_scaling(test_set_z_5)\n",
    "\n",
    "'''\n",
    "show_statistic(test_set_x_1)\n",
    "# do normalization on x of validation test set\n",
    "normalize_one(test_set_x_1, x_mean, x_std)\n",
    "show_statistic(test_set_x_1)\n",
    "normalize_one(test_set_x_2, x_mean, x_std)\n",
    "normalize_one(test_set_x_3, x_mean, x_std)\n",
    "normalize_one(test_set_x_4, x_mean, x_std)\n",
    "normalize_one(test_set_x_5, x_mean, x_std)\n",
    "# do normalization on y of validation test set\n",
    "normalize_one(test_set_y_1, y_mean, y_std)\n",
    "normalize_one(test_set_y_2, y_mean, y_std)\n",
    "normalize_one(test_set_y_3, y_mean, y_std)\n",
    "normalize_one(test_set_y_4, y_mean, y_std)\n",
    "normalize_one(test_set_y_5, y_mean, y_std)\n",
    "# do normalization on z of validation test set\n",
    "normalize_one(test_set_z_1, z_mean, z_std)\n",
    "normalize_one(test_set_z_2, z_mean, z_std)\n",
    "normalize_one(test_set_z_3, z_mean, z_std)\n",
    "normalize_one(test_set_z_4, z_mean, z_std)\n",
    "normalize_one(test_set_z_5, z_mean, z_std)\n",
    "# show_statistic(train_set_x_1)\n",
    "'''\n",
    "test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                     test_set_y_1,\n",
    "                                                     test_set_z_1,\n",
    "                                                     window_size)\n",
    "test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                     test_set_y_2,\n",
    "                                                     test_set_z_2,\n",
    "                                                     window_size)\n",
    "test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                     test_set_y_3,\n",
    "                                                     test_set_z_3,\n",
    "                                                     window_size)\n",
    "test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                     test_set_y_4,\n",
    "                                                     test_set_z_4,\n",
    "                                                     window_size)\n",
    "test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                     test_set_y_5,\n",
    "                                                     test_set_z_5,\n",
    "                                                     window_size)\n",
    "\n",
    "test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "print(len(test_set_1))\n",
    "print(len(test_set_2))\n",
    "print(len(test_set_3))\n",
    "print(len(test_set_4))\n",
    "print(len(test_set_5))\n",
    "show_statistic(test_set_x_1)\n",
    "show_statistic(test_set_x_2)\n",
    "show_statistic(test_set_x_3)\n",
    "show_statistic(test_set_x_4)\n",
    "show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "batch_size = 650\n",
    "test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# learning_rate = 2.2\n",
    "learning_rate = 1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_loss(loader):\n",
    "    total_loss = 0.0\n",
    "    batch = 0\n",
    "    # seq = []\n",
    "    pred = []\n",
    "    # gt = []\n",
    "    for i, (seqs, labels) in enumerate(loader):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        batch = i + 1\n",
    "        \n",
    "    return total_loss/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.05396550909305612, valid_loss: 0.06239989306777716, test_loss: 0.04154685139656067\n",
      "epoch: 200, train_loss: 0.000461296539773078, valid_loss: 0.0010864985815715045, test_loss: 0.002871957840397954\n",
      "epoch: 400, train_loss: 0.00022881138041460267, valid_loss: 0.0006057178761693649, test_loss: 0.002046518959105015\n",
      "epoch: 600, train_loss: 0.0002140673908191578, valid_loss: 0.0006132449561846443, test_loss: 0.001920237671583891\n",
      "epoch: 800, train_loss: 0.00011611338307678427, valid_loss: 0.0003782070998568088, test_loss: 0.0015887103509157896\n",
      "epoch: 1000, train_loss: 0.00011936615859061324, valid_loss: 0.00034116883034585044, test_loss: 0.0014617490814998746\n",
      "epoch: 1200, train_loss: 8.942620221811619e-05, valid_loss: 0.00030858638092468027, test_loss: 0.0013482391368597746\n",
      "epoch: 1400, train_loss: 8.71090084425911e-05, valid_loss: 0.00027511796179169323, test_loss: 0.0012569286627694964\n",
      "epoch: 1600, train_loss: 8.459116876717114e-05, valid_loss: 0.0002609535731608048, test_loss: 0.0011976966634392738\n",
      "epoch: 1800, train_loss: 7.733998199910275e-05, valid_loss: 0.00023912453434604686, test_loss: 0.001135512487962842\n",
      "epoch: 2000, train_loss: 7.715204477184064e-05, valid_loss: 0.0002260919327454758, test_loss: 0.0010990181472152472\n",
      "epoch: 2200, train_loss: 7.235745435233791e-05, valid_loss: 0.00021729243781010155, test_loss: 0.001075061270967126\n",
      "epoch: 2400, train_loss: 7.711545549682342e-05, valid_loss: 0.00021287077834131196, test_loss: 0.0010597460204735398\n",
      "epoch: 2600, train_loss: 7.67400357896501e-05, valid_loss: 0.00022489852926810272, test_loss: 0.0010646464070305228\n",
      "epoch: 2800, train_loss: 6.952938110771356e-05, valid_loss: 0.00019438610343058826, test_loss: 0.0010399805614724755\n",
      "epoch: 3000, train_loss: 6.860417852294631e-05, valid_loss: 0.00019313154189148918, test_loss: 0.0010348159121349454\n",
      "epoch: 3200, train_loss: 6.588655393594915e-05, valid_loss: 0.00019266731214884203, test_loss: 0.0010333516402170062\n",
      "epoch: 3400, train_loss: 6.524669690103717e-05, valid_loss: 0.00018233969512948534, test_loss: 0.0010236677480861545\n",
      "epoch: 3600, train_loss: 6.60338087072887e-05, valid_loss: 0.00019415816859691404, test_loss: 0.0010386507492512465\n",
      "epoch: 3800, train_loss: 6.354649591836885e-05, valid_loss: 0.00019363208502909401, test_loss: 0.0010350951924920082\n",
      "epoch: 4000, train_loss: 6.490917379172363e-05, valid_loss: 0.00019730556141439592, test_loss: 0.001049440703354776\n",
      "epoch: 4200, train_loss: 6.0978802745618545e-05, valid_loss: 0.00017222795395355206, test_loss: 0.0010175175266340375\n",
      "epoch: 4400, train_loss: 5.9146421297858374e-05, valid_loss: 0.00016710549061826896, test_loss: 0.0010143195977434516\n",
      "epoch: 4600, train_loss: 5.900841006223345e-05, valid_loss: 0.00017795235544326715, test_loss: 0.001027204212732613\n",
      "epoch: 4800, train_loss: 6.036350547825956e-05, valid_loss: 0.0001677721866144566, test_loss: 0.0010194557253271341\n",
      "epoch: 5000, train_loss: 5.827783434142475e-05, valid_loss: 0.00016444496668555075, test_loss: 0.001019755145534873\n",
      "epoch: 5200, train_loss: 5.6581899343655095e-05, valid_loss: 0.00015856524032642483, test_loss: 0.0010168652515858412\n",
      "epoch: 5400, train_loss: 5.7095095447342224e-05, valid_loss: 0.00015670734728701063, test_loss: 0.001023263088427484\n",
      "epoch: 5600, train_loss: 5.528649065430121e-05, valid_loss: 0.0001577944312884938, test_loss: 0.0010247535537928343\n",
      "epoch: 5800, train_loss: 5.408396259554138e-05, valid_loss: 0.00014753839514014544, test_loss: 0.0010195281356573105\n",
      "epoch: 6000, train_loss: 5.522863405834263e-05, valid_loss: 0.00014555497091350844, test_loss: 0.0010225223377346992\n",
      "epoch: 6200, train_loss: 5.2432149459491484e-05, valid_loss: 0.00014683336485177279, test_loss: 0.0010260341223329306\n"
     ]
    }
   ],
   "source": [
    "seq_dim = window_size\n",
    "\n",
    "num_epochs = 6400\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "test_loss_1 = []\n",
    "test_loss_2 = []\n",
    "test_loss_3 = []\n",
    "test_loss_4 = []\n",
    "test_loss_5 = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    train_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(train_loader):\n",
    "        # print(\"train: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(seqs.float())\n",
    "        outputs = model(seqs)\n",
    "        # print(outputs.is_cuda)\n",
    "        # print(labels.is_cuda)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs.dtype)\n",
    "        \n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_train_loss += loss.data.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch = i + 1;\n",
    "        \n",
    "        # print(\"loss: \", loss.data)\n",
    "    \n",
    "    total_valid_loss = 0.0\n",
    "    valid_batch = 0\n",
    "    # valid_seq = []\n",
    "    valid_pred = []\n",
    "    # valid_gt = []\n",
    "    for i, (seqs, labels) in enumerate(valid_loader):\n",
    "        # print(\"valid: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_valid_loss += loss.data.item()\n",
    "        # valid_gt.append(labels)\n",
    "        valid_pred.append(outputs)\n",
    "        # valid_seq.append(seqs)\n",
    "        \n",
    "        valid_batch = i + 1\n",
    "    \n",
    "    '''\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    # test_seq = []\n",
    "    test_pred = []\n",
    "    # test_gt = []\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        test_pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        test_batch = i + 1\n",
    "    '''\n",
    "    test_cur_loss_1 = get_epoch_loss(test_loader_1)\n",
    "    test_cur_loss_2 = get_epoch_loss(test_loader_2)\n",
    "    test_cur_loss_3 = get_epoch_loss(test_loader_3)\n",
    "    test_cur_loss_4 = get_epoch_loss(test_loader_4)\n",
    "    test_cur_loss_5 = get_epoch_loss(test_loader_5)\n",
    "    \n",
    "    # print(\"train batch: \", train_batch)\n",
    "    # print(\"valid batch: \", valid_batch)\n",
    "    train_loss.append(total_train_loss/train_batch)\n",
    "    valid_loss.append(total_valid_loss/valid_batch)\n",
    "    # test_loss.append(total_test_loss/test_batch)\n",
    "    test_loss_1.append(test_cur_loss_1)\n",
    "    test_loss_2.append(test_cur_loss_2)\n",
    "    test_loss_3.append(test_cur_loss_3)\n",
    "    test_loss_4.append(test_cur_loss_4)\n",
    "    test_loss_5.append(test_cur_loss_5)\n",
    "    # train_loss.append(total_train_loss)\n",
    "    # valid_loss.append(total_valid_loss)\n",
    "    # print(\"epoch: {}, train_loss: {}, valid_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_valid_loss/valid_batch, total_test_loss/test_batch))\n",
    "    if (epoch % 200 == 0):\n",
    "        print(\"epoch: {}, train_loss: {}, valid_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_valid_loss/valid_batch, test_cur_loss_1))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('1600/without_normalization/window_5/train_loss.npy', np.asarray(train_loss))\n",
    "np.save('1600/without_normalization/window_5/valid_loss.npy', np.asarray(valid_loss))\n",
    "np.save('1600/without_normalization/window_5/test_loss_1.npy', np.asarray(test_loss_1))\n",
    "np.save('1600/without_normalization/window_5/test_loss_2.npy', np.asarray(test_loss_2))\n",
    "np.save('1600/without_normalization/window_5/test_loss_3.npy', np.asarray(test_loss_3))\n",
    "np.save('1600/without_normalization/window_5/test_loss_4.npy', np.asarray(test_loss_4))\n",
    "np.save('1600/without_normalization/window_5/test_loss_5.npy', np.asarray(test_loss_5))\n",
    "\n",
    "  #   array([[2, 3, 4], [1, 2]], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 6400\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "valid_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), valid_loss[starting_epoch:end_epoch])\n",
    "test_loss_curve_1, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_1[starting_epoch:end_epoch])\n",
    "test_loss_curve_2, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_2[starting_epoch:end_epoch])\n",
    "test_loss_curve_3, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_3[starting_epoch:end_epoch])\n",
    "test_loss_curve_4, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_4[starting_epoch:end_epoch])\n",
    "test_loss_curve_5, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_5[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, valid_loss_curve, test_loss_curve_1, test_loss_curve_2, test_loss_curve_3, test_loss_curve_4, test_loss_curve_5], ['Train Loss', 'Validation Loss', 'Test Loss 1', 'Test Loss 2', 'Test Loss 3', 'Test Loss 4', 'Test Loss 5'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x mean: 0.5009409709733069 ; std: 0.2436898615684393\n",
    "y mean: 0.4416590158205868 ; std: 0.23351418998386245\n",
    "z mean: 0.48521343842977466 ; std: 0.2423616199566707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch: 1599, train_loss: 0.0007271119671792763, valid_loss: 0.002470962528605014, test_loss: 0.001238117110915482\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get valid results\n",
    "seq_dim = window_size\n",
    "input_dim = 3\n",
    "# valid_seq = []\n",
    "valid_predd = []\n",
    "# valid_gt = []\n",
    "total_valid_loss = 0.0\n",
    "valid_batch = 0\n",
    "for i, (seqs, labels) in enumerate(valid_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        \n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_valid_loss += loss.data.item()\n",
    "    valid_predd.append(outputs)\n",
    "    valid_batch = i + 1\n",
    "print(total_valid_loss/valid_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of ground true around predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore testing data\n",
    "test_seq = []\n",
    "test_gt = []\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    test_gt.append(labels)\n",
    "    test_seq.append(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = test_seq[i].numpy()\n",
    "        gt = test_gt[i].numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        test = np.append(test, test_seq[i].numpy(), axis = 0)\n",
    "        gt = np.append(gt, test_gt[i].numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test[0][-1,:].reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct analysis matrix l(0-2 col): last sequence point, g(3-5 col): next ground true point, p(6-8 col): predicted point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    last_point = test[i][-1, :].reshape(1, -1)\n",
    "    gt_point = gt[i].reshape(1, -1)\n",
    "    pred_point = pred[i].reshape(1, -1)\n",
    "    row = np.append(np.append(last_point, gt_point, axis = 1), pred_point, axis = 1)\n",
    "    if (i == 0):\n",
    "        analysis = row\n",
    "    else:\n",
    "        analysis = np.append(analysis, row, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rotation matrix\n",
    "# note: input vec_1 and vec_2 have to be normalized before passing to the function\n",
    "def get_rotatin_mat(vec_1, vec_2):\n",
    "    a,b = vec_1.reshape(3), vec_2.reshape(3)\n",
    "    v = np.cross(a,b)\n",
    "    c = np.dot(a,b)\n",
    "    s = np.linalg.norm(v)\n",
    "    # print(\"s\", s)\n",
    "    if (s == 0):\n",
    "        return np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "    I = np.identity(3)\n",
    "    vXStr = '{} {} {}; {} {} {}; {} {} {}'.format(0, -v[2], v[1], v[2], 0, -v[0], -v[1], v[0], 0)\n",
    "    k = np.matrix(vXStr)\n",
    "    r = I + k + np.matmul(k,k) * ((1 -c)/(s**2))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 1, 0])\n",
    "a = a/np.linalg.norm(a)\n",
    "b = np.array([4, 5, 6])\n",
    "b = b/np.linalg.norm(b)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "m = get_rotatin_mat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(m, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(b[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b/np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "a_n = a/np.linalg.norm(a)\n",
    "\n",
    "b = np.array([[4,5,6]])\n",
    "b_n = b/np.linalg.norm(b)\n",
    "\n",
    "c = np.array([[7,8,9]])\n",
    "c_n = c/np.linalg.norm(c)\n",
    "\n",
    "r = get_rotatin_mat(b_n, a_n) # b to a\n",
    "\n",
    "b_p = np.dot(r, b.reshape(3, -1))\n",
    "c_p = np.dot(r, c.reshape(3, -1))\n",
    "c_p = np.asarray(c_p)\n",
    "c_p_n = c_p/np.linalg.norm(c_p)\n",
    "print(b.shape)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"c\", c)\n",
    "print(\"b_p\", b_p)\n",
    "print(\"c_p\", c_p)\n",
    "\n",
    "print(get_rotatin_mat(c_n, b_n))\n",
    "\n",
    "print(get_rotatin_mat(c_p_n, a_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c_p.reshape(1, -1)\n",
    "d = np.asarray(d)\n",
    "d_n = d/np.linalg.norm(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rotatin_mat(d_n, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all point to the same starting point\n",
    "# first sequence last direction\n",
    "# direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = (analysis[0, 3:6] - analysis[0, 0:3]).reshape(1, -1)\n",
    "# normalize\n",
    "direction = direction/np.linalg.norm(direction)\n",
    "# first diff between prediction and ground true\n",
    "diff_correct = (analysis[0, 6:9] - analysis[0, 3:6]).reshape(1, -1)\n",
    "# print(type(diff_correct))\n",
    "# print(diff_correct.shape)\n",
    "for i in range(1, analysis.shape[0]):\n",
    "    # cur_direction = (analysis[i, 0:3] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    cur_direction = (analysis[i, 3:6] - analysis[i, 0:3]).reshape(1, -1)\n",
    "    # normalize\n",
    "    if (np.linalg.norm(cur_direction) == 0):\n",
    "        print(i, np.linalg.norm(cur_direction))\n",
    "    cur_direction = cur_direction/np.linalg.norm(cur_direction)\n",
    "    cur_diff = (analysis[i, 6:9] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    # get rotation matrix from cur_direction to direction\n",
    "    r = get_rotatin_mat(cur_direction, direction)\n",
    "    # apply rotation matrix to the cur_diff\n",
    "    cur_diff = np.dot(r, cur_diff.reshape(3, -1))\n",
    "    cur_diff = cur_diff.reshape(1, -1)\n",
    "    # matrix type to np array type\n",
    "    cur_diff = np.asarray(cur_diff)\n",
    "    # print(cur_diff.shape)\n",
    "    diff_correct = np.append(diff_correct, cur_diff, axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_correct[:, 0], diff_correct[:, 1], diff_correct[:, 2])\n",
    "x = [-direction[0, 0], 0]\n",
    "y = [-direction[0, 1], 0]\n",
    "z = [-direction[0, 2], 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "# ax.arrow(0, 0, 0.5, 0.5, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "# ax.quiver(0, 0, 0, -direction[0, 0], -direction[0, 1], -direction[0, 2], length=5, normalize=True, color='r')\n",
    "# x = np.zeros(10)\n",
    "# y = np.zeros(10)\n",
    "# z = np.arange(10)*10 # remove *100 and the arrow heads will reappear.\n",
    "# dx = np.zeros(10)\n",
    "# dy = np.arange(10)\n",
    "# dz = np.zeros(10)\n",
    "x = np.array([0, -direction[0, 0]])\n",
    "y = np.array([0, -direction[0, 1]])\n",
    "z = np.array([0, -direction[0, 2]])\n",
    "dx = np.array([0, 0])\n",
    "dy = np.array([0, 0])\n",
    "dz = np.array([0, 0])\n",
    "# ax.quiver(x, y, z, dx, dy, dz, length=1)\n",
    "# ax.quiver(-direction[0, 0], -direction[0, 1], -direction[0, 2], 0, 0, 0, length=100, normalize=True)\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note view_angle must be normalized\n",
    "def getProjection(view_angle, point):\n",
    "    v = point - np.array([[0, 0, 0]])\n",
    "    dist = v[0, 0]*view_angle[0, 0] + v[0, 1]*view_angle[0, 1] + v[0, 2]*view_angle[0, 2]\n",
    "    projected_point = point - dist*view_angle\n",
    "    \n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))\n",
    "# normalization\n",
    "view_angle = view_angle/np.linalg.norm(view_angle)\n",
    "print(view_angle)\n",
    "for i in range(diff_correct.shape[0]):\n",
    "    if (i == 0):\n",
    "        diff_corrrect_projection = getProjection(view_angle, diff_correct[i, :])\n",
    "    else:\n",
    "        diff_corrrect_projection = np.append(diff_corrrect_projection, getProjection(view_angle, diff_correct[i, :]), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_corrrect_projection[:, 0], diff_corrrect_projection[:, 1], diff_corrrect_projection[:, 2])\n",
    "x = [-direction[0, 0]*10, 0]\n",
    "y = [-direction[0, 1]*10, 0]\n",
    "z = [-direction[0, 2]*10, 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from 3D to 2D\n",
    "diff_corrrect_projection_2D = np.delete(diff_corrrect_projection, 1, 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from matplotlib.colors import LogNorm\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(diff_corrrect_projection_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(-5., 5.)\n",
    "y = np.linspace(-5., 5.)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -clf.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
    "                 levels=np.logspace(0, 3, 300))\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(diff_corrrect_projection_2D[:, 0], diff_corrrect_projection_2D[:, 1], .8)\n",
    "\n",
    "x = [-direction[0, 0]*5, 0]\n",
    "z = [-direction[0, 2]*5, 0]\n",
    "plt.plot(x, z, label='parametric curve', color='r')\n",
    "\n",
    "plt.title('Negative log-likelihood predicted by a GMM')\n",
    "plt.axis('tight')\n",
    "plt.xlabel(\"Direction 1\")\n",
    "plt.ylabel(\"Direction 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "C = np.array([[0., -0.7], [3.5, .7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_gaussian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(direction[0, 0], 2) + pow(direction[0, 1], 2) + pow(direction[0, 2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[2, 0:3] - analysis[2, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[3, 0:3] - analysis[3, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = direction/np.linalg.norm(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (analysis[1, 0:3] - analysis[1, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d/np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(0.84804916, 2) + pow(0.5299187, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 3:6] - analysis[0, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([13.259178  ,  0.37397736, 20.36138]) - np.asarray([12.658086  ,  1.293065  ,20.224268])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = \n",
    "        gt = test_gt[i].cpu().detach().numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        gt = np.append(gt, test_gt[i].cpu().detach().numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pred - gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff[:, 0], diff[:, 1], diff[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.from_numpy(gt), torch.from_numpy(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i in range(len(test_pred)):\n",
    "    l = criterion(test_gt[i], test_pred[i])\n",
    "    loss += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
