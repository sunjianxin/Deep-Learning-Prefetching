{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistic(l):\n",
    "    max_value = max(l)\n",
    "    min_value = min(l)\n",
    "    mean = sum(l)/len(l)\n",
    "    var = sum([((x - mean) ** 2) for x in l]) / len(l)\n",
    "    std = var**0.5\n",
    "    print(\"min:\", min_value)\n",
    "    print(\"max:\", max_value)\n",
    "    print(\"mean:\", mean)\n",
    "    print(\"std:\", std)\n",
    "\n",
    "def normalization(l):\n",
    "    max_value = max(l)\n",
    "    min_value = min(l)\n",
    "    mean = sum(l)/len(l)\n",
    "    var = sum([((x - mean) ** 2) for x in l]) / len(l)\n",
    "    std = var**0.5\n",
    "    # print(\"min:\", min_value)\n",
    "    # print(\"max:\", max_value)\n",
    "    # print(\"mean:\", mean)\n",
    "    # print(\"std:\", std)\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        l[i] = (l[i] - mean)/std\n",
    "    \n",
    "    return l\n",
    "    \n",
    "    \n",
    "def min_max_scaling(l):\n",
    "    max_value = max(l)\n",
    "    min_value = min(l)\n",
    "    for i in range(len(l)):\n",
    "        l[i] = (l[i] - min_value)/(max_value - min_value)\n",
    "    \n",
    "    return l\n",
    "    \n",
    "\n",
    "def load_data(file_name, test_size):\n",
    "    f = open(file_name)\n",
    "    df = pd.read_csv(f)\n",
    "    data = np.array(df[['x_0','x']])\n",
    "    x = data[:,0].tolist()\n",
    "    x.append(data[-1, -1])\n",
    "    # do normalization\n",
    "    show_statistic(x)\n",
    "    x = min_max_scaling(x)\n",
    "    show_statistic(x)\n",
    "    \n",
    "    data = np.array(df[['y_0','y']])\n",
    "    y = data[:,0].tolist()\n",
    "    y.append(data[-1, -1])\n",
    "    # do normalization\n",
    "    y = min_max_scaling(y)\n",
    "\n",
    "    data = np.array(df[['z_0','z']])\n",
    "    z = data[:,0].tolist()\n",
    "    z.append(data[-1, -1])\n",
    "    # do normalization\n",
    "    z = min_max_scaling(z)\n",
    "    \n",
    "    train_set_x = x[:-test_size]\n",
    "    test_set_x = x[-test_size:]\n",
    "    train_set_y = y[:-test_size]\n",
    "    test_set_y = y[-test_size:]\n",
    "    train_set_z = z[:-test_size]\n",
    "    test_set_z = z[-test_size:]\n",
    "    \n",
    "    return train_set_x, test_set_x, train_set_y, test_set_y, train_set_z, test_set_z\n",
    "\n",
    "def normalize_all(d_1, d_2, d_3, d_4, d_5, l_1, l_2, l_3, l_4, l_5):\n",
    "    d_all = d_1 + d_2 + d_3 + d_4 + d_5\n",
    "    # get mean and std of all traning data\n",
    "    mean = sum(d_all)/len(d_all)\n",
    "    var = sum([((x - mean) ** 2) for x in d_all]) / len(d_all)\n",
    "    std = var**0.5\n",
    "    # do normalization for each training data set\n",
    "    for i in range(len(d_1)):\n",
    "        d_1[i] = (d_1[i] - mean)/std\n",
    "    for i in range(len(d_2)):\n",
    "        d_2[i] = (d_2[i] - mean)/std\n",
    "    for i in range(len(d_3)):\n",
    "        d_3[i] = (d_3[i] - mean)/std\n",
    "    for i in range(len(d_4)):\n",
    "        d_4[i] = (d_4[i] - mean)/std\n",
    "    for i in range(len(d_5)):\n",
    "        d_5[i] = (d_5[i] - mean)/std\n",
    "        \n",
    "    # do normalization for each test data set by using mean and std from traning data sets\n",
    "    for i in range(len(l_1)):\n",
    "        l_1[i] = (l_1[i] - mean)/std\n",
    "    for i in range(len(l_2)):\n",
    "        l_2[i] = (l_2[i] - mean)/std\n",
    "    for i in range(len(l_3)):\n",
    "        l_3[i] = (l_3[i] - mean)/std\n",
    "    for i in range(len(l_4)):\n",
    "        l_4[i] = (l_4[i] - mean)/std\n",
    "    for i in range(len(l_5)):\n",
    "        l_5[i] = (l_5[i] - mean)/std\n",
    "    # return d_1, d_2, d_3, d_4, d_5, l_1, l_2, l_3, l_4, l_5\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: -53.879546999999995\n",
      "max: 55.39066700000001\n",
      "mean: 1.201871477941177\n",
      "std: 24.715028995911588\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5040844751886471\n",
      "std: 0.22618267221396277\n",
      "min: -65.839828\n",
      "max: 58.818431999999994\n",
      "mean: 0.9881137397058822\n",
      "std: 28.473596818315595\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5360891587906473\n",
      "std: 0.22841323806633868\n",
      "min: -53.948696\n",
      "max: 62.748402\n",
      "mean: 4.172788362647053\n",
      "std: 29.38591584163824\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4980542392120766\n",
      "std: 0.2518135955843416\n",
      "min: -59.879162\n",
      "max: 68.182877\n",
      "mean: 2.104038269411762\n",
      "std: 27.857703124497807\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4840091626950572\n",
      "std: 0.21753287189576814\n",
      "min: -60.71685\n",
      "max: 59.075089\n",
      "mean: 1.9414046220588248\n",
      "std: 27.954991497077714\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5230590233793504\n",
      "std: 0.23336287675481832\n"
     ]
    }
   ],
   "source": [
    "train_set_x_1, test_set_x_1, train_set_y_1, test_set_y_1, train_set_z_1, test_set_z_1 = load_data('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, test_set_x_2, train_set_y_2, test_set_y_2, train_set_z_2, test_set_z_2 = load_data('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, test_set_x_3, train_set_y_3, test_set_y_3, train_set_z_3, test_set_z_3 = load_data('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, test_set_x_4, train_set_y_4, test_set_y_4, train_set_z_4, test_set_z_4 = load_data('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, test_set_x_5, train_set_y_5, test_set_y_5, train_set_z_5, test_set_z_5 = load_data('data_preprocessing/test_5_training_xyz.txt', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_statistic(train_set_x_1)\n",
    "x_mean, x_std = normalize_all(train_set_x_1, train_set_x_2, train_set_x_3, train_set_x_4, train_set_x_5, test_set_x_1, test_set_x_2, test_set_x_3, test_set_x_4, test_set_x_5)\n",
    "y_mean, y_std = normalize_all(train_set_y_1, train_set_y_2, train_set_y_3, train_set_y_4, train_set_y_5, test_set_y_1, test_set_y_2, test_set_y_3, test_set_y_4, test_set_y_5)\n",
    "z_mean, z_std = normalize_all(train_set_z_1, train_set_z_2, train_set_z_3, train_set_z_4, train_set_z_5, test_set_z_1, test_set_z_2, test_set_z_3, test_set_z_4, test_set_z_5)\n",
    "# show_statistic(train_set_x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5067009838472856"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2332880326514156"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of input and label pairs\n",
    "def input_data(seq, ws):\n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    \n",
    "    for i in range(L - ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window, label))\n",
    "        \n",
    "    return out\n",
    "\n",
    "def get_tensor(sample_size, data_x, data_y, data_z):\n",
    "    for i in range(sample_size):\n",
    "        # construct dataset\n",
    "        x = np.asarray(data_x[i][0]).reshape(1,-1) # 1 x window_size\n",
    "        y = np.asarray(data_y[i][0]).reshape(1,-1) # 1 x window_size\n",
    "        z = np.asarray(data_z[i][0]).reshape(1,-1) # 1 x window_size\n",
    "        xyz = np.append(np.append(x, y, axis = 0), z, axis = 0) # 3 x window_size\n",
    "        xyz = np.transpose(xyz) # window_size x 3\n",
    "        window_size = xyz.shape[0]\n",
    "        # print(\"window_size\", window_size)\n",
    "        xyz = xyz.reshape(1, window_size, 3) # 1 x window_size x 3\n",
    "        if (i == 0):\n",
    "            prev_xyz = xyz\n",
    "        else:\n",
    "            prev_xyz = np.append(prev_xyz, xyz, axis = 0)\n",
    "        \n",
    "        #construct label\n",
    "        x_label = np.asarray(data_x[i][1]).reshape(1,-1) # 1 x 1\n",
    "        y_label = np.asarray(data_y[i][1]).reshape(1,-1) # 1 x 1\n",
    "        z_label = np.asarray(data_z[i][1]).reshape(1,-1) # 1 x 1\n",
    "        xyz_label = np.append(np.append(x_label, y_label, axis = 0), z_label, axis = 0) # 3 x 1\n",
    "        xyz_label = xyz_label.reshape(1, 3) # 1 x 3 x 1\n",
    "        if (i == 0):\n",
    "            prev_xyz_label = xyz_label\n",
    "        else:\n",
    "            prev_xyz_label = np.append(prev_xyz_label, xyz_label, axis = 0)\n",
    "            \n",
    "    return prev_xyz, prev_xyz_label\n",
    "\n",
    "# make train_dataset\n",
    "def construct_train_test_tensor(train_set_x, train_set_y, train_set_z, test_set_x, test_set_y, test_set_z, window_size):\n",
    "    # sequence to data sample\n",
    "    train_data_x = input_data(train_set_x, window_size)\n",
    "    train_data_y = input_data(train_set_y, window_size)\n",
    "    train_data_z = input_data(train_set_z, window_size)\n",
    "    test_data_x = input_data(test_set_x, window_size)\n",
    "    test_data_y = input_data(test_set_y, window_size)\n",
    "    test_data_z = input_data(test_set_z, window_size)\n",
    "    \n",
    "    \n",
    "    # reconstruct train/test dataset and label\n",
    "    sample_size = len(train_data_x)\n",
    "    train_dataset, train_label = get_tensor(sample_size, train_data_x, train_data_y, train_data_z)\n",
    "    sample_size = len(test_data_x)\n",
    "    test_dataset, test_label = get_tensor(sample_size, test_data_x, test_data_y, test_data_z)\n",
    "    \n",
    "    train_dataset = train_dataset.astype(\"float32\")\n",
    "    train_label = train_label.astype(\"float32\")\n",
    "    test_dataset = test_dataset.astype(\"float32\")\n",
    "    test_label = test_label.astype(\"float32\")\n",
    "    \n",
    "    return train_dataset, train_label, test_dataset, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "train_dataset_1, train_label_1, test_dataset_1, test_label_1 = construct_train_test_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           test_set_x_1,\n",
    "                                                                                           test_set_y_1,\n",
    "                                                                                           test_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, test_dataset_2, test_label_2 = construct_train_test_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           test_set_x_2,\n",
    "                                                                                           test_set_y_2,\n",
    "                                                                                           test_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, test_dataset_3, test_label_3 = construct_train_test_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           test_set_x_3,\n",
    "                                                                                           test_set_y_3,\n",
    "                                                                                           test_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, test_dataset_4, test_label_4 = construct_train_test_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           test_set_x_4,\n",
    "                                                                                           test_set_y_4,\n",
    "                                                                                           test_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, test_dataset_5, test_label_5 = construct_train_test_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           test_set_x_5,\n",
    "                                                                                           test_set_y_5,\n",
    "                                                                                           test_set_z_5,\n",
    "                                                                                           window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 10, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 10, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "test_dataset = np.concatenate((test_dataset_1,\n",
    "                               test_dataset_2,\n",
    "                               test_dataset_3,\n",
    "                               test_dataset_4,\n",
    "                               test_dataset_5), axis=0)\n",
    "test_label = np.concatenate((test_label_1,\n",
    "                             test_label_2,\n",
    "                             test_label_3,\n",
    "                             test_label_4,\n",
    "                             test_label_5), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 10, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 10, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2990*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "390*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-9979ff4634ab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-9979ff4634ab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 23 = 14950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 23 = 14950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-de69bcda9ea3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-de69bcda9ea3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 3 = 1950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 3 = 1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.8337859e-02,  2.5345819e-04,  1.0032148e+00],\n",
       "        [ 1.3741747e-01,  2.5345819e-04,  1.0000477e+00],\n",
       "        [ 1.7205451e-01,  2.5345819e-04,  9.8298705e-01],\n",
       "        ...,\n",
       "        [ 3.3839446e-01,  2.5345819e-04,  8.8670057e-01],\n",
       "        [ 3.7166247e-01,  2.5345819e-04,  8.6744332e-01],\n",
       "        [ 4.0493050e-01,  2.5345819e-04,  8.4818602e-01]],\n",
       "\n",
       "       [[ 1.3741747e-01,  2.5345819e-04,  1.0000477e+00],\n",
       "        [ 1.7205451e-01,  2.5345819e-04,  9.8298705e-01],\n",
       "        [ 2.0532250e-01,  2.5345819e-04,  9.6372980e-01],\n",
       "        ...,\n",
       "        [ 3.7166247e-01,  2.5345819e-04,  8.6744332e-01],\n",
       "        [ 4.0493050e-01,  2.5345819e-04,  8.4818602e-01],\n",
       "        [ 4.3819851e-01,  2.5345819e-04,  8.2892871e-01]],\n",
       "\n",
       "       [[ 1.7205451e-01,  2.5345819e-04,  9.8298705e-01],\n",
       "        [ 2.0532250e-01,  2.5345819e-04,  9.6372980e-01],\n",
       "        [ 2.3859046e-01,  2.5345819e-04,  9.4447249e-01],\n",
       "        ...,\n",
       "        [ 4.0493050e-01,  2.5345819e-04,  8.4818602e-01],\n",
       "        [ 4.3819851e-01,  2.5345819e-04,  8.2892871e-01],\n",
       "        [ 4.7146651e-01,  2.5345819e-04,  8.0967140e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.8215570e-01, -9.4251966e-01, -9.8875052e-01],\n",
       "        [ 4.3210381e-01, -9.4251966e-01, -1.0804528e+00],\n",
       "        [ 2.7044451e-01, -9.4251966e-01, -1.1427628e+00],\n",
       "        ...,\n",
       "        [-5.1833689e-01, -9.4251966e-01, -7.6937962e-01],\n",
       "        [-6.6838872e-01, -9.4251966e-01, -6.7767727e-01],\n",
       "        [-8.1844062e-01, -9.4251966e-01, -5.8597499e-01]],\n",
       "\n",
       "       [[ 4.3210381e-01, -9.4251966e-01, -1.0804528e+00],\n",
       "        [ 2.7044451e-01, -9.4251966e-01, -1.1427628e+00],\n",
       "        [ 9.4414018e-02, -9.4251966e-01, -1.1165528e+00],\n",
       "        ...,\n",
       "        [-6.6838872e-01, -9.4251966e-01, -6.7767727e-01],\n",
       "        [-8.1844062e-01, -9.4251966e-01, -5.8597499e-01],\n",
       "        [-9.2026514e-01, -9.4251966e-01, -4.4908711e-01]],\n",
       "\n",
       "       [[ 2.7044451e-01, -9.4251966e-01, -1.1427628e+00],\n",
       "        [ 9.4414018e-02, -9.4251966e-01, -1.1165528e+00],\n",
       "        [-6.2550768e-02, -9.4251966e-01, -1.0360161e+00],\n",
       "        ...,\n",
       "        [-8.1844062e-01, -9.4251966e-01, -5.8597499e-01],\n",
       "        [-9.2026514e-01, -9.4251966e-01, -4.4908711e-01],\n",
       "        [-9.0904403e-01, -9.4251966e-01, -2.8163695e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainDataSet(Dataset):\n",
    "    def __init__(self, train_dataset, train_label):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_label = train_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.train_dataset[idx]\n",
    "        label = self.train_label[idx]\n",
    "        \n",
    "        return [seq, label]\n",
    "\n",
    "\n",
    "class MyTestDataSet(Dataset):\n",
    "    def __init__(self, test_dataset, test_label):\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_label = test_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.test_dataset[idx]\n",
    "        label = self.test_label[idx]\n",
    "\n",
    "        return [seq, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14950\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(test_dataset, test_label)\n",
    "print(len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 30\n",
    "# batch_size = 50\n",
    "batch_size = 650\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# keep the test data trajectory order\n",
    "test_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd8e48b3828>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "1 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "2 torch.Size([650, 10, 3]) torch.Size([650, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    print(i, seqs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[ 0.4985, -0.3636, -0.1193],\n",
      "        [ 0.4877, -0.3636, -0.1921],\n",
      "        [ 0.4770, -0.3636, -0.2649],\n",
      "        [ 0.4662, -0.3636, -0.3376],\n",
      "        [ 0.4555, -0.3636, -0.4104],\n",
      "        [ 0.4447, -0.3636, -0.4832],\n",
      "        [ 0.4339, -0.3636, -0.5560],\n",
      "        [ 0.4232, -0.3636, -0.6288],\n",
      "        [ 0.4124, -0.3636, -0.7015],\n",
      "        [ 0.4017, -0.3636, -0.7743]])\n",
      "tensor([ 0.3778, -0.3636, -0.8427])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[0])\n",
    "        print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[1.3742e-01, 2.5346e-04, 1.0000e+00],\n",
      "        [1.7205e-01, 2.5346e-04, 9.8299e-01],\n",
      "        [2.0532e-01, 2.5346e-04, 9.6373e-01],\n",
      "        [2.3859e-01, 2.5346e-04, 9.4447e-01],\n",
      "        [2.7186e-01, 2.5346e-04, 9.2522e-01],\n",
      "        [3.0513e-01, 2.5346e-04, 9.0596e-01],\n",
      "        [3.3839e-01, 2.5346e-04, 8.8670e-01],\n",
      "        [3.7166e-01, 2.5346e-04, 8.6744e-01],\n",
      "        [4.0493e-01, 2.5346e-04, 8.4819e-01],\n",
      "        [4.3820e-01, 2.5346e-04, 8.2893e-01]])\n",
      "tensor([4.7147e-01, 2.5346e-04, 8.0967e-01])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[1])\n",
    "        print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        \n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first = True)\n",
    "        # If your input data is of shape (seq_len, batch_size, features)\n",
    "        # then you donâ€™t need batch_first=True and your LSTM will give\n",
    "        # output of shape (seq_len, batch_size, hidden_size).\n",
    "\n",
    "        # If your input data is of shape (batch_size, seq_len, features)\n",
    "        # then you need batch_first=True and your LSTM will give\n",
    "        # output of shape (batch_size, seq_len, hidden_size).\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.hidden = (torch.zeros(self.num_layers, 1, hidden_size), torch.zeros(self.num_layers, 1, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim))\n",
    "        if torch.cuda.is_available():\n",
    "            c0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            c0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim))\n",
    "        # print(\"x.size(0)\", x.size(0))\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # x is (batch_size, seq_len, features)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.24395426451835944, test_loss: 0.024608733287702005\n",
      "epoch: 1, train_loss: 0.023696140831579334, test_loss: 0.021704524134596188\n",
      "epoch: 2, train_loss: 0.020620783709961434, test_loss: 0.01960700734828909\n",
      "epoch: 3, train_loss: 0.018374239823416523, test_loss: 0.017711480613797903\n",
      "epoch: 4, train_loss: 0.016625659013895885, test_loss: 0.016298655265321333\n",
      "epoch: 5, train_loss: 0.015168875415364037, test_loss: 0.015126192631820837\n",
      "epoch: 6, train_loss: 0.013941473692007687, test_loss: 0.01398835649403433\n",
      "epoch: 7, train_loss: 0.012841035328481508, test_loss: 0.012928927627702555\n",
      "epoch: 8, train_loss: 0.011880362851788168, test_loss: 0.011974334872017304\n",
      "epoch: 9, train_loss: 0.01099638851440471, test_loss: 0.011197434893498818\n",
      "epoch: 10, train_loss: 0.010193767066559067, test_loss: 0.010432820922384659\n",
      "epoch: 11, train_loss: 0.009448040074304394, test_loss: 0.009733952038610974\n",
      "epoch: 12, train_loss: 0.008758371554153122, test_loss: 0.009048701729625463\n",
      "epoch: 13, train_loss: 0.008098091484735842, test_loss: 0.008390284764269987\n",
      "epoch: 14, train_loss: 0.007490091653459746, test_loss: 0.007781906286254525\n",
      "epoch: 15, train_loss: 0.006919209521425807, test_loss: 0.007201605631659429\n",
      "epoch: 16, train_loss: 0.006395107337638088, test_loss: 0.00682947567353646\n",
      "epoch: 17, train_loss: 0.005912660418645195, test_loss: 0.00628696793379883\n",
      "epoch: 18, train_loss: 0.0054801838553470116, test_loss: 0.005883855822806557\n",
      "epoch: 19, train_loss: 0.005085317094040954, test_loss: 0.0054778150127579766\n",
      "epoch: 20, train_loss: 0.0047484412586883355, test_loss: 0.005152196390554309\n",
      "epoch: 21, train_loss: 0.004453028319403529, test_loss: 0.004866898525506258\n",
      "epoch: 22, train_loss: 0.0042064916544958305, test_loss: 0.004627551378992696\n",
      "epoch: 23, train_loss: 0.004000221348732062, test_loss: 0.004434410870696108\n",
      "epoch: 24, train_loss: 0.0038240407897240443, test_loss: 0.004272874910384417\n",
      "epoch: 25, train_loss: 0.003676730309329603, test_loss: 0.00414624682161957\n",
      "epoch: 26, train_loss: 0.0035557786974570026, test_loss: 0.004015547533830007\n",
      "epoch: 27, train_loss: 0.003458856635362558, test_loss: 0.003928037787166734\n",
      "epoch: 28, train_loss: 0.003376662042801795, test_loss: 0.0038224695405612388\n",
      "epoch: 29, train_loss: 0.0032975472732568564, test_loss: 0.003751743429650863\n",
      "epoch: 30, train_loss: 0.003236281655161925, test_loss: 0.0037057321751490235\n",
      "epoch: 31, train_loss: 0.003177577123531829, test_loss: 0.003630443437335392\n",
      "epoch: 32, train_loss: 0.0031339184117867894, test_loss: 0.0036040861159563065\n",
      "epoch: 33, train_loss: 0.003081560964979555, test_loss: 0.0035788107585782805\n",
      "epoch: 34, train_loss: 0.003044172676037187, test_loss: 0.003508726522947351\n",
      "epoch: 35, train_loss: 0.0030044228263685236, test_loss: 0.0034487529968221984\n",
      "epoch: 36, train_loss: 0.0029642027903996086, test_loss: 0.0033936839705953994\n",
      "epoch: 37, train_loss: 0.00293191314836883, test_loss: 0.003354514599777758\n",
      "epoch: 38, train_loss: 0.002898597543168327, test_loss: 0.0033205017680302262\n",
      "epoch: 39, train_loss: 0.0028627418265070605, test_loss: 0.0032709541264921427\n",
      "epoch: 40, train_loss: 0.0028353972239014897, test_loss: 0.003214029517645637\n",
      "epoch: 41, train_loss: 0.0028035190239872622, test_loss: 0.003196054215853413\n",
      "epoch: 42, train_loss: 0.002775147334551034, test_loss: 0.003196799079887569\n",
      "epoch: 43, train_loss: 0.0027479326514446216, test_loss: 0.003167944960296154\n",
      "epoch: 44, train_loss: 0.0027221737976145487, test_loss: 0.003095187906486293\n",
      "epoch: 45, train_loss: 0.002695935434135406, test_loss: 0.003093222388997674\n",
      "epoch: 46, train_loss: 0.0026727221177324004, test_loss: 0.0030520495492964983\n",
      "epoch: 47, train_loss: 0.0026488377226759558, test_loss: 0.0029956893995404243\n",
      "epoch: 48, train_loss: 0.0026232900158704624, test_loss: 0.0029968648062398038\n",
      "epoch: 49, train_loss: 0.002600874189975793, test_loss: 0.0029523856937885284\n",
      "epoch: 50, train_loss: 0.0025744117624328837, test_loss: 0.002920612537612518\n",
      "epoch: 51, train_loss: 0.0025572417324165935, test_loss: 0.002867762950093796\n",
      "epoch: 52, train_loss: 0.0025383404294109864, test_loss: 0.00286798644810915\n",
      "epoch: 53, train_loss: 0.0025181393088448954, test_loss: 0.0028222541053158543\n",
      "epoch: 54, train_loss: 0.0024973650098494863, test_loss: 0.002820487793845435\n",
      "epoch: 55, train_loss: 0.0024741047730102487, test_loss: 0.0027697772408525148\n",
      "epoch: 56, train_loss: 0.002457064900385297, test_loss: 0.002775750181172043\n",
      "epoch: 57, train_loss: 0.002438667205000377, test_loss: 0.0027301175092967847\n",
      "epoch: 58, train_loss: 0.0024201134928380666, test_loss: 0.0026990071055479348\n",
      "epoch: 59, train_loss: 0.0024051052093019953, test_loss: 0.0027005781691210964\n",
      "epoch: 60, train_loss: 0.0023849407595622797, test_loss: 0.0026445900245259204\n",
      "epoch: 61, train_loss: 0.002365144349270217, test_loss: 0.002622090008420249\n",
      "epoch: 62, train_loss: 0.002354656816865115, test_loss: 0.0026008946782288453\n",
      "epoch: 63, train_loss: 0.0023357405266522064, test_loss: 0.002588053117506206\n",
      "epoch: 64, train_loss: 0.0023229561806858883, test_loss: 0.002601996607457598\n",
      "epoch: 65, train_loss: 0.002304285820371107, test_loss: 0.002563159466565897\n",
      "epoch: 66, train_loss: 0.002291253914692156, test_loss: 0.002527535291543851\n",
      "epoch: 67, train_loss: 0.0022759419288891163, test_loss: 0.0025360685928414264\n",
      "epoch: 68, train_loss: 0.002263640423062379, test_loss: 0.002477567099655668\n",
      "epoch: 69, train_loss: 0.002250536284207002, test_loss: 0.002476534495751063\n",
      "epoch: 70, train_loss: 0.0022323887248564024, test_loss: 0.002476210725338509\n",
      "epoch: 71, train_loss: 0.0022222124819603305, test_loss: 0.0024300553098631403\n",
      "epoch: 72, train_loss: 0.0022044279511370087, test_loss: 0.0024182370398193598\n",
      "epoch: 73, train_loss: 0.002195449788933215, test_loss: 0.00240358339700227\n",
      "epoch: 74, train_loss: 0.002180206331262446, test_loss: 0.002388217505843689\n",
      "epoch: 75, train_loss: 0.0021673142302619376, test_loss: 0.0023585784365423024\n",
      "epoch: 76, train_loss: 0.002155757205479819, test_loss: 0.0023984075135861835\n",
      "epoch: 77, train_loss: 0.0021447140024974942, test_loss: 0.002325986628420651\n",
      "epoch: 78, train_loss: 0.002131950170935496, test_loss: 0.002330699256466081\n",
      "epoch: 79, train_loss: 0.002121813018279879, test_loss: 0.0023057966609485447\n",
      "epoch: 80, train_loss: 0.0021107005532426033, test_loss: 0.0023204091703519225\n",
      "epoch: 81, train_loss: 0.0020984750413133397, test_loss: 0.0022587732528336346\n",
      "epoch: 82, train_loss: 0.0020909458854357185, test_loss: 0.0022548481938429177\n",
      "epoch: 83, train_loss: 0.002075752445861049, test_loss: 0.002243007824290544\n",
      "epoch: 84, train_loss: 0.0020690279830571103, test_loss: 0.0022417369570272663\n",
      "epoch: 85, train_loss: 0.0020545525526951837, test_loss: 0.002219460322521627\n",
      "epoch: 86, train_loss: 0.0020477631554493437, test_loss: 0.0021956732380203903\n",
      "epoch: 87, train_loss: 0.0020344743920166206, test_loss: 0.0021815053769387305\n",
      "epoch: 88, train_loss: 0.002029973902213185, test_loss: 0.002167449895447741\n",
      "epoch: 89, train_loss: 0.00201662455725929, test_loss: 0.0021650290776354573\n",
      "epoch: 90, train_loss: 0.0020069458430794916, test_loss: 0.002142930073508372\n",
      "epoch: 91, train_loss: 0.0020008904646839137, test_loss: 0.002127820315460364\n",
      "epoch: 92, train_loss: 0.001994182396194209, test_loss: 0.002123073150869459\n",
      "epoch: 93, train_loss: 0.0019863807980942984, test_loss: 0.0021315026873101792\n",
      "epoch: 94, train_loss: 0.001974861167938165, test_loss: 0.002101705972260485\n",
      "epoch: 95, train_loss: 0.00196577794343719, test_loss: 0.002088662178721279\n",
      "epoch: 96, train_loss: 0.001958873760684029, test_loss: 0.0020778975061451397\n",
      "epoch: 97, train_loss: 0.001950062457067163, test_loss: 0.0020625250375208757\n",
      "epoch: 98, train_loss: 0.0019434169036052797, test_loss: 0.0020614094682969153\n",
      "epoch: 99, train_loss: 0.0019365945658848984, test_loss: 0.0020653720324238143\n",
      "epoch: 100, train_loss: 0.0019276837423524778, test_loss: 0.0020281717103595534\n",
      "epoch: 101, train_loss: 0.001919447120441043, test_loss: 0.0020320576732046902\n",
      "epoch: 102, train_loss: 0.0019129000644644966, test_loss: 0.002015509584452957\n",
      "epoch: 103, train_loss: 0.0019045411102959642, test_loss: 0.0020067855366505682\n",
      "epoch: 104, train_loss: 0.001896771698501771, test_loss: 0.002002295533505579\n",
      "epoch: 105, train_loss: 0.0018897389679256341, test_loss: 0.0019795201951637864\n",
      "epoch: 106, train_loss: 0.0018829702571763294, test_loss: 0.00196466032260408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 107, train_loss: 0.0018751033500808737, test_loss: 0.0019751323270611465\n",
      "epoch: 108, train_loss: 0.0018696728182713623, test_loss: 0.001965070356770108\n",
      "epoch: 109, train_loss: 0.0018647004364301329, test_loss: 0.0019540356006473303\n",
      "epoch: 110, train_loss: 0.0018580165671427612, test_loss: 0.0019339955373046298\n",
      "epoch: 111, train_loss: 0.0018514060142004619, test_loss: 0.0019498318433761597\n",
      "epoch: 112, train_loss: 0.0018444466860155048, test_loss: 0.0019251325672181945\n",
      "epoch: 113, train_loss: 0.0018388819982014272, test_loss: 0.001911974938896795\n",
      "epoch: 114, train_loss: 0.0018355347590682948, test_loss: 0.0019256109565806885\n",
      "epoch: 115, train_loss: 0.0018265098839512339, test_loss: 0.001906927830229203\n",
      "epoch: 116, train_loss: 0.0018187625303297586, test_loss: 0.0018937555335772533\n",
      "epoch: 117, train_loss: 0.0018153829732909799, test_loss: 0.0018884532231216629\n",
      "epoch: 118, train_loss: 0.0018094499777678561, test_loss: 0.001876070067131271\n",
      "epoch: 119, train_loss: 0.001804155110300559, test_loss: 0.0018675923153447609\n",
      "epoch: 120, train_loss: 0.0017964867824364615, test_loss: 0.001865430890272061\n",
      "epoch: 121, train_loss: 0.0017955111157473016, test_loss: 0.0018484994458655517\n",
      "epoch: 122, train_loss: 0.0017867417621385791, test_loss: 0.0018534784127647679\n",
      "epoch: 123, train_loss: 0.0017820216016843915, test_loss: 0.0018372951890341938\n",
      "epoch: 124, train_loss: 0.001777181062725899, test_loss: 0.0018413907576662798\n",
      "epoch: 125, train_loss: 0.0017719659701232677, test_loss: 0.0018400867508413892\n",
      "epoch: 126, train_loss: 0.001766903695407445, test_loss: 0.0018165771228571732\n",
      "epoch: 127, train_loss: 0.0017613211353106992, test_loss: 0.0018159372654433052\n",
      "epoch: 128, train_loss: 0.0017566009748565114, test_loss: 0.0018099399361138542\n",
      "epoch: 129, train_loss: 0.001750295875472543, test_loss: 0.001804837801804145\n",
      "epoch: 130, train_loss: 0.0017445161989282656, test_loss: 0.0018028618845467765\n",
      "epoch: 131, train_loss: 0.0017404784094137342, test_loss: 0.0017996157209078472\n",
      "epoch: 132, train_loss: 0.0017358373658245673, test_loss: 0.0018029974113839369\n",
      "epoch: 133, train_loss: 0.0017328068826590543, test_loss: 0.0017856660415418446\n",
      "epoch: 134, train_loss: 0.0017282194306102135, test_loss: 0.0017767967268203695\n",
      "epoch: 135, train_loss: 0.001721705983230925, test_loss: 0.00176866544643417\n",
      "epoch: 136, train_loss: 0.0017189913398950644, test_loss: 0.0017579906658890347\n",
      "epoch: 137, train_loss: 0.001716755092670412, test_loss: 0.0017579313328800101\n",
      "epoch: 138, train_loss: 0.0017096096416935325, test_loss: 0.0017548597922238212\n",
      "epoch: 139, train_loss: 0.0017022107191302855, test_loss: 0.001747178166018178\n",
      "epoch: 140, train_loss: 0.001700736946709778, test_loss: 0.0017387488042004406\n",
      "epoch: 141, train_loss: 0.0016985914376361864, test_loss: 0.0017422162539636095\n",
      "epoch: 142, train_loss: 0.0016927271220914047, test_loss: 0.0017253730911761522\n",
      "epoch: 143, train_loss: 0.0016871113836279383, test_loss: 0.0017243115774666269\n",
      "epoch: 144, train_loss: 0.0016844048185031052, test_loss: 0.0017380704230163246\n",
      "epoch: 145, train_loss: 0.0016812888004452639, test_loss: 0.0017181626559856038\n",
      "epoch: 146, train_loss: 0.0016752961496620076, test_loss: 0.0017014490634513397\n",
      "epoch: 147, train_loss: 0.001671899008848097, test_loss: 0.0017033293067167203\n",
      "epoch: 148, train_loss: 0.0016671296249589195, test_loss: 0.0016943558778924246\n",
      "epoch: 149, train_loss: 0.0016637439991387985, test_loss: 0.001688219509863605\n",
      "epoch: 150, train_loss: 0.0016593783638318596, test_loss: 0.001693563904458036\n",
      "epoch: 151, train_loss: 0.0016554417847858174, test_loss: 0.001686925320730855\n",
      "epoch: 152, train_loss: 0.0016516684807594056, test_loss: 0.0016772129747550935\n",
      "epoch: 153, train_loss: 0.0016480343689656129, test_loss: 0.0016736260828717302\n",
      "epoch: 154, train_loss: 0.0016449760169843617, test_loss: 0.0016706497602475185\n",
      "epoch: 155, train_loss: 0.001641527471213561, test_loss: 0.0016642098295657586\n",
      "epoch: 156, train_loss: 0.001637393178458771, test_loss: 0.001654539684144159\n",
      "epoch: 157, train_loss: 0.0016340678912061064, test_loss: 0.0016526146500837058\n",
      "epoch: 158, train_loss: 0.0016290591169229667, test_loss: 0.0016455723574229826\n",
      "epoch: 159, train_loss: 0.0016250423327817218, test_loss: 0.0016621432538765173\n",
      "epoch: 160, train_loss: 0.0016242684189068234, test_loss: 0.0016434090211987495\n",
      "epoch: 161, train_loss: 0.0016189508403286986, test_loss: 0.001643772595950092\n",
      "epoch: 162, train_loss: 0.0016153335733258207, test_loss: 0.0016295247381397833\n",
      "epoch: 163, train_loss: 0.0016130396347168994, test_loss: 0.0016306466131936759\n",
      "epoch: 164, train_loss: 0.0016069295714892771, test_loss: 0.0016271834804986913\n",
      "epoch: 165, train_loss: 0.0016036281088853011, test_loss: 0.0016201843876236428\n",
      "epoch: 166, train_loss: 0.0016011803198362822, test_loss: 0.0016227845335379243\n",
      "epoch: 167, train_loss: 0.0015967037407276423, test_loss: 0.001612339896382764\n",
      "epoch: 168, train_loss: 0.0015958101721480489, test_loss: 0.001606334320968017\n",
      "epoch: 169, train_loss: 0.0015926430477639256, test_loss: 0.001601723488420248\n",
      "epoch: 170, train_loss: 0.0015895240990768957, test_loss: 0.0015983740837934117\n",
      "epoch: 171, train_loss: 0.0015857396191795883, test_loss: 0.0016116575279738754\n",
      "epoch: 172, train_loss: 0.001581943502811634, test_loss: 0.0015921266555475693\n",
      "epoch: 173, train_loss: 0.0015764117251028833, test_loss: 0.001595338573679328\n",
      "epoch: 174, train_loss: 0.001575376038723018, test_loss: 0.0015836922878709931\n",
      "epoch: 175, train_loss: 0.0015716973601071083, test_loss: 0.0015771753678563982\n",
      "epoch: 176, train_loss: 0.0015676738694310188, test_loss: 0.0015981380517284076\n",
      "epoch: 177, train_loss: 0.0015646925034082453, test_loss: 0.0015678113074197124\n",
      "epoch: 178, train_loss: 0.001564062823055555, test_loss: 0.0015647546291196097\n",
      "epoch: 179, train_loss: 0.0015583175598927166, test_loss: 0.001559701602673158\n",
      "epoch: 180, train_loss: 0.0015565252447824764, test_loss: 0.0015739654287851106\n",
      "epoch: 181, train_loss: 0.001553281028147625, test_loss: 0.0015585219759183626\n",
      "epoch: 182, train_loss: 0.0015485397965201864, test_loss: 0.0015640303123897563\n",
      "epoch: 183, train_loss: 0.0015456804135085447, test_loss: 0.0015536441836350907\n",
      "epoch: 184, train_loss: 0.0015435001822998342, test_loss: 0.0015586052128734689\n",
      "epoch: 185, train_loss: 0.001540859908108478, test_loss: 0.001540560197706024\n",
      "epoch: 186, train_loss: 0.0015368257122843163, test_loss: 0.0015363384348650773\n",
      "epoch: 187, train_loss: 0.001533367711564769, test_loss: 0.0015328302785443764\n",
      "epoch: 188, train_loss: 0.0015325173280080376, test_loss: 0.0015301411331165582\n",
      "epoch: 189, train_loss: 0.0015301053948781412, test_loss: 0.0015329679299611598\n",
      "epoch: 190, train_loss: 0.0015257259466401908, test_loss: 0.0015264955097033333\n",
      "epoch: 191, train_loss: 0.001522296771366635, test_loss: 0.0015228687843773514\n",
      "epoch: 192, train_loss: 0.0015203957395304155, test_loss: 0.0015142764799141635\n",
      "epoch: 193, train_loss: 0.0015201229176929464, test_loss: 0.0015137395336447905\n",
      "epoch: 194, train_loss: 0.0015132079741148198, test_loss: 0.0015091440970233332\n",
      "epoch: 195, train_loss: 0.0015112792151858625, test_loss: 0.001512072728170703\n",
      "epoch: 196, train_loss: 0.0015100137148376393, test_loss: 0.0015089840356570978\n",
      "epoch: 197, train_loss: 0.0015054990780418334, test_loss: 0.001500302052590996\n",
      "epoch: 198, train_loss: 0.001505121725368435, test_loss: 0.0015053745300974697\n",
      "epoch: 199, train_loss: 0.0015022355718943088, test_loss: 0.0014934127102605999\n",
      "epoch: 200, train_loss: 0.0015003151543762374, test_loss: 0.0014899460463008534\n",
      "epoch: 201, train_loss: 0.0014936674598847394, test_loss: 0.0014951064755829673\n",
      "epoch: 202, train_loss: 0.0014910476950361676, test_loss: 0.001482409085535134\n",
      "epoch: 203, train_loss: 0.001489752610011593, test_loss: 0.0014799282556244482\n",
      "epoch: 204, train_loss: 0.0014882522695900304, test_loss: 0.0014785089200207342\n",
      "epoch: 205, train_loss: 0.0014846517461473527, test_loss: 0.0014712008317777265\n",
      "epoch: 206, train_loss: 0.00148452989473615, test_loss: 0.0014763833799709876\n",
      "epoch: 207, train_loss: 0.0014798967977580817, test_loss: 0.0014759991609025747\n",
      "epoch: 208, train_loss: 0.0014751353503569312, test_loss: 0.001464493330180024\n",
      "epoch: 209, train_loss: 0.0014744040697975004, test_loss: 0.0014651025897668053\n",
      "epoch: 210, train_loss: 0.0014712408174882116, test_loss: 0.0014570704273258646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 211, train_loss: 0.001469572712464825, test_loss: 0.001463167155937602\n",
      "epoch: 212, train_loss: 0.0014677578633975077, test_loss: 0.0014551399217452854\n",
      "epoch: 213, train_loss: 0.00146418494557071, test_loss: 0.0014518068443673353\n",
      "epoch: 214, train_loss: 0.0014602345738397992, test_loss: 0.001449004989505435\n",
      "epoch: 215, train_loss: 0.0014586840685376007, test_loss: 0.0014463704428635538\n",
      "epoch: 216, train_loss: 0.0014559272676706314, test_loss: 0.001449293427867815\n",
      "epoch: 217, train_loss: 0.0014544122842261972, test_loss: 0.0014431502592439454\n",
      "epoch: 218, train_loss: 0.0014522928774923735, test_loss: 0.0014299558436808486\n",
      "epoch: 219, train_loss: 0.0014492900252504194, test_loss: 0.001433601777534932\n",
      "epoch: 220, train_loss: 0.001444899563110717, test_loss: 0.001439917366951704\n",
      "epoch: 221, train_loss: 0.0014425580308813116, test_loss: 0.001427491253707558\n",
      "epoch: 222, train_loss: 0.0014428726154501023, test_loss: 0.0014303083395740639\n",
      "epoch: 223, train_loss: 0.0014413458133197348, test_loss: 0.0014226814188684027\n",
      "epoch: 224, train_loss: 0.0014372454986543112, test_loss: 0.0014234102854970843\n",
      "epoch: 225, train_loss: 0.0014351540923361545, test_loss: 0.0014260335398527484\n",
      "epoch: 226, train_loss: 0.001431670657399556, test_loss: 0.0014167982541645567\n",
      "epoch: 227, train_loss: 0.0014290450287618391, test_loss: 0.0014241526369005442\n",
      "epoch: 228, train_loss: 0.0014307413341315544, test_loss: 0.0014084430683093767\n",
      "epoch: 229, train_loss: 0.0014244131212446678, test_loss: 0.0014214284310583025\n",
      "epoch: 230, train_loss: 0.001424688208119377, test_loss: 0.0013979080831632018\n",
      "epoch: 231, train_loss: 0.0014209509169196954, test_loss: 0.0014127077787028004\n",
      "epoch: 232, train_loss: 0.0014191739939396148, test_loss: 0.0014104654643839847\n",
      "epoch: 233, train_loss: 0.0014177488628774881, test_loss: 0.0014008548945033301\n",
      "epoch: 234, train_loss: 0.001416365378131361, test_loss: 0.0013991937642761816\n",
      "epoch: 235, train_loss: 0.0014139611840895984, test_loss: 0.001387403521221131\n",
      "epoch: 236, train_loss: 0.0014089588885721953, test_loss: 0.001388273638440296\n",
      "epoch: 237, train_loss: 0.0014078507210006533, test_loss: 0.0014065519402114053\n",
      "epoch: 238, train_loss: 0.0014062876883975189, test_loss: 0.001384718557043622\n",
      "epoch: 239, train_loss: 0.0014040689996403196, test_loss: 0.0013807799647717427\n",
      "epoch: 240, train_loss: 0.0014017404958038874, test_loss: 0.0013771465552660327\n",
      "epoch: 241, train_loss: 0.001398050119978902, test_loss: 0.001372987850724409\n",
      "epoch: 242, train_loss: 0.0013962665005870488, test_loss: 0.0013751050185722609\n",
      "epoch: 243, train_loss: 0.0013969479199579876, test_loss: 0.0013691684677420806\n",
      "epoch: 244, train_loss: 0.001393871057940566, test_loss: 0.0013849013290988903\n",
      "epoch: 245, train_loss: 0.0013909211772008111, test_loss: 0.0013733113883063197\n",
      "epoch: 246, train_loss: 0.0013881507430873487, test_loss: 0.0013694497659647216\n",
      "epoch: 247, train_loss: 0.0013856205218674047, test_loss: 0.0013746959739364684\n",
      "epoch: 248, train_loss: 0.0013841684786197932, test_loss: 0.0013573440664913505\n",
      "epoch: 249, train_loss: 0.001382131681210645, test_loss: 0.0013572674943134189\n",
      "epoch: 250, train_loss: 0.0013802783442256243, test_loss: 0.0013540974468924105\n",
      "epoch: 251, train_loss: 0.001378005760235955, test_loss: 0.0013497464921480666\n",
      "epoch: 252, train_loss: 0.0013771765895754747, test_loss: 0.0013534985191654414\n",
      "epoch: 253, train_loss: 0.0013733223920850003, test_loss: 0.001351687281082074\n",
      "epoch: 254, train_loss: 0.0013710968651930275, test_loss: 0.0013443623999288927\n",
      "epoch: 255, train_loss: 0.0013694256574482374, test_loss: 0.0013369775260798633\n",
      "epoch: 256, train_loss: 0.001367706157591032, test_loss: 0.001352166533858205\n",
      "epoch: 257, train_loss: 0.001365018639025157, test_loss: 0.0013377684129712482\n",
      "epoch: 258, train_loss: 0.0013631623309186618, test_loss: 0.0013413988732888054\n",
      "epoch: 259, train_loss: 0.0013625393090162265, test_loss: 0.0013318171550054103\n",
      "epoch: 260, train_loss: 0.0013585957070893567, test_loss: 0.0013336017824864637\n",
      "epoch: 261, train_loss: 0.001357986910633095, test_loss: 0.00132998302190875\n",
      "epoch: 262, train_loss: 0.0013559745326030838, test_loss: 0.0013334459896820288\n",
      "epoch: 263, train_loss: 0.0013536752566047337, test_loss: 0.0013215332097994785\n",
      "epoch: 264, train_loss: 0.0013528359274420402, test_loss: 0.0013348055848230918\n",
      "epoch: 265, train_loss: 0.0013518637835817492, test_loss: 0.00131601034081541\n",
      "epoch: 266, train_loss: 0.00134850098275701, test_loss: 0.0013242050481494516\n",
      "epoch: 267, train_loss: 0.0013464590909121478, test_loss: 0.00132403247213612\n",
      "epoch: 268, train_loss: 0.0013453501767640853, test_loss: 0.0013102740243387718\n",
      "epoch: 269, train_loss: 0.0013414802151205747, test_loss: 0.0013049741295011092\n",
      "epoch: 270, train_loss: 0.0013401490615923767, test_loss: 0.0013081385210777323\n",
      "epoch: 271, train_loss: 0.0013380605125111406, test_loss: 0.0013058485540871818\n",
      "epoch: 272, train_loss: 0.001336070205307687, test_loss: 0.0013078896542234968\n",
      "epoch: 273, train_loss: 0.0013360065311881835, test_loss: 0.001310172500476862\n",
      "epoch: 274, train_loss: 0.0013339008235008173, test_loss: 0.001299225075248008\n",
      "epoch: 275, train_loss: 0.0013334297818010268, test_loss: 0.0012974057560010503\n",
      "epoch: 276, train_loss: 0.0013305737343414323, test_loss: 0.0012979873766501744\n",
      "epoch: 277, train_loss: 0.0013278353195030081, test_loss: 0.0013001414384537686\n",
      "epoch: 278, train_loss: 0.0013258463282988448, test_loss: 0.001289610624856626\n",
      "epoch: 279, train_loss: 0.0013246807148275168, test_loss: 0.0012937300586296867\n",
      "epoch: 280, train_loss: 0.001324536592659095, test_loss: 0.0012858141077837597\n",
      "epoch: 281, train_loss: 0.0013219092810607474, test_loss: 0.001295786244251455\n",
      "epoch: 282, train_loss: 0.0013201817536078718, test_loss: 0.0012978759380833556\n",
      "epoch: 283, train_loss: 0.0013209607340800374, test_loss: 0.0012804861762560904\n",
      "epoch: 284, train_loss: 0.0013156543794573972, test_loss: 0.001277384057175368\n",
      "epoch: 285, train_loss: 0.0013145341527769747, test_loss: 0.001284150096277396\n",
      "epoch: 286, train_loss: 0.0013128358607544847, test_loss: 0.0012719767498007666\n",
      "epoch: 287, train_loss: 0.0013111572403906157, test_loss: 0.0012728102738037705\n",
      "epoch: 288, train_loss: 0.0013100896638048732, test_loss: 0.0012705220918481548\n",
      "epoch: 289, train_loss: 0.0013068890295234387, test_loss: 0.0012745970549682777\n",
      "epoch: 290, train_loss: 0.0013059359372836416, test_loss: 0.001287761270456637\n",
      "epoch: 291, train_loss: 0.0013047371454455931, test_loss: 0.0012646666436921805\n",
      "epoch: 292, train_loss: 0.0013032969765608077, test_loss: 0.0012801776465494186\n",
      "epoch: 293, train_loss: 0.001301041880177091, test_loss: 0.001265289126119266\n",
      "epoch: 294, train_loss: 0.0012996028703839882, test_loss: 0.0012641028539898496\n",
      "epoch: 295, train_loss: 0.0012974730394173252, test_loss: 0.0012765192659571767\n",
      "epoch: 296, train_loss: 0.0012957689747133334, test_loss: 0.0012621880353738864\n",
      "epoch: 297, train_loss: 0.0012939824550615056, test_loss: 0.0012564824913473178\n",
      "epoch: 298, train_loss: 0.0012925694995235813, test_loss: 0.001251601429733758\n",
      "epoch: 299, train_loss: 0.0012898574072017293, test_loss: 0.0012579472095239908\n",
      "epoch: 300, train_loss: 0.0012902514781276493, test_loss: 0.0012480213578479986\n",
      "epoch: 301, train_loss: 0.001288262919684791, test_loss: 0.0012555749466021855\n",
      "epoch: 302, train_loss: 0.00128446564928669, test_loss: 0.0012453639549979318\n",
      "epoch: 303, train_loss: 0.0012838521304175906, test_loss: 0.0012498305004555732\n",
      "epoch: 304, train_loss: 0.0012829164574530137, test_loss: 0.001243696208499993\n",
      "epoch: 305, train_loss: 0.0012813694926950595, test_loss: 0.0012400654086377472\n",
      "epoch: 306, train_loss: 0.001280401263932657, test_loss: 0.0012492498790379614\n",
      "epoch: 307, train_loss: 0.001278178720815998, test_loss: 0.0012413181636172037\n",
      "epoch: 308, train_loss: 0.001277848903555423, test_loss: 0.0012342690412576\n",
      "epoch: 309, train_loss: 0.0012758062474186654, test_loss: 0.001234038199375694\n",
      "epoch: 310, train_loss: 0.0012742792023345828, test_loss: 0.0012391660129651427\n",
      "epoch: 311, train_loss: 0.0012714879298785133, test_loss: 0.0012361057451926172\n",
      "epoch: 312, train_loss: 0.0012728816389268184, test_loss: 0.001225539327909549\n",
      "epoch: 313, train_loss: 0.00127129405281385, test_loss: 0.0012248855006570618\n",
      "epoch: 314, train_loss: 0.0012695879609647977, test_loss: 0.0012231415894348174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 315, train_loss: 0.001264998340047896, test_loss: 0.00122917066134202\n",
      "epoch: 316, train_loss: 0.001264258386546989, test_loss: 0.0012233591017623742\n",
      "epoch: 317, train_loss: 0.0012619149992647378, test_loss: 0.0012173124705441296\n",
      "epoch: 318, train_loss: 0.0012617042419784095, test_loss: 0.0012290706217754632\n",
      "epoch: 319, train_loss: 0.0012597563909366727, test_loss: 0.0012233660963829607\n",
      "epoch: 320, train_loss: 0.0012594130745603015, test_loss: 0.0012167355841180931\n",
      "epoch: 321, train_loss: 0.0012569978362475724, test_loss: 0.0012154801030798505\n",
      "epoch: 322, train_loss: 0.0012550907508146179, test_loss: 0.001216695944701011\n",
      "epoch: 323, train_loss: 0.001254880780597096, test_loss: 0.0012094885460101068\n",
      "epoch: 324, train_loss: 0.0012518162737883952, test_loss: 0.0012021438354471077\n",
      "epoch: 325, train_loss: 0.0012519027475956018, test_loss: 0.0012035368320842583\n",
      "epoch: 326, train_loss: 0.0012497367646099756, test_loss: 0.0012159174172362934\n",
      "epoch: 327, train_loss: 0.001248964515980333, test_loss: 0.0012039766103650134\n",
      "epoch: 328, train_loss: 0.0012461053328993528, test_loss: 0.0011985509869797777\n",
      "epoch: 329, train_loss: 0.0012459813607046785, test_loss: 0.00120396365916046\n",
      "epoch: 330, train_loss: 0.0012440950250374558, test_loss: 0.0011977774556726217\n",
      "epoch: 331, train_loss: 0.0012450802327214699, test_loss: 0.00120038526559559\n",
      "epoch: 332, train_loss: 0.0012411846040302644, test_loss: 0.001194299557634319\n",
      "epoch: 333, train_loss: 0.0012395214517195911, test_loss: 0.0012086815549992025\n",
      "epoch: 334, train_loss: 0.0012379220469206896, test_loss: 0.001189029950182885\n",
      "epoch: 335, train_loss: 0.0012369211914989612, test_loss: 0.001187822160621484\n",
      "epoch: 336, train_loss: 0.0012356423514733172, test_loss: 0.0011867046996485442\n",
      "epoch: 337, train_loss: 0.0012334545355533128, test_loss: 0.001192228771590938\n",
      "epoch: 338, train_loss: 0.0012334552694759939, test_loss: 0.0011878746930354585\n",
      "epoch: 339, train_loss: 0.001231311949516606, test_loss: 0.001205452960372592\n",
      "epoch: 340, train_loss: 0.0012308602074525602, test_loss: 0.001190669087615485\n",
      "epoch: 341, train_loss: 0.001228267567373974, test_loss: 0.0011973036841178935\n",
      "epoch: 342, train_loss: 0.0012291345072140837, test_loss: 0.0011818013639034082\n",
      "epoch: 343, train_loss: 0.001224844377366421, test_loss: 0.0011904357913105439\n",
      "epoch: 344, train_loss: 0.0012245932163711152, test_loss: 0.0011910943721886724\n",
      "epoch: 345, train_loss: 0.0012237855496451907, test_loss: 0.0011746184803390254\n",
      "epoch: 346, train_loss: 0.0012218958834874566, test_loss: 0.0011742737842723727\n",
      "epoch: 347, train_loss: 0.0012215308708381717, test_loss: 0.0011712199678489317\n",
      "epoch: 348, train_loss: 0.0012199111718117542, test_loss: 0.0011843067574469994\n",
      "epoch: 349, train_loss: 0.0012200822112271967, test_loss: 0.0011735838876726727\n",
      "epoch: 350, train_loss: 0.0012170447127731597, test_loss: 0.0011651292831326525\n",
      "epoch: 351, train_loss: 0.0012160841598296943, test_loss: 0.001169568965754782\n",
      "epoch: 352, train_loss: 0.0012130686942406971, test_loss: 0.001171992616339897\n",
      "epoch: 353, train_loss: 0.0012122313011154208, test_loss: 0.0011636507406365126\n",
      "epoch: 354, train_loss: 0.0012117235936507907, test_loss: 0.0011596343538258225\n",
      "epoch: 355, train_loss: 0.0012115097184584517, test_loss: 0.0011620433457816641\n",
      "epoch: 356, train_loss: 0.001208972061842518, test_loss: 0.001161912795699512\n",
      "epoch: 357, train_loss: 0.0012073846073294787, test_loss: 0.0011646909988485277\n",
      "epoch: 358, train_loss: 0.0012082219675547726, test_loss: 0.0011558049591258168\n",
      "epoch: 359, train_loss: 0.0012046727984536278, test_loss: 0.0011611237035443385\n",
      "epoch: 360, train_loss: 0.001203780819196254, test_loss: 0.0011487534987584997\n",
      "epoch: 361, train_loss: 0.0012037154342776732, test_loss: 0.0011589400722490002\n",
      "epoch: 362, train_loss: 0.0012005010376805844, test_loss: 0.00115297464071773\n",
      "epoch: 363, train_loss: 0.001199266977061558, test_loss: 0.0011462498126396288\n",
      "epoch: 364, train_loss: 0.0011995006428586075, test_loss: 0.0011454260481211047\n",
      "epoch: 365, train_loss: 0.0011971954954788089, test_loss: 0.0011465388912862788\n",
      "epoch: 366, train_loss: 0.0011966712216077292, test_loss: 0.001147500336325417\n",
      "epoch: 367, train_loss: 0.00119491995808304, test_loss: 0.0011534427273242425\n",
      "epoch: 368, train_loss: 0.0011937255292887921, test_loss: 0.0011454472939173381\n",
      "epoch: 369, train_loss: 0.00119261004526735, test_loss: 0.0011443344604534407\n",
      "epoch: 370, train_loss: 0.0011917704731509414, test_loss: 0.0011367411255681266\n",
      "epoch: 371, train_loss: 0.0011900691391454766, test_loss: 0.0011365090710266184\n",
      "epoch: 372, train_loss: 0.0011899173360965822, test_loss: 0.0011335697878773014\n",
      "epoch: 373, train_loss: 0.0011883307015523314, test_loss: 0.0011512522566287469\n",
      "epoch: 374, train_loss: 0.0011864399142644327, test_loss: 0.0011431188516629238\n",
      "epoch: 375, train_loss: 0.0011855147819480171, test_loss: 0.0011287771728044997\n",
      "epoch: 376, train_loss: 0.0011854727509553018, test_loss: 0.0011337199927462886\n",
      "epoch: 377, train_loss: 0.0011825917298784075, test_loss: 0.00114228596794419\n",
      "epoch: 378, train_loss: 0.0011820815447920365, test_loss: 0.001146157388575375\n",
      "epoch: 379, train_loss: 0.0011805216781795025, test_loss: 0.001131833754091834\n",
      "epoch: 380, train_loss: 0.0011797126068774126, test_loss: 0.0011351268719105672\n",
      "epoch: 381, train_loss: 0.0011804462106817443, test_loss: 0.0011331498293050875\n",
      "epoch: 382, train_loss: 0.0011786915584588828, test_loss: 0.0011349934211466461\n",
      "epoch: 383, train_loss: 0.0011775683454723785, test_loss: 0.001130954857217148\n",
      "epoch: 384, train_loss: 0.0011759086274136992, test_loss: 0.001119737804401666\n",
      "epoch: 385, train_loss: 0.001174698798896988, test_loss: 0.0011221778792484354\n",
      "epoch: 386, train_loss: 0.0011721198401494842, test_loss: 0.0011257289540177833\n",
      "epoch: 387, train_loss: 0.0011718276554070737, test_loss: 0.0011257303607029219\n",
      "epoch: 388, train_loss: 0.0011712036920589921, test_loss: 0.0011096888144190113\n",
      "epoch: 389, train_loss: 0.00116866483585909, test_loss: 0.0011163662711624056\n",
      "epoch: 390, train_loss: 0.001167551672551781, test_loss: 0.0011167629078651469\n",
      "epoch: 391, train_loss: 0.0011684952010198133, test_loss: 0.0011089489174385865\n",
      "epoch: 392, train_loss: 0.0011661907318858025, test_loss: 0.0011119010159745812\n",
      "epoch: 393, train_loss: 0.0011654618100020225, test_loss: 0.0011039048064655315\n",
      "epoch: 394, train_loss: 0.001163681753931324, test_loss: 0.0011082245133972417\n",
      "epoch: 395, train_loss: 0.001165030410250082, test_loss: 0.0011056563525926322\n",
      "epoch: 396, train_loss: 0.0011621432571226487, test_loss: 0.0011060402903240174\n",
      "epoch: 397, train_loss: 0.0011596716991018343, test_loss: 0.001106537344943111\n",
      "epoch: 398, train_loss: 0.001159170915818085, test_loss: 0.0011012315032227586\n",
      "epoch: 399, train_loss: 0.0011583131348268817, test_loss: 0.0010978583886753768\n",
      "epoch: 400, train_loss: 0.0011573532221676862, test_loss: 0.0010987419615654896\n",
      "epoch: 401, train_loss: 0.0011563895791566567, test_loss: 0.0010954860772471875\n",
      "epoch: 402, train_loss: 0.0011554378256930606, test_loss: 0.0010994880479605247\n",
      "epoch: 403, train_loss: 0.0011535874927294967, test_loss: 0.0011009589264479776\n",
      "epoch: 404, train_loss: 0.0011535046505741775, test_loss: 0.0010994829644914716\n",
      "epoch: 405, train_loss: 0.0011511889979769678, test_loss: 0.0011061055314106245\n",
      "epoch: 406, train_loss: 0.001151198010041338, test_loss: 0.0010956650172981124\n",
      "epoch: 407, train_loss: 0.0011521356494125464, test_loss: 0.0010913419052182387\n",
      "epoch: 408, train_loss: 0.0011488244737215016, test_loss: 0.0010952445154543966\n",
      "epoch: 409, train_loss: 0.0011484702674510038, test_loss: 0.0010912580570826929\n",
      "epoch: 410, train_loss: 0.0011475123895291724, test_loss: 0.0010923936691445608\n",
      "epoch: 411, train_loss: 0.0011459639012489629, test_loss: 0.0011135171419785668\n",
      "epoch: 412, train_loss: 0.0011458176228663196, test_loss: 0.0010825022084948917\n",
      "epoch: 413, train_loss: 0.0011440393307408237, test_loss: 0.001084480609279126\n",
      "epoch: 414, train_loss: 0.0011420766645840006, test_loss: 0.001102084342467909\n",
      "epoch: 415, train_loss: 0.0011411409688425128, test_loss: 0.0010806714999489486\n",
      "epoch: 416, train_loss: 0.0011394888709258773, test_loss: 0.0010832746532590438\n",
      "epoch: 417, train_loss: 0.001139821752946338, test_loss: 0.0010772509946643065\n",
      "epoch: 418, train_loss: 0.0011394278034972756, test_loss: 0.0010777991750122358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 419, train_loss: 0.0011377765913493931, test_loss: 0.0010794210441720982\n",
      "epoch: 420, train_loss: 0.0011375169016663795, test_loss: 0.0010850625112652779\n",
      "epoch: 421, train_loss: 0.0011346641037127247, test_loss: 0.0010731337048734229\n",
      "epoch: 422, train_loss: 0.0011353116386327083, test_loss: 0.0010754133303028841\n",
      "epoch: 423, train_loss: 0.0011338900708917367, test_loss: 0.0010884133031746994\n",
      "epoch: 424, train_loss: 0.0011329189494616635, test_loss: 0.0010774564483047773\n",
      "epoch: 425, train_loss: 0.0011317356535152574, test_loss: 0.001071279567743962\n",
      "epoch: 426, train_loss: 0.001130877609324196, test_loss: 0.001072552113328129\n",
      "epoch: 427, train_loss: 0.0011320324948705409, test_loss: 0.0010664682971158375\n",
      "epoch: 428, train_loss: 0.001128832905796235, test_loss: 0.0010677027554872136\n",
      "epoch: 429, train_loss: 0.0011280926992185414, test_loss: 0.0010753555203943204\n",
      "epoch: 430, train_loss: 0.0011286341494111264, test_loss: 0.0010668887213493388\n",
      "epoch: 431, train_loss: 0.0011260944529962928, test_loss: 0.0010677614676145215\n",
      "epoch: 432, train_loss: 0.0011265290794772623, test_loss: 0.0010639388153019051\n",
      "epoch: 433, train_loss: 0.0011259450288691921, test_loss: 0.001065730320988223\n",
      "epoch: 434, train_loss: 0.0011215308104115336, test_loss: 0.0010622660726464044\n",
      "epoch: 435, train_loss: 0.0011232246659979548, test_loss: 0.001057945783638085\n",
      "epoch: 436, train_loss: 0.0011213875765426328, test_loss: 0.0010561170735551666\n",
      "epoch: 437, train_loss: 0.0011207154602743685, test_loss: 0.001057012773041303\n",
      "epoch: 438, train_loss: 0.0011189336782199857, test_loss: 0.0010515590450571228\n",
      "epoch: 439, train_loss: 0.001118288312168063, test_loss: 0.0010564915913467605\n",
      "epoch: 440, train_loss: 0.0011163401422495751, test_loss: 0.0010573626398885\n",
      "epoch: 441, train_loss: 0.0011189719282455094, test_loss: 0.0010614617882917325\n",
      "epoch: 442, train_loss: 0.001115673267201561, test_loss: 0.0010526819969527423\n",
      "epoch: 443, train_loss: 0.0011150332031083171, test_loss: 0.0010519103767971198\n",
      "epoch: 444, train_loss: 0.0011154683357428598, test_loss: 0.001056757231708616\n",
      "epoch: 445, train_loss: 0.001113032684256525, test_loss: 0.00105577001037697\n",
      "epoch: 446, train_loss: 0.0011120380165622287, test_loss: 0.0010492008780905355\n",
      "epoch: 447, train_loss: 0.0011105870058679063, test_loss: 0.0010592390802533676\n",
      "epoch: 448, train_loss: 0.0011118833259071994, test_loss: 0.0010442489195459832\n",
      "epoch: 449, train_loss: 0.001108876801521072, test_loss: 0.0010482348346461852\n",
      "epoch: 450, train_loss: 0.0011077681277959566, test_loss: 0.001045692750873665\n",
      "epoch: 451, train_loss: 0.0011068341023612606, test_loss: 0.0010536688011294852\n",
      "epoch: 452, train_loss: 0.0011056109721286466, test_loss: 0.0010514167758325736\n",
      "epoch: 453, train_loss: 0.0011061413755194972, test_loss: 0.0010440132949346055\n",
      "epoch: 454, train_loss: 0.0011031607940347622, test_loss: 0.0010392211843281984\n",
      "epoch: 455, train_loss: 0.0011038896956723993, test_loss: 0.0010451946291141212\n",
      "epoch: 456, train_loss: 0.0011021564562764504, test_loss: 0.0010377555251276742\n",
      "epoch: 457, train_loss: 0.0011028560010068443, test_loss: 0.0010400350050379832\n",
      "epoch: 458, train_loss: 0.0011001524247188606, test_loss: 0.001039976884688561\n",
      "epoch: 459, train_loss: 0.0011009418243623297, test_loss: 0.0010441794584039599\n",
      "epoch: 460, train_loss: 0.001098304144208036, test_loss: 0.001034321147017181\n",
      "epoch: 461, train_loss: 0.001098745451916171, test_loss: 0.0010323660972062498\n",
      "epoch: 462, train_loss: 0.0011012683035400898, test_loss: 0.0010291868335722636\n",
      "epoch: 463, train_loss: 0.0010957281489897034, test_loss: 0.001029233360895887\n",
      "epoch: 464, train_loss: 0.0010964534139139173, test_loss: 0.001034844055538997\n",
      "epoch: 465, train_loss: 0.0010960821806372185, test_loss: 0.0010319078476944317\n",
      "epoch: 466, train_loss: 0.001093189204212926, test_loss: 0.0010308101579236488\n",
      "epoch: 467, train_loss: 0.001094580230647293, test_loss: 0.0010260202495070796\n",
      "epoch: 468, train_loss: 0.0010934696765616536, test_loss: 0.0010237494037331392\n",
      "epoch: 469, train_loss: 0.0010933770833577475, test_loss: 0.0010324585988807182\n",
      "epoch: 470, train_loss: 0.001091912231893967, test_loss: 0.0010278853975857298\n",
      "epoch: 471, train_loss: 0.0010904785164672396, test_loss: 0.0010229344964803506\n",
      "epoch: 472, train_loss: 0.0010899875500320416, test_loss: 0.0010286002652719617\n",
      "epoch: 473, train_loss: 0.0010873632449859185, test_loss: 0.0010223943778934579\n",
      "epoch: 474, train_loss: 0.0010870957757224855, test_loss: 0.0010198586096521467\n",
      "epoch: 475, train_loss: 0.0010855923528256624, test_loss: 0.0010165076819248497\n",
      "epoch: 476, train_loss: 0.0010848802909174044, test_loss: 0.0010179229235897462\n",
      "epoch: 477, train_loss: 0.0010854106715313442, test_loss: 0.0010186642175540328\n",
      "epoch: 478, train_loss: 0.0010852326294812171, test_loss: 0.0010195081704296172\n",
      "epoch: 479, train_loss: 0.0010836711355849452, test_loss: 0.0010183177849588294\n",
      "epoch: 480, train_loss: 0.001081725840618753, test_loss: 0.0010174911100572597\n",
      "epoch: 481, train_loss: 0.001082063065436871, test_loss: 0.001016748739251246\n",
      "epoch: 482, train_loss: 0.0010800753623935516, test_loss: 0.0010335782717447728\n",
      "epoch: 483, train_loss: 0.0010812140409024837, test_loss: 0.0010120605778259535\n",
      "epoch: 484, train_loss: 0.0010789640693236952, test_loss: 0.0010133997323767592\n",
      "epoch: 485, train_loss: 0.0010771596419584491, test_loss: 0.001012094512892266\n",
      "epoch: 486, train_loss: 0.001076510717646907, test_loss: 0.001012023528649782\n",
      "epoch: 487, train_loss: 0.0010774202250025194, test_loss: 0.001010614214465022\n",
      "epoch: 488, train_loss: 0.0010769950889010468, test_loss: 0.001015628290285046\n",
      "epoch: 489, train_loss: 0.0010737886313227532, test_loss: 0.001004863598306353\n",
      "epoch: 490, train_loss: 0.001074962259735912, test_loss: 0.0010017926688306034\n",
      "epoch: 491, train_loss: 0.0010738666270576093, test_loss: 0.0010159235001386453\n",
      "epoch: 492, train_loss: 0.0010726617841535935, test_loss: 0.001002435067978998\n",
      "epoch: 493, train_loss: 0.001071198876319534, test_loss: 0.0010115401334284495\n",
      "epoch: 494, train_loss: 0.0010704867637959187, test_loss: 0.0010045720749379445\n",
      "epoch: 495, train_loss: 0.001070490099347966, test_loss: 0.0010044220834970474\n",
      "epoch: 496, train_loss: 0.0010702643928158543, test_loss: 0.001011237162553395\n",
      "epoch: 497, train_loss: 0.0010702511341230054, test_loss: 0.0010094690466454874\n",
      "epoch: 498, train_loss: 0.0010686597113659525, test_loss: 0.000997131612772743\n",
      "epoch: 499, train_loss: 0.0010662240042027247, test_loss: 0.0009976457998466988\n",
      "epoch: 500, train_loss: 0.0010662680016020718, test_loss: 0.0010026900272350758\n",
      "epoch: 501, train_loss: 0.0010657840428630943, test_loss: 0.0009934460880079616\n",
      "epoch: 502, train_loss: 0.0010642970244035773, test_loss: 0.001002545264782384\n",
      "epoch: 503, train_loss: 0.0010637635663759124, test_loss: 0.0009953106152048956\n",
      "epoch: 504, train_loss: 0.0010633082432753365, test_loss: 0.0010024122893810272\n",
      "epoch: 505, train_loss: 0.0010618203466393702, test_loss: 0.0009910460115255166\n",
      "epoch: 506, train_loss: 0.0010620949020528276, test_loss: 0.0009954645017084356\n",
      "epoch: 507, train_loss: 0.0010607178783570619, test_loss: 0.000994828500552103\n",
      "epoch: 508, train_loss: 0.0010616126085591056, test_loss: 0.0009964603911309193\n",
      "epoch: 509, train_loss: 0.0010597948136779926, test_loss: 0.0009938308697504301\n",
      "epoch: 510, train_loss: 0.0010589950201708984, test_loss: 0.0009890741494018584\n",
      "epoch: 511, train_loss: 0.001058590289650728, test_loss: 0.0009954388175780575\n",
      "epoch: 512, train_loss: 0.001057151360842197, test_loss: 0.000990652129985392\n",
      "epoch: 513, train_loss: 0.0010555725413091157, test_loss: 0.0009874815683967124\n",
      "epoch: 514, train_loss: 0.001055695286081375, test_loss: 0.000992044229254437\n",
      "epoch: 515, train_loss: 0.001054262375438829, test_loss: 0.0009915976455279936\n",
      "epoch: 516, train_loss: 0.0010548700457033904, test_loss: 0.000988874851221529\n",
      "epoch: 517, train_loss: 0.001053092894954202, test_loss: 0.000987451213101546\n",
      "epoch: 518, train_loss: 0.0010534715408499799, test_loss: 0.0009847348943973582\n",
      "epoch: 519, train_loss: 0.0010502676013857126, test_loss: 0.0009836230165092275\n",
      "epoch: 520, train_loss: 0.001051052941677525, test_loss: 0.000982859846165714\n",
      "epoch: 521, train_loss: 0.0010513711730828104, test_loss: 0.000990497292756724\n",
      "epoch: 522, train_loss: 0.0010496130030926156, test_loss: 0.0009783619995384167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 523, train_loss: 0.0010484305144611585, test_loss: 0.0009853204246610403\n",
      "epoch: 524, train_loss: 0.0010492912817584431, test_loss: 0.0009878254010497283\n",
      "epoch: 525, train_loss: 0.0010486233716263719, test_loss: 0.0009945510925414662\n",
      "epoch: 526, train_loss: 0.0010462011990096905, test_loss: 0.0009760089888004586\n",
      "epoch: 527, train_loss: 0.001047444233731569, test_loss: 0.0009754345956025645\n",
      "epoch: 528, train_loss: 0.0010442609579869263, test_loss: 0.0009882726978200178\n",
      "epoch: 529, train_loss: 0.001044242837689007, test_loss: 0.0009899578581098467\n",
      "epoch: 530, train_loss: 0.0010447951423449685, test_loss: 0.0009715015427597488\n",
      "epoch: 531, train_loss: 0.0010422852887448084, test_loss: 0.0009693434209718058\n",
      "epoch: 532, train_loss: 0.0010433605462110238, test_loss: 0.0009712550478676955\n",
      "epoch: 533, train_loss: 0.0010434768476482966, test_loss: 0.0009730062059437236\n",
      "epoch: 534, train_loss: 0.001040460389998296, test_loss: 0.0009686526997635762\n",
      "epoch: 535, train_loss: 0.001038794876481204, test_loss: 0.0009672835488648465\n",
      "epoch: 536, train_loss: 0.0010408872976372747, test_loss: 0.0009766981578043972\n",
      "epoch: 537, train_loss: 0.0010393006959930062, test_loss: 0.0009715976096534481\n",
      "epoch: 538, train_loss: 0.0010380433776172931, test_loss: 0.0009711843401116008\n",
      "epoch: 539, train_loss: 0.001037922212044182, test_loss: 0.0009692922176327556\n",
      "epoch: 540, train_loss: 0.0010368514897139823, test_loss: 0.0009706934991603097\n",
      "epoch: 541, train_loss: 0.0010374807575515108, test_loss: 0.0009647831854332859\n",
      "epoch: 542, train_loss: 0.0010358185466864834, test_loss: 0.0009716244045800219\n",
      "epoch: 543, train_loss: 0.0010364829491742928, test_loss: 0.000963022374586823\n",
      "epoch: 544, train_loss: 0.0010345290091050708, test_loss: 0.0009619463914229224\n",
      "epoch: 545, train_loss: 0.0010327174981205683, test_loss: 0.000965039842412807\n",
      "epoch: 546, train_loss: 0.0010329159077904794, test_loss: 0.0009616041109741976\n",
      "epoch: 547, train_loss: 0.0010306425822858253, test_loss: 0.0009750297952753803\n",
      "epoch: 548, train_loss: 0.0010319778938656268, test_loss: 0.0009610622946638614\n",
      "epoch: 549, train_loss: 0.0010302634074595635, test_loss: 0.0009608946857042611\n",
      "epoch: 550, train_loss: 0.0010298564599867425, test_loss: 0.0009590541255117083\n",
      "epoch: 551, train_loss: 0.001030526341586981, test_loss: 0.0009634316375013441\n",
      "epoch: 552, train_loss: 0.001028290711869688, test_loss: 0.0009580588618215794\n",
      "epoch: 553, train_loss: 0.001026704839086565, test_loss: 0.0009590292417366678\n",
      "epoch: 554, train_loss: 0.0010274095592372444, test_loss: 0.0009543966637769093\n",
      "epoch: 555, train_loss: 0.0010273879869719563, test_loss: 0.0009571085150431221\n",
      "epoch: 556, train_loss: 0.0010260910949791255, test_loss: 0.0009594186461375406\n",
      "epoch: 557, train_loss: 0.0010251898404575236, test_loss: 0.000959126172044004\n",
      "epoch: 558, train_loss: 0.0010244021541438997, test_loss: 0.0009518273145658895\n",
      "epoch: 559, train_loss: 0.0010251862037441006, test_loss: 0.000957201652151222\n",
      "epoch: 560, train_loss: 0.0010230117882399456, test_loss: 0.0009565130749251693\n",
      "epoch: 561, train_loss: 0.0010238417586230714, test_loss: 0.0009542233650184547\n",
      "epoch: 562, train_loss: 0.001021845202953757, test_loss: 0.000956048946439599\n",
      "epoch: 563, train_loss: 0.0010214166781541121, test_loss: 0.0009565059590386227\n",
      "epoch: 564, train_loss: 0.0010210774084755583, test_loss: 0.000961937170359306\n",
      "epoch: 565, train_loss: 0.0010200439010868254, test_loss: 0.0009502968605374917\n",
      "epoch: 566, train_loss: 0.0010189357284537475, test_loss: 0.0009474091154212753\n",
      "epoch: 567, train_loss: 0.001018681221783323, test_loss: 0.000943845656972068\n",
      "epoch: 568, train_loss: 0.0010185304436954143, test_loss: 0.0009539071713030959\n",
      "epoch: 569, train_loss: 0.0010177908191943297, test_loss: 0.0009486922159946213\n",
      "epoch: 570, train_loss: 0.0010173413446982918, test_loss: 0.0009441685494190702\n",
      "epoch: 571, train_loss: 0.00101442331114136, test_loss: 0.0009469497987690071\n",
      "epoch: 572, train_loss: 0.0010162132498605745, test_loss: 0.0009528833179501817\n",
      "epoch: 573, train_loss: 0.0010150335551968412, test_loss: 0.0009500745169740791\n",
      "epoch: 574, train_loss: 0.001014640056224459, test_loss: 0.0009493439138168469\n",
      "epoch: 575, train_loss: 0.0010157618975347798, test_loss: 0.0009402136832553273\n",
      "epoch: 576, train_loss: 0.0010134020577306333, test_loss: 0.0009409335568003977\n",
      "epoch: 577, train_loss: 0.0010129844253558827, test_loss: 0.0009360658295918256\n",
      "epoch: 578, train_loss: 0.0010108634495459821, test_loss: 0.000942645434406586\n",
      "epoch: 579, train_loss: 0.0010117499091986406, test_loss: 0.0009450620273128152\n",
      "epoch: 580, train_loss: 0.001010926683311877, test_loss: 0.0009386680370274311\n",
      "epoch: 581, train_loss: 0.0010103913930082774, test_loss: 0.0009412009094376117\n",
      "epoch: 582, train_loss: 0.0010094298631884158, test_loss: 0.0009333058793951446\n",
      "epoch: 583, train_loss: 0.0010095792721309092, test_loss: 0.0009512503990360225\n",
      "epoch: 584, train_loss: 0.0010067527055861833, test_loss: 0.000935367148485966\n",
      "epoch: 585, train_loss: 0.0010069978661308794, test_loss: 0.0009340813752108564\n",
      "epoch: 586, train_loss: 0.0010062561638693771, test_loss: 0.0009388911615436276\n",
      "epoch: 587, train_loss: 0.0010057653239725726, test_loss: 0.0009350843853705252\n",
      "epoch: 588, train_loss: 0.0010059895575977862, test_loss: 0.0009284941382550945\n",
      "epoch: 589, train_loss: 0.001004245780084444, test_loss: 0.0009312108062052479\n",
      "epoch: 590, train_loss: 0.0010033108562271557, test_loss: 0.0009326687480400627\n",
      "epoch: 591, train_loss: 0.0010027508985291681, test_loss: 0.0009323752747150138\n",
      "epoch: 592, train_loss: 0.0010029837772574115, test_loss: 0.0009367537762348851\n",
      "epoch: 593, train_loss: 0.0010014401961360936, test_loss: 0.0009368523897137493\n",
      "epoch: 594, train_loss: 0.0010011879798105877, test_loss: 0.0009257181275946399\n",
      "epoch: 595, train_loss: 0.0010009871836265793, test_loss: 0.0009333133687808489\n",
      "epoch: 596, train_loss: 0.000999964897662563, test_loss: 0.0009381853936550518\n",
      "epoch: 597, train_loss: 0.0010010577110654633, test_loss: 0.0009315414742256204\n",
      "epoch: 598, train_loss: 0.0009990961078311439, test_loss: 0.0009269188546265165\n",
      "epoch: 599, train_loss: 0.000998367881908527, test_loss: 0.000927981450028407\n",
      "epoch: 600, train_loss: 0.0009983910055345168, test_loss: 0.0009250134316971526\n",
      "epoch: 601, train_loss: 0.000997455575523655, test_loss: 0.000922390254951703\n",
      "epoch: 602, train_loss: 0.0009963271061322935, test_loss: 0.0009230842357889438\n",
      "epoch: 603, train_loss: 0.0009965534225794609, test_loss: 0.0009264538287728404\n",
      "epoch: 604, train_loss: 0.0009959530991340137, test_loss: 0.0009246170957339928\n",
      "epoch: 605, train_loss: 0.0009944479780919526, test_loss: 0.000922828660501788\n",
      "epoch: 606, train_loss: 0.0009940986030573106, test_loss: 0.0009210293064825237\n",
      "epoch: 607, train_loss: 0.0009939698553036737, test_loss: 0.0009225113947953408\n",
      "epoch: 608, train_loss: 0.0009929986452967253, test_loss: 0.0009404157220463579\n",
      "epoch: 609, train_loss: 0.0009917514904847612, test_loss: 0.0009228232956957072\n",
      "epoch: 610, train_loss: 0.0009924343296164727, test_loss: 0.0009174932929454371\n",
      "epoch: 611, train_loss: 0.0009923245373141506, test_loss: 0.0009203675581375137\n",
      "epoch: 612, train_loss: 0.0009910079458242526, test_loss: 0.0009153225012899687\n",
      "epoch: 613, train_loss: 0.00099152158540876, test_loss: 0.0009178185088482375\n",
      "epoch: 614, train_loss: 0.0009891247963937728, test_loss: 0.0009210146527038887\n",
      "epoch: 615, train_loss: 0.0009884564939926825, test_loss: 0.0009242729429388419\n",
      "epoch: 616, train_loss: 0.0009893447251828468, test_loss: 0.0009227211606533577\n",
      "epoch: 617, train_loss: 0.0009868950203425534, test_loss: 0.0009184452355839312\n",
      "epoch: 618, train_loss: 0.0009876276244935782, test_loss: 0.0009221486058474208\n",
      "epoch: 619, train_loss: 0.0009884345826045003, test_loss: 0.0009115301703180497\n",
      "epoch: 620, train_loss: 0.0009850968945892932, test_loss: 0.0009162501470806698\n",
      "epoch: 621, train_loss: 0.0009851498686222603, test_loss: 0.0009108490339713171\n",
      "epoch: 622, train_loss: 0.0009851473479774659, test_loss: 0.0009151884247936929\n",
      "epoch: 623, train_loss: 0.0009839078133075457, test_loss: 0.0009167316893581301\n",
      "epoch: 624, train_loss: 0.0009848631174384575, test_loss: 0.0009120600298047066\n",
      "epoch: 625, train_loss: 0.0009839309900796609, test_loss: 0.0009228473985179638\n",
      "epoch: 626, train_loss: 0.0009840442495339591, test_loss: 0.0009164962005646279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 627, train_loss: 0.0009832425301125193, test_loss: 0.0009100639920992156\n",
      "epoch: 628, train_loss: 0.0009825057982791054, test_loss: 0.0009145078802248463\n",
      "epoch: 629, train_loss: 0.0009817788503942606, test_loss: 0.0009162106822865704\n",
      "epoch: 630, train_loss: 0.0009798647471420143, test_loss: 0.0009159465601745372\n",
      "epoch: 631, train_loss: 0.0009805536727704432, test_loss: 0.000904743685775126\n",
      "epoch: 632, train_loss: 0.000979057644777324, test_loss: 0.0009069281513802707\n",
      "epoch: 633, train_loss: 0.0009792311567529712, test_loss: 0.0009025415153397868\n",
      "epoch: 634, train_loss: 0.000976649968398978, test_loss: 0.0009159351466223598\n",
      "epoch: 635, train_loss: 0.0009773880972162538, test_loss: 0.0009107281754647071\n",
      "epoch: 636, train_loss: 0.0009766632397456663, test_loss: 0.0009078102836307759\n",
      "epoch: 637, train_loss: 0.000977211442026917, test_loss: 0.0009038461381957555\n",
      "epoch: 638, train_loss: 0.0009740331822641841, test_loss: 0.0009047554194694385\n",
      "epoch: 639, train_loss: 0.0009758383992290044, test_loss: 0.000906232050814045\n",
      "epoch: 640, train_loss: 0.0009755529589829562, test_loss: 0.0009001780078203107\n",
      "epoch: 641, train_loss: 0.0009739967240223094, test_loss: 0.0009031424609323343\n",
      "epoch: 642, train_loss: 0.0009737744897037097, test_loss: 0.000905973618500866\n",
      "epoch: 643, train_loss: 0.0009738905557795712, test_loss: 0.0009036633952443177\n",
      "epoch: 644, train_loss: 0.0009727260280076576, test_loss: 0.0009128412251205494\n",
      "epoch: 645, train_loss: 0.0009717311021750388, test_loss: 0.0009025799323959897\n",
      "epoch: 646, train_loss: 0.0009709144689385657, test_loss: 0.0008957494234588618\n",
      "epoch: 647, train_loss: 0.0009700090386499853, test_loss: 0.0008950684762870272\n",
      "epoch: 648, train_loss: 0.0009716026758289207, test_loss: 0.0008937143332635363\n",
      "epoch: 649, train_loss: 0.0009703919387665455, test_loss: 0.0009019815964469066\n",
      "epoch: 650, train_loss: 0.0009685925805293348, test_loss: 0.0008934557602818435\n",
      "epoch: 651, train_loss: 0.0009682076406138747, test_loss: 0.0009034165511062989\n",
      "epoch: 652, train_loss: 0.0009680573028795745, test_loss: 0.0008941386089039346\n",
      "epoch: 653, train_loss: 0.0009680650318446367, test_loss: 0.000926657963039664\n",
      "epoch: 654, train_loss: 0.0009682874660938978, test_loss: 0.0008988812138947347\n",
      "epoch: 655, train_loss: 0.0009661973580596564, test_loss: 0.0008929333804796139\n",
      "epoch: 656, train_loss: 0.0009658901734561052, test_loss: 0.0008914432158538451\n",
      "epoch: 657, train_loss: 0.000965895060368854, test_loss: 0.0008935219908986861\n",
      "epoch: 658, train_loss: 0.0009645455511813254, test_loss: 0.000896123931549179\n",
      "epoch: 659, train_loss: 0.0009647569892442097, test_loss: 0.000889416269880409\n",
      "epoch: 660, train_loss: 0.0009620518919647387, test_loss: 0.0008947031359033039\n",
      "epoch: 661, train_loss: 0.000963268191654883, test_loss: 0.0008922252260769407\n",
      "epoch: 662, train_loss: 0.0009648159333585721, test_loss: 0.0008857819630065933\n",
      "epoch: 663, train_loss: 0.0009616894531063735, test_loss: 0.0008971058947887892\n",
      "epoch: 664, train_loss: 0.000963744332851923, test_loss: 0.0008910344186006114\n",
      "epoch: 665, train_loss: 0.0009616522685341213, test_loss: 0.0008836857208128398\n",
      "epoch: 666, train_loss: 0.0009620971395634115, test_loss: 0.0008853033068589866\n",
      "epoch: 667, train_loss: 0.0009607236665349616, test_loss: 0.0008818004571367055\n",
      "epoch: 668, train_loss: 0.00095943674829829, test_loss: 0.000884551482158713\n",
      "epoch: 669, train_loss: 0.0009602905838223903, test_loss: 0.0008818738618477558\n",
      "epoch: 670, train_loss: 0.0009579426715271952, test_loss: 0.0008874185150489211\n",
      "epoch: 671, train_loss: 0.0009584389475133756, test_loss: 0.0008820724081791317\n",
      "epoch: 672, train_loss: 0.000957905279431978, test_loss: 0.000884044721412162\n",
      "epoch: 673, train_loss: 0.0009555770312031002, test_loss: 0.0008790684902730087\n",
      "epoch: 674, train_loss: 0.0009559807215776781, test_loss: 0.0008781809932164227\n",
      "epoch: 675, train_loss: 0.0009562271626909142, test_loss: 0.0008854486513882875\n",
      "epoch: 676, train_loss: 0.0009554217103868723, test_loss: 0.0008789045144415771\n",
      "epoch: 677, train_loss: 0.0009551498527719599, test_loss: 0.000883343915726679\n",
      "epoch: 678, train_loss: 0.0009533504666963025, test_loss: 0.0008843157799371207\n",
      "epoch: 679, train_loss: 0.0009537855765539343, test_loss: 0.0008925021538743749\n",
      "epoch: 680, train_loss: 0.0009548076271565388, test_loss: 0.0008781058568274602\n",
      "epoch: 681, train_loss: 0.0009529490438897325, test_loss: 0.000874854827998206\n",
      "epoch: 682, train_loss: 0.0009514327216690973, test_loss: 0.0008740684861550108\n",
      "epoch: 683, train_loss: 0.0009507956691896138, test_loss: 0.0008814764344909539\n",
      "epoch: 684, train_loss: 0.0009504649206064641, test_loss: 0.0008808250471095865\n",
      "epoch: 685, train_loss: 0.0009511337392841992, test_loss: 0.0008757836185395718\n",
      "epoch: 686, train_loss: 0.0009502311485171642, test_loss: 0.0008799674202843258\n",
      "epoch: 687, train_loss: 0.0009509932538292006, test_loss: 0.0008801160001894459\n",
      "epoch: 688, train_loss: 0.0009485836768441874, test_loss: 0.0008808059598474453\n",
      "epoch: 689, train_loss: 0.0009488860124965076, test_loss: 0.0008754977461649105\n",
      "epoch: 690, train_loss: 0.0009493180145712003, test_loss: 0.0008819672705916067\n",
      "epoch: 691, train_loss: 0.0009477050094257878, test_loss: 0.0008704580007664239\n",
      "epoch: 692, train_loss: 0.000947282100400037, test_loss: 0.0008792486963405585\n",
      "epoch: 693, train_loss: 0.0009454700199927648, test_loss: 0.0008730387683802595\n",
      "epoch: 694, train_loss: 0.0009466694698304585, test_loss: 0.0008752277644816786\n",
      "epoch: 695, train_loss: 0.0009470880309970158, test_loss: 0.0008720536716282368\n",
      "epoch: 696, train_loss: 0.0009456861500992724, test_loss: 0.000873661299313729\n",
      "epoch: 697, train_loss: 0.0009453922317808737, test_loss: 0.0008723811867336432\n",
      "epoch: 698, train_loss: 0.0009435296220623928, test_loss: 0.0008717125650340071\n",
      "epoch: 699, train_loss: 0.0009430903212531754, test_loss: 0.0008658929970503474\n",
      "epoch: 700, train_loss: 0.0009439304501385144, test_loss: 0.0008727571742686754\n",
      "epoch: 701, train_loss: 0.0009435347215596424, test_loss: 0.0008630488300696015\n",
      "epoch: 702, train_loss: 0.0009433268771871277, test_loss: 0.0008676276387025913\n",
      "epoch: 703, train_loss: 0.0009424789585982976, test_loss: 0.0008653478337995087\n",
      "epoch: 704, train_loss: 0.0009426173738851819, test_loss: 0.000869519067540144\n",
      "epoch: 705, train_loss: 0.0009407710015733281, test_loss: 0.0008647961367387325\n",
      "epoch: 706, train_loss: 0.0009394387699380193, test_loss: 0.0008724910197391486\n",
      "epoch: 707, train_loss: 0.0009414585479332701, test_loss: 0.0008775675184248636\n",
      "epoch: 708, train_loss: 0.0009394805959385375, test_loss: 0.0008765397409054761\n",
      "epoch: 709, train_loss: 0.0009387739296273693, test_loss: 0.0008667701525458446\n",
      "epoch: 710, train_loss: 0.0009382122231687864, test_loss: 0.0008641095919301733\n",
      "epoch: 711, train_loss: 0.0009398462944259138, test_loss: 0.0008629897541444128\n",
      "epoch: 712, train_loss: 0.000937391335711531, test_loss: 0.0008691614105676612\n",
      "epoch: 713, train_loss: 0.0009380665597627344, test_loss: 0.0008665372006362304\n",
      "epoch: 714, train_loss: 0.000937067721362995, test_loss: 0.0008698159896691019\n",
      "epoch: 715, train_loss: 0.0009372500707800298, test_loss: 0.000860931157755355\n",
      "epoch: 716, train_loss: 0.0009365392464172581, test_loss: 0.0008583854020495588\n",
      "epoch: 717, train_loss: 0.000936083229886287, test_loss: 0.0008637048255574579\n",
      "epoch: 718, train_loss: 0.0009353760143984919, test_loss: 0.0008548926465058079\n",
      "epoch: 719, train_loss: 0.0009359580226768942, test_loss: 0.0008681695908308029\n",
      "epoch: 720, train_loss: 0.0009336416589339142, test_loss: 0.0008634388990079364\n",
      "epoch: 721, train_loss: 0.0009350557356020031, test_loss: 0.0008743951184442267\n",
      "epoch: 722, train_loss: 0.0009325573084187572, test_loss: 0.0008575092360842973\n",
      "epoch: 723, train_loss: 0.0009332612490929339, test_loss: 0.0008757933246670291\n",
      "epoch: 724, train_loss: 0.000932347305301253, test_loss: 0.0008542670936246092\n",
      "epoch: 725, train_loss: 0.0009320343581392714, test_loss: 0.0008638736714298526\n",
      "epoch: 726, train_loss: 0.0009316858789964539, test_loss: 0.0008628834038972855\n",
      "epoch: 727, train_loss: 0.0009313649902849094, test_loss: 0.0008538561717917522\n",
      "epoch: 728, train_loss: 0.0009302906438951259, test_loss: 0.0008515344767753655\n",
      "epoch: 729, train_loss: 0.000929554949676537, test_loss: 0.0008540454873582348\n",
      "epoch: 730, train_loss: 0.0009289892825662442, test_loss: 0.0008516749124585962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 731, train_loss: 0.0009290821010084903, test_loss: 0.0008506437734467909\n",
      "epoch: 732, train_loss: 0.0009288645511412102, test_loss: 0.0008540706379183879\n",
      "epoch: 733, train_loss: 0.0009280872865296576, test_loss: 0.0008509024143374214\n",
      "epoch: 734, train_loss: 0.0009285254105318176, test_loss: 0.0008684920030646026\n",
      "epoch: 735, train_loss: 0.0009267893923527521, test_loss: 0.0008563908631913364\n",
      "epoch: 736, train_loss: 0.0009269084118347129, test_loss: 0.0008541357625896732\n",
      "epoch: 737, train_loss: 0.0009262442118083329, test_loss: 0.0008545006276108325\n",
      "epoch: 738, train_loss: 0.0009254990803563725, test_loss: 0.0008544234005967155\n",
      "epoch: 739, train_loss: 0.0009256011234474895, test_loss: 0.0008525828501054397\n",
      "epoch: 740, train_loss: 0.0009247657017903807, test_loss: 0.0008550759181768323\n",
      "epoch: 741, train_loss: 0.0009241799783447515, test_loss: 0.0008505277801305056\n",
      "epoch: 742, train_loss: 0.0009242575944644277, test_loss: 0.0008510956831742078\n",
      "epoch: 743, train_loss: 0.0009226519968765585, test_loss: 0.0008591631873665998\n",
      "epoch: 744, train_loss: 0.0009229061391164104, test_loss: 0.0008699222526047379\n",
      "epoch: 745, train_loss: 0.0009229658373995968, test_loss: 0.0008499332567832122\n",
      "epoch: 746, train_loss: 0.0009220140662206256, test_loss: 0.0008490240628210207\n",
      "epoch: 747, train_loss: 0.0009218432469819875, test_loss: 0.000850964745040983\n",
      "epoch: 748, train_loss: 0.000920391730134092, test_loss: 0.0008454307680949569\n",
      "epoch: 749, train_loss: 0.0009214410371066111, test_loss: 0.0008403402074084928\n",
      "epoch: 750, train_loss: 0.000919640459039289, test_loss: 0.0008446949990078186\n",
      "epoch: 751, train_loss: 0.0009193462699287287, test_loss: 0.0008420267404289916\n",
      "epoch: 752, train_loss: 0.0009185815984180764, test_loss: 0.0008449972228845581\n",
      "epoch: 753, train_loss: 0.000919685663614908, test_loss: 0.0008467581452957044\n",
      "epoch: 754, train_loss: 0.0009178624048059726, test_loss: 0.0008416754232409099\n",
      "epoch: 755, train_loss: 0.0009188733503991818, test_loss: 0.0008546843697937826\n",
      "epoch: 756, train_loss: 0.0009178317040609925, test_loss: 0.0008424698365464186\n",
      "epoch: 757, train_loss: 0.0009167069321214829, test_loss: 0.0008412648166995496\n",
      "epoch: 758, train_loss: 0.0009154411812272409, test_loss: 0.0008471894495111579\n",
      "epoch: 759, train_loss: 0.000916208798551689, test_loss: 0.0008481452823616564\n",
      "epoch: 760, train_loss: 0.0009147064056773872, test_loss: 0.0008430569772220527\n",
      "epoch: 761, train_loss: 0.000914903463917258, test_loss: 0.0008435392131408056\n",
      "epoch: 762, train_loss: 0.0009146422861427393, test_loss: 0.0008422156873469552\n",
      "epoch: 763, train_loss: 0.0009162661558745996, test_loss: 0.0008454548515146598\n",
      "epoch: 764, train_loss: 0.0009150937498223199, test_loss: 0.0008411161155284693\n",
      "epoch: 765, train_loss: 0.0009135136775591451, test_loss: 0.0008352640046117207\n",
      "epoch: 766, train_loss: 0.0009133109757069336, test_loss: 0.0008357654684611285\n",
      "epoch: 767, train_loss: 0.0009127881570273768, test_loss: 0.0008437684979677821\n",
      "epoch: 768, train_loss: 0.0009127936083013597, test_loss: 0.0008348126284545287\n",
      "epoch: 769, train_loss: 0.0009115733606133448, test_loss: 0.0008384194516111165\n",
      "epoch: 770, train_loss: 0.0009119103500700515, test_loss: 0.0008324273609711478\n",
      "epoch: 771, train_loss: 0.0009117920543877003, test_loss: 0.0008403744626169404\n",
      "epoch: 772, train_loss: 0.0009093554538634161, test_loss: 0.0008373850384183849\n",
      "epoch: 773, train_loss: 0.0009099882419990456, test_loss: 0.0008444008271908388\n",
      "epoch: 774, train_loss: 0.0009115405466771969, test_loss: 0.0008348898506180072\n",
      "epoch: 775, train_loss: 0.0009092442013080354, test_loss: 0.0008299073039476449\n",
      "epoch: 776, train_loss: 0.0009095471139754291, test_loss: 0.0008323045428066204\n",
      "epoch: 777, train_loss: 0.0009086307052158467, test_loss: 0.0008307628013426438\n",
      "epoch: 778, train_loss: 0.0009110190060354122, test_loss: 0.0008368753478862345\n",
      "epoch: 779, train_loss: 0.0009075287911717011, test_loss: 0.0008415560732828453\n",
      "epoch: 780, train_loss: 0.000909081966433998, test_loss: 0.0008278104117683446\n",
      "epoch: 781, train_loss: 0.0009057475532324094, test_loss: 0.0008297179398747782\n",
      "epoch: 782, train_loss: 0.0009064531945825919, test_loss: 0.0008343669954532137\n",
      "epoch: 783, train_loss: 0.0009058698627125958, test_loss: 0.0008290137484436855\n",
      "epoch: 784, train_loss: 0.0009058928598001924, test_loss: 0.0008299923550415164\n",
      "epoch: 785, train_loss: 0.0009044058793021933, test_loss: 0.0008330396231031045\n",
      "epoch: 786, train_loss: 0.0009040079413629744, test_loss: 0.0008306997236407673\n",
      "epoch: 787, train_loss: 0.0009044637351863734, test_loss: 0.0008361702881908665\n",
      "epoch: 788, train_loss: 0.0009046815102919936, test_loss: 0.000834244162736771\n",
      "epoch: 789, train_loss: 0.0009041721401128756, test_loss: 0.0008290274757503843\n",
      "epoch: 790, train_loss: 0.0009044203122713319, test_loss: 0.0008268451056210324\n",
      "epoch: 791, train_loss: 0.0009020377740101969, test_loss: 0.0008228717439730341\n",
      "epoch: 792, train_loss: 0.0009017038518441436, test_loss: 0.000830802857914629\n",
      "epoch: 793, train_loss: 0.0009017893766134006, test_loss: 0.0008278075256384909\n",
      "epoch: 794, train_loss: 0.0009012579204231176, test_loss: 0.0008313535945490003\n",
      "epoch: 795, train_loss: 0.0009001866615701305, test_loss: 0.0008264252392109483\n",
      "epoch: 796, train_loss: 0.0009003520168809463, test_loss: 0.0008302792266476899\n",
      "epoch: 797, train_loss: 0.0009006288044316614, test_loss: 0.0008224267706585427\n",
      "epoch: 798, train_loss: 0.0009003062276979504, test_loss: 0.0008244996424764395\n",
      "epoch: 799, train_loss: 0.0008980670827440917, test_loss: 0.0008236368933770185\n",
      "epoch: 800, train_loss: 0.0008987161184094199, test_loss: 0.00083690277824644\n",
      "epoch: 801, train_loss: 0.0008988574238332069, test_loss: 0.0008476709335809574\n",
      "epoch: 802, train_loss: 0.0008992078870980312, test_loss: 0.0008266581232116247\n",
      "epoch: 803, train_loss: 0.0008977730986257286, test_loss: 0.0008341703990784785\n",
      "epoch: 804, train_loss: 0.0008982611951701667, test_loss: 0.0008263767910345147\n",
      "epoch: 805, train_loss: 0.0008959689572372514, test_loss: 0.0008197169857642924\n",
      "epoch: 806, train_loss: 0.0008964632440398892, test_loss: 0.0008192294869028652\n",
      "epoch: 807, train_loss: 0.0008966128478515084, test_loss: 0.0008227623378237089\n",
      "epoch: 808, train_loss: 0.0008958529240613723, test_loss: 0.0008216551747561122\n",
      "epoch: 809, train_loss: 0.0008955509477780889, test_loss: 0.0008210636539539943\n",
      "epoch: 810, train_loss: 0.0008950810322699988, test_loss: 0.0008245864398001382\n",
      "epoch: 811, train_loss: 0.0008963046357562037, test_loss: 0.0008227875902472684\n",
      "epoch: 812, train_loss: 0.0008943893683507391, test_loss: 0.0008351263240911067\n",
      "epoch: 813, train_loss: 0.0008932657656259835, test_loss: 0.0008157495904015377\n",
      "epoch: 814, train_loss: 0.0008927607736752733, test_loss: 0.0008298052125610411\n",
      "epoch: 815, train_loss: 0.0008924174243989198, test_loss: 0.0008212187385652214\n",
      "epoch: 816, train_loss: 0.0008936549369853152, test_loss: 0.0008271961026669791\n",
      "epoch: 817, train_loss: 0.000892437053034487, test_loss: 0.0008179982930111388\n",
      "epoch: 818, train_loss: 0.0008915149551087424, test_loss: 0.0008240069291787222\n",
      "epoch: 819, train_loss: 0.0008916763447063124, test_loss: 0.0008180898003047332\n",
      "epoch: 820, train_loss: 0.0008913593331311384, test_loss: 0.0008134419137301544\n",
      "epoch: 821, train_loss: 0.0008916969679336509, test_loss: 0.0008369602049545696\n",
      "epoch: 822, train_loss: 0.0008907616315612003, test_loss: 0.0008334239973919466\n",
      "epoch: 823, train_loss: 0.0008908026198775548, test_loss: 0.0008121956003985057\n",
      "epoch: 824, train_loss: 0.0008902931560600257, test_loss: 0.0008113542911208546\n",
      "epoch: 825, train_loss: 0.0008882392025755152, test_loss: 0.00080942334413218\n",
      "epoch: 826, train_loss: 0.0008884470671941729, test_loss: 0.0008166356759223466\n",
      "epoch: 827, train_loss: 0.0008888571477576119, test_loss: 0.0008246024857119968\n",
      "epoch: 828, train_loss: 0.0008880196331554781, test_loss: 0.0008143845140390719\n",
      "epoch: 829, train_loss: 0.0008870474994182587, test_loss: 0.0008184671508691584\n",
      "epoch: 830, train_loss: 0.0008885055557703195, test_loss: 0.0008132010843837634\n",
      "epoch: 831, train_loss: 0.0008873128858597382, test_loss: 0.0008285124737691755\n",
      "epoch: 832, train_loss: 0.0008867543504532913, test_loss: 0.0008150276213806743\n",
      "epoch: 833, train_loss: 0.0008855243517166895, test_loss: 0.0008138815319398418\n",
      "epoch: 834, train_loss: 0.000886484942621673, test_loss: 0.0008095272302549953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 835, train_loss: 0.0008848148673955027, test_loss: 0.0008119927758040527\n",
      "epoch: 836, train_loss: 0.0008857815890856411, test_loss: 0.0008359377146310484\n",
      "epoch: 837, train_loss: 0.0008869035468112839, test_loss: 0.0008102040446829051\n",
      "epoch: 838, train_loss: 0.000883627374170591, test_loss: 0.0008115499949781224\n",
      "epoch: 839, train_loss: 0.0008846442291067671, test_loss: 0.0008130196705072498\n",
      "epoch: 840, train_loss: 0.000883473318738296, test_loss: 0.0008105682985236248\n",
      "epoch: 841, train_loss: 0.0008825402954876747, test_loss: 0.0008087250801812237\n",
      "epoch: 842, train_loss: 0.0008817275621644828, test_loss: 0.0008051975164562464\n",
      "epoch: 843, train_loss: 0.0008827030516998923, test_loss: 0.0008085524314083159\n",
      "epoch: 844, train_loss: 0.0008815582791019393, test_loss: 0.0008073667413555086\n",
      "epoch: 845, train_loss: 0.0008815521141514182, test_loss: 0.0008032093367849787\n",
      "epoch: 846, train_loss: 0.0008813026462398145, test_loss: 0.0008109794677390406\n",
      "epoch: 847, train_loss: 0.0008820318895311135, test_loss: 0.0007997890400777882\n",
      "epoch: 848, train_loss: 0.0008805259712971747, test_loss: 0.0008029584375132496\n",
      "epoch: 849, train_loss: 0.0008802111083439187, test_loss: 0.0007989794248715043\n",
      "epoch: 850, train_loss: 0.0008785628926251893, test_loss: 0.0008046753209782764\n",
      "epoch: 851, train_loss: 0.0008797219716538877, test_loss: 0.0008051239662260438\n",
      "epoch: 852, train_loss: 0.0008803156897952051, test_loss: 0.0008052570240882536\n",
      "epoch: 853, train_loss: 0.0008780410356374215, test_loss: 0.0007998616153296704\n",
      "epoch: 854, train_loss: 0.0008787909780790949, test_loss: 0.0008059535321081057\n",
      "epoch: 855, train_loss: 0.0008777004474288095, test_loss: 0.0007997570161630089\n",
      "epoch: 856, train_loss: 0.0008775554850454564, test_loss: 0.0008025410982857769\n",
      "epoch: 857, train_loss: 0.0008771072329102975, test_loss: 0.0008007545353999982\n",
      "epoch: 858, train_loss: 0.0008778835788531148, test_loss: 0.0008053263882175088\n",
      "epoch: 859, train_loss: 0.00087629909546155, test_loss: 0.0007984861586010084\n",
      "epoch: 860, train_loss: 0.0008763156745218388, test_loss: 0.0007998916213788713\n",
      "epoch: 861, train_loss: 0.0008753943434187576, test_loss: 0.0008035268644259\n",
      "epoch: 862, train_loss: 0.000875118340406081, test_loss: 0.0007982205764468139\n",
      "epoch: 863, train_loss: 0.0008751724709999626, test_loss: 0.0008014630390486369\n",
      "epoch: 864, train_loss: 0.0008744405881177796, test_loss: 0.0007997515058377758\n",
      "epoch: 865, train_loss: 0.0008725625229999423, test_loss: 0.000797888720020031\n",
      "epoch: 866, train_loss: 0.000872858306494258, test_loss: 0.0008015564720456799\n",
      "epoch: 867, train_loss: 0.0008735167819237256, test_loss: 0.0007958848824879775\n",
      "epoch: 868, train_loss: 0.0008714525965447335, test_loss: 0.0008044656618343046\n",
      "epoch: 869, train_loss: 0.0008730031929545752, test_loss: 0.0008029204570145035\n",
      "epoch: 870, train_loss: 0.0008720757981316875, test_loss: 0.0007934078894322738\n",
      "epoch: 871, train_loss: 0.0008719963143053262, test_loss: 0.0007990209463362893\n",
      "epoch: 872, train_loss: 0.0008716824965591987, test_loss: 0.0008245386901156356\n",
      "epoch: 873, train_loss: 0.0008708850769123629, test_loss: 0.000799097329339323\n",
      "epoch: 874, train_loss: 0.0008703422676974341, test_loss: 0.0007970854494487867\n",
      "epoch: 875, train_loss: 0.0008698814249921428, test_loss: 0.0007943691598484293\n",
      "epoch: 876, train_loss: 0.0008716950314524381, test_loss: 0.0007957188984922444\n",
      "epoch: 877, train_loss: 0.0008701560993516898, test_loss: 0.000788110871023188\n",
      "epoch: 878, train_loss: 0.0008694128280141107, test_loss: 0.0007937377425453936\n",
      "epoch: 879, train_loss: 0.0008676676889476569, test_loss: 0.0007898260907192404\n",
      "epoch: 880, train_loss: 0.0008690774430642309, test_loss: 0.0007977411927034458\n",
      "epoch: 881, train_loss: 0.0008683210674106427, test_loss: 0.0007890883668248231\n",
      "epoch: 882, train_loss: 0.0008676071479187711, test_loss: 0.0007923067702601353\n",
      "epoch: 883, train_loss: 0.0008683383234513357, test_loss: 0.0008006698626559228\n",
      "epoch: 884, train_loss: 0.0008667557469933578, test_loss: 0.0007903917139628902\n",
      "epoch: 885, train_loss: 0.0008663440492692525, test_loss: 0.0007894242104763786\n",
      "epoch: 886, train_loss: 0.0008658416184556225, test_loss: 0.0007872906959770868\n",
      "epoch: 887, train_loss: 0.0008661937798656847, test_loss: 0.0007910856559950238\n",
      "epoch: 888, train_loss: 0.0008655384501806744, test_loss: 0.0007888501325699812\n",
      "epoch: 889, train_loss: 0.0008661779676280592, test_loss: 0.0007844496625087535\n",
      "epoch: 890, train_loss: 0.0008648175280541182, test_loss: 0.0007908500022798156\n",
      "epoch: 891, train_loss: 0.0008657197999444021, test_loss: 0.0007892323531753694\n",
      "epoch: 892, train_loss: 0.0008642487607531897, test_loss: 0.0007889287759705136\n",
      "epoch: 893, train_loss: 0.0008652838397487674, test_loss: 0.0007904479280114174\n",
      "epoch: 894, train_loss: 0.0008621280582662185, test_loss: 0.0007843130539792279\n",
      "epoch: 895, train_loss: 0.0008632331433625001, test_loss: 0.0007871599567200368\n",
      "epoch: 896, train_loss: 0.0008625667592834519, test_loss: 0.0007871515651155884\n",
      "epoch: 897, train_loss: 0.0008641388545663136, test_loss: 0.0007883917278377339\n",
      "epoch: 898, train_loss: 0.0008632593342791433, test_loss: 0.0007852846417032803\n",
      "epoch: 899, train_loss: 0.0008619996192662612, test_loss: 0.0007810356958846872\n",
      "epoch: 900, train_loss: 0.0008618603890721241, test_loss: 0.0007917252563250562\n",
      "epoch: 901, train_loss: 0.0008609176400805945, test_loss: 0.0007798483032577982\n",
      "epoch: 902, train_loss: 0.0008594295764139489, test_loss: 0.0007935145161657905\n",
      "epoch: 903, train_loss: 0.0008600342629031967, test_loss: 0.000790720786123226\n",
      "epoch: 904, train_loss: 0.0008607316868501189, test_loss: 0.0008089352825966974\n",
      "epoch: 905, train_loss: 0.000860234535218257, test_loss: 0.0007850050848598281\n",
      "epoch: 906, train_loss: 0.0008590454691211166, test_loss: 0.0007816158710435653\n",
      "epoch: 907, train_loss: 0.0008589643807879285, test_loss: 0.0008003829959003875\n",
      "epoch: 908, train_loss: 0.0008586975366241583, test_loss: 0.0007791077562918266\n",
      "epoch: 909, train_loss: 0.0008575647548043534, test_loss: 0.0007788919707915435\n",
      "epoch: 910, train_loss: 0.0008579063300124329, test_loss: 0.000779899230110459\n",
      "epoch: 911, train_loss: 0.0008581468061058093, test_loss: 0.000783717138498711\n",
      "epoch: 912, train_loss: 0.0008566622956372473, test_loss: 0.0007815567514626309\n",
      "epoch: 913, train_loss: 0.000857650172015976, test_loss: 0.0007937867339933291\n",
      "epoch: 914, train_loss: 0.0008550358976682891, test_loss: 0.0007748676289338619\n",
      "epoch: 915, train_loss: 0.0008556086585209098, test_loss: 0.0008009368272420639\n",
      "epoch: 916, train_loss: 0.000856995552211352, test_loss: 0.0007776426646159962\n",
      "epoch: 917, train_loss: 0.0008557872092553779, test_loss: 0.0007750821726707121\n",
      "epoch: 918, train_loss: 0.0008549556394269609, test_loss: 0.0007800677655419955\n",
      "epoch: 919, train_loss: 0.0008547950887789383, test_loss: 0.000778855440633682\n",
      "epoch: 920, train_loss: 0.0008541251704825655, test_loss: 0.0007845607130244995\n",
      "epoch: 921, train_loss: 0.0008557288497483925, test_loss: 0.0007804787310305983\n",
      "epoch: 922, train_loss: 0.0008537027549565486, test_loss: 0.0007861970661906525\n",
      "epoch: 923, train_loss: 0.0008533599244876076, test_loss: 0.0007820521956697727\n",
      "epoch: 924, train_loss: 0.000852789426141459, test_loss: 0.0007798497875531515\n",
      "epoch: 925, train_loss: 0.0008516804234165212, test_loss: 0.0007826263269331927\n",
      "epoch: 926, train_loss: 0.0008526999483127956, test_loss: 0.0007843789644539356\n",
      "epoch: 927, train_loss: 0.000853166260007445, test_loss: 0.0007825180946383625\n",
      "epoch: 928, train_loss: 0.000852253447469, test_loss: 0.0007789414570045968\n",
      "epoch: 929, train_loss: 0.0008503106326553161, test_loss: 0.0007735677208984271\n",
      "epoch: 930, train_loss: 0.0008504955457401988, test_loss: 0.0007794797226476172\n",
      "epoch: 931, train_loss: 0.0008508666246400579, test_loss: 0.0007694172721433764\n",
      "epoch: 932, train_loss: 0.0008519219573708656, test_loss: 0.0007854775030864403\n",
      "epoch: 933, train_loss: 0.0008500349510501584, test_loss: 0.0007909635023679584\n",
      "epoch: 934, train_loss: 0.0008496252905941852, test_loss: 0.0007711084520754715\n",
      "epoch: 935, train_loss: 0.0008495463977021205, test_loss: 0.0007719266335091864\n",
      "epoch: 936, train_loss: 0.0008494958974947424, test_loss: 0.0007746069216712689\n",
      "epoch: 937, train_loss: 0.0008500039061207486, test_loss: 0.000782159079487125\n",
      "epoch: 938, train_loss: 0.0008487101573416072, test_loss: 0.0007698840345256031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 939, train_loss: 0.0008482062713364544, test_loss: 0.0007677708296493316\n",
      "epoch: 940, train_loss: 0.0008482810884268712, test_loss: 0.0007684817149614295\n",
      "epoch: 941, train_loss: 0.0008492218203964117, test_loss: 0.0007711070599422479\n",
      "epoch: 942, train_loss: 0.0008470491738989949, test_loss: 0.0007748644614669805\n",
      "epoch: 943, train_loss: 0.0008485667007651342, test_loss: 0.0007694634211172039\n",
      "epoch: 944, train_loss: 0.0008460532205746225, test_loss: 0.0007719777446861068\n",
      "epoch: 945, train_loss: 0.0008486362867583723, test_loss: 0.0007764772502317404\n",
      "epoch: 946, train_loss: 0.0008460903849007319, test_loss: 0.0007724735914962366\n",
      "epoch: 947, train_loss: 0.0008468520701053026, test_loss: 0.0007746229287780201\n",
      "epoch: 948, train_loss: 0.0008465983365844611, test_loss: 0.0007713998396260043\n",
      "epoch: 949, train_loss: 0.00084600612298464, test_loss: 0.0007705993242173766\n",
      "epoch: 950, train_loss: 0.0008452689052437959, test_loss: 0.0007747454510536045\n",
      "epoch: 951, train_loss: 0.0008429576941441907, test_loss: 0.0007657204599430164\n",
      "epoch: 952, train_loss: 0.0008439040267030182, test_loss: 0.0007727809958547974\n",
      "epoch: 953, train_loss: 0.0008439579750816135, test_loss: 0.0007666764868190512\n",
      "epoch: 954, train_loss: 0.00084353591133233, test_loss: 0.0007707326779685294\n",
      "epoch: 955, train_loss: 0.0008439322827262399, test_loss: 0.0007657459645997733\n",
      "epoch: 956, train_loss: 0.0008434727787971497, test_loss: 0.0007695100212004036\n",
      "epoch: 957, train_loss: 0.0008442604701723094, test_loss: 0.0007752962944020206\n",
      "epoch: 958, train_loss: 0.0008424263341230867, test_loss: 0.0007640891271876171\n",
      "epoch: 959, train_loss: 0.0008422476188887073, test_loss: 0.0007742847180149207\n",
      "epoch: 960, train_loss: 0.0008404217508581022, test_loss: 0.0007645775331184268\n",
      "epoch: 961, train_loss: 0.0008410786141884391, test_loss: 0.0007680074001351992\n",
      "epoch: 962, train_loss: 0.0008412539230092713, test_loss: 0.0007649121592597415\n",
      "epoch: 963, train_loss: 0.0008408473500900942, test_loss: 0.000771636909727628\n",
      "epoch: 964, train_loss: 0.0008404765040208788, test_loss: 0.0007714410166954622\n",
      "epoch: 965, train_loss: 0.0008401963328335272, test_loss: 0.0007752917639057463\n",
      "epoch: 966, train_loss: 0.00084121494665337, test_loss: 0.0007713036708688984\n",
      "epoch: 967, train_loss: 0.0008393958610806452, test_loss: 0.0007615454087499529\n",
      "epoch: 968, train_loss: 0.0008398298903003982, test_loss: 0.0007706285444631552\n",
      "epoch: 969, train_loss: 0.000840121449943146, test_loss: 0.0007592894689878449\n",
      "epoch: 970, train_loss: 0.0008407404782938893, test_loss: 0.0007606079598190263\n",
      "epoch: 971, train_loss: 0.000837482415828044, test_loss: 0.0007632604295698305\n",
      "epoch: 972, train_loss: 0.0008388526950274472, test_loss: 0.0007599999662488699\n",
      "epoch: 973, train_loss: 0.0008376471017507594, test_loss: 0.0007645998557563871\n",
      "epoch: 974, train_loss: 0.0008387570598405664, test_loss: 0.0007588663089942808\n",
      "epoch: 975, train_loss: 0.0008371642426304195, test_loss: 0.0007588022902685528\n",
      "epoch: 976, train_loss: 0.0008380374359979254, test_loss: 0.0007604996208101511\n",
      "epoch: 977, train_loss: 0.0008366225267072086, test_loss: 0.0007686168731500705\n",
      "epoch: 978, train_loss: 0.0008364686281825213, test_loss: 0.0007588638497206072\n",
      "epoch: 979, train_loss: 0.0008354610333259662, test_loss: 0.0007636024141296124\n",
      "epoch: 980, train_loss: 0.0008354396230298216, test_loss: 0.0007805097508632267\n",
      "epoch: 981, train_loss: 0.0008353709101514971, test_loss: 0.0007610945128059635\n",
      "epoch: 982, train_loss: 0.0008339349094413868, test_loss: 0.0007649180042790249\n",
      "epoch: 983, train_loss: 0.0008341116683922061, test_loss: 0.0007650215654090667\n",
      "epoch: 984, train_loss: 0.0008344468255729779, test_loss: 0.0007589505936872835\n",
      "epoch: 985, train_loss: 0.0008337707081607178, test_loss: 0.0007557635350773732\n",
      "epoch: 986, train_loss: 0.0008326207778578544, test_loss: 0.0007562000925342242\n",
      "epoch: 987, train_loss: 0.0008328577387146652, test_loss: 0.0007583989742367218\n",
      "epoch: 988, train_loss: 0.0008357820890681899, test_loss: 0.0007636549077384794\n",
      "epoch: 989, train_loss: 0.0008329233007869967, test_loss: 0.0007580006640637293\n",
      "epoch: 990, train_loss: 0.0008325787089036211, test_loss: 0.0007563597755506635\n",
      "epoch: 991, train_loss: 0.0008325950323563555, test_loss: 0.000764685576238359\n",
      "epoch: 992, train_loss: 0.0008314162411767503, test_loss: 0.000771518704520228\n",
      "epoch: 993, train_loss: 0.0008319444174918791, test_loss: 0.000761276280779081\n",
      "epoch: 994, train_loss: 0.0008324739148678339, test_loss: 0.0007573071876928831\n",
      "epoch: 995, train_loss: 0.0008315079511426713, test_loss: 0.0007581727065068359\n",
      "epoch: 996, train_loss: 0.0008313028956763446, test_loss: 0.0007516732536411533\n",
      "epoch: 997, train_loss: 0.0008295856380794684, test_loss: 0.0007516791277642673\n",
      "epoch: 998, train_loss: 0.0008301603678694886, test_loss: 0.00075652437711445\n",
      "epoch: 999, train_loss: 0.0008306485428677305, test_loss: 0.0007659172794471184\n",
      "epoch: 1000, train_loss: 0.0008300955587005972, test_loss: 0.0007552147144451737\n",
      "epoch: 1001, train_loss: 0.0008301098765197979, test_loss: 0.0007561095650695885\n",
      "epoch: 1002, train_loss: 0.0008298755679077104, test_loss: 0.0007490505959140137\n",
      "epoch: 1003, train_loss: 0.0008282585666798379, test_loss: 0.0007555891206720844\n",
      "epoch: 1004, train_loss: 0.000828632814101065, test_loss: 0.0007481403784671178\n",
      "epoch: 1005, train_loss: 0.0008284978760892283, test_loss: 0.0007518596685258672\n",
      "epoch: 1006, train_loss: 0.0008281468307477949, test_loss: 0.0007532052744257575\n",
      "epoch: 1007, train_loss: 0.000826480201162074, test_loss: 0.0007485596628005927\n",
      "epoch: 1008, train_loss: 0.0008273226431692424, test_loss: 0.0007517292542615905\n",
      "epoch: 1009, train_loss: 0.0008281222063764606, test_loss: 0.000757737512079378\n",
      "epoch: 1010, train_loss: 0.0008263817922536122, test_loss: 0.0007560066587757319\n",
      "epoch: 1011, train_loss: 0.0008259985732603009, test_loss: 0.000749349890005154\n",
      "epoch: 1012, train_loss: 0.0008254531320467916, test_loss: 0.0007548159483121708\n",
      "epoch: 1013, train_loss: 0.0008259805567238642, test_loss: 0.000749520055251196\n",
      "epoch: 1014, train_loss: 0.0008253569755217303, test_loss: 0.0007601978431921452\n",
      "epoch: 1015, train_loss: 0.0008254110605617905, test_loss: 0.0007479573008216297\n",
      "epoch: 1016, train_loss: 0.0008240796539568058, test_loss: 0.0007449536157461504\n",
      "epoch: 1017, train_loss: 0.0008250154585213116, test_loss: 0.0007523681755022457\n",
      "epoch: 1018, train_loss: 0.0008246452147748483, test_loss: 0.0007596281066071242\n",
      "epoch: 1019, train_loss: 0.0008245822669857222, test_loss: 0.0007475485569254184\n",
      "epoch: 1020, train_loss: 0.0008225881838766129, test_loss: 0.0007538605811229596\n",
      "epoch: 1021, train_loss: 0.0008243304074984853, test_loss: 0.0007434644212480634\n",
      "epoch: 1022, train_loss: 0.0008240636266039118, test_loss: 0.0007439921707070122\n",
      "epoch: 1023, train_loss: 0.0008233663545030614, test_loss: 0.0007459621847374365\n",
      "epoch: 1024, train_loss: 0.0008234683672249641, test_loss: 0.000759500990776966\n",
      "epoch: 1025, train_loss: 0.0008218885252616652, test_loss: 0.0007561961780690277\n",
      "epoch: 1026, train_loss: 0.0008212659791435884, test_loss: 0.0007578598645826181\n",
      "epoch: 1027, train_loss: 0.0008220525721654943, test_loss: 0.0007490485101394976\n",
      "epoch: 1028, train_loss: 0.0008216742299618604, test_loss: 0.000758362478033329\n",
      "epoch: 1029, train_loss: 0.0008213636060447797, test_loss: 0.0007532448944402859\n",
      "epoch: 1030, train_loss: 0.0008200701761423894, test_loss: 0.0007471918652299792\n",
      "epoch: 1031, train_loss: 0.0008219781372210254, test_loss: 0.0007562709070043638\n",
      "epoch: 1032, train_loss: 0.0008206932170255839, test_loss: 0.000741456805068689\n",
      "epoch: 1033, train_loss: 0.0008196050475817174, test_loss: 0.0007474114293775832\n",
      "epoch: 1034, train_loss: 0.0008188625215552747, test_loss: 0.0007450999886107942\n",
      "epoch: 1035, train_loss: 0.0008186657214537263, test_loss: 0.0007429001901376372\n",
      "epoch: 1036, train_loss: 0.000819624988767116, test_loss: 0.0007474016213867193\n",
      "epoch: 1037, train_loss: 0.0008196855753498232, test_loss: 0.0007489532241985822\n",
      "epoch: 1038, train_loss: 0.0008199090003947039, test_loss: 0.0007406764974196752\n",
      "epoch: 1039, train_loss: 0.0008190218612308736, test_loss: 0.0007472588816502442\n",
      "epoch: 1040, train_loss: 0.0008184575733890676, test_loss: 0.0007389339201229935\n",
      "epoch: 1041, train_loss: 0.0008181891171261668, test_loss: 0.0007407997909467667\n",
      "epoch: 1042, train_loss: 0.0008183586431424255, test_loss: 0.0007449436040284733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1043, train_loss: 0.0008170326878114239, test_loss: 0.0007440999179380015\n",
      "epoch: 1044, train_loss: 0.0008177088811467199, test_loss: 0.0007377379030610124\n",
      "epoch: 1045, train_loss: 0.0008177592434272494, test_loss: 0.0007397855467085416\n",
      "epoch: 1046, train_loss: 0.0008156392267783699, test_loss: 0.0007427383825415745\n",
      "epoch: 1047, train_loss: 0.0008175892087266496, test_loss: 0.0007453992196436351\n",
      "epoch: 1048, train_loss: 0.0008157250704243779, test_loss: 0.0007400409376714379\n",
      "epoch: 1049, train_loss: 0.0008161455056245398, test_loss: 0.0007393665970691169\n",
      "epoch: 1050, train_loss: 0.0008147588004763036, test_loss: 0.0007386072441780319\n",
      "epoch: 1051, train_loss: 0.0008157161266907402, test_loss: 0.0007424813035565118\n",
      "epoch: 1052, train_loss: 0.0008152904388818728, test_loss: 0.0007404191856039688\n",
      "epoch: 1053, train_loss: 0.0008140640262433368, test_loss: 0.0007414674570706362\n",
      "epoch: 1054, train_loss: 0.0008144312609068078, test_loss: 0.0007392228192960223\n",
      "epoch: 1055, train_loss: 0.0008148094208951553, test_loss: 0.0007432873050371805\n",
      "epoch: 1056, train_loss: 0.0008135538360954303, test_loss: 0.0007435980757387975\n",
      "epoch: 1057, train_loss: 0.000812264723683019, test_loss: 0.0007376425637630746\n",
      "epoch: 1058, train_loss: 0.000812697983549341, test_loss: 0.0007342111930483952\n",
      "epoch: 1059, train_loss: 0.0008122095111858747, test_loss: 0.0007396987445342044\n",
      "epoch: 1060, train_loss: 0.000811800765602485, test_loss: 0.0007392829817642147\n",
      "epoch: 1061, train_loss: 0.0008129261601108896, test_loss: 0.0007347131128578136\n",
      "epoch: 1062, train_loss: 0.0008117855227876292, test_loss: 0.000747227983083576\n",
      "epoch: 1063, train_loss: 0.000811492485176448, test_loss: 0.0007384203151256467\n",
      "epoch: 1064, train_loss: 0.0008125045164690717, test_loss: 0.0007377219105061764\n",
      "epoch: 1065, train_loss: 0.0008112388085978835, test_loss: 0.0007379367404306928\n",
      "epoch: 1066, train_loss: 0.0008108969701899458, test_loss: 0.0007340439963930597\n",
      "epoch: 1067, train_loss: 0.0008114621817620228, test_loss: 0.0007319309370359406\n",
      "epoch: 1068, train_loss: 0.0008099173074183257, test_loss: 0.0007334459999886652\n",
      "epoch: 1069, train_loss: 0.0008105436142574509, test_loss: 0.0007419991258454198\n",
      "epoch: 1070, train_loss: 0.000809511559529473, test_loss: 0.0007392804303284114\n",
      "epoch: 1071, train_loss: 0.0008086666449620996, test_loss: 0.0007357269641943276\n",
      "epoch: 1072, train_loss: 0.0008102139564352515, test_loss: 0.0007338923217806345\n",
      "epoch: 1073, train_loss: 0.0008094962407915813, test_loss: 0.0007277844075967247\n",
      "epoch: 1074, train_loss: 0.0008088197969102665, test_loss: 0.000732780080094623\n",
      "epoch: 1075, train_loss: 0.000808338310731494, test_loss: 0.0007330274335496748\n",
      "epoch: 1076, train_loss: 0.0008091865861585931, test_loss: 0.0007340309384744614\n",
      "epoch: 1077, train_loss: 0.0008070869953371584, test_loss: 0.0007296200492419302\n",
      "epoch: 1078, train_loss: 0.0008078929335486306, test_loss: 0.0007330474181799218\n",
      "epoch: 1079, train_loss: 0.0008070507927028382, test_loss: 0.0007354990569486594\n",
      "epoch: 1080, train_loss: 0.0008061292844460062, test_loss: 0.0007387879062055921\n",
      "epoch: 1081, train_loss: 0.0008067149446224389, test_loss: 0.0007318443967960775\n",
      "epoch: 1082, train_loss: 0.0008062873562069043, test_loss: 0.0007313929963856936\n",
      "epoch: 1083, train_loss: 0.000805913847769894, test_loss: 0.0007318253046832979\n",
      "epoch: 1084, train_loss: 0.0008071000034839886, test_loss: 0.0007297027429255346\n",
      "epoch: 1085, train_loss: 0.0008060200754856771, test_loss: 0.0007451275741914287\n",
      "epoch: 1086, train_loss: 0.0008066461507595428, test_loss: 0.000733925238212881\n",
      "epoch: 1087, train_loss: 0.0008056887195182397, test_loss: 0.0007328939051755393\n",
      "epoch: 1088, train_loss: 0.0008057237263647435, test_loss: 0.0007350351952482015\n",
      "epoch: 1089, train_loss: 0.00080555198845737, test_loss: 0.0007289879819533477\n",
      "epoch: 1090, train_loss: 0.0008044770296217631, test_loss: 0.0007311039419922357\n",
      "epoch: 1091, train_loss: 0.0008056464012833717, test_loss: 0.0007425540631326536\n",
      "epoch: 1092, train_loss: 0.0008047542398107116, test_loss: 0.0007394653609177718\n",
      "epoch: 1093, train_loss: 0.0008039945830165854, test_loss: 0.0007321207910232866\n",
      "epoch: 1094, train_loss: 0.0008042462742077591, test_loss: 0.0007300824751534188\n",
      "epoch: 1095, train_loss: 0.0008043625769104161, test_loss: 0.0007332567814349508\n",
      "epoch: 1096, train_loss: 0.0008044123356028095, test_loss: 0.0007347690407186747\n",
      "epoch: 1097, train_loss: 0.000803286789248333, test_loss: 0.0007386313664028421\n",
      "epoch: 1098, train_loss: 0.0008035792903367268, test_loss: 0.0007244414640202498\n",
      "epoch: 1099, train_loss: 0.0008024009496338018, test_loss: 0.0007278634341976916\n",
      "epoch: 1100, train_loss: 0.0008024799045296791, test_loss: 0.0007277578891565403\n",
      "epoch: 1101, train_loss: 0.0008009201581286186, test_loss: 0.0007329792279051617\n",
      "epoch: 1102, train_loss: 0.0008018825294287956, test_loss: 0.00073299887299072\n",
      "epoch: 1103, train_loss: 0.0008024355629459023, test_loss: 0.0007252101980460187\n",
      "epoch: 1104, train_loss: 0.0008010329177562634, test_loss: 0.0007253134875403097\n",
      "epoch: 1105, train_loss: 0.0007999245072310061, test_loss: 0.0007337733986787498\n",
      "epoch: 1106, train_loss: 0.0007997799573628151, test_loss: 0.0007246465296096479\n",
      "epoch: 1107, train_loss: 0.0008006449712885786, test_loss: 0.0007346410614748796\n",
      "epoch: 1108, train_loss: 0.0008010311069918554, test_loss: 0.0007319213521744435\n",
      "epoch: 1109, train_loss: 0.0007996344976835762, test_loss: 0.0007255867822095752\n",
      "epoch: 1110, train_loss: 0.0007998939532705623, test_loss: 0.0007245766852671901\n",
      "epoch: 1111, train_loss: 0.0007986635217726555, test_loss: 0.0007292509253602475\n",
      "epoch: 1112, train_loss: 0.0007991140718450365, test_loss: 0.0007293091184692457\n",
      "epoch: 1113, train_loss: 0.0007988024179054344, test_loss: 0.0007263589213835075\n",
      "epoch: 1114, train_loss: 0.0007984460984437686, test_loss: 0.0007181781014272323\n",
      "epoch: 1115, train_loss: 0.0007996175959503845, test_loss: 0.0007235352726032337\n",
      "epoch: 1116, train_loss: 0.0007985828484853972, test_loss: 0.0007199723719774435\n",
      "epoch: 1117, train_loss: 0.0007977889769751092, test_loss: 0.0007197717059170827\n",
      "epoch: 1118, train_loss: 0.0007981688249856234, test_loss: 0.0007446326538532352\n",
      "epoch: 1119, train_loss: 0.0007974938337650636, test_loss: 0.0007209682371467352\n",
      "epoch: 1120, train_loss: 0.0007989329042965951, test_loss: 0.0007330282242037356\n",
      "epoch: 1121, train_loss: 0.0007988054092730517, test_loss: 0.0007226401988494521\n",
      "epoch: 1122, train_loss: 0.0007964212791589291, test_loss: 0.0007242491459085917\n",
      "epoch: 1123, train_loss: 0.0007960724001547889, test_loss: 0.0007210220210254192\n",
      "epoch: 1124, train_loss: 0.0007976633901505367, test_loss: 0.0007277859209959084\n",
      "epoch: 1125, train_loss: 0.0007959972667183889, test_loss: 0.0007159043937766304\n",
      "epoch: 1126, train_loss: 0.0007954674858964332, test_loss: 0.000717133492192564\n",
      "epoch: 1127, train_loss: 0.000795338156066187, test_loss: 0.000720289036204728\n",
      "epoch: 1128, train_loss: 0.0007953257957959305, test_loss: 0.0007189702785884341\n",
      "epoch: 1129, train_loss: 0.0007956410333028306, test_loss: 0.0007153399686406677\n",
      "epoch: 1130, train_loss: 0.0007949204262568737, test_loss: 0.0007237203826662153\n",
      "epoch: 1131, train_loss: 0.0007955463142539172, test_loss: 0.0007291716562273601\n",
      "epoch: 1132, train_loss: 0.0007944215777451577, test_loss: 0.0007323972725619873\n",
      "epoch: 1133, train_loss: 0.000794338257274712, test_loss: 0.0007216913654701784\n",
      "epoch: 1134, train_loss: 0.0007947530438009974, test_loss: 0.0007144975146123519\n",
      "epoch: 1135, train_loss: 0.0007935152237263063, test_loss: 0.0007247053241978089\n",
      "epoch: 1136, train_loss: 0.00079365365166703, test_loss: 0.0007208354412189996\n",
      "epoch: 1137, train_loss: 0.0007938630448694786, test_loss: 0.0007134210657871639\n",
      "epoch: 1138, train_loss: 0.000793303069456116, test_loss: 0.0007159586045114944\n",
      "epoch: 1139, train_loss: 0.0007924388679068373, test_loss: 0.0007163248277114084\n",
      "epoch: 1140, train_loss: 0.0007927468623561056, test_loss: 0.0007178711772818739\n",
      "epoch: 1141, train_loss: 0.000792056894051316, test_loss: 0.0007178396626841277\n",
      "epoch: 1142, train_loss: 0.0007924271403285472, test_loss: 0.0007143258650709564\n",
      "epoch: 1143, train_loss: 0.0007910020181241081, test_loss: 0.0007213120358452821\n",
      "epoch: 1144, train_loss: 0.0007915827850608722, test_loss: 0.0007196053823766609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1145, train_loss: 0.00079123352897232, test_loss: 0.0007125231738124663\n",
      "epoch: 1146, train_loss: 0.0007902213635489992, test_loss: 0.0007300162881923219\n",
      "epoch: 1147, train_loss: 0.0007904139980066406, test_loss: 0.000715636502718553\n",
      "epoch: 1148, train_loss: 0.0007902160413441775, test_loss: 0.0007139719091355801\n",
      "epoch: 1149, train_loss: 0.0007901324222431234, test_loss: 0.000719729796401225\n",
      "epoch: 1150, train_loss: 0.0007896670266864417, test_loss: 0.0007136021498202657\n",
      "epoch: 1151, train_loss: 0.0007904476774653987, test_loss: 0.0007109969107356543\n",
      "epoch: 1152, train_loss: 0.0007902299175443857, test_loss: 0.0007276324856017405\n",
      "epoch: 1153, train_loss: 0.0007895913339503434, test_loss: 0.0007211299719832217\n",
      "epoch: 1154, train_loss: 0.0007902171776589493, test_loss: 0.000712970921692128\n",
      "epoch: 1155, train_loss: 0.0007888746053061408, test_loss: 0.0007180460816016421\n",
      "epoch: 1156, train_loss: 0.0007888660335953792, test_loss: 0.0007150325933859373\n",
      "epoch: 1157, train_loss: 0.0007895554552543099, test_loss: 0.0007147020029757792\n",
      "epoch: 1158, train_loss: 0.0007883945717881231, test_loss: 0.0007138743821997195\n",
      "epoch: 1159, train_loss: 0.0007861168402166146, test_loss: 0.000713421221007593\n",
      "epoch: 1160, train_loss: 0.0007880096015034486, test_loss: 0.000714575454670315\n",
      "epoch: 1161, train_loss: 0.0007877334719523787, test_loss: 0.000712070643203333\n",
      "epoch: 1162, train_loss: 0.0007874898956683667, test_loss: 0.0007209578664818158\n",
      "epoch: 1163, train_loss: 0.0007865523119740512, test_loss: 0.000720818352419883\n",
      "epoch: 1164, train_loss: 0.0007874338518139784, test_loss: 0.0007249161280924454\n",
      "epoch: 1165, train_loss: 0.0007870141669864888, test_loss: 0.000719407272602742\n",
      "epoch: 1166, train_loss: 0.0007867327025767578, test_loss: 0.0007107327789223442\n",
      "epoch: 1167, train_loss: 0.0007851460629709712, test_loss: 0.0007196717730645711\n",
      "epoch: 1168, train_loss: 0.000785225859347164, test_loss: 0.0007187620794866234\n",
      "epoch: 1169, train_loss: 0.0007854217040838431, test_loss: 0.0007111553859431297\n",
      "epoch: 1170, train_loss: 0.0007840903063365461, test_loss: 0.0007219929490626479\n",
      "epoch: 1171, train_loss: 0.0007850631626079912, test_loss: 0.0007128621364245191\n",
      "epoch: 1172, train_loss: 0.000784645561867839, test_loss: 0.0007120127702364698\n",
      "epoch: 1173, train_loss: 0.0007847497776231688, test_loss: 0.0007132017102170115\n",
      "epoch: 1174, train_loss: 0.0007853017203793254, test_loss: 0.0007100435032043606\n",
      "epoch: 1175, train_loss: 0.0007859327597543597, test_loss: 0.000706677048583515\n",
      "epoch: 1176, train_loss: 0.0007841506664155294, test_loss: 0.00071217580504405\n",
      "epoch: 1177, train_loss: 0.0007840561017434558, test_loss: 0.0007332327319697166\n",
      "epoch: 1178, train_loss: 0.0007850341119236597, test_loss: 0.0007101520119855801\n",
      "epoch: 1179, train_loss: 0.0007834384703765745, test_loss: 0.0007173797688058888\n",
      "epoch: 1180, train_loss: 0.0007824115682919712, test_loss: 0.00070668331560834\n",
      "epoch: 1181, train_loss: 0.0007835702094978288, test_loss: 0.0007129763398552313\n",
      "epoch: 1182, train_loss: 0.0007827695770411874, test_loss: 0.0007123608763019244\n",
      "epoch: 1183, train_loss: 0.0007820588754206572, test_loss: 0.0007049909957762187\n",
      "epoch: 1184, train_loss: 0.0007825628424103817, test_loss: 0.0007094307123528173\n",
      "epoch: 1185, train_loss: 0.0007822633209719282, test_loss: 0.0007065714550359795\n",
      "epoch: 1186, train_loss: 0.0007823698030298818, test_loss: 0.0007063814021724587\n",
      "epoch: 1187, train_loss: 0.0007828787606938378, test_loss: 0.0007101783946078891\n",
      "epoch: 1188, train_loss: 0.0007808867021990211, test_loss: 0.0007018336376252895\n",
      "epoch: 1189, train_loss: 0.000780770479215552, test_loss: 0.0007112092813864971\n",
      "epoch: 1190, train_loss: 0.0007808771561426313, test_loss: 0.0007084511792830502\n",
      "epoch: 1191, train_loss: 0.0007819530311162057, test_loss: 0.0007049928584213679\n",
      "epoch: 1192, train_loss: 0.0007808085179219589, test_loss: 0.0007069130272914966\n",
      "epoch: 1193, train_loss: 0.0007798908285938365, test_loss: 0.0007096155798838785\n",
      "epoch: 1194, train_loss: 0.0007826940956242058, test_loss: 0.0007041140488581732\n",
      "epoch: 1195, train_loss: 0.0007803329954976621, test_loss: 0.0007042652626599496\n",
      "epoch: 1196, train_loss: 0.0007790978820792035, test_loss: 0.0007121696156294396\n",
      "epoch: 1197, train_loss: 0.0007801059729662602, test_loss: 0.0007076792389852926\n",
      "epoch: 1198, train_loss: 0.0007789355156052371, test_loss: 0.000721377534015725\n",
      "epoch: 1199, train_loss: 0.0007786779745441416, test_loss: 0.0007031486651006466\n",
      "epoch: 1200, train_loss: 0.0007793102084416087, test_loss: 0.0007041546390003836\n",
      "epoch: 1201, train_loss: 0.0007786047062836587, test_loss: 0.0007069196920686712\n",
      "epoch: 1202, train_loss: 0.0007790513032966334, test_loss: 0.0007053024504178514\n",
      "epoch: 1203, train_loss: 0.0007769667262555627, test_loss: 0.0007151773800918212\n",
      "epoch: 1204, train_loss: 0.0007780552436800107, test_loss: 0.0007083738261523346\n",
      "epoch: 1205, train_loss: 0.0007784773555138837, test_loss: 0.0007035433712493008\n",
      "epoch: 1206, train_loss: 0.0007767491067921662, test_loss: 0.0007048384383476028\n",
      "epoch: 1207, train_loss: 0.0007772565864876884, test_loss: 0.0007074722331405306\n",
      "epoch: 1208, train_loss: 0.000776625456005011, test_loss: 0.0007050410058582202\n",
      "epoch: 1209, train_loss: 0.0007751612549485719, test_loss: 0.0007011025348523011\n",
      "epoch: 1210, train_loss: 0.0007766753716050121, test_loss: 0.0007097638202443098\n",
      "epoch: 1211, train_loss: 0.0007766758992701122, test_loss: 0.0007003184049002206\n",
      "epoch: 1212, train_loss: 0.0007768323386857367, test_loss: 0.0007030499206545452\n",
      "epoch: 1213, train_loss: 0.0007754306324109759, test_loss: 0.0007036077634741863\n",
      "epoch: 1214, train_loss: 0.0007748316679878727, test_loss: 0.0006983808367901171\n",
      "epoch: 1215, train_loss: 0.0007760860532036294, test_loss: 0.0006982673997602736\n",
      "epoch: 1216, train_loss: 0.000775654595244028, test_loss: 0.0007020042539807037\n",
      "epoch: 1217, train_loss: 0.0007747416194710556, test_loss: 0.0007067957194522023\n",
      "epoch: 1218, train_loss: 0.0007756114785518984, test_loss: 0.0007023978687357157\n",
      "epoch: 1219, train_loss: 0.0007760213490616044, test_loss: 0.0007082180042440692\n",
      "epoch: 1220, train_loss: 0.0007741050440413148, test_loss: 0.0007104943651938811\n",
      "epoch: 1221, train_loss: 0.0007747432783893917, test_loss: 0.0006988978517862657\n",
      "epoch: 1222, train_loss: 0.0007737772817642468, test_loss: 0.0006985432410147041\n",
      "epoch: 1223, train_loss: 0.0007732827950309476, test_loss: 0.000702821979454408\n",
      "epoch: 1224, train_loss: 0.000775030019450123, test_loss: 0.0007046038857273137\n",
      "epoch: 1225, train_loss: 0.0007738095414622322, test_loss: 0.000702335285798957\n",
      "epoch: 1226, train_loss: 0.0007726676134473604, test_loss: 0.0006996614344340438\n",
      "epoch: 1227, train_loss: 0.0007718172906290578, test_loss: 0.0007008615551361194\n",
      "epoch: 1228, train_loss: 0.000773341946668275, test_loss: 0.0006995196599746123\n",
      "epoch: 1229, train_loss: 0.0007729237170323082, test_loss: 0.0007029592670733109\n",
      "epoch: 1230, train_loss: 0.0007732648442944754, test_loss: 0.0007028880305976296\n",
      "epoch: 1231, train_loss: 0.0007719937420915812, test_loss: 0.0006987446783265719\n",
      "epoch: 1232, train_loss: 0.0007727954653091729, test_loss: 0.0006977391797893991\n",
      "epoch: 1233, train_loss: 0.0007713736520837183, test_loss: 0.0006960179598536342\n",
      "epoch: 1234, train_loss: 0.0007744170522884182, test_loss: 0.000701337747159414\n",
      "epoch: 1235, train_loss: 0.0007716685371554416, test_loss: 0.0006945849405989671\n",
      "epoch: 1236, train_loss: 0.0007719533421787555, test_loss: 0.0007018369554619616\n",
      "epoch: 1237, train_loss: 0.0007716286585807962, test_loss: 0.0007006344870508959\n",
      "epoch: 1238, train_loss: 0.0007711628694897113, test_loss: 0.0006941636432505524\n",
      "epoch: 1239, train_loss: 0.0007701063574210781, test_loss: 0.000702577429668357\n",
      "epoch: 1240, train_loss: 0.000771254151755863, test_loss: 0.0006987101417810967\n",
      "epoch: 1241, train_loss: 0.0007702745142919214, test_loss: 0.0006944460183149204\n",
      "epoch: 1242, train_loss: 0.0007705012002311971, test_loss: 0.0006970113414960603\n",
      "epoch: 1243, train_loss: 0.0007707774208899101, test_loss: 0.0006966434011701494\n",
      "epoch: 1244, train_loss: 0.0007688928162679076, test_loss: 0.0006902693615605434\n",
      "epoch: 1245, train_loss: 0.0007702168532768669, test_loss: 0.000699115606645743\n",
      "epoch: 1246, train_loss: 0.000768728663071828, test_loss: 0.0006972174305701628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1247, train_loss: 0.0007679850095883012, test_loss: 0.0006911082891747355\n",
      "epoch: 1248, train_loss: 0.0007689245166661947, test_loss: 0.0006983898492762819\n",
      "epoch: 1249, train_loss: 0.0007690329980307623, test_loss: 0.0007044182323928302\n",
      "epoch: 1250, train_loss: 0.000769019708968699, test_loss: 0.0007011956622591242\n",
      "epoch: 1251, train_loss: 0.0007699619771143341, test_loss: 0.0006963227885231996\n",
      "epoch: 1252, train_loss: 0.0007691136257641989, test_loss: 0.0006957789349447315\n",
      "epoch: 1253, train_loss: 0.0007685102514780896, test_loss: 0.000689657720310303\n",
      "epoch: 1254, train_loss: 0.000768694925405409, test_loss: 0.0006905553260973344\n",
      "epoch: 1255, train_loss: 0.0007681796458833244, test_loss: 0.0006942908075870946\n",
      "epoch: 1256, train_loss: 0.0007673198377470608, test_loss: 0.0006951421916407222\n",
      "epoch: 1257, train_loss: 0.0007669481793013604, test_loss: 0.0006936621551479524\n",
      "epoch: 1258, train_loss: 0.0007663776632398367, test_loss: 0.000690710973382617\n",
      "epoch: 1259, train_loss: 0.0007668429676888753, test_loss: 0.0006957066943868995\n",
      "epoch: 1260, train_loss: 0.0007666558072821277, test_loss: 0.0006897098016149054\n",
      "epoch: 1261, train_loss: 0.0007674257351976374, test_loss: 0.0007029401022009552\n",
      "epoch: 1262, train_loss: 0.0007666005125350279, test_loss: 0.0006904365194107717\n",
      "epoch: 1263, train_loss: 0.0007663376695152534, test_loss: 0.0006991689103112245\n",
      "epoch: 1264, train_loss: 0.0007667942377536193, test_loss: 0.0006863875023555011\n",
      "epoch: 1265, train_loss: 0.0007672889547867943, test_loss: 0.0006902528742405897\n",
      "epoch: 1266, train_loss: 0.0007653569792518797, test_loss: 0.0006909163348609582\n",
      "epoch: 1267, train_loss: 0.0007655318553114067, test_loss: 0.0006883804987107093\n",
      "epoch: 1268, train_loss: 0.0007657539580300775, test_loss: 0.0006892507129426425\n",
      "epoch: 1269, train_loss: 0.0007646171927816518, test_loss: 0.0006896367558510974\n",
      "epoch: 1270, train_loss: 0.0007638736050980895, test_loss: 0.000690352069796063\n",
      "epoch: 1271, train_loss: 0.0007636203448308389, test_loss: 0.0006971366528887302\n",
      "epoch: 1272, train_loss: 0.0007646675069775919, test_loss: 0.0006910916951407368\n",
      "epoch: 1273, train_loss: 0.0007645388149008479, test_loss: 0.0006880680690907562\n",
      "epoch: 1274, train_loss: 0.0007656934701473168, test_loss: 0.0006866335267356286\n",
      "epoch: 1275, train_loss: 0.0007630765397587548, test_loss: 0.000686827945173718\n",
      "epoch: 1276, train_loss: 0.0007658247916918734, test_loss: 0.0006991743187730511\n",
      "epoch: 1277, train_loss: 0.0007621260491483238, test_loss: 0.0006928208895260468\n",
      "epoch: 1278, train_loss: 0.0007632333284203449, test_loss: 0.0006920971403208872\n",
      "epoch: 1279, train_loss: 0.0007620516825345871, test_loss: 0.0006854450766695663\n",
      "epoch: 1280, train_loss: 0.000762784339831737, test_loss: 0.0006848568543015668\n",
      "epoch: 1281, train_loss: 0.0007624089168688363, test_loss: 0.0006932708104917159\n",
      "epoch: 1282, train_loss: 0.0007629370185263131, test_loss: 0.0006946613672577465\n",
      "epoch: 1283, train_loss: 0.0007613210979363192, test_loss: 0.0006896364648127928\n",
      "epoch: 1284, train_loss: 0.0007614692187179689, test_loss: 0.0006916878531531742\n",
      "epoch: 1285, train_loss: 0.0007618907029214113, test_loss: 0.0006995350656022007\n",
      "epoch: 1286, train_loss: 0.0007618059335863622, test_loss: 0.0006862035806989297\n",
      "epoch: 1287, train_loss: 0.0007603285421678067, test_loss: 0.0006871615090252211\n",
      "epoch: 1288, train_loss: 0.0007621634792050589, test_loss: 0.0006929199056078991\n",
      "epoch: 1289, train_loss: 0.0007628668808256803, test_loss: 0.0006840729959852373\n",
      "epoch: 1290, train_loss: 0.0007601292543214462, test_loss: 0.0006965126037054384\n",
      "epoch: 1291, train_loss: 0.0007598458677935212, test_loss: 0.0006929625475701565\n",
      "epoch: 1292, train_loss: 0.0007610943917509006, test_loss: 0.000690919851573805\n",
      "epoch: 1293, train_loss: 0.0007606208812870572, test_loss: 0.0006909166113473475\n",
      "epoch: 1294, train_loss: 0.0007604787925906155, test_loss: 0.0006900680746184662\n",
      "epoch: 1295, train_loss: 0.0007608653420744383, test_loss: 0.0006865125130085895\n",
      "epoch: 1296, train_loss: 0.0007593811049288058, test_loss: 0.0006880486810890337\n",
      "epoch: 1297, train_loss: 0.0007592644884109335, test_loss: 0.0006811989733250812\n",
      "epoch: 1298, train_loss: 0.000758919518177762, test_loss: 0.0006891291074377174\n",
      "epoch: 1299, train_loss: 0.0007598578332639906, test_loss: 0.0006865849766957884\n",
      "epoch: 1300, train_loss: 0.0007588042214553317, test_loss: 0.000687872762985838\n",
      "epoch: 1301, train_loss: 0.0007599265954922885, test_loss: 0.0006833696000588437\n",
      "epoch: 1302, train_loss: 0.0007586584226531988, test_loss: 0.0006890224418990935\n",
      "epoch: 1303, train_loss: 0.0007580231986534984, test_loss: 0.0006802180190182602\n",
      "epoch: 1304, train_loss: 0.0007574763604561272, test_loss: 0.000680229117278941\n",
      "epoch: 1305, train_loss: 0.0007591829875625832, test_loss: 0.0006887624088752394\n",
      "epoch: 1306, train_loss: 0.0007590042381628376, test_loss: 0.0006878635516234984\n",
      "epoch: 1307, train_loss: 0.0007583441885957575, test_loss: 0.0006926877395017073\n",
      "epoch: 1308, train_loss: 0.0007582323615560713, test_loss: 0.0006813757499912754\n",
      "epoch: 1309, train_loss: 0.0007567128151371751, test_loss: 0.000681255764599579\n",
      "epoch: 1310, train_loss: 0.0007576038377642956, test_loss: 0.0006838052746995041\n",
      "epoch: 1311, train_loss: 0.0007569866695279336, test_loss: 0.0006796755090666314\n",
      "epoch: 1312, train_loss: 0.000756561721968667, test_loss: 0.0006845215102657676\n",
      "epoch: 1313, train_loss: 0.0007570586167275906, test_loss: 0.0006875391603292277\n",
      "epoch: 1314, train_loss: 0.000756734617702339, test_loss: 0.0006868502435584863\n",
      "epoch: 1315, train_loss: 0.0007555793626638858, test_loss: 0.000677731698184895\n",
      "epoch: 1316, train_loss: 0.0007559434465213639, test_loss: 0.0006791022460674867\n",
      "epoch: 1317, train_loss: 0.0007551736274288725, test_loss: 0.0006871001096442342\n",
      "epoch: 1318, train_loss: 0.0007551952376556785, test_loss: 0.000680828588277412\n",
      "epoch: 1319, train_loss: 0.000755484746111071, test_loss: 0.0006852546503068879\n",
      "epoch: 1320, train_loss: 0.0007547862048301359, test_loss: 0.000675873045111075\n",
      "epoch: 1321, train_loss: 0.0007553123248963739, test_loss: 0.0006926300411578268\n",
      "epoch: 1322, train_loss: 0.0007564233509702203, test_loss: 0.0006808767308636258\n",
      "epoch: 1323, train_loss: 0.0007551614810085005, test_loss: 0.0006829459550014386\n",
      "epoch: 1324, train_loss: 0.0007549305901984158, test_loss: 0.000679024631002297\n",
      "epoch: 1325, train_loss: 0.00075475210779468, test_loss: 0.0006829885145028433\n",
      "epoch: 1326, train_loss: 0.0007556861167793851, test_loss: 0.0006814430671511218\n",
      "epoch: 1327, train_loss: 0.0007541341474279761, test_loss: 0.0006810370541643351\n",
      "epoch: 1328, train_loss: 0.0007537924798468695, test_loss: 0.0006824091396993026\n",
      "epoch: 1329, train_loss: 0.0007536287682697824, test_loss: 0.0006791149401882043\n",
      "epoch: 1330, train_loss: 0.0007540221343768518, test_loss: 0.000675456176395528\n",
      "epoch: 1331, train_loss: 0.0007535187836291026, test_loss: 0.0006766966204547012\n",
      "epoch: 1332, train_loss: 0.0007529259968103598, test_loss: 0.0006867849006084725\n",
      "epoch: 1333, train_loss: 0.0007542003637038009, test_loss: 0.0006809131058010583\n",
      "epoch: 1334, train_loss: 0.0007517495668371735, test_loss: 0.0006758479236547524\n",
      "epoch: 1335, train_loss: 0.0007537149662232918, test_loss: 0.0006767443167821815\n",
      "epoch: 1336, train_loss: 0.0007529340737560035, test_loss: 0.000685667788881498\n",
      "epoch: 1337, train_loss: 0.0007523984569833492, test_loss: 0.0006909446771411846\n",
      "epoch: 1338, train_loss: 0.0007521580201168747, test_loss: 0.0006953868044850727\n",
      "epoch: 1339, train_loss: 0.0007515399585194562, test_loss: 0.0006838903597478444\n",
      "epoch: 1340, train_loss: 0.0007522894049306278, test_loss: 0.000683469979170089\n",
      "epoch: 1341, train_loss: 0.0007507922659304155, test_loss: 0.0006762382496769229\n",
      "epoch: 1342, train_loss: 0.0007515758346847218, test_loss: 0.0006872036659236377\n",
      "epoch: 1343, train_loss: 0.0007515824146811729, test_loss: 0.0006899954460095614\n",
      "epoch: 1344, train_loss: 0.0007516639610833448, test_loss: 0.0006811583055726563\n",
      "epoch: 1345, train_loss: 0.0007513210066067784, test_loss: 0.0006776999701590588\n",
      "epoch: 1346, train_loss: 0.000751320352403285, test_loss: 0.0006780834955861792\n",
      "epoch: 1347, train_loss: 0.0007496259969902103, test_loss: 0.0006848023622296751\n",
      "epoch: 1348, train_loss: 0.0007508663845289012, test_loss: 0.0006780294012666369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1349, train_loss: 0.0007495902182595075, test_loss: 0.0006797244035017987\n",
      "epoch: 1350, train_loss: 0.000750068484036171, test_loss: 0.0006844304443802685\n",
      "epoch: 1351, train_loss: 0.0007510045113082489, test_loss: 0.0006879877570706109\n",
      "epoch: 1352, train_loss: 0.0007498401505670146, test_loss: 0.0006747587855594853\n",
      "epoch: 1353, train_loss: 0.0007493187529136143, test_loss: 0.0006735160714015365\n",
      "epoch: 1354, train_loss: 0.0007498178367867418, test_loss: 0.0006767433612064148\n",
      "epoch: 1355, train_loss: 0.0007491674927144271, test_loss: 0.0006737219955539331\n",
      "epoch: 1356, train_loss: 0.0007494543970096856, test_loss: 0.0006715127237839624\n",
      "epoch: 1357, train_loss: 0.0007513615773464351, test_loss: 0.0006759507523383945\n",
      "epoch: 1358, train_loss: 0.0007494817862449133, test_loss: 0.0006750231890085464\n",
      "epoch: 1359, train_loss: 0.0007485198844021753, test_loss: 0.0006784706347389147\n",
      "epoch: 1360, train_loss: 0.0007475485617760569, test_loss: 0.0006740707952606803\n",
      "epoch: 1361, train_loss: 0.0007499164430950971, test_loss: 0.0006748729792889208\n",
      "epoch: 1362, train_loss: 0.0007491789596236271, test_loss: 0.0006741595231384659\n",
      "epoch: 1363, train_loss: 0.0007475181432116939, test_loss: 0.0006763547280570492\n",
      "epoch: 1364, train_loss: 0.000748278935055208, test_loss: 0.0006719373729235182\n",
      "epoch: 1365, train_loss: 0.0007482735470504216, test_loss: 0.0006941561247610176\n",
      "epoch: 1366, train_loss: 0.0007480367102012362, test_loss: 0.0006718264339724556\n",
      "epoch: 1367, train_loss: 0.0007469560875071455, test_loss: 0.000674418299846972\n",
      "epoch: 1368, train_loss: 0.0007470848377915504, test_loss: 0.0006753482108858103\n",
      "epoch: 1369, train_loss: 0.0007466931748912548, test_loss: 0.0006812452338635921\n",
      "epoch: 1370, train_loss: 0.0007457948775719042, test_loss: 0.0006706033163936809\n",
      "epoch: 1371, train_loss: 0.0007476369551706897, test_loss: 0.0006714576690380151\n",
      "epoch: 1372, train_loss: 0.0007462427589495707, test_loss: 0.0006817117537138984\n",
      "epoch: 1373, train_loss: 0.0007459847231233573, test_loss: 0.0006709745454524333\n",
      "epoch: 1374, train_loss: 0.0007463530789169928, test_loss: 0.0006787090872724851\n",
      "epoch: 1375, train_loss: 0.0007457018591989965, test_loss: 0.0006794316626231497\n",
      "epoch: 1376, train_loss: 0.0007458792559033179, test_loss: 0.0006759156143137565\n",
      "epoch: 1377, train_loss: 0.0007461865392068158, test_loss: 0.0006720910168951377\n",
      "epoch: 1378, train_loss: 0.0007449123279555985, test_loss: 0.0006710910335338364\n",
      "epoch: 1379, train_loss: 0.0007455622645742867, test_loss: 0.0006728198738225425\n",
      "epoch: 1380, train_loss: 0.0007445278916629436, test_loss: 0.0006712833564961329\n",
      "epoch: 1381, train_loss: 0.0007448838821247867, test_loss: 0.0006726793168733517\n",
      "epoch: 1382, train_loss: 0.0007451098341413814, test_loss: 0.0006760826023916403\n",
      "epoch: 1383, train_loss: 0.0007447388412876297, test_loss: 0.0006691968737868592\n",
      "epoch: 1384, train_loss: 0.0007440008516625866, test_loss: 0.0006773764180252329\n",
      "epoch: 1385, train_loss: 0.0007443940570658964, test_loss: 0.0006759393872926012\n",
      "epoch: 1386, train_loss: 0.0007437376593968467, test_loss: 0.0006759202272708839\n",
      "epoch: 1387, train_loss: 0.0007434091075202045, test_loss: 0.0006773969992840042\n",
      "epoch: 1388, train_loss: 0.0007447914078669703, test_loss: 0.0006710613427761322\n",
      "epoch: 1389, train_loss: 0.0007430627858569926, test_loss: 0.0006724390890061235\n",
      "epoch: 1390, train_loss: 0.0007429727348094077, test_loss: 0.0006867076056854179\n",
      "epoch: 1391, train_loss: 0.0007455535536712927, test_loss: 0.0006729204227061322\n",
      "epoch: 1392, train_loss: 0.0007421556004808973, test_loss: 0.0006722511219171187\n",
      "epoch: 1393, train_loss: 0.0007431731792166829, test_loss: 0.0006844549352535978\n",
      "epoch: 1394, train_loss: 0.000742793930998153, test_loss: 0.0006724630608611429\n",
      "epoch: 1395, train_loss: 0.0007424813482667441, test_loss: 0.0006741051523325344\n",
      "epoch: 1396, train_loss: 0.00074321770428113, test_loss: 0.00066952763882\n",
      "epoch: 1397, train_loss: 0.0007419851861651177, test_loss: 0.0006713533269551893\n",
      "epoch: 1398, train_loss: 0.0007432896147846528, test_loss: 0.0006761534605175257\n",
      "epoch: 1399, train_loss: 0.0007428744106816695, test_loss: 0.000676848110742867\n",
      "epoch: 1400, train_loss: 0.0007422249384588846, test_loss: 0.000677208648994565\n",
      "epoch: 1401, train_loss: 0.0007419029918863722, test_loss: 0.0006682181895788138\n",
      "epoch: 1402, train_loss: 0.0007429649729443634, test_loss: 0.0006656375868866841\n",
      "epoch: 1403, train_loss: 0.0007414180663940699, test_loss: 0.0006648775597568601\n",
      "epoch: 1404, train_loss: 0.0007407725484960753, test_loss: 0.0006700619123876095\n",
      "epoch: 1405, train_loss: 0.0007399448431501894, test_loss: 0.0006741960532963276\n",
      "epoch: 1406, train_loss: 0.0007412921796735052, test_loss: 0.0006616201377861822\n",
      "epoch: 1407, train_loss: 0.0007399282311899183, test_loss: 0.0006833598939313864\n",
      "epoch: 1408, train_loss: 0.0007412519645067337, test_loss: 0.0006679469224764034\n",
      "epoch: 1409, train_loss: 0.0007396097087463283, test_loss: 0.000666025453635181\n",
      "epoch: 1410, train_loss: 0.0007401534201800014, test_loss: 0.0006711572738519559\n",
      "epoch: 1411, train_loss: 0.0007401727261426656, test_loss: 0.000671409594360739\n",
      "epoch: 1412, train_loss: 0.0007400718699816776, test_loss: 0.0006677508305680627\n",
      "epoch: 1413, train_loss: 0.0007388985629760376, test_loss: 0.000670411226262028\n",
      "epoch: 1414, train_loss: 0.0007406412965476351, test_loss: 0.0006702165652920181\n",
      "epoch: 1415, train_loss: 0.0007395849401212257, test_loss: 0.0006709935017473375\n",
      "epoch: 1416, train_loss: 0.0007393028986964213, test_loss: 0.0006657587218796834\n",
      "epoch: 1417, train_loss: 0.0007388894901732388, test_loss: 0.0006664274454427263\n",
      "epoch: 1418, train_loss: 0.0007382001798680943, test_loss: 0.0006641292954251791\n",
      "epoch: 1419, train_loss: 0.0007409567010852144, test_loss: 0.0006672838499071077\n",
      "epoch: 1420, train_loss: 0.0007403165496804792, test_loss: 0.0006656300538452342\n",
      "epoch: 1421, train_loss: 0.0007394483849487227, test_loss: 0.0006721499667037278\n",
      "epoch: 1422, train_loss: 0.0007380286545452217, test_loss: 0.0006688952416880056\n",
      "epoch: 1423, train_loss: 0.0007383740005974213, test_loss: 0.0006697790037530164\n",
      "epoch: 1424, train_loss: 0.0007402047466830877, test_loss: 0.0006689669923313583\n",
      "epoch: 1425, train_loss: 0.0007383213935257947, test_loss: 0.0006699225632473826\n",
      "epoch: 1426, train_loss: 0.0007391531696117928, test_loss: 0.0006707502761855721\n",
      "epoch: 1427, train_loss: 0.0007374608502014662, test_loss: 0.0006785202422179282\n",
      "epoch: 1428, train_loss: 0.0007373030391096582, test_loss: 0.0006633513742902627\n",
      "epoch: 1429, train_loss: 0.0007371222904032987, test_loss: 0.0006698486346673841\n",
      "epoch: 1430, train_loss: 0.0007377007606682246, test_loss: 0.0006739672923382992\n",
      "epoch: 1431, train_loss: 0.0007370562689965996, test_loss: 0.000672474066959694\n",
      "epoch: 1432, train_loss: 0.0007373043892743146, test_loss: 0.0006646403441360841\n",
      "epoch: 1433, train_loss: 0.000736378333227628, test_loss: 0.0006612482344886909\n",
      "epoch: 1434, train_loss: 0.0007357877583, test_loss: 0.0006625303746356318\n",
      "epoch: 1435, train_loss: 0.0007357584469465781, test_loss: 0.0006695689177528644\n",
      "epoch: 1436, train_loss: 0.0007359628887016974, test_loss: 0.0006692258612019941\n",
      "epoch: 1437, train_loss: 0.0007353941606276709, test_loss: 0.0006698374636471272\n",
      "epoch: 1438, train_loss: 0.000736991614204548, test_loss: 0.0006596967917478954\n",
      "epoch: 1439, train_loss: 0.000738908797401287, test_loss: 0.0006665097995816419\n",
      "epoch: 1440, train_loss: 0.0007356005320932878, test_loss: 0.0006641845053915555\n",
      "epoch: 1441, train_loss: 0.0007351972491723363, test_loss: 0.0006679915240965784\n",
      "epoch: 1442, train_loss: 0.000736597335755663, test_loss: 0.0006796028804577267\n",
      "epoch: 1443, train_loss: 0.0007360683723717281, test_loss: 0.0006759506747281799\n",
      "epoch: 1444, train_loss: 0.0007360107948720131, test_loss: 0.0006590931394991154\n",
      "epoch: 1445, train_loss: 0.0007354363966125833, test_loss: 0.0006718033497842649\n",
      "epoch: 1446, train_loss: 0.0007346195723031364, test_loss: 0.0006650880823144689\n",
      "epoch: 1447, train_loss: 0.0007349507675668144, test_loss: 0.0006630196003243327\n",
      "epoch: 1448, train_loss: 0.0007348390683309053, test_loss: 0.0006729649079109853\n",
      "epoch: 1449, train_loss: 0.0007351004776706839, test_loss: 0.0006707474919191251\n",
      "epoch: 1450, train_loss: 0.0007345101267160119, test_loss: 0.000663401098184598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1451, train_loss: 0.0007367814314025252, test_loss: 0.000657123537772956\n",
      "epoch: 1452, train_loss: 0.0007330042628935822, test_loss: 0.0006613396738733476\n",
      "epoch: 1453, train_loss: 0.0007345669513122867, test_loss: 0.0006674452257963518\n",
      "epoch: 1454, train_loss: 0.0007330877402716357, test_loss: 0.0006740519747836515\n",
      "epoch: 1455, train_loss: 0.0007346272784912878, test_loss: 0.0006593580862196783\n",
      "epoch: 1456, train_loss: 0.0007333839856310869, test_loss: 0.0006580703581372896\n",
      "epoch: 1457, train_loss: 0.0007330159651641936, test_loss: 0.0006738184796025356\n",
      "epoch: 1458, train_loss: 0.0007346454278930374, test_loss: 0.0006690450585059201\n",
      "epoch: 1459, train_loss: 0.0007330951326445717, test_loss: 0.0006698967238965755\n",
      "epoch: 1460, train_loss: 0.0007335060331767992, test_loss: 0.0006646478626256188\n",
      "epoch: 1461, train_loss: 0.0007346820481040555, test_loss: 0.0006578946825660145\n",
      "epoch: 1462, train_loss: 0.0007319301392113709, test_loss: 0.0006666292708056668\n",
      "epoch: 1463, train_loss: 0.0007315489195246736, test_loss: 0.0006593770473652208\n",
      "epoch: 1464, train_loss: 0.000733277471938535, test_loss: 0.0006629316631006077\n",
      "epoch: 1465, train_loss: 0.0007318986868283347, test_loss: 0.0006602197827305645\n",
      "epoch: 1466, train_loss: 0.0007315690758253407, test_loss: 0.0006583436479559168\n",
      "epoch: 1467, train_loss: 0.0007323832466246803, test_loss: 0.0006638358851584295\n",
      "epoch: 1468, train_loss: 0.0007325643395154697, test_loss: 0.0006586426170542836\n",
      "epoch: 1469, train_loss: 0.0007319616409443805, test_loss: 0.0006644067179877311\n",
      "epoch: 1470, train_loss: 0.0007322076353770883, test_loss: 0.0006674279769261678\n",
      "epoch: 1471, train_loss: 0.0007317592200074021, test_loss: 0.000660167284271059\n",
      "epoch: 1472, train_loss: 0.0007312655836121057, test_loss: 0.0006589083738314608\n",
      "epoch: 1473, train_loss: 0.0007315265386790523, test_loss: 0.0006659035958970586\n",
      "epoch: 1474, train_loss: 0.0007307343210255647, test_loss: 0.0006651620788034052\n",
      "epoch: 1475, train_loss: 0.0007313613466027638, test_loss: 0.0006640619297589486\n",
      "epoch: 1476, train_loss: 0.0007310859041551695, test_loss: 0.0006673682122103249\n",
      "epoch: 1477, train_loss: 0.0007300456130431722, test_loss: 0.0006565026463552689\n",
      "epoch: 1478, train_loss: 0.0007312488299288342, test_loss: 0.0006605821060171971\n",
      "epoch: 1479, train_loss: 0.0007309032447190713, test_loss: 0.0006596842916527142\n",
      "epoch: 1480, train_loss: 0.0007296466855737178, test_loss: 0.0006636551746244853\n",
      "epoch: 1481, train_loss: 0.0007302587036974728, test_loss: 0.0006659033873196071\n",
      "epoch: 1482, train_loss: 0.000729193340268229, test_loss: 0.0006649797918119779\n",
      "epoch: 1483, train_loss: 0.0007293732677404162, test_loss: 0.0006620412023039535\n",
      "epoch: 1484, train_loss: 0.0007285812398945184, test_loss: 0.0006617367422829071\n",
      "epoch: 1485, train_loss: 0.0007298932911868652, test_loss: 0.000658855788060464\n",
      "epoch: 1486, train_loss: 0.0007301409951532664, test_loss: 0.0006585978650643179\n",
      "epoch: 1487, train_loss: 0.0007297819045007876, test_loss: 0.0006569643980280185\n",
      "epoch: 1488, train_loss: 0.0007285518791911232, test_loss: 0.0006558810030886283\n",
      "epoch: 1489, train_loss: 0.0007286383960213836, test_loss: 0.0006625062087550759\n",
      "epoch: 1490, train_loss: 0.0007302013109438121, test_loss: 0.0006554340119085585\n",
      "epoch: 1491, train_loss: 0.0007269707275554538, test_loss: 0.0006619698106078431\n",
      "epoch: 1492, train_loss: 0.0007295047942771698, test_loss: 0.0006553254740235085\n",
      "epoch: 1493, train_loss: 0.0007297445136709543, test_loss: 0.0006573826831299812\n",
      "epoch: 1494, train_loss: 0.000728055714558729, test_loss: 0.0006576739106094465\n",
      "epoch: 1495, train_loss: 0.0007284387007213967, test_loss: 0.000654322526922139\n",
      "epoch: 1496, train_loss: 0.0007287527449712481, test_loss: 0.0006586700910702348\n",
      "epoch: 1497, train_loss: 0.0007283264257357982, test_loss: 0.000653297004949612\n",
      "epoch: 1498, train_loss: 0.0007287905571739311, test_loss: 0.0006694747717119753\n",
      "epoch: 1499, train_loss: 0.0007277862839501998, test_loss: 0.0006664042448392138\n",
      "epoch: 1500, train_loss: 0.0007278401589365271, test_loss: 0.0006571342722357562\n",
      "epoch: 1501, train_loss: 0.0007276895959638869, test_loss: 0.0006574939373725405\n",
      "epoch: 1502, train_loss: 0.0007290411740541458, test_loss: 0.0006555368114883701\n",
      "epoch: 1503, train_loss: 0.0007276765840209049, test_loss: 0.0006587774308475977\n",
      "epoch: 1504, train_loss: 0.0007275093445534129, test_loss: 0.0006577823854361972\n",
      "epoch: 1505, train_loss: 0.0007274916848552454, test_loss: 0.0006514195605025938\n",
      "epoch: 1506, train_loss: 0.0007266913611522835, test_loss: 0.0006556729592072467\n",
      "epoch: 1507, train_loss: 0.0007275312407569879, test_loss: 0.0006556739875425895\n",
      "epoch: 1508, train_loss: 0.0007263773308216554, test_loss: 0.0006603267781126002\n",
      "epoch: 1509, train_loss: 0.0007264882505810617, test_loss: 0.000667972364074861\n",
      "epoch: 1510, train_loss: 0.000727044289385009, test_loss: 0.000650984191452153\n",
      "epoch: 1511, train_loss: 0.0007254578079522142, test_loss: 0.0006532322392255688\n",
      "epoch: 1512, train_loss: 0.0007268646289088318, test_loss: 0.0006603096359564612\n",
      "epoch: 1513, train_loss: 0.0007258509462901756, test_loss: 0.000652939381931598\n",
      "epoch: 1514, train_loss: 0.0007254528058895274, test_loss: 0.0006612661672988907\n",
      "epoch: 1515, train_loss: 0.0007263179805190505, test_loss: 0.0006514708026467512\n",
      "epoch: 1516, train_loss: 0.0007262879997775283, test_loss: 0.0006549922448660558\n",
      "epoch: 1517, train_loss: 0.0007255302144862387, test_loss: 0.0006509350593357036\n",
      "epoch: 1518, train_loss: 0.000725271394121987, test_loss: 0.000655931857181713\n",
      "epoch: 1519, train_loss: 0.0007252387332972949, test_loss: 0.0006563716839688519\n",
      "epoch: 1520, train_loss: 0.0007250681342354611, test_loss: 0.000651475633882607\n",
      "epoch: 1521, train_loss: 0.0007256480849995885, test_loss: 0.0006573866558028385\n",
      "epoch: 1522, train_loss: 0.0007237928081809988, test_loss: 0.0006550416825727249\n",
      "epoch: 1523, train_loss: 0.0007243008269300765, test_loss: 0.0006547024240717292\n",
      "epoch: 1524, train_loss: 0.000725222399721489, test_loss: 0.0006510714544371391\n",
      "epoch: 1525, train_loss: 0.0007235098658028343, test_loss: 0.0006524223960392798\n",
      "epoch: 1526, train_loss: 0.0007236298899996378, test_loss: 0.0006513582969394823\n",
      "epoch: 1527, train_loss: 0.0007235774233856279, test_loss: 0.0006564141076523811\n",
      "epoch: 1528, train_loss: 0.0007235206607931658, test_loss: 0.0006616444491858905\n",
      "epoch: 1529, train_loss: 0.0007238545601823083, test_loss: 0.0006522413605125621\n",
      "epoch: 1530, train_loss: 0.0007249138193994598, test_loss: 0.0006592619853715102\n",
      "epoch: 1531, train_loss: 0.0007230003893314658, test_loss: 0.0006494484017215049\n",
      "epoch: 1532, train_loss: 0.0007224898942260314, test_loss: 0.0006524462466283391\n",
      "epoch: 1533, train_loss: 0.0007234731228495746, test_loss: 0.0006476891915857171\n",
      "epoch: 1534, train_loss: 0.0007225303283042234, test_loss: 0.0006549071064606929\n",
      "epoch: 1535, train_loss: 0.0007228896416642743, test_loss: 0.0006493246764875948\n",
      "epoch: 1536, train_loss: 0.0007228741900610697, test_loss: 0.0006524991234376406\n",
      "epoch: 1537, train_loss: 0.0007219909343605294, test_loss: 0.0006480262139424061\n",
      "epoch: 1538, train_loss: 0.000722483604002501, test_loss: 0.0006516804229856158\n",
      "epoch: 1539, train_loss: 0.0007227013272273799, test_loss: 0.0006568896011837447\n",
      "epoch: 1540, train_loss: 0.0007224433622626668, test_loss: 0.0006496309603486831\n",
      "epoch: 1541, train_loss: 0.0007222235385004593, test_loss: 0.0006476193035875136\n",
      "epoch: 1542, train_loss: 0.0007211081367289728, test_loss: 0.0006498459066885213\n",
      "epoch: 1543, train_loss: 0.000722082877157094, test_loss: 0.0006641026072126502\n",
      "epoch: 1544, train_loss: 0.0007220331551608346, test_loss: 0.0006465589152260994\n",
      "epoch: 1545, train_loss: 0.0007227550959214568, test_loss: 0.0006545454234583303\n",
      "epoch: 1546, train_loss: 0.0007205331968852197, test_loss: 0.0006529822033674767\n",
      "epoch: 1547, train_loss: 0.0007216452542469715, test_loss: 0.0006503087497549132\n",
      "epoch: 1548, train_loss: 0.0007209488388110439, test_loss: 0.0006524348815825457\n",
      "epoch: 1549, train_loss: 0.0007224927489321841, test_loss: 0.0006492725175727779\n",
      "epoch: 1550, train_loss: 0.0007209488438725796, test_loss: 0.000648627188638784\n",
      "epoch: 1551, train_loss: 0.0007210817749854987, test_loss: 0.0006468984065577388\n",
      "epoch: 1552, train_loss: 0.0007202968978242058, test_loss: 0.0006554049759870395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1553, train_loss: 0.000720916323505504, test_loss: 0.0006473373456780488\n",
      "epoch: 1554, train_loss: 0.0007192898432865901, test_loss: 0.0006483441053812081\n",
      "epoch: 1555, train_loss: 0.0007200161382333254, test_loss: 0.0006471253582276404\n",
      "epoch: 1556, train_loss: 0.0007204311297518079, test_loss: 0.0006580036376059676\n",
      "epoch: 1557, train_loss: 0.0007206564591995076, test_loss: 0.0006478885431230689\n",
      "epoch: 1558, train_loss: 0.0007193864338383403, test_loss: 0.0006549836883399015\n",
      "epoch: 1559, train_loss: 0.0007207081501331667, test_loss: 0.0006516615491515646\n",
      "epoch: 1560, train_loss: 0.0007204259973545761, test_loss: 0.0006506770053723206\n",
      "epoch: 1561, train_loss: 0.0007193686817671456, test_loss: 0.0006504226475954056\n",
      "epoch: 1562, train_loss: 0.0007188644047583575, test_loss: 0.0006428450821355606\n",
      "epoch: 1563, train_loss: 0.0007196417933775355, test_loss: 0.0006574398915593823\n",
      "epoch: 1564, train_loss: 0.000720109231263885, test_loss: 0.0006464741066641485\n",
      "epoch: 1565, train_loss: 0.0007189027813222746, test_loss: 0.0006451403848283613\n",
      "epoch: 1566, train_loss: 0.0007191098057260008, test_loss: 0.0006458938878495246\n",
      "epoch: 1567, train_loss: 0.0007198771370736802, test_loss: 0.0006504582803851614\n",
      "epoch: 1568, train_loss: 0.0007188619182789294, test_loss: 0.0006467058847192675\n",
      "epoch: 1569, train_loss: 0.0007183261041063815, test_loss: 0.0006501766280659164\n",
      "epoch: 1570, train_loss: 0.0007177113793263941, test_loss: 0.0006432045969025543\n",
      "epoch: 1571, train_loss: 0.0007190112335830117, test_loss: 0.0006477135126867021\n",
      "epoch: 1572, train_loss: 0.0007178630659599667, test_loss: 0.0006520054303109646\n",
      "epoch: 1573, train_loss: 0.0007188716301006143, test_loss: 0.0006423882829646269\n",
      "epoch: 1574, train_loss: 0.0007193534225022987, test_loss: 0.0006462861104713132\n",
      "epoch: 1575, train_loss: 0.0007177273244293326, test_loss: 0.0006507591654857\n",
      "epoch: 1576, train_loss: 0.000718231652053478, test_loss: 0.0006486197332075486\n",
      "epoch: 1577, train_loss: 0.0007185504085930955, test_loss: 0.0006509130907943472\n",
      "epoch: 1578, train_loss: 0.0007165657171635362, test_loss: 0.000648618375028794\n",
      "epoch: 1579, train_loss: 0.0007167842907264181, test_loss: 0.000642967046587728\n",
      "epoch: 1580, train_loss: 0.0007176147963669475, test_loss: 0.0006495680039127668\n",
      "epoch: 1581, train_loss: 0.0007177776550752637, test_loss: 0.0006450618675444275\n",
      "epoch: 1582, train_loss: 0.0007171152746709793, test_loss: 0.0006518355221487582\n",
      "epoch: 1583, train_loss: 0.0007184653735274206, test_loss: 0.0006463665389067804\n",
      "epoch: 1584, train_loss: 0.0007178054745410285, test_loss: 0.0006470151759761696\n",
      "epoch: 1585, train_loss: 0.0007182458990111785, test_loss: 0.0006456162082031369\n",
      "epoch: 1586, train_loss: 0.0007165499783181788, test_loss: 0.0006450917474770298\n",
      "epoch: 1587, train_loss: 0.0007168488746569694, test_loss: 0.0006446746459308391\n",
      "epoch: 1588, train_loss: 0.0007161893957780431, test_loss: 0.0006452397501561791\n",
      "epoch: 1589, train_loss: 0.0007175563014638813, test_loss: 0.0006590153789147735\n",
      "epoch: 1590, train_loss: 0.0007168110358812239, test_loss: 0.0006494738481706008\n",
      "epoch: 1591, train_loss: 0.0007161124946002889, test_loss: 0.0006469146076900264\n",
      "epoch: 1592, train_loss: 0.0007176889238231208, test_loss: 0.0006450686293343703\n",
      "epoch: 1593, train_loss: 0.0007151603653176647, test_loss: 0.0006577496533282101\n",
      "epoch: 1594, train_loss: 0.0007160800375024099, test_loss: 0.0006428314324390764\n",
      "epoch: 1595, train_loss: 0.0007181913040213934, test_loss: 0.0006430155384199073\n",
      "epoch: 1596, train_loss: 0.0007166871218942106, test_loss: 0.0006453366659116\n",
      "epoch: 1597, train_loss: 0.0007162038869548427, test_loss: 0.00064107868335365\n",
      "epoch: 1598, train_loss: 0.0007154289531804945, test_loss: 0.0006524913381629934\n",
      "epoch: 1599, train_loss: 0.000715806267892375, test_loss: 0.0006484806799562648\n"
     ]
    }
   ],
   "source": [
    "seq_dim = 10 # = window_size\n",
    "\n",
    "num_epochs = 1600 # 400 # 200 will overfit, 100 is good, see the plot: plt.plot(train_loss[20:]) and plt.plot(test_loss[20:])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    train_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(train_loader):\n",
    "        # print(\"train: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(seqs.float())\n",
    "        outputs = model(seqs)\n",
    "        # print(outputs.is_cuda)\n",
    "        # print(labels.is_cuda)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs.dtype)\n",
    "        \n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_train_loss += loss.data.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch = i + 1;\n",
    "        \n",
    "        # print(\"loss: \", loss.data)\n",
    "    \n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    # test_seq = []\n",
    "    test_pred = []\n",
    "    # test_gt = []\n",
    "    for i, (seqs, labels) in enumerate(test_loader):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        test_pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        test_batch = i + 1\n",
    "    \n",
    "    # print(\"train batch: \", train_batch)\n",
    "    # print(\"test batch: \", test_batch)\n",
    "    train_loss.append(total_train_loss/train_batch)\n",
    "    test_loss.append(total_test_loss/test_batch)\n",
    "    # train_loss.append(total_train_loss)\n",
    "    # test_loss.append(total_test_loss)\n",
    "    print(\"epoch: {}, train_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_test_loss/test_batch))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "124/(199/39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 1600\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "test_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, test_loss_curve], ['Train Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x mean: 0.5067009838472856 ; std: 0.2332880326514156\n",
      "y mean: 0.49486543289715496 ; std: 0.23134116200489124\n",
      "z mean: 0.4910597234688982 ; std: 0.23926200186407484\n"
     ]
    }
   ],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006484806799562648\n"
     ]
    }
   ],
   "source": [
    "# get test results\n",
    "seq_dim = 10 # = window_size\n",
    "input_dim = 3\n",
    "# test_seq = []\n",
    "test_predd = []\n",
    "# test_gt = []\n",
    "total_test_loss = 0.0\n",
    "test_batch = 0\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        \n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_test_loss += loss.data.item()\n",
    "    test_predd.append(outputs)\n",
    "    test_batch = i + 1\n",
    "print(total_test_loss/test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.8338e-02,  2.5346e-04,  1.0032e+00],\n",
      "         [ 1.3742e-01,  2.5346e-04,  1.0000e+00],\n",
      "         [ 1.7205e-01,  2.5346e-04,  9.8299e-01],\n",
      "         ...,\n",
      "         [ 3.3839e-01,  2.5346e-04,  8.8670e-01],\n",
      "         [ 3.7166e-01,  2.5346e-04,  8.6744e-01],\n",
      "         [ 4.0493e-01,  2.5346e-04,  8.4819e-01]],\n",
      "\n",
      "        [[ 1.3742e-01,  2.5346e-04,  1.0000e+00],\n",
      "         [ 1.7205e-01,  2.5346e-04,  9.8299e-01],\n",
      "         [ 2.0532e-01,  2.5346e-04,  9.6373e-01],\n",
      "         ...,\n",
      "         [ 3.7166e-01,  2.5346e-04,  8.6744e-01],\n",
      "         [ 4.0493e-01,  2.5346e-04,  8.4819e-01],\n",
      "         [ 4.3820e-01,  2.5346e-04,  8.2893e-01]],\n",
      "\n",
      "        [[ 1.7205e-01,  2.5346e-04,  9.8299e-01],\n",
      "         [ 2.0532e-01,  2.5346e-04,  9.6373e-01],\n",
      "         [ 2.3859e-01,  2.5346e-04,  9.4447e-01],\n",
      "         ...,\n",
      "         [ 4.0493e-01,  2.5346e-04,  8.4819e-01],\n",
      "         [ 4.3820e-01,  2.5346e-04,  8.2893e-01],\n",
      "         [ 4.7147e-01,  2.5346e-04,  8.0967e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.6829e-01,  5.7823e-01, -1.5301e+00],\n",
      "         [-3.3062e-01,  5.7823e-01, -1.4990e+00],\n",
      "         [-3.9295e-01,  5.7823e-01, -1.4678e+00],\n",
      "         ...,\n",
      "         [-7.0459e-01,  5.7823e-01, -1.3121e+00],\n",
      "         [-7.6692e-01,  5.7823e-01, -1.2810e+00],\n",
      "         [-8.2925e-01,  5.7823e-01, -1.2498e+00]],\n",
      "\n",
      "        [[-3.3062e-01,  5.7823e-01, -1.4990e+00],\n",
      "         [-3.9295e-01,  5.7823e-01, -1.4678e+00],\n",
      "         [-4.5528e-01,  5.7823e-01, -1.4367e+00],\n",
      "         ...,\n",
      "         [-7.6692e-01,  5.7823e-01, -1.2810e+00],\n",
      "         [-8.2925e-01,  5.7823e-01, -1.2498e+00],\n",
      "         [-8.9158e-01,  5.7823e-01, -1.2187e+00]],\n",
      "\n",
      "        [[-3.9295e-01,  5.7823e-01, -1.4678e+00],\n",
      "         [-4.5528e-01,  5.7823e-01, -1.4367e+00],\n",
      "         [-5.1761e-01,  5.7823e-01, -1.4055e+00],\n",
      "         ...,\n",
      "         [-8.2925e-01,  5.7823e-01, -1.2498e+00],\n",
      "         [-8.9158e-01,  5.7823e-01, -1.2187e+00],\n",
      "         [-9.5391e-01,  5.7823e-01, -1.1875e+00]]])\n",
      "tensor([[[-0.4553,  0.5782, -1.4367],\n",
      "         [-0.5176,  0.5782, -1.4055],\n",
      "         [-0.5799,  0.5782, -1.3744],\n",
      "         ...,\n",
      "         [-0.8916,  0.5782, -1.2187],\n",
      "         [-0.9539,  0.5782, -1.1875],\n",
      "         [-1.0162,  0.5782, -1.1564]],\n",
      "\n",
      "        [[-0.5176,  0.5782, -1.4055],\n",
      "         [-0.5799,  0.5782, -1.3744],\n",
      "         [-0.6423,  0.5782, -1.3433],\n",
      "         ...,\n",
      "         [-0.9539,  0.5782, -1.1875],\n",
      "         [-1.0162,  0.5782, -1.1564],\n",
      "         [-1.0786,  0.5782, -1.1253]],\n",
      "\n",
      "        [[-0.5799,  0.5782, -1.3744],\n",
      "         [-0.6423,  0.5782, -1.3433],\n",
      "         [-0.7046,  0.5782, -1.3121],\n",
      "         ...,\n",
      "         [-1.0162,  0.5782, -1.1564],\n",
      "         [-1.0786,  0.5782, -1.1253],\n",
      "         [-1.1409,  0.5782, -1.0941]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4572, -1.3433, -0.1563],\n",
      "         [ 1.4786, -1.1883, -0.3047],\n",
      "         [ 1.4485, -1.1377, -0.4517],\n",
      "         ...,\n",
      "         [ 1.3233, -1.1599, -1.1567],\n",
      "         [ 1.2863, -1.1599, -1.2685],\n",
      "         [ 1.1534, -1.1182, -1.2869]],\n",
      "\n",
      "        [[ 1.4786, -1.1883, -0.3047],\n",
      "         [ 1.4485, -1.1377, -0.4517],\n",
      "         [ 1.4161, -1.1155, -0.5981],\n",
      "         ...,\n",
      "         [ 1.2863, -1.1599, -1.2685],\n",
      "         [ 1.1534, -1.1182, -1.2869],\n",
      "         [ 1.0205, -1.0765, -1.3053]],\n",
      "\n",
      "        [[ 1.4485, -1.1377, -0.4517],\n",
      "         [ 1.4161, -1.1155, -0.5981],\n",
      "         [ 1.3693, -1.1247, -0.7393],\n",
      "         ...,\n",
      "         [ 1.1534, -1.1182, -1.2869],\n",
      "         [ 1.0205, -1.0765, -1.3053],\n",
      "         [ 0.8919, -1.0725, -1.2904]]])\n",
      "tensor([[[ 1.4161, -1.1155, -0.5981],\n",
      "         [ 1.3693, -1.1247, -0.7393],\n",
      "         [ 1.3165, -1.1495, -0.8780],\n",
      "         ...,\n",
      "         [ 1.0205, -1.0765, -1.3053],\n",
      "         [ 0.8919, -1.0725, -1.2904],\n",
      "         [ 0.7700, -1.1917, -1.2292]],\n",
      "\n",
      "        [[ 1.3693, -1.1247, -0.7393],\n",
      "         [ 1.3165, -1.1495, -0.8780],\n",
      "         [ 1.3279, -1.1599, -1.0059],\n",
      "         ...,\n",
      "         [ 0.8919, -1.0725, -1.2904],\n",
      "         [ 0.7700, -1.1917, -1.2292],\n",
      "         [ 0.6437, -1.3071, -1.2636]],\n",
      "\n",
      "        [[ 1.3165, -1.1495, -0.8780],\n",
      "         [ 1.3279, -1.1599, -1.0059],\n",
      "         [ 1.3233, -1.1599, -1.1567],\n",
      "         ...,\n",
      "         [ 0.7700, -1.1917, -1.2292],\n",
      "         [ 0.6437, -1.3071, -1.2636],\n",
      "         [ 0.5245, -1.3897, -1.3321]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5822, -0.9425, -0.9888],\n",
      "         [ 0.4321, -0.9425, -1.0805],\n",
      "         [ 0.2704, -0.9425, -1.1428],\n",
      "         ...,\n",
      "         [-0.5183, -0.9425, -0.7694],\n",
      "         [-0.6684, -0.9425, -0.6777],\n",
      "         [-0.8184, -0.9425, -0.5860]],\n",
      "\n",
      "        [[ 0.4321, -0.9425, -1.0805],\n",
      "         [ 0.2704, -0.9425, -1.1428],\n",
      "         [ 0.0944, -0.9425, -1.1166],\n",
      "         ...,\n",
      "         [-0.6684, -0.9425, -0.6777],\n",
      "         [-0.8184, -0.9425, -0.5860],\n",
      "         [-0.9203, -0.9425, -0.4491]],\n",
      "\n",
      "        [[ 0.2704, -0.9425, -1.1428],\n",
      "         [ 0.0944, -0.9425, -1.1166],\n",
      "         [-0.0626, -0.9425, -1.0360],\n",
      "         ...,\n",
      "         [-0.8184, -0.9425, -0.5860],\n",
      "         [-0.9203, -0.9425, -0.4491],\n",
      "         [-0.9090, -0.9425, -0.2816]]])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    print(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predd[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of ground true around predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore testing data\n",
    "test_seq = []\n",
    "test_gt = []\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    test_gt.append(labels)\n",
    "    test_seq.append(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = test_seq[i].numpy()\n",
    "        gt = test_gt[i].numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        test = np.append(test, test_seq[i].numpy(), axis = 0)\n",
    "        gt = np.append(gt, test_gt[i].numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test[0][-1,:].reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct analysis matrix l(0-2 col): last sequence point, g(3-5 col): next ground true point, p(6-8 col): predicted point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    last_point = test[i][-1, :].reshape(1, -1)\n",
    "    gt_point = gt[i].reshape(1, -1)\n",
    "    pred_point = pred[i].reshape(1, -1)\n",
    "    row = np.append(np.append(last_point, gt_point, axis = 1), pred_point, axis = 1)\n",
    "    if (i == 0):\n",
    "        analysis = row\n",
    "    else:\n",
    "        analysis = np.append(analysis, row, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rotation matrix\n",
    "# note: input vec_1 and vec_2 have to be normalized before passing to the function\n",
    "def get_rotatin_mat(vec_1, vec_2):\n",
    "    a,b = vec_1.reshape(3), vec_2.reshape(3)\n",
    "    v = np.cross(a,b)\n",
    "    c = np.dot(a,b)\n",
    "    s = np.linalg.norm(v)\n",
    "    # print(\"s\", s)\n",
    "    if (s == 0):\n",
    "        return np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "    I = np.identity(3)\n",
    "    vXStr = '{} {} {}; {} {} {}; {} {} {}'.format(0, -v[2], v[1], v[2], 0, -v[0], -v[1], v[0], 0)\n",
    "    k = np.matrix(vXStr)\n",
    "    r = I + k + np.matmul(k,k) * ((1 -c)/(s**2))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 1, 0])\n",
    "a = a/np.linalg.norm(a)\n",
    "b = np.array([4, 5, 6])\n",
    "b = b/np.linalg.norm(b)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "m = get_rotatin_mat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(m, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(b[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b/np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "a_n = a/np.linalg.norm(a)\n",
    "\n",
    "b = np.array([[4,5,6]])\n",
    "b_n = b/np.linalg.norm(b)\n",
    "\n",
    "c = np.array([[7,8,9]])\n",
    "c_n = c/np.linalg.norm(c)\n",
    "\n",
    "r = get_rotatin_mat(b_n, a_n) # b to a\n",
    "\n",
    "b_p = np.dot(r, b.reshape(3, -1))\n",
    "c_p = np.dot(r, c.reshape(3, -1))\n",
    "c_p = np.asarray(c_p)\n",
    "c_p_n = c_p/np.linalg.norm(c_p)\n",
    "print(b.shape)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"c\", c)\n",
    "print(\"b_p\", b_p)\n",
    "print(\"c_p\", c_p)\n",
    "\n",
    "print(get_rotatin_mat(c_n, b_n))\n",
    "\n",
    "print(get_rotatin_mat(c_p_n, a_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c_p.reshape(1, -1)\n",
    "d = np.asarray(d)\n",
    "d_n = d/np.linalg.norm(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rotatin_mat(d_n, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all point to the same starting point\n",
    "# first sequence last direction\n",
    "# direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = (analysis[0, 3:6] - analysis[0, 0:3]).reshape(1, -1)\n",
    "# normalize\n",
    "direction = direction/np.linalg.norm(direction)\n",
    "# first diff between prediction and ground true\n",
    "diff_correct = (analysis[0, 6:9] - analysis[0, 3:6]).reshape(1, -1)\n",
    "# print(type(diff_correct))\n",
    "# print(diff_correct.shape)\n",
    "for i in range(1, analysis.shape[0]):\n",
    "    # cur_direction = (analysis[i, 0:3] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    cur_direction = (analysis[i, 3:6] - analysis[i, 0:3]).reshape(1, -1)\n",
    "    # normalize\n",
    "    if (np.linalg.norm(cur_direction) == 0):\n",
    "        print(i, np.linalg.norm(cur_direction))\n",
    "    cur_direction = cur_direction/np.linalg.norm(cur_direction)\n",
    "    cur_diff = (analysis[i, 6:9] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    # get rotation matrix from cur_direction to direction\n",
    "    r = get_rotatin_mat(cur_direction, direction)\n",
    "    # apply rotation matrix to the cur_diff\n",
    "    cur_diff = np.dot(r, cur_diff.reshape(3, -1))\n",
    "    cur_diff = cur_diff.reshape(1, -1)\n",
    "    # matrix type to np array type\n",
    "    cur_diff = np.asarray(cur_diff)\n",
    "    # print(cur_diff.shape)\n",
    "    diff_correct = np.append(diff_correct, cur_diff, axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_correct[:, 0], diff_correct[:, 1], diff_correct[:, 2])\n",
    "x = [-direction[0, 0], 0]\n",
    "y = [-direction[0, 1], 0]\n",
    "z = [-direction[0, 2], 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "# ax.arrow(0, 0, 0.5, 0.5, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "# ax.quiver(0, 0, 0, -direction[0, 0], -direction[0, 1], -direction[0, 2], length=5, normalize=True, color='r')\n",
    "# x = np.zeros(10)\n",
    "# y = np.zeros(10)\n",
    "# z = np.arange(10)*10 # remove *100 and the arrow heads will reappear.\n",
    "# dx = np.zeros(10)\n",
    "# dy = np.arange(10)\n",
    "# dz = np.zeros(10)\n",
    "x = np.array([0, -direction[0, 0]])\n",
    "y = np.array([0, -direction[0, 1]])\n",
    "z = np.array([0, -direction[0, 2]])\n",
    "dx = np.array([0, 0])\n",
    "dy = np.array([0, 0])\n",
    "dz = np.array([0, 0])\n",
    "# ax.quiver(x, y, z, dx, dy, dz, length=1)\n",
    "# ax.quiver(-direction[0, 0], -direction[0, 1], -direction[0, 2], 0, 0, 0, length=100, normalize=True)\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note view_angle must be normalized\n",
    "def getProjection(view_angle, point):\n",
    "    v = point - np.array([[0, 0, 0]])\n",
    "    dist = v[0, 0]*view_angle[0, 0] + v[0, 1]*view_angle[0, 1] + v[0, 2]*view_angle[0, 2]\n",
    "    projected_point = point - dist*view_angle\n",
    "    \n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))\n",
    "# normalization\n",
    "view_angle = view_angle/np.linalg.norm(view_angle)\n",
    "print(view_angle)\n",
    "for i in range(diff_correct.shape[0]):\n",
    "    if (i == 0):\n",
    "        diff_corrrect_projection = getProjection(view_angle, diff_correct[i, :])\n",
    "    else:\n",
    "        diff_corrrect_projection = np.append(diff_corrrect_projection, getProjection(view_angle, diff_correct[i, :]), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_corrrect_projection[:, 0], diff_corrrect_projection[:, 1], diff_corrrect_projection[:, 2])\n",
    "x = [-direction[0, 0]*10, 0]\n",
    "y = [-direction[0, 1]*10, 0]\n",
    "z = [-direction[0, 2]*10, 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from 3D to 2D\n",
    "diff_corrrect_projection_2D = np.delete(diff_corrrect_projection, 1, 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from matplotlib.colors import LogNorm\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(diff_corrrect_projection_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(-5., 5.)\n",
    "y = np.linspace(-5., 5.)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -clf.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
    "                 levels=np.logspace(0, 3, 300))\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(diff_corrrect_projection_2D[:, 0], diff_corrrect_projection_2D[:, 1], .8)\n",
    "\n",
    "x = [-direction[0, 0]*5, 0]\n",
    "z = [-direction[0, 2]*5, 0]\n",
    "plt.plot(x, z, label='parametric curve', color='r')\n",
    "\n",
    "plt.title('Negative log-likelihood predicted by a GMM')\n",
    "plt.axis('tight')\n",
    "plt.xlabel(\"Direction 1\")\n",
    "plt.ylabel(\"Direction 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "C = np.array([[0., -0.7], [3.5, .7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_gaussian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(direction[0, 0], 2) + pow(direction[0, 1], 2) + pow(direction[0, 2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[2, 0:3] - analysis[2, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[3, 0:3] - analysis[3, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = direction/np.linalg.norm(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (analysis[1, 0:3] - analysis[1, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d/np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(0.84804916, 2) + pow(0.5299187, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 3:6] - analysis[0, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([13.259178  ,  0.37397736, 20.36138]) - np.asarray([12.658086  ,  1.293065  ,20.224268])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = \n",
    "        gt = test_gt[i].cpu().detach().numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        gt = np.append(gt, test_gt[i].cpu().detach().numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pred - gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff[:, 0], diff[:, 1], diff[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.from_numpy(gt), torch.from_numpy(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i in range(len(test_pred)):\n",
    "    l = criterion(test_gt[i], test_pred[i])\n",
    "    loss += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
