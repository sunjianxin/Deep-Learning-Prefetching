{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, show_statistic, min_max_scaling, normalize_one, normalize_all, construct_train_valid_tensor, construct_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14950\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "# load each of the 5 dataset and do min_max_scaling on each of them\n",
    "train_set_x_1, valid_set_x_1, train_set_y_1, valid_set_y_1, train_set_z_1, valid_set_z_1 = load_data_2('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, valid_set_x_2, train_set_y_2, valid_set_y_2, train_set_z_2, valid_set_z_2 = load_data_2('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, valid_set_x_3, train_set_y_3, valid_set_y_3, train_set_z_3, valid_set_z_3 = load_data_2('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, valid_set_x_4, train_set_y_4, valid_set_y_4, train_set_z_4, valid_set_z_4 = load_data_2('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, valid_set_x_5, train_set_y_5, valid_set_y_5, train_set_z_5, valid_set_z_5 = load_data_2('data_preprocessing/test_5_training_xyz.txt', 400)\n",
    "\n",
    "# do min-max-scaling for each data set\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_1)\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_2)\n",
    "min_max_scaling(train_set_x_3)\n",
    "min_max_scaling(train_set_x_4)\n",
    "min_max_scaling(train_set_x_5)\n",
    "\n",
    "min_max_scaling(valid_set_x_1)\n",
    "min_max_scaling(valid_set_x_2)\n",
    "min_max_scaling(valid_set_x_3)\n",
    "min_max_scaling(valid_set_x_4)\n",
    "min_max_scaling(valid_set_x_5)\n",
    "\n",
    "min_max_scaling(train_set_y_1)\n",
    "min_max_scaling(train_set_y_2)\n",
    "min_max_scaling(train_set_y_3)\n",
    "min_max_scaling(train_set_y_4)\n",
    "min_max_scaling(train_set_y_5)\n",
    "\n",
    "min_max_scaling(valid_set_y_1)\n",
    "min_max_scaling(valid_set_y_2)\n",
    "min_max_scaling(valid_set_y_3)\n",
    "min_max_scaling(valid_set_y_4)\n",
    "min_max_scaling(valid_set_y_5)\n",
    "\n",
    "min_max_scaling(train_set_z_1)\n",
    "min_max_scaling(train_set_z_2)\n",
    "min_max_scaling(train_set_z_3)\n",
    "min_max_scaling(train_set_z_4)\n",
    "min_max_scaling(train_set_z_5)\n",
    "\n",
    "min_max_scaling(valid_set_z_1)\n",
    "min_max_scaling(valid_set_z_2)\n",
    "min_max_scaling(valid_set_z_3)\n",
    "min_max_scaling(valid_set_z_4)\n",
    "min_max_scaling(valid_set_z_5)\n",
    "\n",
    "\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "x_mean, x_std = normalize_all(train_set_x_1, train_set_x_2, train_set_x_3, train_set_x_4, train_set_x_5, valid_set_x_1, valid_set_x_2, valid_set_x_3, valid_set_x_4, valid_set_x_5)\n",
    "y_mean, y_std = normalize_all(train_set_y_1, train_set_y_2, train_set_y_3, train_set_y_4, train_set_y_5, valid_set_y_1, valid_set_y_2, valid_set_y_3, valid_set_y_4, valid_set_y_5)\n",
    "z_mean, z_std = normalize_all(train_set_z_1, train_set_z_2, train_set_z_3, train_set_z_4, train_set_z_5, valid_set_z_1, valid_set_z_2, valid_set_z_3, valid_set_z_4, valid_set_z_5)\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "\n",
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)\n",
    "\n",
    "\n",
    "window_size = 10\n",
    "train_dataset_1, train_label_1, valid_dataset_1, valid_label_1 = construct_train_valid_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           valid_set_x_1,\n",
    "                                                                                           valid_set_y_1,\n",
    "                                                                                           valid_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, valid_dataset_2, valid_label_2 = construct_train_valid_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           valid_set_x_2,\n",
    "                                                                                           valid_set_y_2,\n",
    "                                                                                           valid_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, valid_dataset_3, valid_label_3 = construct_train_valid_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           valid_set_x_3,\n",
    "                                                                                           valid_set_y_3,\n",
    "                                                                                           valid_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, valid_dataset_4, valid_label_4 = construct_train_valid_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           valid_set_x_4,\n",
    "                                                                                           valid_set_y_4,\n",
    "                                                                                           valid_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, valid_dataset_5, valid_label_5 = construct_train_valid_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           valid_set_x_5,\n",
    "                                                                                           valid_set_y_5,\n",
    "                                                                                           valid_set_z_5,\n",
    "                                                                                           window_size)\n",
    "\n",
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "valid_dataset = np.concatenate((valid_dataset_1,\n",
    "                               valid_dataset_2,\n",
    "                               valid_dataset_3,\n",
    "                               valid_dataset_4,\n",
    "                               valid_dataset_5), axis=0)\n",
    "valid_label = np.concatenate((valid_label_1,\n",
    "                             valid_label_2,\n",
    "                             valid_label_3,\n",
    "                             valid_label_4,\n",
    "                             valid_label_5), axis=0)\n",
    "\n",
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(valid_dataset, valid_label)\n",
    "print(len(valid_set))\n",
    "\n",
    "# batch_size = 30\n",
    "# batch_size = 50\n",
    "batch_size = 650\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# keep the valid data trajectory order\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle valid data for using continous trajectory later on\n",
    "# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "min: -1.5190000000000001\n",
      "max: 1.41144\n",
      "mean: -0.058533702725000004\n",
      "std: 0.7861266362964188\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4999879488895008\n",
      "std: 0.3247696467306799\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.49462499118347864\n",
      "std: 0.2888055402177855\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5101133475610831\n",
      "std: 0.25434808389556507\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4851962413697185\n",
      "std: 0.2642834718504606\n"
     ]
    }
   ],
   "source": [
    "test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "# do min-max-scaling for each test data set\n",
    "show_statistic(test_set_x_1)\n",
    "min_max_scaling(test_set_x_1)\n",
    "show_statistic(test_set_x_1)\n",
    "min_max_scaling(test_set_x_2)\n",
    "min_max_scaling(test_set_x_3)\n",
    "min_max_scaling(test_set_x_4)\n",
    "min_max_scaling(test_set_x_5)\n",
    "\n",
    "min_max_scaling(test_set_y_1)\n",
    "min_max_scaling(test_set_y_2)\n",
    "min_max_scaling(test_set_y_3)\n",
    "min_max_scaling(test_set_y_4)\n",
    "min_max_scaling(test_set_y_5)\n",
    "\n",
    "min_max_scaling(test_set_z_1)\n",
    "min_max_scaling(test_set_z_2)\n",
    "min_max_scaling(test_set_z_3)\n",
    "min_max_scaling(test_set_z_4)\n",
    "min_max_scaling(test_set_z_5)\n",
    "\n",
    "'''\n",
    "show_statistic(test_set_x_1)\n",
    "# do normalization on x of validation test set\n",
    "normalize_one(test_set_x_1, x_mean, x_std)\n",
    "show_statistic(test_set_x_1)\n",
    "normalize_one(test_set_x_2, x_mean, x_std)\n",
    "normalize_one(test_set_x_3, x_mean, x_std)\n",
    "normalize_one(test_set_x_4, x_mean, x_std)\n",
    "normalize_one(test_set_x_5, x_mean, x_std)\n",
    "# do normalization on y of validation test set\n",
    "normalize_one(test_set_y_1, y_mean, y_std)\n",
    "normalize_one(test_set_y_2, y_mean, y_std)\n",
    "normalize_one(test_set_y_3, y_mean, y_std)\n",
    "normalize_one(test_set_y_4, y_mean, y_std)\n",
    "normalize_one(test_set_y_5, y_mean, y_std)\n",
    "# do normalization on z of validation test set\n",
    "normalize_one(test_set_z_1, z_mean, z_std)\n",
    "normalize_one(test_set_z_2, z_mean, z_std)\n",
    "normalize_one(test_set_z_3, z_mean, z_std)\n",
    "normalize_one(test_set_z_4, z_mean, z_std)\n",
    "normalize_one(test_set_z_5, z_mean, z_std)\n",
    "# show_statistic(train_set_x_1)\n",
    "'''\n",
    "\n",
    "window_size = 10\n",
    "test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                     test_set_y_1,\n",
    "                                                     test_set_z_1,\n",
    "                                                     window_size)\n",
    "test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                     test_set_y_2,\n",
    "                                                     test_set_z_2,\n",
    "                                                     window_size)\n",
    "test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                     test_set_y_3,\n",
    "                                                     test_set_z_3,\n",
    "                                                     window_size)\n",
    "test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                     test_set_y_4,\n",
    "                                                     test_set_z_4,\n",
    "                                                     window_size)\n",
    "test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                     test_set_y_5,\n",
    "                                                     test_set_z_5,\n",
    "                                                     window_size)\n",
    "\n",
    "test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "print(len(test_set_1))\n",
    "print(len(test_set_2))\n",
    "print(len(test_set_3))\n",
    "print(len(test_set_4))\n",
    "print(len(test_set_5))\n",
    "show_statistic(test_set_x_1)\n",
    "show_statistic(test_set_x_2)\n",
    "show_statistic(test_set_x_3)\n",
    "show_statistic(test_set_x_4)\n",
    "show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "batch_size = 650\n",
    "test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7ff375efd128>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(train_set_x_5, train_set_y_5, train_set_z_5, 'gray')\n",
    "# ax.plot3D(valid_test_set_x_1, valid_test_set_y_1, valid_test_set_z_1, 'gray')\n",
    "# ax.plot3D(train_set_x_2, train_set_y_2, train_set_z_2, 'red')\n",
    "# ax.plot3D(test_set_x_1, test_set_y_1, test_set_z_1, 'red')\n",
    "# ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = np.linspace(0, 15, 1000)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "ax.plot3D(train_set_x_5, train_set_y_5, train_set_z_5, 'gray')\n",
    "# ax.plot3D(train_set_x_5, train_set_y_5, train_set_z_5, 'gray')\n",
    "# ax.plot3D(valid_test_set_x_1, valid_test_set_y_1, valid_test_set_z_1, 'gray')\n",
    "# ax.plot3D(train_set_x_2, train_set_y_2, train_set_z_2, 'red')\n",
    "ax.plot3D(valid_set_x_1, valid_set_y_1, valid_set_z_1, 'red')\n",
    "# ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 10, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 10, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 10, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 10, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2990*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "390*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-9979ff4634ab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-9979ff4634ab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 23 = 14950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 23 = 14950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-de69bcda9ea3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-de69bcda9ea3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 3 = 1950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 3 = 1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 10, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "1 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "2 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "3 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "4 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "5 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "6 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "7 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "8 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "9 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "10 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "11 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "12 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "13 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "14 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "15 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "16 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "17 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "18 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "19 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "20 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "21 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "22 torch.Size([650, 10, 3]) torch.Size([650, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    print(i, seqs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "1 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "2 torch.Size([650, 10, 3]) torch.Size([650, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(valid_loader):\n",
    "    print(i, seqs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[0.0807, 0.1743, 0.3415],\n",
      "        [0.0800, 0.1743, 0.3502],\n",
      "        [0.0794, 0.1743, 0.3588],\n",
      "        [0.0787, 0.1743, 0.3675],\n",
      "        [0.0781, 0.1743, 0.3762],\n",
      "        [0.0775, 0.1743, 0.3849],\n",
      "        [0.0768, 0.1743, 0.3935],\n",
      "        [0.0762, 0.1743, 0.4022],\n",
      "        [0.0755, 0.1743, 0.4109],\n",
      "        [0.0755, 0.1743, 0.4196]])\n",
      "tensor([0.0790, 0.1743, 0.4276])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[0])\n",
    "        print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[0.4875, 0.4896, 0.8459],\n",
      "        [0.5034, 0.4896, 0.8386],\n",
      "        [0.5187, 0.4896, 0.8304],\n",
      "        [0.5340, 0.4896, 0.8221],\n",
      "        [0.5493, 0.4896, 0.8139],\n",
      "        [0.5645, 0.4896, 0.8056],\n",
      "        [0.5798, 0.4896, 0.7974],\n",
      "        [0.5951, 0.4896, 0.7892],\n",
      "        [0.6104, 0.4896, 0.7809],\n",
      "        [0.6256, 0.4896, 0.7727]])\n",
      "tensor([0.6409, 0.4896, 0.7644])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(valid_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[1])\n",
    "        print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_loss(loader):\n",
    "    total_loss = 0.0\n",
    "    batch = 0\n",
    "    # seq = []\n",
    "    pred = []\n",
    "    # gt = []\n",
    "    for i, (seqs, labels) in enumerate(loader):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        batch = i + 1\n",
    "        \n",
    "    return total_loss/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.06445725051605183, valid_loss: 0.07854614655176799, test_loss: 0.05505218356847763\n",
      "epoch: 1, train_loss: 0.04305348901644997, valid_loss: 0.06468262275060017, test_loss: 0.04479163885116577\n",
      "epoch: 2, train_loss: 0.03436505713540575, valid_loss: 0.049621593207120895, test_loss: 0.03422139212489128\n",
      "epoch: 3, train_loss: 0.024944122232820675, valid_loss: 0.03411031203965346, test_loss: 0.023301055654883385\n",
      "epoch: 4, train_loss: 0.015682877122384052, valid_loss: 0.0200374120225509, test_loss: 0.014545844867825508\n",
      "epoch: 5, train_loss: 0.008307058343906765, valid_loss: 0.010259409124652544, test_loss: 0.00897747091948986\n",
      "epoch: 6, train_loss: 0.00396032954323227, valid_loss: 0.005170484771952033, test_loss: 0.006745105143636465\n",
      "epoch: 7, train_loss: 0.0021311737414773393, valid_loss: 0.0032607623919223747, test_loss: 0.006119127850979567\n",
      "epoch: 8, train_loss: 0.0015509452053305247, valid_loss: 0.0027131186216138303, test_loss: 0.005984291434288025\n",
      "epoch: 9, train_loss: 0.0013999919123623681, valid_loss: 0.002559785032644868, test_loss: 0.005982929840683937\n",
      "epoch: 10, train_loss: 0.0013584898096387801, valid_loss: 0.0025121767345505455, test_loss: 0.005992831662297249\n",
      "epoch: 11, train_loss: 0.0013415359325059082, valid_loss: 0.0024874476948753, test_loss: 0.005977094639092684\n",
      "epoch: 12, train_loss: 0.001327124540694058, valid_loss: 0.0024786319894095263, test_loss: 0.005915287882089615\n",
      "epoch: 13, train_loss: 0.0013151471756155724, valid_loss: 0.0024582632274056473, test_loss: 0.005888098850846291\n",
      "epoch: 14, train_loss: 0.0013085328012137957, valid_loss: 0.0024460028895797827, test_loss: 0.005845694337040186\n",
      "epoch: 15, train_loss: 0.0012954037545410836, valid_loss: 0.0024248140204387405, test_loss: 0.005834205541759729\n",
      "epoch: 16, train_loss: 0.0012853981916914167, valid_loss: 0.0024274990622264645, test_loss: 0.005790325812995434\n",
      "epoch: 17, train_loss: 0.0012753660912099092, valid_loss: 0.00239518533150355, test_loss: 0.005795618053525686\n",
      "epoch: 18, train_loss: 0.0012641369425894125, valid_loss: 0.0023858574180242917, test_loss: 0.0057334317825734615\n",
      "epoch: 19, train_loss: 0.0012526063289007416, valid_loss: 0.002369015167156855, test_loss: 0.005714887753129005\n",
      "epoch: 20, train_loss: 0.001243640830659348, valid_loss: 0.0023574242756391564, test_loss: 0.005675233900547028\n",
      "epoch: 21, train_loss: 0.001233853602215, valid_loss: 0.0023441926071730754, test_loss: 0.005648890044540167\n",
      "epoch: 22, train_loss: 0.0012236058246344328, valid_loss: 0.0023309410316869617, test_loss: 0.005623802542686462\n",
      "epoch: 23, train_loss: 0.001213253997063831, valid_loss: 0.0023201051517389715, test_loss: 0.005588855128735304\n",
      "epoch: 24, train_loss: 0.0012049901542132316, valid_loss: 0.0023051218789381287, test_loss: 0.005569742061197758\n",
      "epoch: 25, train_loss: 0.0011964595987987907, valid_loss: 0.002288961705441276, test_loss: 0.005549332592636347\n",
      "epoch: 26, train_loss: 0.0011885449158675644, valid_loss: 0.0022832415997982025, test_loss: 0.005506915505975485\n",
      "epoch: 27, train_loss: 0.001178852965771828, valid_loss: 0.0022624854658109448, test_loss: 0.005497760605067015\n",
      "epoch: 28, train_loss: 0.0011693998543626587, valid_loss: 0.002247977963027855, test_loss: 0.005473524797707796\n",
      "epoch: 29, train_loss: 0.0011604637560515623, valid_loss: 0.002239979454316199, test_loss: 0.005442047957330942\n",
      "epoch: 30, train_loss: 0.001152248207844146, valid_loss: 0.002229540143162012, test_loss: 0.005420245695859194\n",
      "epoch: 31, train_loss: 0.001144005228643832, valid_loss: 0.00222481928843384, test_loss: 0.005372337065637112\n",
      "epoch: 32, train_loss: 0.0011351094277494628, valid_loss: 0.002201588843793919, test_loss: 0.005371917504817247\n",
      "epoch: 33, train_loss: 0.0011274911837814295, valid_loss: 0.0021922068553976715, test_loss: 0.005350472871214151\n",
      "epoch: 34, train_loss: 0.0011209351132097452, valid_loss: 0.0021793040020080903, test_loss: 0.005336489994078875\n",
      "epoch: 35, train_loss: 0.0011117383584623103, valid_loss: 0.002167666098102927, test_loss: 0.005303490441292524\n",
      "epoch: 36, train_loss: 0.001104557350196916, valid_loss: 0.0021610095767149082, test_loss: 0.005294862203299999\n",
      "epoch: 37, train_loss: 0.0010975148609798887, valid_loss: 0.002148560539353639, test_loss: 0.005259842146188021\n",
      "epoch: 38, train_loss: 0.0010879531037062407, valid_loss: 0.0021385365592626235, test_loss: 0.005231082439422607\n",
      "epoch: 39, train_loss: 0.0010826620779445639, valid_loss: 0.0021301423645733544, test_loss: 0.005207964684814215\n",
      "epoch: 40, train_loss: 0.0010733895216380124, valid_loss: 0.002118601648059363, test_loss: 0.005179697647690773\n",
      "epoch: 41, train_loss: 0.001066997336001014, valid_loss: 0.002123855675260226, test_loss: 0.005172924138605595\n",
      "epoch: 42, train_loss: 0.0010616274160818884, valid_loss: 0.0021020955173298717, test_loss: 0.005134746432304382\n",
      "epoch: 43, train_loss: 0.0010536161489258318, valid_loss: 0.002084077966477101, test_loss: 0.005137789528816938\n",
      "epoch: 44, train_loss: 0.001045837239159838, valid_loss: 0.002074892952805385, test_loss: 0.005134040489792824\n",
      "epoch: 45, train_loss: 0.0010391929915741734, valid_loss: 0.002068102437381943, test_loss: 0.005090859718620777\n",
      "epoch: 46, train_loss: 0.0010338332984879937, valid_loss: 0.002055693640916919, test_loss: 0.005076803732663393\n",
      "epoch: 47, train_loss: 0.001026252590868946, valid_loss: 0.002043237764155492, test_loss: 0.0050598937086761\n",
      "epoch: 48, train_loss: 0.0010189673453366952, valid_loss: 0.002042783977231011, test_loss: 0.005031957756727934\n",
      "epoch: 49, train_loss: 0.0010124464321922026, valid_loss: 0.0020313934752872833, test_loss: 0.0050021689385175705\n",
      "epoch: 50, train_loss: 0.0010068393742625156, valid_loss: 0.0020170280477032065, test_loss: 0.004991563502699137\n",
      "epoch: 51, train_loss: 0.000999662635402511, valid_loss: 0.0020182478280427554, test_loss: 0.004956613294780254\n",
      "epoch: 52, train_loss: 0.0009948340846913989, valid_loss: 0.001998343844509994, test_loss: 0.004957662895321846\n",
      "epoch: 53, train_loss: 0.0009889028429904063, valid_loss: 0.001991184758177648, test_loss: 0.004932385869324207\n",
      "epoch: 54, train_loss: 0.000980973430990201, valid_loss: 0.001992860964188973, test_loss: 0.004900560714304447\n",
      "epoch: 55, train_loss: 0.0009753580847957536, valid_loss: 0.001986108865821734, test_loss: 0.004881018772721291\n",
      "epoch: 56, train_loss: 0.000969363477728937, valid_loss: 0.0019774735750009618, test_loss: 0.004868651740252972\n",
      "epoch: 57, train_loss: 0.0009648273040985931, valid_loss: 0.0019574838709862283, test_loss: 0.00487543037161231\n",
      "epoch: 58, train_loss: 0.0009577711639196976, valid_loss: 0.0019505803744929533, test_loss: 0.0048477016389369965\n",
      "epoch: 59, train_loss: 0.000952258356846869, valid_loss: 0.0019439688088217129, test_loss: 0.0048376210033893585\n",
      "epoch: 60, train_loss: 0.0009474556376833631, valid_loss: 0.0019319578035113711, test_loss: 0.004827770870178938\n",
      "epoch: 61, train_loss: 0.000942077545914799, valid_loss: 0.0019248432848447312, test_loss: 0.004801440984010696\n",
      "epoch: 62, train_loss: 0.0009362810954411069, valid_loss: 0.001917471536823238, test_loss: 0.004771318286657333\n",
      "epoch: 63, train_loss: 0.0009300111180535802, valid_loss: 0.001908549398649484, test_loss: 0.004766588099300861\n",
      "epoch: 64, train_loss: 0.0009253486464528934, valid_loss: 0.0019052899345600356, test_loss: 0.004736655857414007\n",
      "epoch: 65, train_loss: 0.0009193236018409548, valid_loss: 0.0018953608620601396, test_loss: 0.004722508601844311\n",
      "epoch: 66, train_loss: 0.0009137428967970545, valid_loss: 0.001889131769227485, test_loss: 0.004706129897385836\n",
      "epoch: 67, train_loss: 0.0009090418132710392, valid_loss: 0.0018793511747693021, test_loss: 0.004707109648734331\n",
      "epoch: 68, train_loss: 0.0009047789296702198, valid_loss: 0.0018688779867564638, test_loss: 0.004683568142354488\n",
      "epoch: 69, train_loss: 0.0008978166582022349, valid_loss: 0.0018652723908113937, test_loss: 0.004657728597521782\n",
      "epoch: 70, train_loss: 0.0008946300266594019, valid_loss: 0.0018579767202027142, test_loss: 0.004651346243917942\n",
      "epoch: 71, train_loss: 0.000889256475088389, valid_loss: 0.0018585494253784418, test_loss: 0.004614687990397215\n",
      "epoch: 72, train_loss: 0.0008856616838349272, valid_loss: 0.0018434941302984953, test_loss: 0.004625885747373104\n",
      "epoch: 73, train_loss: 0.0008792999787661045, valid_loss: 0.001834134425735101, test_loss: 0.004598364233970642\n",
      "epoch: 74, train_loss: 0.0008752531721256673, valid_loss: 0.0018330708553548902, test_loss: 0.004586661700159311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 75, train_loss: 0.0008699987691057765, valid_loss: 0.001821023868008827, test_loss: 0.004569836426526308\n",
      "epoch: 76, train_loss: 0.0008635975385044256, valid_loss: 0.0018142685391164075, test_loss: 0.0045651947148144245\n",
      "epoch: 77, train_loss: 0.0008598211849027354, valid_loss: 0.0018033521870772045, test_loss: 0.0045506879687309265\n",
      "epoch: 78, train_loss: 0.0008547018540253782, valid_loss: 0.0018049406838448097, test_loss: 0.0045270356349647045\n",
      "epoch: 79, train_loss: 0.0008505406187158887, valid_loss: 0.0018034007613702367, test_loss: 0.004499344155192375\n",
      "epoch: 80, train_loss: 0.0008457069533229198, valid_loss: 0.001782669850702708, test_loss: 0.004529255907982588\n",
      "epoch: 81, train_loss: 0.000841978831095216, valid_loss: 0.00177644479360121, test_loss: 0.004488338716328144\n",
      "epoch: 82, train_loss: 0.0008367520134450625, valid_loss: 0.0017759697705817719, test_loss: 0.004466468933969736\n",
      "epoch: 83, train_loss: 0.0008318798247036402, valid_loss: 0.0017640764126554132, test_loss: 0.004464781377464533\n",
      "epoch: 84, train_loss: 0.0008282774867004026, valid_loss: 0.0017629127542022616, test_loss: 0.004436934366822243\n",
      "epoch: 85, train_loss: 0.0008245698712847155, valid_loss: 0.0017563485889695585, test_loss: 0.004436546936631203\n",
      "epoch: 86, train_loss: 0.0008202984640577241, valid_loss: 0.0017461221820364397, test_loss: 0.004418374039232731\n",
      "epoch: 87, train_loss: 0.0008150067119656698, valid_loss: 0.0017371713765896857, test_loss: 0.004411756526678801\n",
      "epoch: 88, train_loss: 0.0008100760827326905, valid_loss: 0.0017335829033982009, test_loss: 0.004392370581626892\n",
      "epoch: 89, train_loss: 0.0008069605126207614, valid_loss: 0.0017271290028778215, test_loss: 0.004390094429254532\n",
      "epoch: 90, train_loss: 0.0008032955659512917, valid_loss: 0.0017197695463740577, test_loss: 0.004368000663816929\n",
      "epoch: 91, train_loss: 0.0007980819411940225, valid_loss: 0.001715544203761965, test_loss: 0.004350433591753244\n",
      "epoch: 92, train_loss: 0.0007941791656143639, valid_loss: 0.0017112031443199764, test_loss: 0.004344085231423378\n",
      "epoch: 93, train_loss: 0.0007899685727391878, valid_loss: 0.0017124947044067085, test_loss: 0.004329543560743332\n",
      "epoch: 94, train_loss: 0.0007860978746422283, valid_loss: 0.0016946200145563732, test_loss: 0.004319036845117807\n",
      "epoch: 95, train_loss: 0.0007818750961197783, valid_loss: 0.0016927788868391265, test_loss: 0.00430862233042717\n",
      "epoch: 96, train_loss: 0.0007775427834333285, valid_loss: 0.0016880860105933, test_loss: 0.004296019207686186\n",
      "epoch: 97, train_loss: 0.0007741673059923494, valid_loss: 0.0016807314823381603, test_loss: 0.004291468299925327\n",
      "epoch: 98, train_loss: 0.0007706676513644988, valid_loss: 0.0016813341741605352, test_loss: 0.004253442399203777\n",
      "epoch: 99, train_loss: 0.000765808769400515, valid_loss: 0.001670906242604057, test_loss: 0.004242686554789543\n",
      "epoch: 100, train_loss: 0.00076319140371988, valid_loss: 0.001668038438462342, test_loss: 0.004227319732308388\n",
      "epoch: 101, train_loss: 0.000758227520197144, valid_loss: 0.0016588151338510215, test_loss: 0.0042213150300085545\n",
      "epoch: 102, train_loss: 0.0007549211428419727, valid_loss: 0.001655488827964291, test_loss: 0.004224297124892473\n",
      "epoch: 103, train_loss: 0.0007508502306376139, valid_loss: 0.0016469346301164478, test_loss: 0.004214962478727102\n",
      "epoch: 104, train_loss: 0.0007473531017160934, valid_loss: 0.0016385898925364017, test_loss: 0.00419768737629056\n",
      "epoch: 105, train_loss: 0.0007435527058196781, valid_loss: 0.0016349151943965505, test_loss: 0.004187384154647589\n",
      "epoch: 106, train_loss: 0.0007405020018189174, valid_loss: 0.0016288751891503732, test_loss: 0.004177413415163755\n",
      "epoch: 107, train_loss: 0.0007361585082000364, valid_loss: 0.0016366683897407104, test_loss: 0.004151840228587389\n",
      "epoch: 108, train_loss: 0.0007322649572692487, valid_loss: 0.0016169437246086698, test_loss: 0.004164292477071285\n",
      "epoch: 109, train_loss: 0.0007283952360486854, valid_loss: 0.001611150267611568, test_loss: 0.0041346438229084015\n",
      "epoch: 110, train_loss: 0.0007253896961312579, valid_loss: 0.001609953256168713, test_loss: 0.004121311940252781\n",
      "epoch: 111, train_loss: 0.0007224314739806173, valid_loss: 0.0016040980990510434, test_loss: 0.004107166547328234\n",
      "epoch: 112, train_loss: 0.0007176484897449288, valid_loss: 0.0015975365628643583, test_loss: 0.004101028200238943\n",
      "epoch: 113, train_loss: 0.0007147058545935737, valid_loss: 0.0015917734417598695, test_loss: 0.004093641880899668\n",
      "epoch: 114, train_loss: 0.0007105481547667929, valid_loss: 0.0015835197700653225, test_loss: 0.004086237400770187\n",
      "epoch: 115, train_loss: 0.0007072204987154058, valid_loss: 0.0015797575761098415, test_loss: 0.00409000413492322\n",
      "epoch: 116, train_loss: 0.000704467688869361, valid_loss: 0.001575714200347041, test_loss: 0.0040563540533185005\n",
      "epoch: 117, train_loss: 0.0007009355847359351, valid_loss: 0.0015700491688524683, test_loss: 0.0040685273706912994\n",
      "epoch: 118, train_loss: 0.0006976492029776716, valid_loss: 0.0015639872969283413, test_loss: 0.004042154643684626\n",
      "epoch: 119, train_loss: 0.0006940741871443132, valid_loss: 0.0015593717301574845, test_loss: 0.004030063282698393\n",
      "epoch: 120, train_loss: 0.0006903738957708297, valid_loss: 0.001553630824976911, test_loss: 0.0040153563022613525\n",
      "epoch: 121, train_loss: 0.000686940701637903, valid_loss: 0.0015527309054353584, test_loss: 0.004001534078270197\n",
      "epoch: 122, train_loss: 0.00068372631010235, valid_loss: 0.0015406524762511253, test_loss: 0.00400242954492569\n",
      "epoch: 123, train_loss: 0.0006800303336880777, valid_loss: 0.0015414220882424463, test_loss: 0.003985249903053045\n",
      "epoch: 124, train_loss: 0.000676426866962372, valid_loss: 0.001551701008186986, test_loss: 0.003977688029408455\n",
      "epoch: 125, train_loss: 0.0006744782971825613, valid_loss: 0.0015338211281535525, test_loss: 0.003957367967814207\n",
      "epoch: 126, train_loss: 0.0006711258244214822, valid_loss: 0.0015213224299562473, test_loss: 0.003956400789320469\n",
      "epoch: 127, train_loss: 0.0006669885630759856, valid_loss: 0.001523444176806758, test_loss: 0.00393511401489377\n",
      "epoch: 128, train_loss: 0.0006643661409211547, valid_loss: 0.0015158871926056843, test_loss: 0.0039454009383916855\n",
      "epoch: 129, train_loss: 0.0006613762487414415, valid_loss: 0.0015160983020905405, test_loss: 0.003916699439287186\n",
      "epoch: 130, train_loss: 0.000658086242923594, valid_loss: 0.001507782736249889, test_loss: 0.003909681458026171\n",
      "epoch: 131, train_loss: 0.0006548301671105235, valid_loss: 0.0014988774491939694, test_loss: 0.003903563367202878\n",
      "epoch: 132, train_loss: 0.0006518163262744961, valid_loss: 0.0014929432848778863, test_loss: 0.0039031945634633303\n",
      "epoch: 133, train_loss: 0.0006491574585316298, valid_loss: 0.001492273062467575, test_loss: 0.0038899502251297235\n",
      "epoch: 134, train_loss: 0.0006453242335144593, valid_loss: 0.0014843014747990917, test_loss: 0.003878462128341198\n",
      "epoch: 135, train_loss: 0.0006422684047087703, valid_loss: 0.0014828870286388944, test_loss: 0.0038624946027994156\n",
      "epoch: 136, train_loss: 0.0006395625254220289, valid_loss: 0.0014769209956284612, test_loss: 0.003856664290651679\n",
      "epoch: 137, train_loss: 0.0006368571522888606, valid_loss: 0.0014703898729446034, test_loss: 0.0038439661730080843\n",
      "epoch: 138, train_loss: 0.0006327139613542544, valid_loss: 0.0014677922396610181, test_loss: 0.0038281285669654608\n",
      "epoch: 139, train_loss: 0.0006296689819504062, valid_loss: 0.0014642235861780744, test_loss: 0.003823857521638274\n",
      "epoch: 140, train_loss: 0.0006273658718387394, valid_loss: 0.0014605475104569148, test_loss: 0.0038163422141224146\n",
      "epoch: 141, train_loss: 0.0006239460882447336, valid_loss: 0.0014513106628631551, test_loss: 0.0038081654347479343\n",
      "epoch: 142, train_loss: 0.0006202241132521759, valid_loss: 0.0014495856303256005, test_loss: 0.0037920144386589527\n",
      "epoch: 143, train_loss: 0.0006175315280890335, valid_loss: 0.0014477191240681957, test_loss: 0.0037928044330328703\n",
      "epoch: 144, train_loss: 0.0006149708516085925, valid_loss: 0.0014421513866788398, test_loss: 0.003772022435441613\n",
      "epoch: 145, train_loss: 0.000611897380820111, valid_loss: 0.0014369036071002483, test_loss: 0.003767322050407529\n",
      "epoch: 146, train_loss: 0.0006088382949161788, valid_loss: 0.0014314696309156716, test_loss: 0.003749461844563484\n",
      "epoch: 147, train_loss: 0.0006058809053881661, valid_loss: 0.0014286533987615258, test_loss: 0.0037414105609059334\n",
      "epoch: 148, train_loss: 0.0006031530236055994, valid_loss: 0.001422788870210449, test_loss: 0.0037412995006889105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 149, train_loss: 0.0006005652501936192, valid_loss: 0.0014219141157809645, test_loss: 0.0037342258729040623\n",
      "epoch: 150, train_loss: 0.0005972033528530079, valid_loss: 0.0014196165817944955, test_loss: 0.003721993649378419\n",
      "epoch: 151, train_loss: 0.0005943940157759125, valid_loss: 0.001410271836599956, test_loss: 0.0037032384425401688\n",
      "epoch: 152, train_loss: 0.0005917248781770468, valid_loss: 0.001403925222499917, test_loss: 0.0037048638332635164\n",
      "epoch: 153, train_loss: 0.0005888060980435947, valid_loss: 0.0013992389528236042, test_loss: 0.0036959440913051367\n",
      "epoch: 154, train_loss: 0.0005855385325444133, valid_loss: 0.0013995192615160097, test_loss: 0.0036814918275922537\n",
      "epoch: 155, train_loss: 0.0005832483951488267, valid_loss: 0.001391563118280222, test_loss: 0.0036716770846396685\n",
      "epoch: 156, train_loss: 0.0005803214937812933, valid_loss: 0.0013875936662467818, test_loss: 0.00366038060747087\n",
      "epoch: 157, train_loss: 0.0005771987944962862, valid_loss: 0.0013870065061685939, test_loss: 0.003657473484054208\n",
      "epoch: 158, train_loss: 0.0005748727277893087, valid_loss: 0.0013851210727201153, test_loss: 0.0036356367636471987\n",
      "epoch: 159, train_loss: 0.0005717572943388444, valid_loss: 0.001377464592223987, test_loss: 0.0036411138717085123\n",
      "epoch: 160, train_loss: 0.0005689372799521232, valid_loss: 0.001372532860841602, test_loss: 0.00362715357914567\n",
      "epoch: 161, train_loss: 0.0005652556289493551, valid_loss: 0.0013647904173315812, test_loss: 0.0036178450100123882\n",
      "epoch: 162, train_loss: 0.0005637465510517359, valid_loss: 0.001363432869159927, test_loss: 0.003610642859712243\n",
      "epoch: 163, train_loss: 0.0005606499503103449, valid_loss: 0.0013582211492272715, test_loss: 0.003602559445425868\n",
      "epoch: 164, train_loss: 0.0005571886785733311, valid_loss: 0.0013604082923848182, test_loss: 0.0035891071893274784\n",
      "epoch: 165, train_loss: 0.0005549772202199244, valid_loss: 0.0013536349773251761, test_loss: 0.00357625144533813\n",
      "epoch: 166, train_loss: 0.0005525067175293099, valid_loss: 0.001345272728940472, test_loss: 0.003572946647182107\n",
      "epoch: 167, train_loss: 0.0005495557523555244, valid_loss: 0.0013400208457217861, test_loss: 0.0035725997295230627\n",
      "epoch: 168, train_loss: 0.0005467727327334654, valid_loss: 0.0013381302608953167, test_loss: 0.0035558028612285852\n",
      "epoch: 169, train_loss: 0.0005439412311914012, valid_loss: 0.001331675312637041, test_loss: 0.0035512300673872232\n",
      "epoch: 170, train_loss: 0.0005415100515718856, valid_loss: 0.0013352450720655422, test_loss: 0.0035440553911030293\n",
      "epoch: 171, train_loss: 0.0005393072130138297, valid_loss: 0.0013242933006646733, test_loss: 0.0035298368893563747\n",
      "epoch: 172, train_loss: 0.0005362736612922796, valid_loss: 0.0013206508107638608, test_loss: 0.003524971893057227\n",
      "epoch: 173, train_loss: 0.0005330090340145904, valid_loss: 0.0013185775799987216, test_loss: 0.003510349662974477\n",
      "epoch: 174, train_loss: 0.0005311188009648544, valid_loss: 0.001313239277806133, test_loss: 0.0035013859160244465\n",
      "epoch: 175, train_loss: 0.000528330188345812, valid_loss: 0.0013081055755416553, test_loss: 0.003505238564684987\n",
      "epoch: 176, train_loss: 0.0005255511141908557, valid_loss: 0.0013049783495565255, test_loss: 0.003505451139062643\n",
      "epoch: 177, train_loss: 0.0005231875971810001, valid_loss: 0.0013028011308051646, test_loss: 0.0034815222024917603\n",
      "epoch: 178, train_loss: 0.0005201092268259305, valid_loss: 0.0012951116271627445, test_loss: 0.0034766581375151873\n",
      "epoch: 179, train_loss: 0.0005173401694501872, valid_loss: 0.0012913214159198105, test_loss: 0.00346631882712245\n",
      "epoch: 180, train_loss: 0.0005151980465439999, valid_loss: 0.0012911218800581992, test_loss: 0.0034761102870106697\n",
      "epoch: 181, train_loss: 0.0005123548414659402, valid_loss: 0.0012891657922106485, test_loss: 0.0034555874299257994\n",
      "epoch: 182, train_loss: 0.0005093089066972227, valid_loss: 0.0012814155585753422, test_loss: 0.0034357563126832247\n",
      "epoch: 183, train_loss: 0.0005071013431955615, valid_loss: 0.0012803234955451142, test_loss: 0.003430586075410247\n",
      "epoch: 184, train_loss: 0.0005044560631453667, valid_loss: 0.0012729479931294918, test_loss: 0.003427334362640977\n",
      "epoch: 185, train_loss: 0.0005016104310609238, valid_loss: 0.0012714133384482313, test_loss: 0.0034161482471972704\n",
      "epoch: 186, train_loss: 0.0004993086204985561, valid_loss: 0.0012647341257737328, test_loss: 0.0034093924332410097\n",
      "epoch: 187, train_loss: 0.0004972035530954599, valid_loss: 0.001261129043996334, test_loss: 0.0033959681168198586\n",
      "epoch: 188, train_loss: 0.0004943117661320645, valid_loss: 0.0012560273753479123, test_loss: 0.0033915650565177202\n",
      "epoch: 189, train_loss: 0.0004919697125644788, valid_loss: 0.001254029426490888, test_loss: 0.0033834741916507483\n",
      "epoch: 190, train_loss: 0.000488906714093426, valid_loss: 0.0012497484276536852, test_loss: 0.003381132148206234\n",
      "epoch: 191, train_loss: 0.00048668388753076613, valid_loss: 0.0012522065662778914, test_loss: 0.003365875920280814\n",
      "epoch: 192, train_loss: 0.0004841821969223573, valid_loss: 0.001243826807088529, test_loss: 0.003362825606018305\n",
      "epoch: 193, train_loss: 0.00048171455906096685, valid_loss: 0.0012370697416675587, test_loss: 0.0033526457846164703\n",
      "epoch: 194, train_loss: 0.0004793558808792706, valid_loss: 0.0012353880059284468, test_loss: 0.003347103949636221\n",
      "epoch: 195, train_loss: 0.0004765205211815951, valid_loss: 0.0012343114261360217, test_loss: 0.0033302020747214556\n",
      "epoch: 196, train_loss: 0.00047425283422774595, valid_loss: 0.0012282900279387832, test_loss: 0.00332053960300982\n",
      "epoch: 197, train_loss: 0.0004714162407807358, valid_loss: 0.001229184524466594, test_loss: 0.003326983656734228\n",
      "epoch: 198, train_loss: 0.00046940676047993094, valid_loss: 0.0012232487303360056, test_loss: 0.0033092519734054804\n",
      "epoch: 199, train_loss: 0.0004669055583097203, valid_loss: 0.0012168083145904045, test_loss: 0.003303890349343419\n",
      "epoch: 200, train_loss: 0.00046455920623291445, valid_loss: 0.001213057985296473, test_loss: 0.003291268600150943\n",
      "epoch: 201, train_loss: 0.0004617664001315184, valid_loss: 0.001208059712856387, test_loss: 0.003291812026873231\n",
      "epoch: 202, train_loss: 0.0004593272549201451, valid_loss: 0.0012053280467322718, test_loss: 0.0032794601283967495\n",
      "epoch: 203, train_loss: 0.0004568060040069015, valid_loss: 0.0012019877225005378, test_loss: 0.003271831199526787\n",
      "epoch: 204, train_loss: 0.00045431786312964624, valid_loss: 0.001197839427428941, test_loss: 0.0032587547320872545\n",
      "epoch: 205, train_loss: 0.00045225754476395315, valid_loss: 0.0011944799528767665, test_loss: 0.003260975005105138\n",
      "epoch: 206, train_loss: 0.00044968964547202313, valid_loss: 0.0011931777698919177, test_loss: 0.003252866677939892\n",
      "epoch: 207, train_loss: 0.0004476315415282126, valid_loss: 0.0011873734280622255, test_loss: 0.003241102211177349\n",
      "epoch: 208, train_loss: 0.00044488189387661606, valid_loss: 0.0011834377752772223, test_loss: 0.0032345657236874104\n",
      "epoch: 209, train_loss: 0.00044289245599192446, valid_loss: 0.0011799256026279181, test_loss: 0.0032298138830810785\n",
      "epoch: 210, train_loss: 0.0004403340192410447, valid_loss: 0.00117905464139767, test_loss: 0.003226376138627529\n",
      "epoch: 211, train_loss: 0.0004379618229627933, valid_loss: 0.0011722012519991647, test_loss: 0.0032133678905665874\n",
      "epoch: 212, train_loss: 0.00043560481888404036, valid_loss: 0.0011682757370484371, test_loss: 0.0032010257709771395\n",
      "epoch: 213, train_loss: 0.0004333455302833539, valid_loss: 0.0011647229936594765, test_loss: 0.003201450454071164\n",
      "epoch: 214, train_loss: 0.00043114808985315585, valid_loss: 0.0011638009940118839, test_loss: 0.0031905092764645815\n",
      "epoch: 215, train_loss: 0.00042843526867015856, valid_loss: 0.0011602785380091518, test_loss: 0.003172538708895445\n",
      "epoch: 216, train_loss: 0.0004272487890177771, valid_loss: 0.001157008179385836, test_loss: 0.0031784698367118835\n",
      "epoch: 217, train_loss: 0.0004240716820198071, valid_loss: 0.0011508578318171203, test_loss: 0.003164143767207861\n",
      "epoch: 218, train_loss: 0.0004217258917734675, valid_loss: 0.0011481803376227617, test_loss: 0.003159832675009966\n",
      "epoch: 219, train_loss: 0.00041913398814833033, valid_loss: 0.0011479314222621422, test_loss: 0.003153124125674367\n",
      "epoch: 220, train_loss: 0.0004169419547036776, valid_loss: 0.0011461300988836836, test_loss: 0.003138195490464568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 221, train_loss: 0.0004147523077731223, valid_loss: 0.0011390305298846215, test_loss: 0.0031379994470626116\n",
      "epoch: 222, train_loss: 0.00041219571215586495, valid_loss: 0.0011350557712527614, test_loss: 0.003132504178211093\n",
      "epoch: 223, train_loss: 0.0004101777550480936, valid_loss: 0.0011336874740663916, test_loss: 0.0031239227391779423\n",
      "epoch: 224, train_loss: 0.0004077696345199871, valid_loss: 0.0011287210412168254, test_loss: 0.0031125799287110567\n",
      "epoch: 225, train_loss: 0.00040557289890863973, valid_loss: 0.0011276014071578782, test_loss: 0.003099881811067462\n",
      "epoch: 226, train_loss: 0.0004032944966836468, valid_loss: 0.0011236413071552913, test_loss: 0.0030988268554210663\n",
      "epoch: 227, train_loss: 0.00040117950613974875, valid_loss: 0.0011175321463573102, test_loss: 0.0030964056495577097\n",
      "epoch: 228, train_loss: 0.00039875078895736647, valid_loss: 0.0011146407535610099, test_loss: 0.0030844591092318296\n",
      "epoch: 229, train_loss: 0.00039660600494876826, valid_loss: 0.0011132447058722998, test_loss: 0.003074603620916605\n",
      "epoch: 230, train_loss: 0.00039455439242453355, valid_loss: 0.0011078306803635012, test_loss: 0.0030703339725732803\n",
      "epoch: 231, train_loss: 0.0003921831869419016, valid_loss: 0.0011048933132163559, test_loss: 0.0030637478921562433\n",
      "epoch: 232, train_loss: 0.00038993652434209764, valid_loss: 0.0011000616796081886, test_loss: 0.0030542623717337847\n",
      "epoch: 233, train_loss: 0.0003881607212267978, valid_loss: 0.0010964081545049946, test_loss: 0.0030504537280648947\n",
      "epoch: 234, train_loss: 0.00038584348057275235, valid_loss: 0.0010936342393203329, test_loss: 0.0030471463687717915\n",
      "epoch: 235, train_loss: 0.00038363951582299626, valid_loss: 0.0010952847175455342, test_loss: 0.003033121582120657\n",
      "epoch: 236, train_loss: 0.00038160785334184766, valid_loss: 0.0010935052220399182, test_loss: 0.00303240236826241\n",
      "epoch: 237, train_loss: 0.00037973631760510415, valid_loss: 0.001083906822411033, test_loss: 0.003023819997906685\n",
      "epoch: 238, train_loss: 0.00037761285911192715, valid_loss: 0.001081159760360606, test_loss: 0.00301588443107903\n",
      "epoch: 239, train_loss: 0.0003752336095836337, valid_loss: 0.0010764835266551624, test_loss: 0.0030130096711218357\n",
      "epoch: 240, train_loss: 0.0003731167994176402, valid_loss: 0.0010752560240992655, test_loss: 0.00299762818031013\n",
      "epoch: 241, train_loss: 0.00037151454117796993, valid_loss: 0.0010716407171760995, test_loss: 0.002997332252562046\n",
      "epoch: 242, train_loss: 0.000368801089833774, valid_loss: 0.0010681250132620335, test_loss: 0.002989366417750716\n",
      "epoch: 243, train_loss: 0.00036743185331073147, valid_loss: 0.0010659526791035507, test_loss: 0.0029811139684170485\n",
      "epoch: 244, train_loss: 0.00036479973831497455, valid_loss: 0.0010610200455024217, test_loss: 0.0029780587647110224\n",
      "epoch: 245, train_loss: 0.00036267194967797917, valid_loss: 0.001059322103780384, test_loss: 0.0029679089784622192\n",
      "epoch: 246, train_loss: 0.00036081979076540017, valid_loss: 0.0010576116377099727, test_loss: 0.0029648703057318926\n",
      "epoch: 247, train_loss: 0.00035877539725650263, valid_loss: 0.0010531530570005998, test_loss: 0.002959990408271551\n",
      "epoch: 248, train_loss: 0.00035667425760513413, valid_loss: 0.0010504579743913685, test_loss: 0.00295281526632607\n",
      "epoch: 249, train_loss: 0.000354561739621441, valid_loss: 0.0010473342748203625, test_loss: 0.0029440801590681076\n",
      "epoch: 250, train_loss: 0.0003526357265756182, valid_loss: 0.0010432911755439516, test_loss: 0.0029341045301407576\n",
      "epoch: 251, train_loss: 0.0003506052434322951, valid_loss: 0.0010433185525471345, test_loss: 0.0029299354646354914\n",
      "epoch: 252, train_loss: 0.00034895861070886576, valid_loss: 0.0010366102505940944, test_loss: 0.00292897317558527\n",
      "epoch: 253, train_loss: 0.0003466443538301341, valid_loss: 0.0010353034352495645, test_loss: 0.002922008978202939\n",
      "epoch: 254, train_loss: 0.0003445054158953059, valid_loss: 0.0010311423781483124, test_loss: 0.0029092186596244574\n",
      "epoch: 255, train_loss: 0.0003432989937921419, valid_loss: 0.0010323018747537087, test_loss: 0.0028998074121773243\n",
      "epoch: 256, train_loss: 0.00034076352502502823, valid_loss: 0.0010252838643888633, test_loss: 0.0028997755143791437\n",
      "epoch: 257, train_loss: 0.00033886538838724727, valid_loss: 0.0010244019164626177, test_loss: 0.0028964057564735413\n",
      "epoch: 258, train_loss: 0.00033710853566172654, valid_loss: 0.0010197547477825235, test_loss: 0.002890456933528185\n",
      "epoch: 259, train_loss: 0.0003356031071313697, valid_loss: 0.0010175296095743154, test_loss: 0.0028812268283218145\n",
      "epoch: 260, train_loss: 0.00033358480330840075, valid_loss: 0.0010161775717278942, test_loss: 0.0028732973150908947\n",
      "epoch: 261, train_loss: 0.0003317047123108869, valid_loss: 0.0010117502533830702, test_loss: 0.0028679370880126953\n",
      "epoch: 262, train_loss: 0.00032981394906528294, valid_loss: 0.0010111275332747027, test_loss: 0.0028668451122939587\n",
      "epoch: 263, train_loss: 0.00032797665461776376, valid_loss: 0.0010047337758199622, test_loss: 0.0028519947081804276\n",
      "epoch: 264, train_loss: 0.000326090922538677, valid_loss: 0.0010050253816492234, test_loss: 0.0028533556032925844\n",
      "epoch: 265, train_loss: 0.0003247989212547469, valid_loss: 0.0009997547410118084, test_loss: 0.002846943447366357\n",
      "epoch: 266, train_loss: 0.0003224058512031384, valid_loss: 0.0009977069566957653, test_loss: 0.0028350395150482655\n",
      "epoch: 267, train_loss: 0.0003209127791468864, valid_loss: 0.0009946094990785543, test_loss: 0.0028313782531768084\n",
      "epoch: 268, train_loss: 0.00031935779818171716, valid_loss: 0.0009938595661272605, test_loss: 0.002830303506925702\n",
      "epoch: 269, train_loss: 0.0003174977837656827, valid_loss: 0.0009920606001590688, test_loss: 0.0028260720428079367\n",
      "epoch: 270, train_loss: 0.00031567847801615363, valid_loss: 0.0009866659092949703, test_loss: 0.0028106574900448322\n",
      "epoch: 271, train_loss: 0.0003144740008080945, valid_loss: 0.000984303905473401, test_loss: 0.0028102505020797253\n",
      "epoch: 272, train_loss: 0.000312264311461426, valid_loss: 0.0009846213700560231, test_loss: 0.002806201111525297\n",
      "epoch: 273, train_loss: 0.00031055301888440937, valid_loss: 0.0009790357968692358, test_loss: 0.002796475775539875\n",
      "epoch: 274, train_loss: 0.0003083822755244277, valid_loss: 0.0009786883698931585, test_loss: 0.0027895865496248007\n",
      "epoch: 275, train_loss: 0.0003069068093875261, valid_loss: 0.000977274122609136, test_loss: 0.0027882056310772896\n",
      "epoch: 276, train_loss: 0.0003053578927748553, valid_loss: 0.000968973928441604, test_loss: 0.0027837224770337343\n",
      "epoch: 277, train_loss: 0.0003034974215552211, valid_loss: 0.0009684649315507462, test_loss: 0.0027722108643501997\n",
      "epoch: 278, train_loss: 0.00030206434135122794, valid_loss: 0.0009684951898331443, test_loss: 0.0027673617005348206\n",
      "epoch: 279, train_loss: 0.00030017745144584256, valid_loss: 0.0009630665639027333, test_loss: 0.0027634173166006804\n",
      "epoch: 280, train_loss: 0.0002987739470605131, valid_loss: 0.0009603262345384186, test_loss: 0.002761754672974348\n",
      "epoch: 281, train_loss: 0.0002970249447769121, valid_loss: 0.0009579122755288457, test_loss: 0.0027521997690200806\n",
      "epoch: 282, train_loss: 0.000295319186989218, valid_loss: 0.000955326958016182, test_loss: 0.0027492144145071507\n",
      "epoch: 283, train_loss: 0.00029410228118016994, valid_loss: 0.0009548944605436797, test_loss: 0.002743388991802931\n",
      "epoch: 284, train_loss: 0.0002924203929622941, valid_loss: 0.0009515002505698552, test_loss: 0.0027366913855075836\n",
      "epoch: 285, train_loss: 0.0002911517892599754, valid_loss: 0.0009483889007242396, test_loss: 0.0027328431606292725\n",
      "epoch: 286, train_loss: 0.0002896536733833668, valid_loss: 0.0009484276282213008, test_loss: 0.002734746318310499\n",
      "epoch: 287, train_loss: 0.00028782234901724303, valid_loss: 0.0009423214505659416, test_loss: 0.002723111305385828\n",
      "epoch: 288, train_loss: 0.0002864837872988099, valid_loss: 0.0009426080650882795, test_loss: 0.0027147172950208187\n",
      "epoch: 289, train_loss: 0.0002847716810799244, valid_loss: 0.0009391990315634757, test_loss: 0.0027134825941175222\n",
      "epoch: 290, train_loss: 0.0002834932871021168, valid_loss: 0.0009358412789879367, test_loss: 0.0027067221235483885\n",
      "epoch: 291, train_loss: 0.000282117719863258, valid_loss: 0.0009346481917115549, test_loss: 0.002702557248994708\n",
      "epoch: 292, train_loss: 0.00028077409660909325, valid_loss: 0.0009317813334443296, test_loss: 0.00269494391977787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 293, train_loss: 0.0002790835025497591, valid_loss: 0.0009286265121772885, test_loss: 0.002691777190193534\n",
      "epoch: 294, train_loss: 0.00027755353949032724, valid_loss: 0.0009294383926317096, test_loss: 0.0026852358132600784\n",
      "epoch: 295, train_loss: 0.0002763601578012838, valid_loss: 0.0009234264628806462, test_loss: 0.0026857363991439342\n",
      "epoch: 296, train_loss: 0.0002754443020144801, valid_loss: 0.0009252752352040261, test_loss: 0.0026820714119821787\n",
      "epoch: 297, train_loss: 0.0002738507496162682, valid_loss: 0.0009226883266819641, test_loss: 0.0026733765844255686\n",
      "epoch: 298, train_loss: 0.00027267843403893966, valid_loss: 0.0009176083209846789, test_loss: 0.0026721993926912546\n",
      "epoch: 299, train_loss: 0.00027124556641944724, valid_loss: 0.0009169371624011546, test_loss: 0.0026637970004230738\n",
      "epoch: 300, train_loss: 0.00026968199982429326, valid_loss: 0.0009168467125467336, test_loss: 0.0026638717390596867\n",
      "epoch: 301, train_loss: 0.0002684322332598917, valid_loss: 0.0009111613277733946, test_loss: 0.002656001364812255\n",
      "epoch: 302, train_loss: 0.000266915129031986, valid_loss: 0.0009109290792063499, test_loss: 0.0026550546754151583\n",
      "epoch: 303, train_loss: 0.00026588078712254924, valid_loss: 0.0009095431984557459, test_loss: 0.0026447216514497995\n",
      "epoch: 304, train_loss: 0.00026470643997131646, valid_loss: 0.0009111300605582073, test_loss: 0.0026408066041767597\n",
      "epoch: 305, train_loss: 0.0002632174589013195, valid_loss: 0.000905739672210378, test_loss: 0.0026377441827207804\n",
      "epoch: 306, train_loss: 0.0002620460025196814, valid_loss: 0.0009037991258082911, test_loss: 0.0026331248227506876\n",
      "epoch: 307, train_loss: 0.0002611600768590427, valid_loss: 0.0009017581226847445, test_loss: 0.0026281706523150206\n",
      "epoch: 308, train_loss: 0.0002595998274678688, valid_loss: 0.0008985817548818886, test_loss: 0.002624901244416833\n",
      "epoch: 309, train_loss: 0.00025841791964763695, valid_loss: 0.0008968221712469434, test_loss: 0.0026180450804531574\n",
      "epoch: 310, train_loss: 0.0002570131327956915, valid_loss: 0.0008942588901845738, test_loss: 0.002615944016724825\n",
      "epoch: 311, train_loss: 0.0002560739730656876, valid_loss: 0.0008953646077619245, test_loss: 0.002615637145936489\n",
      "epoch: 312, train_loss: 0.00025504405114232844, valid_loss: 0.0008895103965187445, test_loss: 0.002607592148706317\n",
      "epoch: 313, train_loss: 0.0002537470395707161, valid_loss: 0.000890508687007241, test_loss: 0.002609007526189089\n",
      "epoch: 314, train_loss: 0.00025279573215495634, valid_loss: 0.0008874534881518533, test_loss: 0.002599736675620079\n",
      "epoch: 315, train_loss: 0.0002517120016779265, valid_loss: 0.000883272480374823, test_loss: 0.002594433957710862\n",
      "epoch: 316, train_loss: 0.0002504112319160334, valid_loss: 0.000885755080768528, test_loss: 0.0025896658189594746\n",
      "epoch: 317, train_loss: 0.0002495941836336304, valid_loss: 0.0008821368828648701, test_loss: 0.0025887941010296345\n",
      "epoch: 318, train_loss: 0.000248334662857182, valid_loss: 0.0008801942506882673, test_loss: 0.002586155664175749\n",
      "epoch: 319, train_loss: 0.00024708050541053325, valid_loss: 0.0008785150178785747, test_loss: 0.002579769818112254\n",
      "epoch: 320, train_loss: 0.0002460029145763458, valid_loss: 0.0008767313653758416, test_loss: 0.002574947662651539\n",
      "epoch: 321, train_loss: 0.0002452865503627159, valid_loss: 0.0008735099011876931, test_loss: 0.0025715583469718695\n",
      "epoch: 322, train_loss: 0.00024403889129525456, valid_loss: 0.0008730188662108654, test_loss: 0.0025711182970553637\n",
      "epoch: 323, train_loss: 0.000243532435295334, valid_loss: 0.0008712562994332984, test_loss: 0.0025640763342380524\n",
      "epoch: 324, train_loss: 0.0002420808406263266, valid_loss: 0.0008710794354556128, test_loss: 0.00256286165677011\n",
      "epoch: 325, train_loss: 0.0002414004506452171, valid_loss: 0.00086898248991929, test_loss: 0.002555527724325657\n",
      "epoch: 326, train_loss: 0.0002405202413564953, valid_loss: 0.0008682717743795365, test_loss: 0.002553460421040654\n",
      "epoch: 327, train_loss: 0.00023927609183137184, valid_loss: 0.0008649110386613756, test_loss: 0.0025500364135950804\n",
      "epoch: 328, train_loss: 0.00023854049884349755, valid_loss: 0.0008622061965676645, test_loss: 0.002549814758822322\n",
      "epoch: 329, train_loss: 0.00023810944833752254, valid_loss: 0.0008603605141009515, test_loss: 0.0025447786320000887\n",
      "epoch: 330, train_loss: 0.00023639892637932107, valid_loss: 0.0008612406430377936, test_loss: 0.002542054047808051\n",
      "epoch: 331, train_loss: 0.0002362356064385613, valid_loss: 0.000857234001159668, test_loss: 0.0025387927889823914\n",
      "epoch: 332, train_loss: 0.00023501741246866953, valid_loss: 0.0008596596017014235, test_loss: 0.002533975290134549\n",
      "epoch: 333, train_loss: 0.00023394455986461887, valid_loss: 0.0008571603733192509, test_loss: 0.002532794140279293\n",
      "epoch: 334, train_loss: 0.0002333115041002874, valid_loss: 0.0008537649458351856, test_loss: 0.002526183146983385\n",
      "epoch: 335, train_loss: 0.0002318955976870316, valid_loss: 0.0008522317705986401, test_loss: 0.0025254327338188887\n",
      "epoch: 336, train_loss: 0.0002313027545587038, valid_loss: 0.0008519651019014418, test_loss: 0.0025217661168426275\n",
      "epoch: 337, train_loss: 0.00023038807937510958, valid_loss: 0.0008495974713393176, test_loss: 0.0025174336042255163\n",
      "epoch: 338, train_loss: 0.00022981950314715505, valid_loss: 0.0008463835595951726, test_loss: 0.002512809820473194\n",
      "epoch: 339, train_loss: 0.0002293100690661484, valid_loss: 0.000846003526627707, test_loss: 0.002510571852326393\n",
      "epoch: 340, train_loss: 0.00022817381756359956, valid_loss: 0.000845102253758038, test_loss: 0.0025086975656449795\n",
      "epoch: 341, train_loss: 0.00022755510872229934, valid_loss: 0.0008427148374418417, test_loss: 0.0025032279081642628\n",
      "epoch: 342, train_loss: 0.00022657233120812833, valid_loss: 0.0008427041029790416, test_loss: 0.0025019817985594273\n",
      "epoch: 343, train_loss: 0.0002257302934911264, valid_loss: 0.0008408939756918699, test_loss: 0.0024977868888527155\n",
      "epoch: 344, train_loss: 0.00022489578101237345, valid_loss: 0.0008420194644713774, test_loss: 0.0024945377372205257\n",
      "epoch: 345, train_loss: 0.00022462465478937426, valid_loss: 0.0008379816669427479, test_loss: 0.002490643411874771\n",
      "epoch: 346, train_loss: 0.0002238137531094253, valid_loss: 0.0008380222037279358, test_loss: 0.0024858450051397085\n",
      "epoch: 347, train_loss: 0.00022283196639087376, valid_loss: 0.0008395361655857414, test_loss: 0.002491722349077463\n",
      "epoch: 348, train_loss: 0.0002220232525597448, valid_loss: 0.0008326661676013222, test_loss: 0.002482987241819501\n",
      "epoch: 349, train_loss: 0.0002212031787443582, valid_loss: 0.0008332722936756909, test_loss: 0.0024832014460116625\n",
      "epoch: 350, train_loss: 0.0002206534965965978, valid_loss: 0.0008312523665760333, test_loss: 0.0024799704551696777\n",
      "epoch: 351, train_loss: 0.00022018998013505632, valid_loss: 0.0008318710412519673, test_loss: 0.002474255161359906\n",
      "epoch: 352, train_loss: 0.0002195134925979959, valid_loss: 0.0008308330579893664, test_loss: 0.0024693869054317474\n",
      "epoch: 353, train_loss: 0.0002187007263748218, valid_loss: 0.0008269773097708821, test_loss: 0.0024656623136252165\n",
      "epoch: 354, train_loss: 0.00021804826151903558, valid_loss: 0.0008256372578519707, test_loss: 0.0024693040177226067\n",
      "epoch: 355, train_loss: 0.0002176225319524984, valid_loss: 0.0008262423749935502, test_loss: 0.0024632762651890516\n",
      "epoch: 356, train_loss: 0.00021683793254297873, valid_loss: 0.0008234577884043878, test_loss: 0.0024595940485596657\n",
      "epoch: 357, train_loss: 0.00021636521863832098, valid_loss: 0.0008254133038766062, test_loss: 0.0024592557456344366\n",
      "epoch: 358, train_loss: 0.0002159119859520022, valid_loss: 0.0008260475588031113, test_loss: 0.002456651534885168\n",
      "epoch: 359, train_loss: 0.00021478897226346737, valid_loss: 0.0008186096141192442, test_loss: 0.002450467087328434\n",
      "epoch: 360, train_loss: 0.00021443754540877822, valid_loss: 0.0008249230110474551, test_loss: 0.0024531728122383356\n",
      "epoch: 361, train_loss: 0.00021394532877425462, valid_loss: 0.0008241823573674386, test_loss: 0.0024492633529007435\n",
      "epoch: 362, train_loss: 0.00021350318970887557, valid_loss: 0.0008170131671552857, test_loss: 0.002443175297230482\n",
      "epoch: 363, train_loss: 0.00021308474242687225, valid_loss: 0.0008217089101284122, test_loss: 0.0024427229072898626\n",
      "epoch: 364, train_loss: 0.00021227268038985684, valid_loss: 0.0008139765850501135, test_loss: 0.002438556170091033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 365, train_loss: 0.00021159515840143126, valid_loss: 0.0008162573067238554, test_loss: 0.002435247180983424\n",
      "epoch: 366, train_loss: 0.00021098214758398092, valid_loss: 0.0008138124782514448, test_loss: 0.0024350571911782026\n",
      "epoch: 367, train_loss: 0.00021047997427598128, valid_loss: 0.0008107933099381626, test_loss: 0.002435122849419713\n",
      "epoch: 368, train_loss: 0.00021033042297779542, valid_loss: 0.0008158721369303142, test_loss: 0.0024319614749401808\n",
      "epoch: 369, train_loss: 0.0002098717295271142, valid_loss: 0.0008095072698779404, test_loss: 0.0024266806431114674\n",
      "epoch: 370, train_loss: 0.0002090051882829436, valid_loss: 0.0008083739958237857, test_loss: 0.0024258375633507967\n",
      "epoch: 371, train_loss: 0.00020833913004025817, valid_loss: 0.0008086253171010563, test_loss: 0.00242503616027534\n",
      "epoch: 372, train_loss: 0.0002083054100892142, valid_loss: 0.0008052682048097873, test_loss: 0.0024189380928874016\n",
      "epoch: 373, train_loss: 0.00020771700782579896, valid_loss: 0.000805256839763994, test_loss: 0.0024194319266825914\n",
      "epoch: 374, train_loss: 0.0002069582184523587, valid_loss: 0.00080457104195375, test_loss: 0.0024154444690793753\n",
      "epoch: 375, train_loss: 0.00020668669300072867, valid_loss: 0.0008061046100920066, test_loss: 0.0024141750764101744\n",
      "epoch: 376, train_loss: 0.00020619219234820616, valid_loss: 0.0008038271577485526, test_loss: 0.0024128269869834185\n",
      "epoch: 377, train_loss: 0.00020557663178213102, valid_loss: 0.0008024776228315508, test_loss: 0.0024110914673656225\n",
      "epoch: 378, train_loss: 0.00020532449580850485, valid_loss: 0.0008018751395866275, test_loss: 0.0024090285878628492\n",
      "epoch: 379, train_loss: 0.00020486122546413833, valid_loss: 0.000800221828588595, test_loss: 0.0024035789538174868\n",
      "epoch: 380, train_loss: 0.0002043164276983589, valid_loss: 0.0008028112303387994, test_loss: 0.0024054148234426975\n",
      "epoch: 381, train_loss: 0.00020365382395381027, valid_loss: 0.0007980977146265408, test_loss: 0.0024010364431887865\n",
      "epoch: 382, train_loss: 0.00020388338864153332, valid_loss: 0.0007966708993383994, test_loss: 0.002397318836301565\n",
      "epoch: 383, train_loss: 0.00020330853864003944, valid_loss: 0.0007976532069733366, test_loss: 0.0023967232555150986\n",
      "epoch: 384, train_loss: 0.0002028305607382208, valid_loss: 0.0007975844103687754, test_loss: 0.0023969351314008236\n",
      "epoch: 385, train_loss: 0.0002023693061504594, valid_loss: 0.000795447442214936, test_loss: 0.0023960245307534933\n",
      "epoch: 386, train_loss: 0.00020194736261264947, valid_loss: 0.0007935484172776341, test_loss: 0.0023910931777209044\n",
      "epoch: 387, train_loss: 0.0002015235773085252, valid_loss: 0.0007929032775185382, test_loss: 0.0023871595039963722\n",
      "epoch: 388, train_loss: 0.00020115781238849235, valid_loss: 0.0007917957021466767, test_loss: 0.0023868130519986153\n",
      "epoch: 389, train_loss: 0.00020070713957386982, valid_loss: 0.0007915299453694994, test_loss: 0.002384184394031763\n",
      "epoch: 390, train_loss: 0.00020048790548057497, valid_loss: 0.00079075660808788, test_loss: 0.002380277495831251\n",
      "epoch: 391, train_loss: 0.0001998705430077794, valid_loss: 0.0007906892472722878, test_loss: 0.002382077509537339\n",
      "epoch: 392, train_loss: 0.00019946486534773493, valid_loss: 0.0007901823313053077, test_loss: 0.0023789559490978718\n",
      "epoch: 393, train_loss: 0.00019921006574093, valid_loss: 0.000788747493061237, test_loss: 0.002376815304160118\n",
      "epoch: 394, train_loss: 0.00019892694810709065, valid_loss: 0.000785075380311658, test_loss: 0.002375803655013442\n",
      "epoch: 395, train_loss: 0.0001988809811376521, valid_loss: 0.0007850923138903454, test_loss: 0.0023733661510050297\n",
      "epoch: 396, train_loss: 0.00019854122934781986, valid_loss: 0.0007840625767130405, test_loss: 0.0023694171104580164\n",
      "epoch: 397, train_loss: 0.0001978207551671759, valid_loss: 0.0007850444283879673, test_loss: 0.0023673372343182564\n",
      "epoch: 398, train_loss: 0.00019760660412113953, valid_loss: 0.0007838518843830874, test_loss: 0.0023689311929047108\n",
      "epoch: 399, train_loss: 0.00019751045139223012, valid_loss: 0.0007837720962318903, test_loss: 0.002365510445088148\n",
      "epoch: 400, train_loss: 0.00019700268888339886, valid_loss: 0.0007838116580387577, test_loss: 0.0023654436226934195\n",
      "epoch: 401, train_loss: 0.00019706233212238422, valid_loss: 0.0007792497587312633, test_loss: 0.002363551640883088\n",
      "epoch: 402, train_loss: 0.00019632270635591576, valid_loss: 0.0007822380527310694, test_loss: 0.002361409366130829\n",
      "epoch: 403, train_loss: 0.00019609700678341576, valid_loss: 0.000779645381650577, test_loss: 0.002358571859076619\n",
      "epoch: 404, train_loss: 0.00019585450038151896, valid_loss: 0.0007808533215817685, test_loss: 0.0023588205222040415\n",
      "epoch: 405, train_loss: 0.00019552614021054268, valid_loss: 0.0007777604429672161, test_loss: 0.0023542791604995728\n",
      "epoch: 406, train_loss: 0.00019539364692046428, valid_loss: 0.0007758710756509876, test_loss: 0.002352286595851183\n",
      "epoch: 407, train_loss: 0.00019510120910126716, valid_loss: 0.0007770295584729562, test_loss: 0.0023499561939388514\n",
      "epoch: 408, train_loss: 0.0001946083294055627, valid_loss: 0.0007781325839459896, test_loss: 0.0023494595661759377\n",
      "epoch: 409, train_loss: 0.00019430977282235804, valid_loss: 0.0007777514595848819, test_loss: 0.0023505862336605787\n",
      "epoch: 410, train_loss: 0.00019403140354172692, valid_loss: 0.0007740850851405412, test_loss: 0.002346278168261051\n",
      "epoch: 411, train_loss: 0.00019366324198452512, valid_loss: 0.0007731394968383635, test_loss: 0.00234416825696826\n",
      "epoch: 412, train_loss: 0.0001936236715308674, valid_loss: 0.0007726916956016794, test_loss: 0.002342349849641323\n",
      "epoch: 413, train_loss: 0.0001934925651015795, valid_loss: 0.0007733405169953281, test_loss: 0.002341892570257187\n",
      "epoch: 414, train_loss: 0.00019309847266413271, valid_loss: 0.0007734766113571823, test_loss: 0.002338198246434331\n",
      "epoch: 415, train_loss: 0.00019267061915066418, valid_loss: 0.0007722441660007462, test_loss: 0.0023373658768832684\n",
      "epoch: 416, train_loss: 0.00019251399435629338, valid_loss: 0.0007699192259072637, test_loss: 0.0023372231516987085\n",
      "epoch: 417, train_loss: 0.00019234445631888735, valid_loss: 0.0007713983310774589, test_loss: 0.0023333674762398005\n",
      "epoch: 418, train_loss: 0.00019197170078531718, valid_loss: 0.0007694318725649888, test_loss: 0.002332081086933613\n",
      "epoch: 419, train_loss: 0.00019185489572523892, valid_loss: 0.0007691045029787347, test_loss: 0.0023307802621275187\n",
      "epoch: 420, train_loss: 0.00019174076315602693, valid_loss: 0.0007711043726885691, test_loss: 0.0023301136679947376\n",
      "epoch: 421, train_loss: 0.00019108117798485023, valid_loss: 0.0007684061953720326, test_loss: 0.0023284005001187325\n",
      "epoch: 422, train_loss: 0.00019110751758410555, valid_loss: 0.000768679844137902, test_loss: 0.0023253175895661116\n",
      "epoch: 423, train_loss: 0.00019088434941216332, valid_loss: 0.0007660554207783813, test_loss: 0.0023244181647896767\n",
      "epoch: 424, train_loss: 0.00019090384897856933, valid_loss: 0.0007644009359258538, test_loss: 0.0023227857891470194\n",
      "epoch: 425, train_loss: 0.00019063483657943004, valid_loss: 0.0007643798065449422, test_loss: 0.0023208113852888346\n",
      "epoch: 426, train_loss: 0.0001900757216271418, valid_loss: 0.0007635226793354377, test_loss: 0.0023211368825286627\n",
      "epoch: 427, train_loss: 0.00019045618081809548, valid_loss: 0.0007663743162993342, test_loss: 0.0023229022044688463\n",
      "epoch: 428, train_loss: 0.00018972264091565233, valid_loss: 0.0007630227385864904, test_loss: 0.0023185396566987038\n",
      "epoch: 429, train_loss: 0.00018948548961855957, valid_loss: 0.0007652586306600521, test_loss: 0.0023165694437921047\n",
      "epoch: 430, train_loss: 0.00018940596909606424, valid_loss: 0.0007626580093832066, test_loss: 0.002314362209290266\n",
      "epoch: 431, train_loss: 0.00018911591209673688, valid_loss: 0.0007597022098101055, test_loss: 0.00231233355589211\n",
      "epoch: 432, train_loss: 0.00018882896541111657, valid_loss: 0.0007612660944384212, test_loss: 0.0023115472868084908\n",
      "epoch: 433, train_loss: 0.00018876169507047567, valid_loss: 0.000758291338570416, test_loss: 0.0023095582146197557\n",
      "epoch: 434, train_loss: 0.00018856327780826098, valid_loss: 0.0007589552114950493, test_loss: 0.0023079016245901585\n",
      "epoch: 435, train_loss: 0.0001882792351787667, valid_loss: 0.0007589442878573512, test_loss: 0.0023053071927279234\n",
      "epoch: 436, train_loss: 0.00018799801435544276, valid_loss: 0.0007567745148359487, test_loss: 0.002306800102815032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 437, train_loss: 0.00018813904329284054, valid_loss: 0.0007568823493784294, test_loss: 0.0023045267444103956\n",
      "epoch: 438, train_loss: 0.00018813687189401168, valid_loss: 0.0007592459296574816, test_loss: 0.002303862478584051\n",
      "epoch: 439, train_loss: 0.00018738093849210799, valid_loss: 0.0007574758395397415, test_loss: 0.002301733708009124\n",
      "epoch: 440, train_loss: 0.00018742692634038141, valid_loss: 0.000758892337519986, test_loss: 0.0023026741109788418\n",
      "epoch: 441, train_loss: 0.00018741777508377868, valid_loss: 0.0007547214772785082, test_loss: 0.0022970642894506454\n",
      "epoch: 442, train_loss: 0.0001870440648180311, valid_loss: 0.0007553315760257343, test_loss: 0.0022971287835389376\n",
      "epoch: 443, train_loss: 0.0001870043925009668, valid_loss: 0.000757109218587478, test_loss: 0.0022951082792133093\n",
      "epoch: 444, train_loss: 0.0001866573603246766, valid_loss: 0.0007525966745257998, test_loss: 0.002296639606356621\n",
      "epoch: 445, train_loss: 0.00018656229835671738, valid_loss: 0.0007570574768275643, test_loss: 0.002292993478477001\n",
      "epoch: 446, train_loss: 0.00018617585389976105, valid_loss: 0.0007526880411508804, test_loss: 0.0022897461894899607\n",
      "epoch: 447, train_loss: 0.0001863972290226704, valid_loss: 0.0007509958328834424, test_loss: 0.002289159456267953\n",
      "epoch: 448, train_loss: 0.00018622522728274697, valid_loss: 0.0007490436352478961, test_loss: 0.002288716845214367\n",
      "epoch: 449, train_loss: 0.0001858365962423546, valid_loss: 0.0007508970059764882, test_loss: 0.0022862472105771303\n",
      "epoch: 450, train_loss: 0.00018576657369165966, valid_loss: 0.0007495284711088365, test_loss: 0.0022867140360176563\n",
      "epoch: 451, train_loss: 0.00018546536170027179, valid_loss: 0.0007499133595653499, test_loss: 0.002284655813127756\n",
      "epoch: 452, train_loss: 0.00018533605527962842, valid_loss: 0.000753044310840778, test_loss: 0.0022851922549307346\n",
      "epoch: 453, train_loss: 0.00018547308813456607, valid_loss: 0.0007499145819262291, test_loss: 0.0022821740712970495\n",
      "epoch: 454, train_loss: 0.00018514120450202861, valid_loss: 0.0007504248060286045, test_loss: 0.0022806571796536446\n",
      "epoch: 455, train_loss: 0.00018486288900021464, valid_loss: 0.0007480188117673, test_loss: 0.002278778003528714\n",
      "epoch: 456, train_loss: 0.00018477206037420294, valid_loss: 0.0007502825368040552, test_loss: 0.0022799514699727297\n",
      "epoch: 457, train_loss: 0.00018478471737699178, valid_loss: 0.0007482715494309863, test_loss: 0.002276577753946185\n",
      "epoch: 458, train_loss: 0.00018429048498586306, valid_loss: 0.0007450916358114531, test_loss: 0.002276245504617691\n",
      "epoch: 459, train_loss: 0.00018486138509141037, valid_loss: 0.0007473238511011004, test_loss: 0.002275838516652584\n",
      "epoch: 460, train_loss: 0.00018415981448134002, valid_loss: 0.0007448200922226533, test_loss: 0.002270119497552514\n",
      "epoch: 461, train_loss: 0.00018406840947294688, valid_loss: 0.000743172825120079, test_loss: 0.002270122990012169\n",
      "epoch: 462, train_loss: 0.00018379195851435804, valid_loss: 0.0007424729410558939, test_loss: 0.0022710722405463457\n",
      "epoch: 463, train_loss: 0.00018367868197737667, valid_loss: 0.0007440906386667242, test_loss: 0.0022692540660500526\n",
      "epoch: 464, train_loss: 0.0001834992062472536, valid_loss: 0.0007412116683553904, test_loss: 0.0022653888445347548\n",
      "epoch: 465, train_loss: 0.0001834147855254781, valid_loss: 0.0007435797257736946, test_loss: 0.0022667928133159876\n",
      "epoch: 466, train_loss: 0.00018346471504470253, valid_loss: 0.0007400088215945289, test_loss: 0.002262385794892907\n",
      "epoch: 467, train_loss: 0.00018304933568604452, valid_loss: 0.0007412230382518222, test_loss: 0.002262584865093231\n",
      "epoch: 468, train_loss: 0.00018296006411496225, valid_loss: 0.0007405269522375116, test_loss: 0.0022608377039432526\n",
      "epoch: 469, train_loss: 0.00018298789813264233, valid_loss: 0.000738468806957826, test_loss: 0.002259548520669341\n",
      "epoch: 470, train_loss: 0.00018319049369260344, valid_loss: 0.0007429291338970264, test_loss: 0.0022603096440434456\n",
      "epoch: 471, train_loss: 0.00018256509539914197, valid_loss: 0.0007399324094876647, test_loss: 0.002255875151604414\n",
      "epoch: 472, train_loss: 0.00018246751215369642, valid_loss: 0.0007393966419234251, test_loss: 0.002258084248751402\n",
      "epoch: 473, train_loss: 0.00018250377489375356, valid_loss: 0.0007383539874960358, test_loss: 0.0022555713076144457\n",
      "epoch: 474, train_loss: 0.0001820328356900617, valid_loss: 0.0007377696407881255, test_loss: 0.002254774793982506\n",
      "epoch: 475, train_loss: 0.0001821378283911263, valid_loss: 0.000735784507317779, test_loss: 0.002250429941341281\n",
      "epoch: 476, train_loss: 0.00018207990164544594, valid_loss: 0.0007351241268528005, test_loss: 0.002249357756227255\n",
      "epoch: 477, train_loss: 0.00018175860421489114, valid_loss: 0.000736727398665001, test_loss: 0.002249351004138589\n",
      "epoch: 478, train_loss: 0.00018165271498931006, valid_loss: 0.0007343281370898088, test_loss: 0.002247444586828351\n",
      "epoch: 479, train_loss: 0.00018155324315834466, valid_loss: 0.0007362805918091908, test_loss: 0.002247909316793084\n",
      "epoch: 480, train_loss: 0.00018157104204874486, valid_loss: 0.0007340062196211269, test_loss: 0.002246664837002754\n",
      "epoch: 481, train_loss: 0.0001814616995904109, valid_loss: 0.0007347004381396497, test_loss: 0.002245531417429447\n",
      "epoch: 482, train_loss: 0.0001811338069787978, valid_loss: 0.0007334188703680411, test_loss: 0.002241891110315919\n",
      "epoch: 483, train_loss: 0.00018095928839023185, valid_loss: 0.0007307821797439829, test_loss: 0.0022403495386242867\n",
      "epoch: 484, train_loss: 0.00018106149091972443, valid_loss: 0.0007312155212275684, test_loss: 0.002239563502371311\n",
      "epoch: 485, train_loss: 0.00018067757596524999, valid_loss: 0.0007328980524713794, test_loss: 0.0022395169362425804\n",
      "epoch: 486, train_loss: 0.0001807518071829058, valid_loss: 0.000729722921581318, test_loss: 0.002237254986539483\n",
      "epoch: 487, train_loss: 0.0001805924630784632, valid_loss: 0.0007304402679437771, test_loss: 0.0022364796604961157\n",
      "epoch: 488, train_loss: 0.00018024645459777474, valid_loss: 0.0007307659931636105, test_loss: 0.0022336486726999283\n",
      "epoch: 489, train_loss: 0.00018029125045438337, valid_loss: 0.0007333285854353259, test_loss: 0.0022356754634529352\n",
      "epoch: 490, train_loss: 0.0001801651410262465, valid_loss: 0.0007279556981908778, test_loss: 0.0022315983660519123\n",
      "epoch: 491, train_loss: 0.00017994761013972533, valid_loss: 0.0007293369417311624, test_loss: 0.0022305806633085012\n",
      "epoch: 492, train_loss: 0.00017964658289439166, valid_loss: 0.0007261864617854977, test_loss: 0.0022291424684226513\n",
      "epoch: 493, train_loss: 0.00017968634125756344, valid_loss: 0.0007279283599928021, test_loss: 0.0022275412920862436\n",
      "epoch: 494, train_loss: 0.0001796992734813577, valid_loss: 0.0007295373604089642, test_loss: 0.0022281890269368887\n",
      "epoch: 495, train_loss: 0.0001795083162901194, valid_loss: 0.0007272060902323574, test_loss: 0.002227426040917635\n",
      "epoch: 496, train_loss: 0.00017937262031330687, valid_loss: 0.0007255257320745537, test_loss: 0.00222477107308805\n",
      "epoch: 497, train_loss: 0.00017917737157246017, valid_loss: 0.0007282971928361803, test_loss: 0.002224220661446452\n",
      "epoch: 498, train_loss: 0.00017953905183315763, valid_loss: 0.0007261587258350725, test_loss: 0.0022219230886548758\n",
      "epoch: 499, train_loss: 0.00017930552775910854, valid_loss: 0.0007266904237136865, test_loss: 0.002221914939582348\n",
      "epoch: 500, train_loss: 0.0001788361038526763, valid_loss: 0.000726485270812797, test_loss: 0.0022195710334926844\n",
      "epoch: 501, train_loss: 0.00017879925017832252, valid_loss: 0.0007250596875868117, test_loss: 0.0022188846487551928\n",
      "epoch: 502, train_loss: 0.00017859395871789235, valid_loss: 0.0007234425186955681, test_loss: 0.0022199624218046665\n",
      "epoch: 503, train_loss: 0.0001786068156513426, valid_loss: 0.0007234244791713232, test_loss: 0.0022141956724226475\n",
      "epoch: 504, train_loss: 0.00017855906322517473, valid_loss: 0.0007221877652530869, test_loss: 0.0022149179130792618\n",
      "epoch: 505, train_loss: 0.00017826221933912325, valid_loss: 0.000721030441733698, test_loss: 0.002213057829067111\n",
      "epoch: 506, train_loss: 0.00017840483316989696, valid_loss: 0.0007208799264238527, test_loss: 0.0022120417561382055\n",
      "epoch: 507, train_loss: 0.0001781517994063704, valid_loss: 0.0007212894997792318, test_loss: 0.0022108645644038916\n",
      "epoch: 508, train_loss: 0.00017828905054003647, valid_loss: 0.0007231771014630795, test_loss: 0.0022121320944279432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 509, train_loss: 0.000177815935508672, valid_loss: 0.0007192882961438348, test_loss: 0.002208125311881304\n",
      "epoch: 510, train_loss: 0.00017794771195875237, valid_loss: 0.0007216426359567171, test_loss: 0.002208396792411804\n",
      "epoch: 511, train_loss: 0.00017781089611715922, valid_loss: 0.0007208421496519198, test_loss: 0.0022065492812544107\n",
      "epoch: 512, train_loss: 0.00017770376491724798, valid_loss: 0.0007206039105464394, test_loss: 0.002207593759521842\n",
      "epoch: 513, train_loss: 0.00017759610352146885, valid_loss: 0.0007194174395408481, test_loss: 0.002202763920649886\n",
      "epoch: 514, train_loss: 0.00017749239803220996, valid_loss: 0.0007176712970249355, test_loss: 0.002201184630393982\n",
      "epoch: 515, train_loss: 0.00017758846439866593, valid_loss: 0.0007177845254773274, test_loss: 0.0022009031381458044\n",
      "epoch: 516, train_loss: 0.0001776223539111569, valid_loss: 0.0007179841098453229, test_loss: 0.002202139003202319\n",
      "epoch: 517, train_loss: 0.00017707141397176716, valid_loss: 0.0007168876957924416, test_loss: 0.0021988265216350555\n",
      "epoch: 518, train_loss: 0.0001769592724841736, valid_loss: 0.0007166198047343642, test_loss: 0.002196876099333167\n",
      "epoch: 519, train_loss: 0.00017683440060152307, valid_loss: 0.0007145130221033469, test_loss: 0.002195926383137703\n",
      "epoch: 520, train_loss: 0.0001767973482618918, valid_loss: 0.0007152011918757731, test_loss: 0.002195000182837248\n",
      "epoch: 521, train_loss: 0.00017681349329549172, valid_loss: 0.0007173813064582646, test_loss: 0.002194116823375225\n",
      "epoch: 522, train_loss: 0.00017672032434189612, valid_loss: 0.0007153278468952825, test_loss: 0.0021915158722549677\n",
      "epoch: 523, train_loss: 0.0001764813635457793, valid_loss: 0.0007154653673448289, test_loss: 0.00219036266207695\n",
      "epoch: 524, train_loss: 0.00017638828759790277, valid_loss: 0.0007131318367707232, test_loss: 0.0021892080549150705\n",
      "epoch: 525, train_loss: 0.00017638696210823306, valid_loss: 0.0007123761412609989, test_loss: 0.0021889267954975367\n",
      "epoch: 526, train_loss: 0.0001761036957917816, valid_loss: 0.0007123269751900807, test_loss: 0.0021886283066123724\n",
      "epoch: 527, train_loss: 0.0001760161521024597, valid_loss: 0.0007154394746369993, test_loss: 0.0021897924598306417\n",
      "epoch: 528, train_loss: 0.00017619224166517833, valid_loss: 0.0007120190178587412, test_loss: 0.0021862501744180918\n",
      "epoch: 529, train_loss: 0.0001758928227228234, valid_loss: 0.0007100428435175369, test_loss: 0.0021840324625372887\n",
      "epoch: 530, train_loss: 0.00017559301594029301, valid_loss: 0.0007094032431875045, test_loss: 0.0021833032369613647\n",
      "epoch: 531, train_loss: 0.0001756926307596428, valid_loss: 0.0007097946314994866, test_loss: 0.002181084593757987\n",
      "epoch: 532, train_loss: 0.00017532306652435142, valid_loss: 0.0007095766438093657, test_loss: 0.0021806848235428333\n",
      "epoch: 533, train_loss: 0.00017555342144434056, valid_loss: 0.000709045329131186, test_loss: 0.0021799083333462477\n",
      "epoch: 534, train_loss: 0.00017520689352354765, valid_loss: 0.0007077759219100699, test_loss: 0.0021772996988147497\n",
      "epoch: 535, train_loss: 0.0001751355385235713, valid_loss: 0.0007080310654904073, test_loss: 0.002176743233576417\n",
      "epoch: 536, train_loss: 0.00017515441615377432, valid_loss: 0.0007082373922457919, test_loss: 0.0021750133018940687\n",
      "epoch: 537, train_loss: 0.00017502276054785952, valid_loss: 0.0007063932668340082, test_loss: 0.0021744375117123127\n",
      "epoch: 538, train_loss: 0.00017526127529613998, valid_loss: 0.0007066509278956801, test_loss: 0.0021727813873440027\n",
      "epoch: 539, train_loss: 0.000174946619233157, valid_loss: 0.0007060126566405719, test_loss: 0.002171562286093831\n",
      "epoch: 540, train_loss: 0.0001747576904523632, valid_loss: 0.0007057875579145426, test_loss: 0.002170683816075325\n",
      "epoch: 541, train_loss: 0.00017462849475281394, valid_loss: 0.0007049723547728112, test_loss: 0.0021689326968044043\n",
      "epoch: 542, train_loss: 0.00017490668308354267, valid_loss: 0.0007056560328540703, test_loss: 0.0021698761265724897\n",
      "epoch: 543, train_loss: 0.00017464885035145056, valid_loss: 0.0007056390556196371, test_loss: 0.002166606253013015\n",
      "epoch: 544, train_loss: 0.00017454256379531452, valid_loss: 0.0007056488587598627, test_loss: 0.0021673718001693487\n",
      "epoch: 545, train_loss: 0.00017423529820719168, valid_loss: 0.0007048636228622248, test_loss: 0.0021645568776875734\n",
      "epoch: 546, train_loss: 0.0001742302056695537, valid_loss: 0.000705006968928501, test_loss: 0.0021637750323861837\n",
      "epoch: 547, train_loss: 0.0001739931834416221, valid_loss: 0.0007047966015913213, test_loss: 0.002165211131796241\n",
      "epoch: 548, train_loss: 0.0001739541121819259, valid_loss: 0.000704010933986865, test_loss: 0.002162329852581024\n",
      "epoch: 549, train_loss: 0.00017403105575004187, valid_loss: 0.0007004224656460186, test_loss: 0.0021601011976599693\n",
      "epoch: 550, train_loss: 0.0001736852604865461, valid_loss: 0.000701781590275156, test_loss: 0.002159221563488245\n",
      "epoch: 551, train_loss: 0.00017386722016532946, valid_loss: 0.0007042497406170393, test_loss: 0.0021600895561277866\n",
      "epoch: 552, train_loss: 0.00017400676670544982, valid_loss: 0.0007030294364085421, test_loss: 0.0021575649734586477\n",
      "epoch: 553, train_loss: 0.00017337760895632368, valid_loss: 0.0006992345006437972, test_loss: 0.002156040631234646\n",
      "epoch: 554, train_loss: 0.0001736976789644636, valid_loss: 0.0007030058477539569, test_loss: 0.0021552860271185637\n",
      "epoch: 555, train_loss: 0.00017359930738482785, valid_loss: 0.0007003812594727302, test_loss: 0.0021538420114666224\n",
      "epoch: 556, train_loss: 0.00017318807532200995, valid_loss: 0.0007019180678374445, test_loss: 0.0021538895089179277\n",
      "epoch: 557, train_loss: 0.00017299981909277648, valid_loss: 0.0006986659864196554, test_loss: 0.0021499190479516983\n",
      "epoch: 558, train_loss: 0.00017318819743155947, valid_loss: 0.0006987557474834224, test_loss: 0.002149015199393034\n",
      "epoch: 559, train_loss: 0.00017290403585597548, valid_loss: 0.0006970502630186578, test_loss: 0.0021494929678738117\n",
      "epoch: 560, train_loss: 0.00017283679208839717, valid_loss: 0.0006971911595125372, test_loss: 0.002147626131772995\n",
      "epoch: 561, train_loss: 0.00017288439583195293, valid_loss: 0.0006966195214772597, test_loss: 0.002146482700482011\n",
      "epoch: 562, train_loss: 0.00017261375867984378, valid_loss: 0.0007004668489874651, test_loss: 0.002147647552192211\n",
      "epoch: 563, train_loss: 0.00017256934307110697, valid_loss: 0.0006975410312103728, test_loss: 0.0021467634942382574\n",
      "epoch: 564, train_loss: 0.00017265385236975777, valid_loss: 0.0006955643912078813, test_loss: 0.0021427408792078495\n",
      "epoch: 565, train_loss: 0.00017246974723251617, valid_loss: 0.0006969357006407032, test_loss: 0.0021424812730401754\n",
      "epoch: 566, train_loss: 0.00017211155468628377, valid_loss: 0.0006937295547686517, test_loss: 0.002141306409612298\n",
      "epoch: 567, train_loss: 0.00017222247476203611, valid_loss: 0.0006953824340598658, test_loss: 0.002140212571248412\n",
      "epoch: 568, train_loss: 0.00017219074399453467, valid_loss: 0.0006925338772513593, test_loss: 0.0021384393330663443\n",
      "epoch: 569, train_loss: 0.00017213056866160554, valid_loss: 0.0006934199142657841, test_loss: 0.0021382104605436325\n",
      "epoch: 570, train_loss: 0.00017214182361899674, valid_loss: 0.0006931635434739292, test_loss: 0.0021356188226491213\n",
      "epoch: 571, train_loss: 0.00017234421165599286, valid_loss: 0.0006940688860292236, test_loss: 0.002134763402864337\n",
      "epoch: 572, train_loss: 0.0001717092704696014, valid_loss: 0.0006932577671250328, test_loss: 0.0021355771459639072\n",
      "epoch: 573, train_loss: 0.00017204497492892423, valid_loss: 0.0006922542767521614, test_loss: 0.0021315242629498243\n",
      "epoch: 574, train_loss: 0.0001715165316177856, valid_loss: 0.0006941293783408279, test_loss: 0.0021327706053853035\n",
      "epoch: 575, train_loss: 0.00017139467387966326, valid_loss: 0.0006922914374930164, test_loss: 0.002129851607605815\n",
      "epoch: 576, train_loss: 0.00017168095623871878, valid_loss: 0.0006942423933651298, test_loss: 0.002132218796759844\n",
      "epoch: 577, train_loss: 0.00017140668743472222, valid_loss: 0.0006895235565025359, test_loss: 0.002128149615600705\n",
      "epoch: 578, train_loss: 0.00017145825499344778, valid_loss: 0.0006910833714452261, test_loss: 0.0021275540348142385\n",
      "epoch: 579, train_loss: 0.0001710301382801212, valid_loss: 0.0006908351254727071, test_loss: 0.002126730512827635\n",
      "epoch: 580, train_loss: 0.0001710687533689096, valid_loss: 0.0006887843968191495, test_loss: 0.002124417806044221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 581, train_loss: 0.00017103430329133636, valid_loss: 0.0006892596769224232, test_loss: 0.0021242310758680105\n",
      "epoch: 582, train_loss: 0.0001708041748750712, valid_loss: 0.0006869947634792576, test_loss: 0.0021226610988378525\n",
      "epoch: 583, train_loss: 0.00017072755524524203, valid_loss: 0.0006887317285872996, test_loss: 0.0021209469996392727\n",
      "epoch: 584, train_loss: 0.00017061636279559815, valid_loss: 0.0006877723790239543, test_loss: 0.0021211442071944475\n",
      "epoch: 585, train_loss: 0.00017056572023252755, valid_loss: 0.0006872326339362189, test_loss: 0.0021184482611715794\n",
      "epoch: 586, train_loss: 0.0001703710589461717, valid_loss: 0.0006868422303038338, test_loss: 0.0021171611733734608\n",
      "epoch: 587, train_loss: 0.0001704448975784623, valid_loss: 0.0006890323420520872, test_loss: 0.002118062926456332\n",
      "epoch: 588, train_loss: 0.00017053787767350593, valid_loss: 0.0006888561426118637, test_loss: 0.002118402160704136\n",
      "epoch: 589, train_loss: 0.00017048012460177034, valid_loss: 0.00068652220943477, test_loss: 0.002116444520652294\n",
      "epoch: 590, train_loss: 0.00017007840316459212, valid_loss: 0.0006834689556853846, test_loss: 0.00211318233050406\n",
      "epoch: 591, train_loss: 0.0001700482167981808, valid_loss: 0.0006858009631590297, test_loss: 0.002112492686137557\n",
      "epoch: 592, train_loss: 0.000170089117170352, valid_loss: 0.0006839304405730218, test_loss: 0.0021113259717822075\n",
      "epoch: 593, train_loss: 0.0001703281589510405, valid_loss: 0.0006888478431695452, test_loss: 0.0021140272729098797\n",
      "epoch: 594, train_loss: 0.00017005271207460243, valid_loss: 0.0006869356099438543, test_loss: 0.0021101864986121655\n",
      "epoch: 595, train_loss: 0.00016956254283128226, valid_loss: 0.0006823322667817896, test_loss: 0.0021091378293931484\n",
      "epoch: 596, train_loss: 0.0001695774196338354, valid_loss: 0.0006809481031571826, test_loss: 0.002106456086039543\n",
      "epoch: 597, train_loss: 0.00016990780220468244, valid_loss: 0.0006849839701317251, test_loss: 0.0021071338560432196\n",
      "epoch: 598, train_loss: 0.00016940931622546327, valid_loss: 0.0006819554400863126, test_loss: 0.0021053815726190805\n",
      "epoch: 599, train_loss: 0.00016942096946244735, valid_loss: 0.0006830798326215396, test_loss: 0.0021040027495473623\n",
      "epoch: 600, train_loss: 0.00016938866642506227, valid_loss: 0.0006797556125093251, test_loss: 0.0021025852765887976\n",
      "epoch: 601, train_loss: 0.00016934088046622017, valid_loss: 0.0006843985077769806, test_loss: 0.002105333376675844\n",
      "epoch: 602, train_loss: 0.00016921310516013563, valid_loss: 0.000682273156902132, test_loss: 0.0021002432331442833\n",
      "epoch: 603, train_loss: 0.00016892930928050825, valid_loss: 0.000680843693165419, test_loss: 0.002098861848935485\n",
      "epoch: 604, train_loss: 0.00016899258353868905, valid_loss: 0.0006805935311907282, test_loss: 0.0020986346062272787\n",
      "epoch: 605, train_loss: 0.00016893377292233154, valid_loss: 0.0006789459293941036, test_loss: 0.002099225064739585\n",
      "epoch: 606, train_loss: 0.00016888699167833218, valid_loss: 0.0006788234556249032, test_loss: 0.0020963235292583704\n",
      "epoch: 607, train_loss: 0.00016860331917363826, valid_loss: 0.0006794151073942581, test_loss: 0.0020949437748640776\n",
      "epoch: 608, train_loss: 0.0001686570809081035, valid_loss: 0.0006772443933490043, test_loss: 0.002092720940709114\n",
      "epoch: 609, train_loss: 0.00016868919065809283, valid_loss: 0.0006764224041641379, test_loss: 0.002092478098347783\n",
      "epoch: 610, train_loss: 0.00016837922980219287, valid_loss: 0.0006766928563592955, test_loss: 0.0020915621425956488\n",
      "epoch: 611, train_loss: 0.00016868396462244993, valid_loss: 0.0006762926204828545, test_loss: 0.0020902713295072317\n",
      "epoch: 612, train_loss: 0.0001685020308840372, valid_loss: 0.0006784444024863964, test_loss: 0.002090803813189268\n",
      "epoch: 613, train_loss: 0.0001683938468846938, valid_loss: 0.0006747066994042447, test_loss: 0.0020872841123491526\n",
      "epoch: 614, train_loss: 0.00016821560680198118, valid_loss: 0.0006771158659830689, test_loss: 0.0020891025196760893\n",
      "epoch: 615, train_loss: 0.00016815366309500584, valid_loss: 0.0006756223010597751, test_loss: 0.00208462355658412\n",
      "epoch: 616, train_loss: 0.0001680901990292351, valid_loss: 0.0006740506069036201, test_loss: 0.002084789564833045\n",
      "epoch: 617, train_loss: 0.00016801332505723542, valid_loss: 0.0006743902922607958, test_loss: 0.002083738800138235\n",
      "epoch: 618, train_loss: 0.00016787563926418838, valid_loss: 0.0006735712959198281, test_loss: 0.0020820731297135353\n",
      "epoch: 619, train_loss: 0.0001678166394729329, valid_loss: 0.0006737335061188787, test_loss: 0.0020818794146180153\n",
      "epoch: 620, train_loss: 0.0001675807244559426, valid_loss: 0.0006727976724505424, test_loss: 0.0020798128098249435\n",
      "epoch: 621, train_loss: 0.00016788354411761722, valid_loss: 0.0006755280337529257, test_loss: 0.0020801874343305826\n",
      "epoch: 622, train_loss: 0.0001676582538968195, valid_loss: 0.0006731298732726524, test_loss: 0.0020791240967810154\n",
      "epoch: 623, train_loss: 0.00016736128726852652, valid_loss: 0.0006734456934888536, test_loss: 0.002076867502182722\n",
      "epoch: 624, train_loss: 0.00016756675651539928, valid_loss: 0.0006722869778362414, test_loss: 0.0020774430595338345\n",
      "epoch: 625, train_loss: 0.00016735104651635757, valid_loss: 0.0006701390229864046, test_loss: 0.0020743149798363447\n",
      "epoch: 626, train_loss: 0.00016717494683056745, valid_loss: 0.0006699564352553958, test_loss: 0.002075694501399994\n",
      "epoch: 627, train_loss: 0.00016737446908930156, valid_loss: 0.000670287748410677, test_loss: 0.002072232309728861\n",
      "epoch: 628, train_loss: 0.0001669778607522502, valid_loss: 0.0006707163750737285, test_loss: 0.00207241578027606\n",
      "epoch: 629, train_loss: 0.00016678161995040009, valid_loss: 0.0006700099426476905, test_loss: 0.0020708125084638596\n",
      "epoch: 630, train_loss: 0.0001670635272445076, valid_loss: 0.0006709243267929802, test_loss: 0.0020706173963844776\n",
      "epoch: 631, train_loss: 0.0001667075886634057, valid_loss: 0.0006685026213138675, test_loss: 0.0020697785075753927\n",
      "epoch: 632, train_loss: 0.00016704273002687842, valid_loss: 0.0006687025694797436, test_loss: 0.0020666818600147963\n",
      "epoch: 633, train_loss: 0.00016668411452606645, valid_loss: 0.0006680110673187301, test_loss: 0.0020657842978835106\n",
      "epoch: 634, train_loss: 0.0001665787523328934, valid_loss: 0.0006681368346714104, test_loss: 0.002065614564344287\n",
      "epoch: 635, train_loss: 0.00016646163725375158, valid_loss: 0.0006690788189492499, test_loss: 0.0020686869975179434\n",
      "epoch: 636, train_loss: 0.00016633795736276585, valid_loss: 0.000666457781335339, test_loss: 0.0020636324770748615\n",
      "epoch: 637, train_loss: 0.00016618990807530835, valid_loss: 0.0006660347086532662, test_loss: 0.0020620471332222223\n",
      "epoch: 638, train_loss: 0.00016629306407159439, valid_loss: 0.0006695129122817889, test_loss: 0.0020632583182305098\n",
      "epoch: 639, train_loss: 0.0001660182695005737, valid_loss: 0.0006653655824872354, test_loss: 0.002059544902294874\n",
      "epoch: 640, train_loss: 0.0001660852608240574, valid_loss: 0.0006643273712446293, test_loss: 0.002057962818071246\n",
      "epoch: 641, train_loss: 0.00016607195847546276, valid_loss: 0.0006666151117921496, test_loss: 0.002059569815173745\n",
      "epoch: 642, train_loss: 0.00016581923473099977, valid_loss: 0.0006633610028075054, test_loss: 0.002057149773463607\n",
      "epoch: 643, train_loss: 0.00016594708437839037, valid_loss: 0.0006665079611896848, test_loss: 0.002057040110230446\n",
      "epoch: 644, train_loss: 0.0001658421831013149, valid_loss: 0.0006642805819865316, test_loss: 0.0020578301046043634\n",
      "epoch: 645, train_loss: 0.00016565327203589612, valid_loss: 0.0006640313416331386, test_loss: 0.002054321113973856\n",
      "epoch: 646, train_loss: 0.00016557005849039263, valid_loss: 0.0006628237946036583, test_loss: 0.0020520463585853577\n",
      "epoch: 647, train_loss: 0.00016563227868375733, valid_loss: 0.0006622407527174801, test_loss: 0.0020528146997094154\n",
      "epoch: 648, train_loss: 0.00016551050319559062, valid_loss: 0.0006616750082078701, test_loss: 0.0020505900029093027\n",
      "epoch: 649, train_loss: 0.00016546377446502447, valid_loss: 0.0006631655560340732, test_loss: 0.0020505001302808523\n",
      "epoch: 650, train_loss: 0.00016545658012467396, valid_loss: 0.000664383711409755, test_loss: 0.0020486663561314344\n",
      "epoch: 651, train_loss: 0.00016527030105783564, valid_loss: 0.0006607487933554997, test_loss: 0.0020472996402531862\n",
      "epoch: 652, train_loss: 0.0001651097209896366, valid_loss: 0.0006611959252040833, test_loss: 0.0020469939336180687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 653, train_loss: 0.00016512325268906906, valid_loss: 0.00066137891553808, test_loss: 0.002045405562967062\n",
      "epoch: 654, train_loss: 0.00016484556165422597, valid_loss: 0.0006609295038894439, test_loss: 0.00204412336461246\n",
      "epoch: 655, train_loss: 0.00016495727987089396, valid_loss: 0.0006623613056338703, test_loss: 0.002045571571215987\n",
      "epoch: 656, train_loss: 0.0001647600539676521, valid_loss: 0.0006600146637841439, test_loss: 0.0020440106745809317\n",
      "epoch: 657, train_loss: 0.00016469195037169138, valid_loss: 0.0006607184332096949, test_loss: 0.002043287269771099\n",
      "epoch: 658, train_loss: 0.00016466570188007924, valid_loss: 0.0006583837045279021, test_loss: 0.002039841841906309\n",
      "epoch: 659, train_loss: 0.00016456267242218652, valid_loss: 0.0006590892201832806, test_loss: 0.002039283746853471\n",
      "epoch: 660, train_loss: 0.00016458178478110906, valid_loss: 0.000659731769701466, test_loss: 0.0020395000465214252\n",
      "epoch: 661, train_loss: 0.0001642074462522388, valid_loss: 0.0006572677084477618, test_loss: 0.0020370602142065763\n",
      "epoch: 662, train_loss: 0.00016418415749364573, valid_loss: 0.0006578129347568998, test_loss: 0.002036420861259103\n",
      "epoch: 663, train_loss: 0.00016408481757895535, valid_loss: 0.0006570486778703829, test_loss: 0.002035203855484724\n",
      "epoch: 664, train_loss: 0.00016404029093277842, valid_loss: 0.0006597148312721401, test_loss: 0.002035083482041955\n",
      "epoch: 665, train_loss: 0.0001639715239592909, valid_loss: 0.0006568473860776672, test_loss: 0.002033993834629655\n",
      "epoch: 666, train_loss: 0.00016374832320371237, valid_loss: 0.0006547441007569432, test_loss: 0.00203252537176013\n",
      "epoch: 667, train_loss: 0.00016375672914917865, valid_loss: 0.0006554121112761399, test_loss: 0.002030234085395932\n",
      "epoch: 668, train_loss: 0.0001639618228933693, valid_loss: 0.0006547443384382253, test_loss: 0.0020302191842347383\n",
      "epoch: 669, train_loss: 0.0001639384591607782, valid_loss: 0.000654458039207384, test_loss: 0.0020283020567148924\n",
      "epoch: 670, train_loss: 0.00016373412875944504, valid_loss: 0.0006546005703664074, test_loss: 0.002028876217082143\n",
      "epoch: 671, train_loss: 0.0001635168582256681, valid_loss: 0.0006528679465797419, test_loss: 0.0020273150876164436\n",
      "epoch: 672, train_loss: 0.0001635921374975663, valid_loss: 0.0006515571052053323, test_loss: 0.002025183755904436\n",
      "epoch: 673, train_loss: 0.00016370132684444442, valid_loss: 0.0006531260053937634, test_loss: 0.002023746958002448\n",
      "epoch: 674, train_loss: 0.00016338219005695504, valid_loss: 0.00065245597215835, test_loss: 0.0020233907271176577\n",
      "epoch: 675, train_loss: 0.0001631589010408472, valid_loss: 0.0006543543131556362, test_loss: 0.0020246054045856\n",
      "epoch: 676, train_loss: 0.00016319254348712528, valid_loss: 0.0006529120340322455, test_loss: 0.002024127636104822\n",
      "epoch: 677, train_loss: 0.0001630204319751457, valid_loss: 0.0006509893234275902, test_loss: 0.0020201746374368668\n",
      "epoch: 678, train_loss: 0.00016337994969470182, valid_loss: 0.0006538919551530853, test_loss: 0.0020212349481880665\n",
      "epoch: 679, train_loss: 0.00016296681006560507, valid_loss: 0.0006528332354112839, test_loss: 0.002019325038418174\n",
      "epoch: 680, train_loss: 0.0001628212023364461, valid_loss: 0.0006508649530587718, test_loss: 0.0020171788055449724\n",
      "epoch: 681, train_loss: 0.00016286662455810153, valid_loss: 0.0006489652344801774, test_loss: 0.0020165485329926014\n",
      "epoch: 682, train_loss: 0.0001628078430455745, valid_loss: 0.0006486486333111922, test_loss: 0.002016717102378607\n",
      "epoch: 683, train_loss: 0.00016267453801170316, valid_loss: 0.0006504187961885085, test_loss: 0.002015063539147377\n",
      "epoch: 684, train_loss: 0.00016244350927476972, valid_loss: 0.0006493187344555432, test_loss: 0.0020144348964095116\n",
      "epoch: 685, train_loss: 0.00016255395609693116, valid_loss: 0.0006493922361793617, test_loss: 0.002014376223087311\n",
      "epoch: 686, train_loss: 0.0001625102709310696, valid_loss: 0.0006488031261445334, test_loss: 0.0020117228850722313\n",
      "epoch: 687, train_loss: 0.00016234960766387698, valid_loss: 0.0006478485593106598, test_loss: 0.002010114025324583\n",
      "epoch: 688, train_loss: 0.00016212405044706944, valid_loss: 0.0006478803261416033, test_loss: 0.0020095608197152615\n",
      "epoch: 689, train_loss: 0.0001622273772701864, valid_loss: 0.0006478151044575498, test_loss: 0.002007844392210245\n",
      "epoch: 690, train_loss: 0.00016234151300285822, valid_loss: 0.0006474459126669293, test_loss: 0.0020077312365174294\n",
      "epoch: 691, train_loss: 0.0001619784041237005, valid_loss: 0.0006456445844378322, test_loss: 0.0020062755793333054\n",
      "epoch: 692, train_loss: 0.0001620548535593907, valid_loss: 0.0006472548314680656, test_loss: 0.0020060776732861996\n",
      "epoch: 693, train_loss: 0.00016186963235352027, valid_loss: 0.0006460982112912461, test_loss: 0.002005862770602107\n",
      "epoch: 694, train_loss: 0.00016180405003504583, valid_loss: 0.0006474217273838198, test_loss: 0.002003659028559923\n",
      "epoch: 695, train_loss: 0.0001618373853093742, valid_loss: 0.0006453866565910479, test_loss: 0.002002021297812462\n",
      "epoch: 696, train_loss: 0.00016160683045872366, valid_loss: 0.0006461666392472883, test_loss: 0.002001178218051791\n",
      "epoch: 697, train_loss: 0.00016188341871146923, valid_loss: 0.0006451164226746187, test_loss: 0.0020004098769277334\n",
      "epoch: 698, train_loss: 0.0001615728938106812, valid_loss: 0.0006459786624570066, test_loss: 0.0020014268811792135\n",
      "epoch: 699, train_loss: 0.00016123272507684305, valid_loss: 0.0006423952193775525, test_loss: 0.0019976720213890076\n",
      "epoch: 700, train_loss: 0.000161400662087228, valid_loss: 0.000643171176003913, test_loss: 0.0019963292870670557\n",
      "epoch: 701, train_loss: 0.00016115533356281483, valid_loss: 0.0006445305625675246, test_loss: 0.001997296465560794\n",
      "epoch: 702, train_loss: 0.0001612928586521024, valid_loss: 0.000641654662710304, test_loss: 0.001995063154026866\n",
      "epoch: 703, train_loss: 0.00016117603809107095, valid_loss: 0.0006435679921802754, test_loss: 0.0019958659540861845\n",
      "epoch: 704, train_loss: 0.00016111743752844632, valid_loss: 0.0006413286901079118, test_loss: 0.0019931781571358442\n",
      "epoch: 705, train_loss: 0.00016119199774592468, valid_loss: 0.0006435826071538031, test_loss: 0.0019933641888201237\n",
      "epoch: 706, train_loss: 0.00016085634959618682, valid_loss: 0.0006433987631074464, test_loss: 0.001991827739402652\n",
      "epoch: 707, train_loss: 0.00016084511931378233, valid_loss: 0.0006434204745649671, test_loss: 0.0019916926976293325\n",
      "epoch: 708, train_loss: 0.0001606354540537881, valid_loss: 0.0006395991804311052, test_loss: 0.0019896584562957287\n",
      "epoch: 709, train_loss: 0.00016068646168012333, valid_loss: 0.000640583758164818, test_loss: 0.0019886032678186893\n",
      "epoch: 710, train_loss: 0.00016047808806077862, valid_loss: 0.0006388899055309594, test_loss: 0.001986769028007984\n",
      "epoch: 711, train_loss: 0.00016044419443578985, valid_loss: 0.0006384301523212343, test_loss: 0.0019862705375999212\n",
      "epoch: 712, train_loss: 0.00016040693330780968, valid_loss: 0.0006387982236143822, test_loss: 0.0019866551738232374\n",
      "epoch: 713, train_loss: 0.0001601874486684961, valid_loss: 0.0006385579666433235, test_loss: 0.001984367612749338\n",
      "epoch: 714, train_loss: 0.00016025949899694356, valid_loss: 0.0006407670201345658, test_loss: 0.001984434900805354\n",
      "epoch: 715, train_loss: 0.00016008025578111815, valid_loss: 0.0006373918877216056, test_loss: 0.001981932669878006\n",
      "epoch: 716, train_loss: 0.0001600956716372267, valid_loss: 0.0006369783271414539, test_loss: 0.001983082154765725\n",
      "epoch: 717, train_loss: 0.0001599205469297569, valid_loss: 0.0006372448503194997, test_loss: 0.001981389243155718\n",
      "epoch: 718, train_loss: 0.00015993357975157383, valid_loss: 0.0006385109008988366, test_loss: 0.001979931490495801\n",
      "epoch: 719, train_loss: 0.00015985678866222176, valid_loss: 0.0006358290993375704, test_loss: 0.00197811727412045\n",
      "epoch: 720, train_loss: 0.0001597670020798788, valid_loss: 0.0006347642192849889, test_loss: 0.001976925879716873\n",
      "epoch: 721, train_loss: 0.0001595158303288095, valid_loss: 0.0006377630876765276, test_loss: 0.0019786383491009474\n",
      "epoch: 722, train_loss: 0.00015967448290093276, valid_loss: 0.0006348647293634713, test_loss: 0.001976174768060446\n",
      "epoch: 723, train_loss: 0.00016004446534561399, valid_loss: 0.0006353232311084867, test_loss: 0.0019754490349441767\n",
      "epoch: 724, train_loss: 0.0001599561143413429, valid_loss: 0.0006346625790077572, test_loss: 0.001973915146663785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 725, train_loss: 0.00015945392901219589, valid_loss: 0.0006372129379694039, test_loss: 0.0019765920005738735\n",
      "epoch: 726, train_loss: 0.0001597477986133131, valid_loss: 0.0006338166228185097, test_loss: 0.0019718469120562077\n",
      "epoch: 727, train_loss: 0.00015910358026461756, valid_loss: 0.0006338688593435412, test_loss: 0.0019708338659256697\n",
      "epoch: 728, train_loss: 0.0001593586626847315, valid_loss: 0.0006329512010173252, test_loss: 0.0019694697111845016\n",
      "epoch: 729, train_loss: 0.00015929018263705075, valid_loss: 0.0006332554912660271, test_loss: 0.0019691602792590857\n",
      "epoch: 730, train_loss: 0.0001591330614958323, valid_loss: 0.0006323119208294278, test_loss: 0.001967744203284383\n",
      "epoch: 731, train_loss: 0.00015910666780141384, valid_loss: 0.0006340059723394612, test_loss: 0.001967957243323326\n",
      "epoch: 732, train_loss: 0.0001589291129241009, valid_loss: 0.0006317840695070723, test_loss: 0.0019656179938465357\n",
      "epoch: 733, train_loss: 0.00015864419180181122, valid_loss: 0.0006305795880810668, test_loss: 0.0019651807378977537\n",
      "epoch: 734, train_loss: 0.00015886299914437467, valid_loss: 0.0006300363990400607, test_loss: 0.001963265473023057\n",
      "epoch: 735, train_loss: 0.00015893828569223052, valid_loss: 0.0006304219908391436, test_loss: 0.001962060108780861\n",
      "epoch: 736, train_loss: 0.00015846876783104128, valid_loss: 0.0006301710333597536, test_loss: 0.0019623874686658382\n",
      "epoch: 737, train_loss: 0.00015876610996201634, valid_loss: 0.0006312446785159409, test_loss: 0.0019615585915744305\n",
      "epoch: 738, train_loss: 0.0001587931860147206, valid_loss: 0.0006307972216745839, test_loss: 0.0019598733633756638\n",
      "epoch: 739, train_loss: 0.00015835428506682587, valid_loss: 0.000630584186486279, test_loss: 0.001959657296538353\n",
      "epoch: 740, train_loss: 0.00015818149182925245, valid_loss: 0.000629608208934466, test_loss: 0.001958222594112158\n",
      "epoch: 741, train_loss: 0.0001582689228994043, valid_loss: 0.0006298011721810326, test_loss: 0.0019582125823944807\n",
      "epoch: 742, train_loss: 0.0001583499979460612, valid_loss: 0.0006275692188258594, test_loss: 0.0019558710046112537\n",
      "epoch: 743, train_loss: 0.00015848509855973333, valid_loss: 0.0006275634659687057, test_loss: 0.0019543368835002184\n",
      "epoch: 744, train_loss: 0.0001582090790970656, valid_loss: 0.000627621445649614, test_loss: 0.0019539373461157084\n",
      "epoch: 745, train_loss: 0.00015827379874004376, valid_loss: 0.000628200791349324, test_loss: 0.001953688682988286\n",
      "epoch: 746, train_loss: 0.000157938450169952, valid_loss: 0.0006300481084811812, test_loss: 0.0019539897330105305\n",
      "epoch: 747, train_loss: 0.0001579695561541366, valid_loss: 0.0006276382337091491, test_loss: 0.001954274484887719\n",
      "epoch: 748, train_loss: 0.00015781872175664276, valid_loss: 0.0006284511813040202, test_loss: 0.0019507809774950147\n",
      "epoch: 749, train_loss: 0.00015788906856197053, valid_loss: 0.0006252320454223081, test_loss: 0.0019488305551931262\n",
      "epoch: 750, train_loss: 0.0001575501465476523, valid_loss: 0.0006255355304650342, test_loss: 0.0019489317201077938\n",
      "epoch: 751, train_loss: 0.00015750144919603252, valid_loss: 0.000625486010297512, test_loss: 0.0019472386920824647\n",
      "epoch: 752, train_loss: 0.00015747849386610577, valid_loss: 0.0006236935538860658, test_loss: 0.001946585369296372\n",
      "epoch: 753, train_loss: 0.00015745255444451922, valid_loss: 0.0006280374994579082, test_loss: 0.0019482322968542576\n",
      "epoch: 754, train_loss: 0.0001575826355964755, valid_loss: 0.0006261708573826278, test_loss: 0.0019467327510938048\n",
      "epoch: 755, train_loss: 0.0001573791080812478, valid_loss: 0.0006245186814339831, test_loss: 0.0019465459045022726\n",
      "epoch: 756, train_loss: 0.0001573328298273618, valid_loss: 0.0006242756304952005, test_loss: 0.0019436037400737405\n",
      "epoch: 757, train_loss: 0.00015710885465681633, valid_loss: 0.0006230621559855839, test_loss: 0.0019426899962127209\n",
      "epoch: 758, train_loss: 0.00015701808044231376, valid_loss: 0.0006240143654091904, test_loss: 0.0019416436553001404\n",
      "epoch: 759, train_loss: 0.00015703009304833478, valid_loss: 0.0006240046495804563, test_loss: 0.001942328060977161\n",
      "epoch: 760, train_loss: 0.0001570756525641469, valid_loss: 0.0006242859332511822, test_loss: 0.0019402164034545422\n",
      "epoch: 761, train_loss: 0.00015703833038139197, valid_loss: 0.0006235636198349918, test_loss: 0.0019395166309550405\n",
      "epoch: 762, train_loss: 0.00015679050651986313, valid_loss: 0.0006205852647932867, test_loss: 0.001936948043294251\n",
      "epoch: 763, train_loss: 0.00015698483785735849, valid_loss: 0.0006210295881222313, test_loss: 0.001936988322995603\n",
      "epoch: 764, train_loss: 0.00015666789081676498, valid_loss: 0.0006243613703797261, test_loss: 0.001936920452862978\n",
      "epoch: 765, train_loss: 0.00015647345603413314, valid_loss: 0.00062134886199298, test_loss: 0.0019355743424966931\n",
      "epoch: 766, train_loss: 0.00015649636644293028, valid_loss: 0.0006190411901722351, test_loss: 0.0019329237984493375\n",
      "epoch: 767, train_loss: 0.00015675823543342236, valid_loss: 0.0006190035201143473, test_loss: 0.0019328441703692079\n",
      "epoch: 768, train_loss: 0.000156609881821129, valid_loss: 0.0006220844904116044, test_loss: 0.001934634754434228\n",
      "epoch: 769, train_loss: 0.00015628967135537255, valid_loss: 0.0006189473739747579, test_loss: 0.0019306429894641042\n",
      "epoch: 770, train_loss: 0.00015629240585005155, valid_loss: 0.0006224691266349206, test_loss: 0.0019313694210723042\n",
      "epoch: 771, train_loss: 0.00015622626866072255, valid_loss: 0.0006208402386012798, test_loss: 0.0019300603307783604\n",
      "epoch: 772, train_loss: 0.00015606278548543543, valid_loss: 0.0006199526154280951, test_loss: 0.001930759521201253\n",
      "epoch: 773, train_loss: 0.00015602119421363687, valid_loss: 0.0006186656197921062, test_loss: 0.0019264117581769824\n",
      "epoch: 774, train_loss: 0.00015618321428875154, valid_loss: 0.0006164596367549772, test_loss: 0.0019262894056737423\n",
      "epoch: 775, train_loss: 0.00015587203866427603, valid_loss: 0.0006178861027971531, test_loss: 0.0019260623957961798\n",
      "epoch: 776, train_loss: 0.00015578647719888502, valid_loss: 0.0006179939906966562, test_loss: 0.001923734089359641\n",
      "epoch: 777, train_loss: 0.00015595672226956358, valid_loss: 0.0006180782026300827, test_loss: 0.0019242062699049711\n",
      "epoch: 778, train_loss: 0.0001555929285013522, valid_loss: 0.0006187582621350884, test_loss: 0.0019259355030953884\n",
      "epoch: 779, train_loss: 0.000155790343895838, valid_loss: 0.0006168147907980407, test_loss: 0.0019214695785194635\n",
      "epoch: 780, train_loss: 0.00015570373595793447, valid_loss: 0.0006155904751115789, test_loss: 0.0019202177645638585\n",
      "epoch: 781, train_loss: 0.00015550977727600738, valid_loss: 0.0006162647138504932, test_loss: 0.0019191327737644315\n",
      "epoch: 782, train_loss: 0.00015567173661269328, valid_loss: 0.000614371276848639, test_loss: 0.0019187800353392959\n",
      "epoch: 783, train_loss: 0.00015527106196437356, valid_loss: 0.0006146449692702541, test_loss: 0.0019176644273102283\n",
      "epoch: 784, train_loss: 0.00015541354988944596, valid_loss: 0.00061494281302051, test_loss: 0.0019185100682079792\n",
      "epoch: 785, train_loss: 0.00015543810339927998, valid_loss: 0.0006158455216791481, test_loss: 0.001919285045005381\n",
      "epoch: 786, train_loss: 0.00015515551501227057, valid_loss: 0.0006125758566971248, test_loss: 0.0019160324009135365\n",
      "epoch: 787, train_loss: 0.0001550588210153839, valid_loss: 0.0006146932234211514, test_loss: 0.0019154760520905256\n",
      "epoch: 788, train_loss: 0.00015506997853821224, valid_loss: 0.0006119137494048724, test_loss: 0.0019133109599351883\n",
      "epoch: 789, train_loss: 0.00015508799412561095, valid_loss: 0.0006131532427389175, test_loss: 0.0019125003600493073\n",
      "epoch: 790, train_loss: 0.00015490949305746219, valid_loss: 0.000611206409909452, test_loss: 0.0019125115359202027\n",
      "epoch: 791, train_loss: 0.0001548072285241569, valid_loss: 0.0006138752457142497, test_loss: 0.001911427010782063\n",
      "epoch: 792, train_loss: 0.00015481616086933923, valid_loss: 0.0006130368516702825, test_loss: 0.0019105254905298352\n",
      "epoch: 793, train_loss: 0.00015472254143906352, valid_loss: 0.0006115382323817661, test_loss: 0.0019086775137111545\n",
      "epoch: 794, train_loss: 0.0001547311618671064, valid_loss: 0.0006122027843957767, test_loss: 0.001908494858071208\n",
      "epoch: 795, train_loss: 0.00015443974331466725, valid_loss: 0.0006117139467581486, test_loss: 0.0019102495862171054\n",
      "epoch: 796, train_loss: 0.0001548095403806023, valid_loss: 0.0006091095953403661, test_loss: 0.0019062417559325695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 797, train_loss: 0.00015468409591172215, valid_loss: 0.0006096126162447035, test_loss: 0.0019048105459660292\n",
      "epoch: 798, train_loss: 0.0001543460678911525, valid_loss: 0.0006098963203839958, test_loss: 0.0019045022781938314\n",
      "epoch: 799, train_loss: 0.0001543942685795786, valid_loss: 0.0006105098145781085, test_loss: 0.0019034430151805282\n",
      "epoch: 800, train_loss: 0.00015466712363337612, valid_loss: 0.0006089723998835931, test_loss: 0.0019026328809559345\n",
      "epoch: 801, train_loss: 0.00015404309859782782, valid_loss: 0.000608715801111733, test_loss: 0.001901826704852283\n",
      "epoch: 802, train_loss: 0.00015418902330566198, valid_loss: 0.0006084377091610804, test_loss: 0.001901151961646974\n",
      "epoch: 803, train_loss: 0.00015405150106348827, valid_loss: 0.0006094128863575558, test_loss: 0.001900564762763679\n",
      "epoch: 804, train_loss: 0.00015399421207131007, valid_loss: 0.000609044567681849, test_loss: 0.001900088507682085\n",
      "epoch: 805, train_loss: 0.000153874386223438, valid_loss: 0.0006072960338012005, test_loss: 0.0018988663796335459\n",
      "epoch: 806, train_loss: 0.00015368406014243627, valid_loss: 0.0006073028974545499, test_loss: 0.0018962703179568052\n",
      "epoch: 807, train_loss: 0.00015377791271970162, valid_loss: 0.0006062642302519331, test_loss: 0.001895453780889511\n",
      "epoch: 808, train_loss: 0.0001535965556800163, valid_loss: 0.0006054909705805281, test_loss: 0.001894672866910696\n",
      "epoch: 809, train_loss: 0.00015373005843012714, valid_loss: 0.0006062737472044925, test_loss: 0.0018940214067697525\n",
      "epoch: 810, train_loss: 0.00015353863304683372, valid_loss: 0.0006059380199682588, test_loss: 0.0018935437547042966\n",
      "epoch: 811, train_loss: 0.00015342199881358638, valid_loss: 0.0006058657163521275, test_loss: 0.001891796593554318\n",
      "epoch: 812, train_loss: 0.00015355735503481296, valid_loss: 0.0006068187503842637, test_loss: 0.0018923276802524924\n",
      "epoch: 813, train_loss: 0.0001534688414287065, valid_loss: 0.0006065559039901321, test_loss: 0.0018924274481832981\n",
      "epoch: 814, train_loss: 0.0001531804768803894, valid_loss: 0.0006050581869203597, test_loss: 0.0018902672454714775\n",
      "epoch: 815, train_loss: 0.0001530866850415528, valid_loss: 0.0006038691693296035, test_loss: 0.001888985512778163\n",
      "epoch: 816, train_loss: 0.00015327334150914913, valid_loss: 0.0006031885325986271, test_loss: 0.0018886413890868425\n",
      "epoch: 817, train_loss: 0.0001529957358877215, valid_loss: 0.0006037469865987077, test_loss: 0.0018891989020630717\n",
      "epoch: 818, train_loss: 0.0001529698018440167, valid_loss: 0.0006064799429926401, test_loss: 0.0018891782965511084\n",
      "epoch: 819, train_loss: 0.00015291800999598902, valid_loss: 0.0006018723991777127, test_loss: 0.0018850862979888916\n",
      "epoch: 820, train_loss: 0.00015289625931156638, valid_loss: 0.0006040078975881139, test_loss: 0.0018846542807295918\n",
      "epoch: 821, train_loss: 0.00015287374971312997, valid_loss: 0.0006023202877258882, test_loss: 0.0018832453060895205\n",
      "epoch: 822, train_loss: 0.0001528581304465542, valid_loss: 0.0006038267310941592, test_loss: 0.0018847776809707284\n",
      "epoch: 823, train_loss: 0.00015258705420622036, valid_loss: 0.0006029241776559502, test_loss: 0.0018831496126949787\n",
      "epoch: 824, train_loss: 0.00015270555836057451, valid_loss: 0.0006019117475564902, test_loss: 0.0018815280636772513\n",
      "epoch: 825, train_loss: 0.00015291104121032455, valid_loss: 0.0006024230145461237, test_loss: 0.0018805742729455233\n",
      "epoch: 826, train_loss: 0.00015249617243408346, valid_loss: 0.0006009318167343736, test_loss: 0.0018791970796883106\n",
      "epoch: 827, train_loss: 0.00015233359717176822, valid_loss: 0.0006006179901305586, test_loss: 0.0018789046443998814\n",
      "epoch: 828, train_loss: 0.0001521933594156745, valid_loss: 0.0006001399694165835, test_loss: 0.0018782691331580281\n",
      "epoch: 829, train_loss: 0.00015214327710134017, valid_loss: 0.0006003570451866835, test_loss: 0.0018763154512271285\n",
      "epoch: 830, train_loss: 0.0001522347010904923, valid_loss: 0.0005998545238981023, test_loss: 0.0018754244083538651\n",
      "epoch: 831, train_loss: 0.00015237335553493998, valid_loss: 0.0005989125881266469, test_loss: 0.0018749609589576721\n",
      "epoch: 832, train_loss: 0.0001521511025519272, valid_loss: 0.0005982813454465941, test_loss: 0.0018734557088464499\n",
      "epoch: 833, train_loss: 0.0001519623170758638, valid_loss: 0.000601460250133338, test_loss: 0.001873543718829751\n",
      "epoch: 834, train_loss: 0.00015218004789304635, valid_loss: 0.0006003282226932546, test_loss: 0.001875374000519514\n",
      "epoch: 835, train_loss: 0.00015177877282005048, valid_loss: 0.0005992138370250663, test_loss: 0.0018718247301876545\n",
      "epoch: 836, train_loss: 0.00015187323784313935, valid_loss: 0.000600177954765968, test_loss: 0.0018724449910223484\n",
      "epoch: 837, train_loss: 0.00015165335903673068, valid_loss: 0.000596591623131341, test_loss: 0.0018712800228968263\n",
      "epoch: 838, train_loss: 0.00015177998157805237, valid_loss: 0.000597722032883515, test_loss: 0.001869782223366201\n",
      "epoch: 839, train_loss: 0.0001519011946030609, valid_loss: 0.0005963897298594626, test_loss: 0.001868019811809063\n",
      "epoch: 840, train_loss: 0.00015152881204140493, valid_loss: 0.0005982394165281827, test_loss: 0.0018673576414585114\n",
      "epoch: 841, train_loss: 0.00015147364288366035, valid_loss: 0.0005948532295102874, test_loss: 0.0018668657867237926\n",
      "epoch: 842, train_loss: 0.00015135345292156157, valid_loss: 0.0005964756661948437, test_loss: 0.001865106401965022\n",
      "epoch: 843, train_loss: 0.0001513556550059508, valid_loss: 0.0005961428881467631, test_loss: 0.0018653704319149256\n",
      "epoch: 844, train_loss: 0.00015156362433512896, valid_loss: 0.0005942922798567452, test_loss: 0.0018645969685167074\n",
      "epoch: 845, train_loss: 0.00015107141434387103, valid_loss: 0.0005964044636736313, test_loss: 0.0018644292140379548\n",
      "epoch: 846, train_loss: 0.00015102868833903062, valid_loss: 0.0005967305284381533, test_loss: 0.00186361453961581\n",
      "epoch: 847, train_loss: 0.00015101110140043917, valid_loss: 0.0005946440651314333, test_loss: 0.0018621572526171803\n",
      "epoch: 848, train_loss: 0.00015115586068967113, valid_loss: 0.0005955870195369547, test_loss: 0.0018607840174809098\n",
      "epoch: 849, train_loss: 0.00015118828805102765, valid_loss: 0.0005936253704324675, test_loss: 0.0018589539686217904\n",
      "epoch: 850, train_loss: 0.00015091029553846255, valid_loss: 0.0005939528200542554, test_loss: 0.001859988085925579\n",
      "epoch: 851, train_loss: 0.00015119011210196692, valid_loss: 0.0005936156449024566, test_loss: 0.0018584412755444646\n",
      "epoch: 852, train_loss: 0.00015063860653595918, valid_loss: 0.0005922015916439705, test_loss: 0.0018583768978714943\n",
      "epoch: 853, train_loss: 0.0001506950366487929, valid_loss: 0.0005922675530503815, test_loss: 0.0018562206532806158\n",
      "epoch: 854, train_loss: 0.0001504887123187275, valid_loss: 0.0005928665971926724, test_loss: 0.0018577312584966421\n",
      "epoch: 855, train_loss: 0.00015044666487602112, valid_loss: 0.0005929892019291098, test_loss: 0.001854411093518138\n",
      "epoch: 856, train_loss: 0.00015047795212645408, valid_loss: 0.0005938998753360162, test_loss: 0.001854834845289588\n",
      "epoch: 857, train_loss: 0.00015048814036518982, valid_loss: 0.0005905875441385433, test_loss: 0.0018516298150643706\n",
      "epoch: 858, train_loss: 0.0001505562091630924, valid_loss: 0.0005913878121646121, test_loss: 0.0018528900109231472\n",
      "epoch: 859, train_loss: 0.00015047533499613485, valid_loss: 0.0005924954457441345, test_loss: 0.0018518009455874562\n",
      "epoch: 860, train_loss: 0.00015015320367756826, valid_loss: 0.0005906066750564302, test_loss: 0.001849455526098609\n",
      "epoch: 861, train_loss: 0.00015052029124015698, valid_loss: 0.0005901640082205025, test_loss: 0.0018500344594940543\n",
      "epoch: 862, train_loss: 0.0001502442066102167, valid_loss: 0.0005884522712828281, test_loss: 0.0018476633122190833\n",
      "epoch: 863, train_loss: 0.0001500394519889201, valid_loss: 0.0005891647330524089, test_loss: 0.0018472905503585935\n",
      "epoch: 864, train_loss: 0.00015008486978173175, valid_loss: 0.0005880958633497357, test_loss: 0.0018468820489943027\n",
      "epoch: 865, train_loss: 0.00014982501559880683, valid_loss: 0.0005908596649533138, test_loss: 0.0018466855399310589\n",
      "epoch: 866, train_loss: 0.0001500578465588067, valid_loss: 0.0005881653220664399, test_loss: 0.0018446858739480376\n",
      "epoch: 867, train_loss: 0.00014966101772860503, valid_loss: 0.0005879322682934193, test_loss: 0.001844843034632504\n",
      "epoch: 868, train_loss: 0.00014963103382362297, valid_loss: 0.0005875714656819279, test_loss: 0.0018434013472869992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 869, train_loss: 0.00014956142473712808, valid_loss: 0.00058645460861347, test_loss: 0.0018424298614263535\n",
      "epoch: 870, train_loss: 0.00014955618066976413, valid_loss: 0.0005862998975014003, test_loss: 0.0018411424243822694\n",
      "epoch: 871, train_loss: 0.00014953015362034026, valid_loss: 0.0005877447062327216, test_loss: 0.0018427326576784253\n",
      "epoch: 872, train_loss: 0.0001497262786395605, valid_loss: 0.0005896816631623855, test_loss: 0.0018415675731375813\n",
      "epoch: 873, train_loss: 0.00014951635713931984, valid_loss: 0.0005879624804947525, test_loss: 0.0018406568560749292\n",
      "epoch: 874, train_loss: 0.00014929292798194143, valid_loss: 0.0005868422158528119, test_loss: 0.0018387107411399484\n",
      "epoch: 875, train_loss: 0.00014932976172649828, valid_loss: 0.0005850228578007469, test_loss: 0.0018365803407505155\n",
      "epoch: 876, train_loss: 0.00014929561217760911, valid_loss: 0.0005861538035484651, test_loss: 0.0018369641620665789\n",
      "epoch: 877, train_loss: 0.00014909557775472817, valid_loss: 0.0005852266986039467, test_loss: 0.0018356783548370004\n",
      "epoch: 878, train_loss: 0.0001489924657305338, valid_loss: 0.0005852225149283186, test_loss: 0.001834942726418376\n",
      "epoch: 879, train_loss: 0.0001493124942973499, valid_loss: 0.0005831113982518824, test_loss: 0.0018338948721066117\n",
      "epoch: 880, train_loss: 0.00014889732752319264, valid_loss: 0.0005847580929791244, test_loss: 0.0018351491307839751\n",
      "epoch: 881, train_loss: 0.00014892453011429018, valid_loss: 0.0005839006892832307, test_loss: 0.0018323062686249614\n",
      "epoch: 882, train_loss: 0.00014886736860954323, valid_loss: 0.0005826113823180398, test_loss: 0.0018315317574888468\n",
      "epoch: 883, train_loss: 0.00014877569091091257, valid_loss: 0.0005833512938503796, test_loss: 0.001831107889302075\n",
      "epoch: 884, train_loss: 0.00014873539064707154, valid_loss: 0.0005831791956249314, test_loss: 0.0018302114913240075\n",
      "epoch: 885, train_loss: 0.00014864390212333882, valid_loss: 0.0005816568009322509, test_loss: 0.0018286427948623896\n",
      "epoch: 886, train_loss: 0.00014912734521836367, valid_loss: 0.0005814155786841487, test_loss: 0.0018279717769473791\n",
      "epoch: 887, train_loss: 0.00014848584807781583, valid_loss: 0.000581959837290924, test_loss: 0.0018275302136316895\n",
      "epoch: 888, train_loss: 0.00014833755362222132, valid_loss: 0.0005833216297711866, test_loss: 0.0018275179900228977\n",
      "epoch: 889, train_loss: 0.0001483068601531989, valid_loss: 0.0005810238775059892, test_loss: 0.0018254200695082545\n",
      "epoch: 890, train_loss: 0.00014840377907207966, valid_loss: 0.0005814714337854335, test_loss: 0.0018251428846269846\n",
      "epoch: 891, train_loss: 0.00014846214427329275, valid_loss: 0.0005806526896776631, test_loss: 0.0018237975891679525\n",
      "epoch: 892, train_loss: 0.00014845797198612, valid_loss: 0.0005847591431423401, test_loss: 0.0018284408142790198\n",
      "epoch: 893, train_loss: 0.0001481595969855097, valid_loss: 0.0005798340183294689, test_loss: 0.001822906662710011\n",
      "epoch: 894, train_loss: 0.00014803903183707482, valid_loss: 0.0005790911333557839, test_loss: 0.001821016427129507\n",
      "epoch: 895, train_loss: 0.0001481441308303898, valid_loss: 0.0005805622349726036, test_loss: 0.0018213068833574653\n",
      "epoch: 896, train_loss: 0.00014798402591385758, valid_loss: 0.0005791329458588734, test_loss: 0.0018197840545326471\n",
      "epoch: 897, train_loss: 0.00014807592948352027, valid_loss: 0.000579172638632978, test_loss: 0.0018186330562457442\n",
      "epoch: 898, train_loss: 0.00014794394582682088, valid_loss: 0.0005789497614993403, test_loss: 0.0018182772910222411\n",
      "epoch: 899, train_loss: 0.00014788173354210576, valid_loss: 0.0005779476171786276, test_loss: 0.0018174248980358243\n",
      "epoch: 900, train_loss: 0.0001477016741536436, valid_loss: 0.0005780295032309368, test_loss: 0.0018167525995522738\n",
      "epoch: 901, train_loss: 0.00014755684368656543, valid_loss: 0.0005781688717737173, test_loss: 0.0018161407206207514\n",
      "epoch: 902, train_loss: 0.0001475438279474316, valid_loss: 0.0005769062481704168, test_loss: 0.001814654329791665\n",
      "epoch: 903, train_loss: 0.00014776917004897058, valid_loss: 0.0005798306301585399, test_loss: 0.0018153856508433819\n",
      "epoch: 904, train_loss: 0.00014736045610017913, valid_loss: 0.0005767784702281157, test_loss: 0.0018143358174711466\n",
      "epoch: 905, train_loss: 0.00014737403746593097, valid_loss: 0.0005768780223055122, test_loss: 0.0018127347575500607\n",
      "epoch: 906, train_loss: 0.00014734145193158284, valid_loss: 0.0005770699062850326, test_loss: 0.0018120802706107497\n",
      "epoch: 907, train_loss: 0.00014728393453760478, valid_loss: 0.0005753050645580515, test_loss: 0.0018120028544217348\n",
      "epoch: 908, train_loss: 0.00014722881251541168, valid_loss: 0.0005773318844148889, test_loss: 0.0018107579089701176\n",
      "epoch: 909, train_loss: 0.0001470330864084763, valid_loss: 0.0005746965155897973, test_loss: 0.0018093336839228868\n",
      "epoch: 910, train_loss: 0.00014709767445152545, valid_loss: 0.0005763516358759565, test_loss: 0.0018098867731168866\n",
      "epoch: 911, train_loss: 0.0001470002009781361, valid_loss: 0.0005774493280720586, test_loss: 0.00180913507938385\n",
      "epoch: 912, train_loss: 0.0001468900382346676, valid_loss: 0.000574950645386707, test_loss: 0.001806613290682435\n",
      "epoch: 913, train_loss: 0.00014723261309605417, valid_loss: 0.0005743710013727347, test_loss: 0.001805772539228201\n",
      "epoch: 914, train_loss: 0.00014684786646798983, valid_loss: 0.0005739463401065829, test_loss: 0.0018065578769892454\n",
      "epoch: 915, train_loss: 0.00014675845032679322, valid_loss: 0.0005732589391603445, test_loss: 0.0018040150171145797\n",
      "epoch: 916, train_loss: 0.00014679597813443726, valid_loss: 0.0005723276699427515, test_loss: 0.001803470659069717\n",
      "epoch: 917, train_loss: 0.0001467180855283716, valid_loss: 0.0005727052218086707, test_loss: 0.0018023950979113579\n",
      "epoch: 918, train_loss: 0.0001465830114877621, valid_loss: 0.0005754260976876443, test_loss: 0.0018045486649498343\n",
      "epoch: 919, train_loss: 0.00014658893917879576, valid_loss: 0.0005719956316170283, test_loss: 0.0018013096414506435\n",
      "epoch: 920, train_loss: 0.00014648865908384323, valid_loss: 0.0005731155421623649, test_loss: 0.0018005080055445433\n",
      "epoch: 921, train_loss: 0.00014635321302035743, valid_loss: 0.0005720928553879882, test_loss: 0.0017994449008256197\n",
      "epoch: 922, train_loss: 0.0001466435189839736, valid_loss: 0.0005723313637039004, test_loss: 0.0017999578267335892\n",
      "epoch: 923, train_loss: 0.00014640467491952458, valid_loss: 0.0005721808556700125, test_loss: 0.0017982299905270338\n",
      "epoch: 924, train_loss: 0.00014633197228565973, valid_loss: 0.0005707328915984059, test_loss: 0.0017980709671974182\n",
      "epoch: 925, train_loss: 0.00014616612189338016, valid_loss: 0.0005717561677253494, test_loss: 0.0017971107736229897\n",
      "epoch: 926, train_loss: 0.0001460622800590025, valid_loss: 0.0005688148788370503, test_loss: 0.0017954098293557763\n",
      "epoch: 927, train_loss: 0.00014614391187458986, valid_loss: 0.0005701978103995012, test_loss: 0.0017954944632947445\n",
      "epoch: 928, train_loss: 0.00014592926606343573, valid_loss: 0.0005690991941567821, test_loss: 0.0017935116775333881\n",
      "epoch: 929, train_loss: 0.0001458935920432534, valid_loss: 0.0005706464629232263, test_loss: 0.0017945090075954795\n",
      "epoch: 930, train_loss: 0.00014583717679868087, valid_loss: 0.0005695289849730519, test_loss: 0.0017938470700755715\n",
      "epoch: 931, train_loss: 0.00014575033761186364, valid_loss: 0.0005681147740688175, test_loss: 0.00179123820271343\n",
      "epoch: 932, train_loss: 0.00014587631290930364, valid_loss: 0.0005691865953849629, test_loss: 0.001790553331375122\n",
      "epoch: 933, train_loss: 0.00014552639755572233, valid_loss: 0.0005692113603193624, test_loss: 0.0017908966401591897\n",
      "epoch: 934, train_loss: 0.00014563392640253448, valid_loss: 0.0005692647561469736, test_loss: 0.0017895709024742246\n",
      "epoch: 935, train_loss: 0.00014553442673312256, valid_loss: 0.0005689397366950288, test_loss: 0.0017895825440064073\n",
      "epoch: 936, train_loss: 0.00014564090879107624, valid_loss: 0.0005669778765877709, test_loss: 0.0017870469018816948\n",
      "epoch: 937, train_loss: 0.00014539815532296652, valid_loss: 0.0005683482838018487, test_loss: 0.0017878968501463532\n",
      "epoch: 938, train_loss: 0.00014527972835703227, valid_loss: 0.0005675636493833736, test_loss: 0.0017867438727989793\n",
      "epoch: 939, train_loss: 0.0001451623424857288, valid_loss: 0.0005660389482121294, test_loss: 0.0017857576021924615\n",
      "epoch: 940, train_loss: 0.00014551883562133932, valid_loss: 0.0005674431207201754, test_loss: 0.0017852815799415112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 941, train_loss: 0.00014528948636523083, valid_loss: 0.0005657541623804718, test_loss: 0.0017836750485002995\n",
      "epoch: 942, train_loss: 0.00014507526319231027, valid_loss: 0.0005646067341634383, test_loss: 0.001782348845154047\n",
      "epoch: 943, train_loss: 0.00014507304212716207, valid_loss: 0.000565482204062088, test_loss: 0.0017827190458774567\n",
      "epoch: 944, train_loss: 0.00014495637372854853, valid_loss: 0.000566718852496706, test_loss: 0.0017816930776461959\n",
      "epoch: 945, train_loss: 0.000145126676690542, valid_loss: 0.0005648047081194818, test_loss: 0.0017804695526137948\n",
      "epoch: 946, train_loss: 0.00014497775934970653, valid_loss: 0.0005663441600821292, test_loss: 0.0017803587252274156\n",
      "epoch: 947, train_loss: 0.00014504028544963703, valid_loss: 0.0005644046250381507, test_loss: 0.0017795133171603084\n",
      "epoch: 948, train_loss: 0.00014496970866077945, valid_loss: 0.0005632941902149469, test_loss: 0.0017783065559342504\n",
      "epoch: 949, train_loss: 0.0001448848042459956, valid_loss: 0.0005634989793179557, test_loss: 0.00177775917109102\n",
      "epoch: 950, train_loss: 0.00014475698370243543, valid_loss: 0.0005640048354204433, test_loss: 0.001776806078851223\n",
      "epoch: 951, train_loss: 0.00014480340178124607, valid_loss: 0.0005645032482182918, test_loss: 0.0017767007229849696\n",
      "epoch: 952, train_loss: 0.00014445462970204812, valid_loss: 0.0005627076849729443, test_loss: 0.0017751982668414712\n",
      "epoch: 953, train_loss: 0.00014436907741069064, valid_loss: 0.0005640138430559697, test_loss: 0.001774958218447864\n",
      "epoch: 954, train_loss: 0.00014459906695106918, valid_loss: 0.0005632896475920764, test_loss: 0.0017740341136232018\n",
      "epoch: 955, train_loss: 0.00014437065629349291, valid_loss: 0.0005616499571866976, test_loss: 0.0017726098885759711\n",
      "epoch: 956, train_loss: 0.000144554564030841, valid_loss: 0.00056239416153403, test_loss: 0.0017718893941491842\n",
      "epoch: 957, train_loss: 0.00014421576697318613, valid_loss: 0.0005620987431029789, test_loss: 0.0017710761167109013\n",
      "epoch: 958, train_loss: 0.00014427577021390039, valid_loss: 0.0005624715292166608, test_loss: 0.001772581017576158\n",
      "epoch: 959, train_loss: 0.00014416947986161256, valid_loss: 0.0005647214517618219, test_loss: 0.0017722899792715907\n",
      "epoch: 960, train_loss: 0.0001441301210434176, valid_loss: 0.0005598695206572302, test_loss: 0.0017687375657260418\n",
      "epoch: 961, train_loss: 0.00014415549040954235, valid_loss: 0.0005603999476685809, test_loss: 0.0017680200980976224\n",
      "epoch: 962, train_loss: 0.00014417285021171784, valid_loss: 0.0005605533709361529, test_loss: 0.0017674140399321914\n",
      "epoch: 963, train_loss: 0.0001439854848127731, valid_loss: 0.0005593354653683491, test_loss: 0.0017659373115748167\n",
      "epoch: 964, train_loss: 0.0001438905582408947, valid_loss: 0.0005589638385572471, test_loss: 0.0017655757255852222\n",
      "epoch: 965, train_loss: 0.0001437199825886637, valid_loss: 0.0005596390716770353, test_loss: 0.0017657382413744926\n",
      "epoch: 966, train_loss: 0.00014369823190424106, valid_loss: 0.0005602947542987143, test_loss: 0.0017642106395214796\n",
      "epoch: 967, train_loss: 0.00014361004128738347, valid_loss: 0.0005589944945919948, test_loss: 0.0017636902630329132\n",
      "epoch: 968, train_loss: 0.0001436195019303578, valid_loss: 0.0005587731187309449, test_loss: 0.0017626340268179774\n",
      "epoch: 969, train_loss: 0.0001435497947766081, valid_loss: 0.0005593020614469424, test_loss: 0.0017621368169784546\n",
      "epoch: 970, train_loss: 0.00014369348323468685, valid_loss: 0.0005580476718023419, test_loss: 0.0017616123659536242\n",
      "epoch: 971, train_loss: 0.00014357066443746748, valid_loss: 0.0005585513766466951, test_loss: 0.0017610095674172044\n",
      "epoch: 972, train_loss: 0.00014333385732480446, valid_loss: 0.0005567336944902005, test_loss: 0.0017595606623217463\n",
      "epoch: 973, train_loss: 0.00014342381884647614, valid_loss: 0.0005561992026438626, test_loss: 0.001758155645802617\n",
      "epoch: 974, train_loss: 0.00014313896732030275, valid_loss: 0.0005560045853295984, test_loss: 0.001757874502800405\n",
      "epoch: 975, train_loss: 0.00014305201583066145, valid_loss: 0.0005573398593696766, test_loss: 0.001757337711751461\n",
      "epoch: 976, train_loss: 0.00014313078344971672, valid_loss: 0.0005550170002000717, test_loss: 0.001757279271259904\n",
      "epoch: 977, train_loss: 0.0001431857599527575, valid_loss: 0.0005546776883420534, test_loss: 0.0017553505022078753\n",
      "epoch: 978, train_loss: 0.000143062641575892, valid_loss: 0.0005558963724373219, test_loss: 0.001755623146891594\n",
      "epoch: 979, train_loss: 0.00014302269213974637, valid_loss: 0.0005553287895357547, test_loss: 0.0017545977607369423\n",
      "epoch: 980, train_loss: 0.00014292920715651354, valid_loss: 0.0005554945867819091, test_loss: 0.0017527787713333964\n",
      "epoch: 981, train_loss: 0.00014289840170101303, valid_loss: 0.0005561707148444839, test_loss: 0.0017532859928905964\n",
      "epoch: 982, train_loss: 0.00014293558690595725, valid_loss: 0.0005539065605262294, test_loss: 0.0017517704982310534\n",
      "epoch: 983, train_loss: 0.00014276708300357038, valid_loss: 0.0005549120590634024, test_loss: 0.0017515377840027213\n",
      "epoch: 984, train_loss: 0.0001426705585681307, valid_loss: 0.0005530943138486085, test_loss: 0.001749386079609394\n",
      "epoch: 985, train_loss: 0.00014271260980698887, valid_loss: 0.0005529340851353481, test_loss: 0.0017496119253337383\n",
      "epoch: 986, train_loss: 0.0001424155261306821, valid_loss: 0.0005540314038322928, test_loss: 0.0017504565184935927\n",
      "epoch: 987, train_loss: 0.00014238179795464257, valid_loss: 0.0005576246039709076, test_loss: 0.0017517998348921537\n",
      "epoch: 988, train_loss: 0.00014263963543192443, valid_loss: 0.0005525124724954367, test_loss: 0.0017470995662733912\n",
      "epoch: 989, train_loss: 0.00014242591936161497, valid_loss: 0.0005531305844973152, test_loss: 0.0017467335565015674\n",
      "epoch: 990, train_loss: 0.00014239643496694043, valid_loss: 0.0005517426034202799, test_loss: 0.0017452880274504423\n",
      "epoch: 991, train_loss: 0.0001424525436722552, valid_loss: 0.0005506634455135403, test_loss: 0.0017450648592785\n",
      "epoch: 992, train_loss: 0.00014231509577138755, valid_loss: 0.0005523587412123258, test_loss: 0.0017456519417464733\n",
      "epoch: 993, train_loss: 0.00014224027330308908, valid_loss: 0.0005500210318132304, test_loss: 0.0017434312030673027\n",
      "epoch: 994, train_loss: 0.00014201327544667393, valid_loss: 0.0005537409961107187, test_loss: 0.0017446493729948997\n",
      "epoch: 995, train_loss: 0.00014212520150265291, valid_loss: 0.0005511240039292412, test_loss: 0.0017411843873560429\n",
      "epoch: 996, train_loss: 0.00014222045897242978, valid_loss: 0.0005524794469238259, test_loss: 0.001744318986311555\n",
      "epoch: 997, train_loss: 0.00014197734993143493, valid_loss: 0.0005513985015568323, test_loss: 0.0017410983564332128\n",
      "epoch: 998, train_loss: 0.00014195186193556646, valid_loss: 0.0005512827750256596, test_loss: 0.0017403505044057965\n",
      "epoch: 999, train_loss: 0.0001417887194649033, valid_loss: 0.0005503065913217142, test_loss: 0.0017382813384756446\n",
      "epoch: 1000, train_loss: 0.00014189984073670095, valid_loss: 0.0005497425663634203, test_loss: 0.001737932674586773\n",
      "epoch: 1001, train_loss: 0.00014175070290271518, valid_loss: 0.0005494136518488327, test_loss: 0.001738815219141543\n",
      "epoch: 1002, train_loss: 0.00014161853576534548, valid_loss: 0.0005483902205014601, test_loss: 0.0017371329013258219\n",
      "epoch: 1003, train_loss: 0.0001417836481224462, valid_loss: 0.000550845844069651, test_loss: 0.0017364457016810775\n",
      "epoch: 1004, train_loss: 0.00014166939692349052, valid_loss: 0.0005496890977762329, test_loss: 0.001735862111672759\n",
      "epoch: 1005, train_loss: 0.00014145052156654066, valid_loss: 0.000548207261696613, test_loss: 0.001735786092467606\n",
      "epoch: 1006, train_loss: 0.0001414117587441781, valid_loss: 0.0005485664563214717, test_loss: 0.0017339588375762105\n",
      "epoch: 1007, train_loss: 0.00014125281639913422, valid_loss: 0.0005482667935818123, test_loss: 0.0017357912147417665\n",
      "epoch: 1008, train_loss: 0.00014122706172360188, valid_loss: 0.0005479665123857558, test_loss: 0.0017324653454124928\n",
      "epoch: 1009, train_loss: 0.0001412718240475363, valid_loss: 0.0005478160989393169, test_loss: 0.001731497817672789\n",
      "epoch: 1010, train_loss: 0.00014122197994172734, valid_loss: 0.0005486399095389061, test_loss: 0.0017332724528387189\n",
      "epoch: 1011, train_loss: 0.00014126816297547245, valid_loss: 0.0005458646555780433, test_loss: 0.0017294861609116197\n",
      "epoch: 1012, train_loss: 0.00014100714242490739, valid_loss: 0.0005463584190389762, test_loss: 0.00173054332844913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1013, train_loss: 0.00014100331115870452, valid_loss: 0.0005467356143829724, test_loss: 0.001728423056192696\n",
      "epoch: 1014, train_loss: 0.00014103431622852045, valid_loss: 0.0005444664711831138, test_loss: 0.0017275389982387424\n",
      "epoch: 1015, train_loss: 0.0001409114328967205, valid_loss: 0.0005443503954059755, test_loss: 0.001727021299302578\n",
      "epoch: 1016, train_loss: 0.0001408215659624978, valid_loss: 0.0005450920289149508, test_loss: 0.0017259640153497458\n",
      "epoch: 1017, train_loss: 0.00014093039277687913, valid_loss: 0.0005439657397801057, test_loss: 0.0017249244265258312\n",
      "epoch: 1018, train_loss: 0.00014091688227262753, valid_loss: 0.0005448454442860869, test_loss: 0.001725562266074121\n",
      "epoch: 1019, train_loss: 0.00014068440656916684, valid_loss: 0.0005449235638176712, test_loss: 0.0017237701686099172\n",
      "epoch: 1020, train_loss: 0.0001405222216777949, valid_loss: 0.0005453653502627276, test_loss: 0.0017235721461474895\n",
      "epoch: 1021, train_loss: 0.0001405093694895344, valid_loss: 0.000545514099940192, test_loss: 0.0017229188233613968\n",
      "epoch: 1022, train_loss: 0.00014059075993313655, valid_loss: 0.0005433529901589887, test_loss: 0.0017215203261002898\n",
      "epoch: 1023, train_loss: 0.00014047380808852208, valid_loss: 0.0005424757400760427, test_loss: 0.0017209053039550781\n",
      "epoch: 1024, train_loss: 0.0001405257878460638, valid_loss: 0.0005421363530331291, test_loss: 0.0017204192699864507\n",
      "epoch: 1025, train_loss: 0.0001403277343817298, valid_loss: 0.0005424014088930562, test_loss: 0.0017199708381667733\n",
      "epoch: 1026, train_loss: 0.0001402650055040241, valid_loss: 0.0005426622907786319, test_loss: 0.0017218600260093808\n",
      "epoch: 1027, train_loss: 0.0001402443439988217, valid_loss: 0.0005414147623620617, test_loss: 0.0017178863054141402\n",
      "epoch: 1028, train_loss: 0.00014058065014820465, valid_loss: 0.0005420762196687671, test_loss: 0.0017182060983031988\n",
      "epoch: 1029, train_loss: 0.00014017583326558056, valid_loss: 0.0005421078143020471, test_loss: 0.0017178672133013606\n",
      "epoch: 1030, train_loss: 0.00014015746305433467, valid_loss: 0.0005416312318023605, test_loss: 0.0017169693019241095\n",
      "epoch: 1031, train_loss: 0.00014015967747621724, valid_loss: 0.0005417728389147669, test_loss: 0.0017151712672784925\n",
      "epoch: 1032, train_loss: 0.000140075836616629, valid_loss: 0.0005409894971914279, test_loss: 0.0017154795350506902\n",
      "epoch: 1033, train_loss: 0.00013977179617580512, valid_loss: 0.0005396345344100458, test_loss: 0.0017133050132542849\n",
      "epoch: 1034, train_loss: 0.00013988563074710092, valid_loss: 0.0005397875162695224, test_loss: 0.0017124332953244448\n",
      "epoch: 1035, train_loss: 0.00013985075550052622, valid_loss: 0.00053921482564571, test_loss: 0.0017116249073296785\n",
      "epoch: 1036, train_loss: 0.00013983931484804287, valid_loss: 0.0005401433542526016, test_loss: 0.0017115464434027672\n",
      "epoch: 1037, train_loss: 0.00013968081570372146, valid_loss: 0.0005385452774741376, test_loss: 0.0017115692608058453\n",
      "epoch: 1038, train_loss: 0.00013979122171725106, valid_loss: 0.0005402899526719315, test_loss: 0.0017119356198236346\n",
      "epoch: 1039, train_loss: 0.00013951619210116485, valid_loss: 0.000541028389610195, test_loss: 0.001711523625999689\n",
      "epoch: 1040, train_loss: 0.00013955445636225784, valid_loss: 0.0005395290524271937, test_loss: 0.0017109215259552002\n",
      "epoch: 1041, train_loss: 0.0001396606628828601, valid_loss: 0.0005386770887222762, test_loss: 0.0017077862285077572\n",
      "epoch: 1042, train_loss: 0.00013943160403236422, valid_loss: 0.000537485429958906, test_loss: 0.0017070234753191471\n",
      "epoch: 1043, train_loss: 0.00013955656986977175, valid_loss: 0.000538288283375247, test_loss: 0.0017072958871722221\n",
      "epoch: 1044, train_loss: 0.00013941305666736773, valid_loss: 0.0005383429670473561, test_loss: 0.0017057800432667136\n",
      "epoch: 1045, train_loss: 0.00013910109334139398, valid_loss: 0.0005386856209952384, test_loss: 0.0017060786485671997\n",
      "epoch: 1046, train_loss: 0.0001391559183147088, valid_loss: 0.0005385528978270789, test_loss: 0.0017052588518708944\n",
      "epoch: 1047, train_loss: 0.00013936962869079054, valid_loss: 0.0005368981704426309, test_loss: 0.0017047139117494226\n",
      "epoch: 1048, train_loss: 0.00013904101797379553, valid_loss: 0.0005361851338723985, test_loss: 0.0017029973678290844\n",
      "epoch: 1049, train_loss: 0.00013908047612462923, valid_loss: 0.0005370643193600699, test_loss: 0.0017022088868543506\n",
      "epoch: 1050, train_loss: 0.0001390225636145181, valid_loss: 0.0005362489682738669, test_loss: 0.0017026134300976992\n",
      "epoch: 1051, train_loss: 0.00013908596504378417, valid_loss: 0.0005366261223874366, test_loss: 0.001701086526736617\n",
      "epoch: 1052, train_loss: 0.00013888125281284928, valid_loss: 0.0005345181562006474, test_loss: 0.001699370564892888\n",
      "epoch: 1053, train_loss: 0.00013870850101659965, valid_loss: 0.000537263459894651, test_loss: 0.00170042272657156\n",
      "epoch: 1054, train_loss: 0.00013888515494055235, valid_loss: 0.0005354370708422115, test_loss: 0.001698553329333663\n",
      "epoch: 1055, train_loss: 0.00013868822576984277, valid_loss: 0.0005355895118555054, test_loss: 0.001699737855233252\n",
      "epoch: 1056, train_loss: 0.00013865828235714656, valid_loss: 0.0005343332062087333, test_loss: 0.0016980160726234317\n",
      "epoch: 1057, train_loss: 0.00013852460688212886, valid_loss: 0.0005342160026581647, test_loss: 0.0016969430726021528\n",
      "epoch: 1058, train_loss: 0.00013865203262590197, valid_loss: 0.0005347637682765102, test_loss: 0.0016965600661933422\n",
      "epoch: 1059, train_loss: 0.0001385090122905398, valid_loss: 0.0005346061904371405, test_loss: 0.0016964887036010623\n",
      "epoch: 1060, train_loss: 0.00013836913453617498, valid_loss: 0.0005330309965453731, test_loss: 0.0016941435169428587\n",
      "epoch: 1061, train_loss: 0.00013843519991496578, valid_loss: 0.0005351140498532914, test_loss: 0.001695699873380363\n",
      "epoch: 1062, train_loss: 0.0001384112544221115, valid_loss: 0.0005344307549724666, test_loss: 0.0016939295455813408\n",
      "epoch: 1063, train_loss: 0.00013818547101792595, valid_loss: 0.0005328700402363514, test_loss: 0.0016924336086958647\n",
      "epoch: 1064, train_loss: 0.0001381642109861233, valid_loss: 0.000532264937646687, test_loss: 0.0016917749308049679\n",
      "epoch: 1065, train_loss: 0.00013835646772666064, valid_loss: 0.0005316502817246752, test_loss: 0.0016900176415219903\n",
      "epoch: 1066, train_loss: 0.0001383603742832075, valid_loss: 0.0005314033284472922, test_loss: 0.0016900089103728533\n",
      "epoch: 1067, train_loss: 0.00013802593489147156, valid_loss: 0.0005333537507491807, test_loss: 0.001691159326583147\n",
      "epoch: 1068, train_loss: 0.00013795334297870085, valid_loss: 0.0005321216570640294, test_loss: 0.0016884597716853023\n",
      "epoch: 1069, train_loss: 0.00013798309247130933, valid_loss: 0.0005320941442429709, test_loss: 0.0016882854979485273\n",
      "epoch: 1070, train_loss: 0.00013779963774140924, valid_loss: 0.0005296128583722748, test_loss: 0.001687244395725429\n",
      "epoch: 1071, train_loss: 0.00013779567867142919, valid_loss: 0.0005302276598134389, test_loss: 0.001685791532509029\n",
      "epoch: 1072, train_loss: 0.00013777153957483318, valid_loss: 0.0005299703067673059, test_loss: 0.001685373717918992\n",
      "epoch: 1073, train_loss: 0.00013788983051199466, valid_loss: 0.0005293679181098317, test_loss: 0.0016850641695782542\n",
      "epoch: 1074, train_loss: 0.0001376661931989593, valid_loss: 0.0005302990951652949, test_loss: 0.0016844283090904355\n",
      "epoch: 1075, train_loss: 0.00013759715258619627, valid_loss: 0.0005311147542670369, test_loss: 0.0016846639337018132\n",
      "epoch: 1076, train_loss: 0.00013746241387232894, valid_loss: 0.0005298046429137079, test_loss: 0.0016836859285831451\n",
      "epoch: 1077, train_loss: 0.00013780065669182122, valid_loss: 0.0005305523057662261, test_loss: 0.0016833986155688763\n",
      "epoch: 1078, train_loss: 0.00013761065296737638, valid_loss: 0.0005309030190498257, test_loss: 0.0016832128167152405\n",
      "epoch: 1079, train_loss: 0.00013772256700220802, valid_loss: 0.0005289855689625256, test_loss: 0.0016810332890599966\n",
      "epoch: 1080, train_loss: 0.00013744920073330158, valid_loss: 0.0005290767051822817, test_loss: 0.0016802202444523573\n",
      "epoch: 1081, train_loss: 0.00013734634684742954, valid_loss: 0.0005283008140395395, test_loss: 0.001679117907769978\n",
      "epoch: 1082, train_loss: 0.00013755728951250404, valid_loss: 0.0005292960680283917, test_loss: 0.0016792155802249908\n",
      "epoch: 1083, train_loss: 0.0001371914486694352, valid_loss: 0.0005275861622067168, test_loss: 0.0016780138248577714\n",
      "epoch: 1084, train_loss: 0.00013711723959599823, valid_loss: 0.0005290738845360465, test_loss: 0.0016791225643828511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1085, train_loss: 0.0001371294233451962, valid_loss: 0.0005268682046638181, test_loss: 0.0016780068399384618\n",
      "epoch: 1086, train_loss: 0.0001369341122841908, valid_loss: 0.0005274377399473451, test_loss: 0.0016772195231169462\n",
      "epoch: 1087, train_loss: 0.0001370279787806794, valid_loss: 0.0005287341482471675, test_loss: 0.0016766602639108896\n",
      "epoch: 1088, train_loss: 0.00013697870473033223, valid_loss: 0.0005255538490018807, test_loss: 0.001674703904427588\n",
      "epoch: 1089, train_loss: 0.00013712447347959665, valid_loss: 0.0005257872398942709, test_loss: 0.0016740810824558139\n",
      "epoch: 1090, train_loss: 0.00013679872474471188, valid_loss: 0.0005267875142938768, test_loss: 0.001674859900958836\n",
      "epoch: 1091, train_loss: 0.0001368811461598734, valid_loss: 0.0005255739621740455, test_loss: 0.0016725255409255624\n",
      "epoch: 1092, train_loss: 0.000136780184339327, valid_loss: 0.000526011366067299, test_loss: 0.0016730953939259052\n",
      "epoch: 1093, train_loss: 0.00013685544905931002, valid_loss: 0.0005244819427995632, test_loss: 0.0016718311235308647\n",
      "epoch: 1094, train_loss: 0.0001366081354638521, valid_loss: 0.0005242543144656034, test_loss: 0.0016699485713616014\n",
      "epoch: 1095, train_loss: 0.00013670712961238283, valid_loss: 0.0005236010498871716, test_loss: 0.0016693182988092303\n",
      "epoch: 1096, train_loss: 0.00013656961401347476, valid_loss: 0.0005244562877730156, test_loss: 0.0016700827982276678\n",
      "epoch: 1097, train_loss: 0.00013643792107873395, valid_loss: 0.0005231640073664797, test_loss: 0.0016691484488546848\n",
      "epoch: 1098, train_loss: 0.00013630011918671104, valid_loss: 0.000524538993583216, test_loss: 0.0016689249314367771\n",
      "epoch: 1099, train_loss: 0.00013652153543459818, valid_loss: 0.0005253135022940114, test_loss: 0.0016691182972863317\n",
      "epoch: 1100, train_loss: 0.00013635101893906602, valid_loss: 0.0005237767909420654, test_loss: 0.0016660576220601797\n",
      "epoch: 1101, train_loss: 0.00013654324436135104, valid_loss: 0.0005245899058839617, test_loss: 0.0016660939436405897\n",
      "epoch: 1102, train_loss: 0.00013627693292421654, valid_loss: 0.0005246321597951464, test_loss: 0.0016664727590978146\n",
      "epoch: 1103, train_loss: 0.0001361603742976592, valid_loss: 0.0005244444861697654, test_loss: 0.0016667136223986745\n",
      "epoch: 1104, train_loss: 0.00013595762088105244, valid_loss: 0.00052211867053605, test_loss: 0.001663470291532576\n",
      "epoch: 1105, train_loss: 0.00013612264877377564, valid_loss: 0.000521668431853565, test_loss: 0.0016643211711198092\n",
      "epoch: 1106, train_loss: 0.00013597692083314305, valid_loss: 0.0005231744459403368, test_loss: 0.0016632932238280773\n",
      "epoch: 1107, train_loss: 0.00013615119868241575, valid_loss: 0.0005226031401737904, test_loss: 0.0016626972937956452\n",
      "epoch: 1108, train_loss: 0.00013596466337533101, valid_loss: 0.000520679859619122, test_loss: 0.001660437905229628\n",
      "epoch: 1109, train_loss: 0.00013598345622481048, valid_loss: 0.0005203534916896994, test_loss: 0.0016605197452008724\n",
      "epoch: 1110, train_loss: 0.00013575409146774885, valid_loss: 0.000521888563525863, test_loss: 0.0016599129885435104\n",
      "epoch: 1111, train_loss: 0.00013566018732608822, valid_loss: 0.0005209872348738523, test_loss: 0.0016594943590462208\n",
      "epoch: 1112, train_loss: 0.00013588208821602166, valid_loss: 0.0005198846265557222, test_loss: 0.0016590565210208297\n",
      "epoch: 1113, train_loss: 0.00013563739238357496, valid_loss: 0.0005212210647490186, test_loss: 0.001659601111896336\n",
      "epoch: 1114, train_loss: 0.00013562485179610795, valid_loss: 0.0005200532565747077, test_loss: 0.0016564117977395654\n",
      "epoch: 1115, train_loss: 0.0001355408657337134, valid_loss: 0.0005206094332000551, test_loss: 0.0016564202960580587\n",
      "epoch: 1116, train_loss: 0.0001353409667069133, valid_loss: 0.0005189433407698137, test_loss: 0.0016552803572267294\n",
      "epoch: 1117, train_loss: 0.00013549088338470978, valid_loss: 0.0005197136973341306, test_loss: 0.0016556947957724333\n",
      "epoch: 1118, train_loss: 0.0001354052874376066, valid_loss: 0.0005192764559372639, test_loss: 0.001655330415815115\n",
      "epoch: 1119, train_loss: 0.00013551375330122107, valid_loss: 0.0005193391164842373, test_loss: 0.0016539645148441195\n",
      "epoch: 1120, train_loss: 0.00013510953494519725, valid_loss: 0.0005187385128616976, test_loss: 0.0016534373862668872\n",
      "epoch: 1121, train_loss: 0.00013529587348508042, valid_loss: 0.0005171705100413723, test_loss: 0.0016522204969078302\n",
      "epoch: 1122, train_loss: 0.00013537160355000472, valid_loss: 0.0005178550806400987, test_loss: 0.001652125851251185\n",
      "epoch: 1123, train_loss: 0.0001350868077007244, valid_loss: 0.0005173079019490009, test_loss: 0.0016502276994287968\n",
      "epoch: 1124, train_loss: 0.00013506385363618156, valid_loss: 0.000516768008916794, test_loss: 0.0016495700692757964\n",
      "epoch: 1125, train_loss: 0.00013484518991469446, valid_loss: 0.0005186856506043114, test_loss: 0.001651161233894527\n",
      "epoch: 1126, train_loss: 0.00013506142156826252, valid_loss: 0.0005186334504590681, test_loss: 0.0016500386409461498\n",
      "epoch: 1127, train_loss: 0.00013495804191536635, valid_loss: 0.0005177880811970681, test_loss: 0.0016497289761900902\n",
      "epoch: 1128, train_loss: 0.00013502685887151924, valid_loss: 0.0005165045707447765, test_loss: 0.0016475385054945946\n",
      "epoch: 1129, train_loss: 0.0001349191943122803, valid_loss: 0.000516463801128945, test_loss: 0.001647154800593853\n",
      "epoch: 1130, train_loss: 0.00013476967116888693, valid_loss: 0.0005177123239263892, test_loss: 0.0016467759851366282\n",
      "epoch: 1131, train_loss: 0.00013464720826089868, valid_loss: 0.0005159673140345452, test_loss: 0.0016461830819025636\n",
      "epoch: 1132, train_loss: 0.00013453246988912883, valid_loss: 0.0005159090966723549, test_loss: 0.0016445177607238293\n",
      "epoch: 1133, train_loss: 0.00013457398428885347, valid_loss: 0.0005165220087898584, test_loss: 0.0016454122960567474\n",
      "epoch: 1134, train_loss: 0.0001346305836467882, valid_loss: 0.000516353616452155, test_loss: 0.0016450168332085013\n",
      "epoch: 1135, train_loss: 0.00013456634105355278, valid_loss: 0.0005162413848059563, test_loss: 0.0016439479077234864\n",
      "epoch: 1136, train_loss: 0.00013452119531828666, valid_loss: 0.0005145607792655937, test_loss: 0.0016420186730101705\n",
      "epoch: 1137, train_loss: 0.00013437957291582438, valid_loss: 0.0005144925162312575, test_loss: 0.0016421290347352624\n",
      "epoch: 1138, train_loss: 0.0001342185822305391, valid_loss: 0.0005141979563632049, test_loss: 0.0016418726881965995\n",
      "epoch: 1139, train_loss: 0.0001347685557329501, valid_loss: 0.0005126453809983408, test_loss: 0.001640039379708469\n",
      "epoch: 1140, train_loss: 0.00013416460063561553, valid_loss: 0.000514697348990012, test_loss: 0.0016393535770475864\n",
      "epoch: 1141, train_loss: 0.0001345446435152553, valid_loss: 0.0005143577218404971, test_loss: 0.0016393436817452312\n",
      "epoch: 1142, train_loss: 0.00013424181594493112, valid_loss: 0.0005125895477249287, test_loss: 0.0016387811629101634\n",
      "epoch: 1143, train_loss: 0.00013404115547334936, valid_loss: 0.0005128237608005293, test_loss: 0.001637787208892405\n",
      "epoch: 1144, train_loss: 0.00013452931591967607, valid_loss: 0.0005138365255940395, test_loss: 0.0016376998974010348\n",
      "epoch: 1145, train_loss: 0.00013408002522065425, valid_loss: 0.0005127443461484896, test_loss: 0.0016363845206797123\n",
      "epoch: 1146, train_loss: 0.00013397398731246105, valid_loss: 0.0005135160196611347, test_loss: 0.0016363634495064616\n",
      "epoch: 1147, train_loss: 0.00013410602759509146, valid_loss: 0.0005126990193578725, test_loss: 0.0016355100087821484\n",
      "epoch: 1148, train_loss: 0.00013385282079031205, valid_loss: 0.0005123250011820346, test_loss: 0.0016361249145120382\n",
      "epoch: 1149, train_loss: 0.0001339194132015109, valid_loss: 0.0005141534808596285, test_loss: 0.001635019900277257\n",
      "epoch: 1150, train_loss: 0.00013388438990501606, valid_loss: 0.0005110178220396241, test_loss: 0.0016342357266694307\n",
      "epoch: 1151, train_loss: 0.00013394788086019537, valid_loss: 0.0005140164673017958, test_loss: 0.001634363317862153\n",
      "epoch: 1152, train_loss: 0.00013364249879605663, valid_loss: 0.0005101913363129521, test_loss: 0.0016314295353367925\n",
      "epoch: 1153, train_loss: 0.00013358781649478024, valid_loss: 0.0005104676723325005, test_loss: 0.0016319239512085915\n",
      "epoch: 1154, train_loss: 0.00013359392292129442, valid_loss: 0.0005117305845487863, test_loss: 0.0016308167250826955\n",
      "epoch: 1155, train_loss: 0.00013361677607146862, valid_loss: 0.0005101881203396866, test_loss: 0.001629170379601419\n",
      "epoch: 1156, train_loss: 0.00013372585975878832, valid_loss: 0.0005101297477570673, test_loss: 0.0016292347572743893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1157, train_loss: 0.00013343011454233655, valid_loss: 0.0005103929700756756, test_loss: 0.0016295325476676226\n",
      "epoch: 1158, train_loss: 0.00013346163683783507, valid_loss: 0.0005092446978475588, test_loss: 0.001628325553610921\n",
      "epoch: 1159, train_loss: 0.00013331347461486155, valid_loss: 0.0005096908910976102, test_loss: 0.001627850579097867\n",
      "epoch: 1160, train_loss: 0.00013318730444829586, valid_loss: 0.0005093693422774473, test_loss: 0.0016278394032269716\n",
      "epoch: 1161, train_loss: 0.00013327814636083886, valid_loss: 0.0005094111305273449, test_loss: 0.0016263358993455768\n",
      "epoch: 1162, train_loss: 0.0001330656244966161, valid_loss: 0.0005096141370207382, test_loss: 0.001626968733035028\n",
      "epoch: 1163, train_loss: 0.00013320268550634628, valid_loss: 0.0005084991862531751, test_loss: 0.0016242144629359245\n",
      "epoch: 1164, train_loss: 0.0001330464833502091, valid_loss: 0.0005067642026309235, test_loss: 0.0016237258678302169\n",
      "epoch: 1165, train_loss: 0.000133071283926256, valid_loss: 0.000509260088923232, test_loss: 0.0016231960617005825\n",
      "epoch: 1166, train_loss: 0.00013302111493312228, valid_loss: 0.0005070376840497678, test_loss: 0.0016227426240220666\n",
      "epoch: 1167, train_loss: 0.00013287057094124106, valid_loss: 0.0005069269294229647, test_loss: 0.0016215809155255556\n",
      "epoch: 1168, train_loss: 0.00013282681586291722, valid_loss: 0.0005062817047776965, test_loss: 0.0016221454134210944\n",
      "epoch: 1169, train_loss: 0.00013283384981585422, valid_loss: 0.0005064721165884597, test_loss: 0.0016205962747335434\n",
      "epoch: 1170, train_loss: 0.0001329741261661579, valid_loss: 0.0005077764944871888, test_loss: 0.0016205584397539496\n",
      "epoch: 1171, train_loss: 0.00013267021384545723, valid_loss: 0.0005059485750583311, test_loss: 0.0016191257163882256\n",
      "epoch: 1172, train_loss: 0.00013289675964346236, valid_loss: 0.0005053038354769038, test_loss: 0.0016189508605748415\n",
      "epoch: 1173, train_loss: 0.00013259849694567612, valid_loss: 0.0005058995569318844, test_loss: 0.001617876230739057\n",
      "epoch: 1174, train_loss: 0.00013248677461651033, valid_loss: 0.0005052028112307502, test_loss: 0.0016175373457372189\n",
      "epoch: 1175, train_loss: 0.00013247712827844384, valid_loss: 0.0005057663681024375, test_loss: 0.0016174445627257228\n",
      "epoch: 1176, train_loss: 0.00013243662555317354, valid_loss: 0.000504225718032103, test_loss: 0.0016156587516888976\n",
      "epoch: 1177, train_loss: 0.0001324468631418827, valid_loss: 0.0005060725088696927, test_loss: 0.001617256784811616\n",
      "epoch: 1178, train_loss: 0.00013232402251679045, valid_loss: 0.0005043952138900446, test_loss: 0.0016162494430318475\n",
      "epoch: 1179, train_loss: 0.00013236592254226866, valid_loss: 0.0005042486397239069, test_loss: 0.00161419075448066\n",
      "epoch: 1180, train_loss: 0.0001323142584980182, valid_loss: 0.0005029636740800925, test_loss: 0.001614288310520351\n",
      "epoch: 1181, train_loss: 0.0001321855939433748, valid_loss: 0.0005051813835355764, test_loss: 0.0016151568852365017\n",
      "epoch: 1182, train_loss: 0.00013235264613404465, valid_loss: 0.0005052620520776449, test_loss: 0.001613716478459537\n",
      "epoch: 1183, train_loss: 0.0001322008073380541, valid_loss: 0.0005035882568336092, test_loss: 0.001612228574231267\n",
      "epoch: 1184, train_loss: 0.00013207965062263057, valid_loss: 0.0005032167294605946, test_loss: 0.0016106079565361142\n",
      "epoch: 1185, train_loss: 0.000132037851511521, valid_loss: 0.0005017412549932487, test_loss: 0.0016105582471936941\n",
      "epoch: 1186, train_loss: 0.00013185506964952725, valid_loss: 0.0005023163127286049, test_loss: 0.001609971048310399\n",
      "epoch: 1187, train_loss: 0.0001318676890071441, valid_loss: 0.0005015097825283495, test_loss: 0.0016090500866994262\n",
      "epoch: 1188, train_loss: 0.00013197940247862235, valid_loss: 0.0005026532477738025, test_loss: 0.0016086187679320574\n",
      "epoch: 1189, train_loss: 0.00013177277951794878, valid_loss: 0.0005018809048730569, test_loss: 0.001607421669177711\n",
      "epoch: 1190, train_loss: 0.00013176549754976094, valid_loss: 0.0005027713244392847, test_loss: 0.0016085569513961673\n",
      "epoch: 1191, train_loss: 0.00013163571186495298, valid_loss: 0.0005018919982830994, test_loss: 0.0016063004732131958\n",
      "epoch: 1192, train_loss: 0.00013172105505161554, valid_loss: 0.0005039170355303213, test_loss: 0.001608530874364078\n",
      "epoch: 1193, train_loss: 0.00013179204562101918, valid_loss: 0.0005003135520382784, test_loss: 0.0016063106013461947\n",
      "epoch: 1194, train_loss: 0.00013158459415021554, valid_loss: 0.0005016289821166234, test_loss: 0.0016061102505773306\n",
      "epoch: 1195, train_loss: 0.0001316820543370736, valid_loss: 0.0005017266739741899, test_loss: 0.001604509074240923\n",
      "epoch: 1196, train_loss: 0.00013173940216960466, valid_loss: 0.0005000562038427839, test_loss: 0.0016029331600293517\n",
      "epoch: 1197, train_loss: 0.0001314252475150051, valid_loss: 0.000501824397360906, test_loss: 0.0016051999991759658\n",
      "epoch: 1198, train_loss: 0.0001315189558385021, valid_loss: 0.0005009926608181559, test_loss: 0.0016034652944654226\n",
      "epoch: 1199, train_loss: 0.000131508642010411, valid_loss: 0.000501880419809216, test_loss: 0.0016042182687669992\n",
      "epoch: 1200, train_loss: 0.0001313740785522184, valid_loss: 0.0004986343895628428, test_loss: 0.001600855146534741\n",
      "epoch: 1201, train_loss: 0.00013153193519784784, valid_loss: 0.0004987492842095284, test_loss: 0.0016004502540454268\n",
      "epoch: 1202, train_loss: 0.00013124241883380583, valid_loss: 0.0004986966305295937, test_loss: 0.0016004570061340928\n",
      "epoch: 1203, train_loss: 0.00013112867853361303, valid_loss: 0.0004981825792735132, test_loss: 0.001598736853338778\n",
      "epoch: 1204, train_loss: 0.0001310912332922706, valid_loss: 0.0004986066899922056, test_loss: 0.0015984768979251385\n",
      "epoch: 1205, train_loss: 0.000131102032078754, valid_loss: 0.0004981713330683609, test_loss: 0.001599007286131382\n",
      "epoch: 1206, train_loss: 0.00013115079902649006, valid_loss: 0.0004977230782969855, test_loss: 0.0015964305493980646\n",
      "epoch: 1207, train_loss: 0.00013107225696212086, valid_loss: 0.0004980427377934878, test_loss: 0.0015969905070960522\n",
      "epoch: 1208, train_loss: 0.00013082001944143406, valid_loss: 0.0004987370266462676, test_loss: 0.0015968277584761381\n",
      "epoch: 1209, train_loss: 0.0001308108083954405, valid_loss: 0.0004982495787165438, test_loss: 0.0015963742043823004\n",
      "epoch: 1210, train_loss: 0.00013084022351034471, valid_loss: 0.0004966898341081105, test_loss: 0.001595652080141008\n",
      "epoch: 1211, train_loss: 0.0001310070773520831, valid_loss: 0.0004971374340433007, test_loss: 0.001594948349520564\n",
      "epoch: 1212, train_loss: 0.00013057990777863029, valid_loss: 0.0004968611150009868, test_loss: 0.0015941900201141834\n",
      "epoch: 1213, train_loss: 0.0001307242169075281, valid_loss: 0.0004967197455698624, test_loss: 0.0015945469494909048\n",
      "epoch: 1214, train_loss: 0.00013077508154547894, valid_loss: 0.0004967101340298541, test_loss: 0.0015922968741506338\n",
      "epoch: 1215, train_loss: 0.00013068723036786136, valid_loss: 0.0004967458880855702, test_loss: 0.0015914837131276727\n",
      "epoch: 1216, train_loss: 0.00013051807795620888, valid_loss: 0.0004955435991481257, test_loss: 0.001591367763467133\n",
      "epoch: 1217, train_loss: 0.00013040013310155305, valid_loss: 0.0004961335313661644, test_loss: 0.001590223517268896\n",
      "epoch: 1218, train_loss: 0.00013035099697805455, valid_loss: 0.000495049850239108, test_loss: 0.0015893075615167618\n",
      "epoch: 1219, train_loss: 0.0001303743015539468, valid_loss: 0.0004937915146001615, test_loss: 0.0015888873022049665\n",
      "epoch: 1220, train_loss: 0.0001304347979780489, valid_loss: 0.0004949335683098374, test_loss: 0.0015883168671280146\n",
      "epoch: 1221, train_loss: 0.0001301973793686003, valid_loss: 0.0004939848877256736, test_loss: 0.0015878576086834073\n",
      "epoch: 1222, train_loss: 0.00013014351703611243, valid_loss: 0.0004944358127734935, test_loss: 0.0015877938130870461\n",
      "epoch: 1223, train_loss: 0.00013017314853166917, valid_loss: 0.0004937823808480365, test_loss: 0.0015865417663007975\n",
      "epoch: 1224, train_loss: 0.0001301726088454218, valid_loss: 0.0004950773606348472, test_loss: 0.001586327445693314\n",
      "epoch: 1225, train_loss: 0.00013023579705749518, valid_loss: 0.0004947033715628398, test_loss: 0.0015867056790739298\n",
      "epoch: 1226, train_loss: 0.00013015511016735968, valid_loss: 0.0004929075003019534, test_loss: 0.001584679470397532\n",
      "epoch: 1227, train_loss: 0.0001299538207999633, valid_loss: 0.0004935183363462178, test_loss: 0.0015849798219278455\n",
      "epoch: 1228, train_loss: 0.0001298780957965747, valid_loss: 0.0004940539705179011, test_loss: 0.001583664445206523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1229, train_loss: 0.000130082309782586, valid_loss: 0.0004969483537327809, test_loss: 0.001587343867868185\n",
      "epoch: 1230, train_loss: 0.00012986762063203218, valid_loss: 0.0004919347338727675, test_loss: 0.0015824561705812812\n",
      "epoch: 1231, train_loss: 0.0001299056447865239, valid_loss: 0.0004947685447405092, test_loss: 0.0015832905191928148\n",
      "epoch: 1232, train_loss: 0.00013000071181601885, valid_loss: 0.0004923939995933324, test_loss: 0.0015821224078536034\n",
      "epoch: 1233, train_loss: 0.00012978929399927753, valid_loss: 0.0004906727723816099, test_loss: 0.0015803955029696226\n",
      "epoch: 1234, train_loss: 0.00012966594342446035, valid_loss: 0.0004912097550307711, test_loss: 0.001580443000420928\n",
      "epoch: 1235, train_loss: 0.00012956191209883875, valid_loss: 0.0004932938027195632, test_loss: 0.0015800020191818476\n",
      "epoch: 1236, train_loss: 0.00012949410049022055, valid_loss: 0.0004911108092831759, test_loss: 0.0015789901372045279\n",
      "epoch: 1237, train_loss: 0.00012959169322604558, valid_loss: 0.0004904293188398393, test_loss: 0.0015776794170960784\n",
      "epoch: 1238, train_loss: 0.00012979609796868476, valid_loss: 0.0004896064104589944, test_loss: 0.0015777001390233636\n",
      "epoch: 1239, train_loss: 0.00012944901739145914, valid_loss: 0.0004907035593835948, test_loss: 0.0015773483319208026\n",
      "epoch: 1240, train_loss: 0.00012939376914721873, valid_loss: 0.0004900986023130827, test_loss: 0.0015774370403960347\n",
      "epoch: 1241, train_loss: 0.0001294961058074082, valid_loss: 0.0004908000191790052, test_loss: 0.0015776667278259993\n",
      "epoch: 1242, train_loss: 0.0001293613576031614, valid_loss: 0.0004896235089593878, test_loss: 0.0015754069900140166\n",
      "epoch: 1243, train_loss: 0.0001290732739700774, valid_loss: 0.0004895332822343335, test_loss: 0.0015740984817966819\n",
      "epoch: 1244, train_loss: 0.00012918069557560122, valid_loss: 0.0004898031717554355, test_loss: 0.0015741019742563367\n",
      "epoch: 1245, train_loss: 0.0001292249289690517, valid_loss: 0.00048825117604186136, test_loss: 0.0015737806679680943\n",
      "epoch: 1246, train_loss: 0.00012941718096439928, valid_loss: 0.0004888860809539134, test_loss: 0.001572771929204464\n",
      "epoch: 1247, train_loss: 0.00012917160347569734, valid_loss: 0.0004895832268327164, test_loss: 0.0015737850917503238\n",
      "epoch: 1248, train_loss: 0.00012899842461237037, valid_loss: 0.0004894389203400351, test_loss: 0.0015720577212050557\n",
      "epoch: 1249, train_loss: 0.00012882639535034642, valid_loss: 0.0004878929248661734, test_loss: 0.001571026397868991\n",
      "epoch: 1250, train_loss: 0.00012894120268554062, valid_loss: 0.0004907691715440402, test_loss: 0.0015737328212708235\n",
      "epoch: 1251, train_loss: 0.0001290159112692851, valid_loss: 0.0004875731174251996, test_loss: 0.001570104737766087\n",
      "epoch: 1252, train_loss: 0.0001287824419230911, valid_loss: 0.0004887980127629513, test_loss: 0.0015693050809204578\n",
      "epoch: 1253, train_loss: 0.00012897250986577052, valid_loss: 0.0004870545769032712, test_loss: 0.0015683022793382406\n",
      "epoch: 1254, train_loss: 0.00012880497872728208, valid_loss: 0.0004870003419152151, test_loss: 0.0015685748076066375\n",
      "epoch: 1255, train_loss: 0.00012878865907069942, valid_loss: 0.0004886999862113347, test_loss: 0.001568628940731287\n",
      "epoch: 1256, train_loss: 0.00012891798890094557, valid_loss: 0.00048715710969797027, test_loss: 0.0015673341695219278\n",
      "epoch: 1257, train_loss: 0.0001286230152523469, valid_loss: 0.0004857820094912313, test_loss: 0.0015663054073229432\n",
      "epoch: 1258, train_loss: 0.00012856540833245558, valid_loss: 0.0004855766746914014, test_loss: 0.0015655449824407697\n",
      "epoch: 1259, train_loss: 0.00012854374179606447, valid_loss: 0.0004871944959935111, test_loss: 0.0015659468481317163\n",
      "epoch: 1260, train_loss: 0.00012838239901769987, valid_loss: 0.00048708159738453105, test_loss: 0.0015659276396036148\n",
      "epoch: 1261, train_loss: 0.00012841633724747226, valid_loss: 0.0004875549760375482, test_loss: 0.0015643705846741796\n",
      "epoch: 1262, train_loss: 0.00012847046720866194, valid_loss: 0.0004849948866952521, test_loss: 0.0015636547468602657\n",
      "epoch: 1263, train_loss: 0.00012844704083956617, valid_loss: 0.00048440484776316833, test_loss: 0.0015625323867425323\n",
      "epoch: 1264, train_loss: 0.0001282409664228275, valid_loss: 0.00048669613655268523, test_loss: 0.0015638573095202446\n",
      "epoch: 1265, train_loss: 0.0001282024573099435, valid_loss: 0.00048501881974516436, test_loss: 0.0015618759207427502\n",
      "epoch: 1266, train_loss: 0.00012823651923099533, valid_loss: 0.0004834497449337505, test_loss: 0.0015612543793395162\n",
      "epoch: 1267, train_loss: 0.00012821967602181047, valid_loss: 0.0004834840753270934, test_loss: 0.0015600259648635983\n",
      "epoch: 1268, train_loss: 0.00012808938924496508, valid_loss: 0.000484428809916911, test_loss: 0.0015600493643432856\n",
      "epoch: 1269, train_loss: 0.00012821491185130307, valid_loss: 0.0004838540795996475, test_loss: 0.0015599785838276148\n",
      "epoch: 1270, train_loss: 0.000127964640737248, valid_loss: 0.0004834625869989395, test_loss: 0.0015581956831738353\n",
      "epoch: 1271, train_loss: 0.00012789851019118467, valid_loss: 0.0004842744383495301, test_loss: 0.0015586328227072954\n",
      "epoch: 1272, train_loss: 0.00012804926423679876, valid_loss: 0.00048303526515762013, test_loss: 0.001557648298330605\n",
      "epoch: 1273, train_loss: 0.0001278530095157254, valid_loss: 0.00048540929371180636, test_loss: 0.0015584173379465938\n",
      "epoch: 1274, train_loss: 0.00012812788064247403, valid_loss: 0.0004829145036637783, test_loss: 0.0015564431669190526\n",
      "epoch: 1275, train_loss: 0.00012802127236247307, valid_loss: 0.00048356551997130737, test_loss: 0.0015558669110760093\n",
      "epoch: 1276, train_loss: 0.000127784447218089, valid_loss: 0.00048217997634007287, test_loss: 0.0015547832008451223\n",
      "epoch: 1277, train_loss: 0.00012781671672279987, valid_loss: 0.0004827226318108539, test_loss: 0.001556025817990303\n",
      "epoch: 1278, train_loss: 0.0001277137030822063, valid_loss: 0.00048182546015596017, test_loss: 0.0015544622438028455\n",
      "epoch: 1279, train_loss: 0.0001277307402114789, valid_loss: 0.0004820218334013286, test_loss: 0.0015546653885394335\n",
      "epoch: 1280, train_loss: 0.0001275453219220609, valid_loss: 0.00048185048945015296, test_loss: 0.0015526995994150639\n",
      "epoch: 1281, train_loss: 0.0001276298239447541, valid_loss: 0.00048252147826133296, test_loss: 0.001552953035570681\n",
      "epoch: 1282, train_loss: 0.0001274827078779471, valid_loss: 0.00048075369219683733, test_loss: 0.0015519738662987947\n",
      "epoch: 1283, train_loss: 0.00012751420961095667, valid_loss: 0.0004800934111699462, test_loss: 0.0015511655947193503\n",
      "epoch: 1284, train_loss: 0.00012745651759399587, valid_loss: 0.0004798920320657392, test_loss: 0.00155105150770396\n",
      "epoch: 1285, train_loss: 0.00012727836830258045, valid_loss: 0.0004815151041839272, test_loss: 0.0015503145987167954\n",
      "epoch: 1286, train_loss: 0.00012744430315923756, valid_loss: 0.0004806262392473097, test_loss: 0.0015491769881919026\n",
      "epoch: 1287, train_loss: 0.0001273230942467268, valid_loss: 0.0004798410882358439, test_loss: 0.0015485864132642746\n",
      "epoch: 1288, train_loss: 0.0001273639120525964, valid_loss: 0.00047905185298683744, test_loss: 0.001548044616356492\n",
      "epoch: 1289, train_loss: 0.00012726484830794936, valid_loss: 0.00048004691780079156, test_loss: 0.001549241947941482\n",
      "epoch: 1290, train_loss: 0.00012704849897367555, valid_loss: 0.0004796849049550171, test_loss: 0.001547515275888145\n",
      "epoch: 1291, train_loss: 0.00012701938090764958, valid_loss: 0.00047950959560694173, test_loss: 0.0015467553166672587\n",
      "epoch: 1292, train_loss: 0.0001269388449145481, valid_loss: 0.0004780885477278692, test_loss: 0.001546433544717729\n",
      "epoch: 1293, train_loss: 0.00012703776820157856, valid_loss: 0.00048133533709915355, test_loss: 0.0015472674276679754\n",
      "epoch: 1294, train_loss: 0.00012694192137923497, valid_loss: 0.0004806668487920736, test_loss: 0.0015459582209587097\n",
      "epoch: 1295, train_loss: 0.00012686897167915723, valid_loss: 0.00047891156282275915, test_loss: 0.0015443400479853153\n",
      "epoch: 1296, train_loss: 0.0001269578449706466, valid_loss: 0.0004772367368180615, test_loss: 0.001543501392006874\n",
      "epoch: 1297, train_loss: 0.00012689178465339154, valid_loss: 0.0004773507122687685, test_loss: 0.0015425124438479543\n",
      "epoch: 1298, train_loss: 0.000126708262541797, valid_loss: 0.00047724212587733444, test_loss: 0.0015421389834955335\n",
      "epoch: 1299, train_loss: 0.00012668314055823114, valid_loss: 0.0004788763593145025, test_loss: 0.0015427806647494435\n",
      "epoch: 1300, train_loss: 0.00012671067499626508, valid_loss: 0.0004773000133961129, test_loss: 0.0015407494502142072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1301, train_loss: 0.0001265840483424456, valid_loss: 0.0004767092371669908, test_loss: 0.0015406482852995396\n",
      "epoch: 1302, train_loss: 0.00012660380921063378, valid_loss: 0.00047693211915126693, test_loss: 0.0015406133607029915\n",
      "epoch: 1303, train_loss: 0.00012645313614959141, valid_loss: 0.0004769077374173018, test_loss: 0.001540077500976622\n",
      "epoch: 1304, train_loss: 0.00012633226857439655, valid_loss: 0.00047754233673913404, test_loss: 0.0015414580702781677\n",
      "epoch: 1305, train_loss: 0.0001266081463140638, valid_loss: 0.00047763144781735417, test_loss: 0.0015397705137729645\n",
      "epoch: 1306, train_loss: 0.00012653604916641083, valid_loss: 0.0004774394741010231, test_loss: 0.001540660159662366\n",
      "epoch: 1307, train_loss: 0.00012650653155540803, valid_loss: 0.00047481933628053713, test_loss: 0.0015369585016742349\n",
      "epoch: 1308, train_loss: 0.00012626128749537241, valid_loss: 0.0004758842818167371, test_loss: 0.001537094940431416\n",
      "epoch: 1309, train_loss: 0.00012652047735173255, valid_loss: 0.000475895203029116, test_loss: 0.0015365072758868337\n",
      "epoch: 1310, train_loss: 0.00012656665658932584, valid_loss: 0.00047435254479448, test_loss: 0.0015356444055214524\n",
      "epoch: 1311, train_loss: 0.00012608367789020437, valid_loss: 0.000475050306704361, test_loss: 0.0015354921342805028\n",
      "epoch: 1312, train_loss: 0.00012606720575485784, valid_loss: 0.0004762067401316017, test_loss: 0.001535960123874247\n",
      "epoch: 1313, train_loss: 0.00012630561452723393, valid_loss: 0.00047472616521796834, test_loss: 0.0015338564990088344\n",
      "epoch: 1314, train_loss: 0.00012609203828184906, valid_loss: 0.0004740509587766913, test_loss: 0.0015338058583438396\n",
      "epoch: 1315, train_loss: 0.0001261365323443897, valid_loss: 0.00047474014475786436, test_loss: 0.001533961738459766\n",
      "epoch: 1316, train_loss: 0.0001260989245012119, valid_loss: 0.00047471483897728223, test_loss: 0.0015325978165492415\n",
      "epoch: 1317, train_loss: 0.00012600325767973277, valid_loss: 0.0004737167788941103, test_loss: 0.0015315262135118246\n",
      "epoch: 1318, train_loss: 0.0001260357695054668, valid_loss: 0.00047331688256235793, test_loss: 0.0015318274963647127\n",
      "epoch: 1319, train_loss: 0.0001257515437116482, valid_loss: 0.00047239531462158385, test_loss: 0.0015308704460039735\n",
      "epoch: 1320, train_loss: 0.0001258136468569991, valid_loss: 0.00047280416038120165, test_loss: 0.0015295002376660705\n",
      "epoch: 1321, train_loss: 0.00012573683931765592, valid_loss: 0.0004732169521351655, test_loss: 0.0015299618244171143\n",
      "epoch: 1322, train_loss: 0.00012587650986540172, valid_loss: 0.00047282374968441826, test_loss: 0.0015295004704967141\n",
      "epoch: 1323, train_loss: 0.0001256813500177763, valid_loss: 0.00047340753614359227, test_loss: 0.0015294115291908383\n",
      "epoch: 1324, train_loss: 0.00012564674176721147, valid_loss: 0.00047260818973882124, test_loss: 0.0015285720583051443\n",
      "epoch: 1325, train_loss: 0.00012558633676709851, valid_loss: 0.00047233678681853536, test_loss: 0.0015275584300979972\n",
      "epoch: 1326, train_loss: 0.00012570934157575601, valid_loss: 0.000471155906173711, test_loss: 0.0015271996380761266\n",
      "epoch: 1327, train_loss: 0.0001254865710507147, valid_loss: 0.000472669824375771, test_loss: 0.0015270033618435264\n",
      "epoch: 1328, train_loss: 0.00012553993229116514, valid_loss: 0.00047243491038292024, test_loss: 0.0015263133682310581\n",
      "epoch: 1329, train_loss: 0.00012559463199147064, valid_loss: 0.00047171399880123016, test_loss: 0.0015255275648087263\n",
      "epoch: 1330, train_loss: 0.00012537570854993132, valid_loss: 0.0004734396788990125, test_loss: 0.0015269070863723755\n",
      "epoch: 1331, train_loss: 0.000125472666695714, valid_loss: 0.0004706482626109694, test_loss: 0.0015239801723510027\n",
      "epoch: 1332, train_loss: 0.00012528524087468648, valid_loss: 0.00047051671814794344, test_loss: 0.0015233461745083332\n",
      "epoch: 1333, train_loss: 0.00012518167394496825, valid_loss: 0.0004703740560216829, test_loss: 0.0015227955300360918\n",
      "epoch: 1334, train_loss: 0.00012527325895422584, valid_loss: 0.00046940845156010863, test_loss: 0.001522014383226633\n",
      "epoch: 1335, train_loss: 0.0001252303959715747, valid_loss: 0.00046968292250918847, test_loss: 0.0015212228754535317\n",
      "epoch: 1336, train_loss: 0.00012504742556737492, valid_loss: 0.0004698724126986538, test_loss: 0.0015211408026516438\n",
      "epoch: 1337, train_loss: 0.00012508087440991127, valid_loss: 0.00047061609317703795, test_loss: 0.001521044410765171\n",
      "epoch: 1338, train_loss: 0.00012504376322992712, valid_loss: 0.000473115112981759, test_loss: 0.0015233076410368085\n",
      "epoch: 1339, train_loss: 0.00012503828443384364, valid_loss: 0.00046873101625048247, test_loss: 0.0015194090083241463\n",
      "epoch: 1340, train_loss: 0.0001249718919533062, valid_loss: 0.00046891738505413133, test_loss: 0.0015199263580143452\n",
      "epoch: 1341, train_loss: 0.00012488778188328624, valid_loss: 0.0004681305363192223, test_loss: 0.001518433215096593\n",
      "epoch: 1342, train_loss: 0.00012484828988343233, valid_loss: 0.00046845487668178976, test_loss: 0.001517627970315516\n",
      "epoch: 1343, train_loss: 0.00012476615824119148, valid_loss: 0.00047114621217284974, test_loss: 0.0015213214792311192\n",
      "epoch: 1344, train_loss: 0.00012502577295020708, valid_loss: 0.0004677397907168294, test_loss: 0.0015169657999649644\n",
      "epoch: 1345, train_loss: 0.00012483821331482866, valid_loss: 0.00046934559698759887, test_loss: 0.0015182050410658121\n",
      "epoch: 1346, train_loss: 0.00012486350865599334, valid_loss: 0.0004678095259199229, test_loss: 0.0015157778980210423\n",
      "epoch: 1347, train_loss: 0.00012460528238949811, valid_loss: 0.00046805483725620434, test_loss: 0.001515827956609428\n",
      "epoch: 1348, train_loss: 0.00012467519232102305, valid_loss: 0.00046741521267297986, test_loss: 0.001514475210569799\n",
      "epoch: 1349, train_loss: 0.00012450396088356882, valid_loss: 0.00046686605977204937, test_loss: 0.0015140729956328869\n",
      "epoch: 1350, train_loss: 0.00012454253041853562, valid_loss: 0.00046861302689649165, test_loss: 0.0015147326048463583\n",
      "epoch: 1351, train_loss: 0.00012441776894063324, valid_loss: 0.0004670846901717596, test_loss: 0.0015130864921957254\n",
      "epoch: 1352, train_loss: 0.0001244945109963579, valid_loss: 0.000465637259670378, test_loss: 0.001512926071882248\n",
      "epoch: 1353, train_loss: 0.0001245506152728289, valid_loss: 0.00046656630487026024, test_loss: 0.0015126185026019812\n",
      "epoch: 1354, train_loss: 0.00012444403831108028, valid_loss: 0.00046629940091709915, test_loss: 0.0015122167533263564\n",
      "epoch: 1355, train_loss: 0.000124346865366375, valid_loss: 0.0004662578576244414, test_loss: 0.0015118973096832633\n",
      "epoch: 1356, train_loss: 0.00012426024920347592, valid_loss: 0.0004660082995542325, test_loss: 0.00151041557546705\n",
      "epoch: 1357, train_loss: 0.00012421097483678275, valid_loss: 0.00046819233587787795, test_loss: 0.001510831294581294\n",
      "epoch: 1358, train_loss: 0.0001244011816553488, valid_loss: 0.00046565216810752946, test_loss: 0.0015097467694431543\n",
      "epoch: 1359, train_loss: 0.00012403255032436192, valid_loss: 0.0004668467663577758, test_loss: 0.001510749338194728\n",
      "epoch: 1360, train_loss: 0.0001239969421041441, valid_loss: 0.0004660957104836901, test_loss: 0.0015098489820957184\n",
      "epoch: 1361, train_loss: 0.00012400321967383522, valid_loss: 0.00046554067860900733, test_loss: 0.0015103128971531987\n",
      "epoch: 1362, train_loss: 0.00012405349369383538, valid_loss: 0.0004659257538150996, test_loss: 0.001509105321019888\n",
      "epoch: 1363, train_loss: 0.00012434909718728665, valid_loss: 0.0004639206381398253, test_loss: 0.0015066041378304362\n",
      "epoch: 1364, train_loss: 0.0001240511514681756, valid_loss: 0.00046510241615275544, test_loss: 0.0015066075138747692\n",
      "epoch: 1365, train_loss: 0.0001240140975468144, valid_loss: 0.0004631703147121395, test_loss: 0.0015058640856295824\n",
      "epoch: 1366, train_loss: 0.00012376795791130027, valid_loss: 0.0004639915253695411, test_loss: 0.0015056602424010634\n",
      "epoch: 1367, train_loss: 0.00012373737358164203, valid_loss: 0.00046282829862320796, test_loss: 0.001504498883150518\n",
      "epoch: 1368, train_loss: 0.0001237859937448419, valid_loss: 0.0004627613670891151, test_loss: 0.0015042007435113192\n",
      "epoch: 1369, train_loss: 0.0001237162018103687, valid_loss: 0.00046271806058939546, test_loss: 0.0015043019084259868\n",
      "epoch: 1370, train_loss: 0.0001238559837119006, valid_loss: 0.00046223267903163406, test_loss: 0.0015029265778139234\n",
      "epoch: 1371, train_loss: 0.0001235744949435289, valid_loss: 0.0004624587090802379, test_loss: 0.0015025653410702944\n",
      "epoch: 1372, train_loss: 0.00012358567366153812, valid_loss: 0.0004626719625472712, test_loss: 0.0015025873435661197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1373, train_loss: 0.00012346511167656306, valid_loss: 0.00046357227984117344, test_loss: 0.0015028357738628983\n",
      "epoch: 1374, train_loss: 0.00012348728246845144, valid_loss: 0.000461828521414039, test_loss: 0.0015014293603599072\n",
      "epoch: 1375, train_loss: 0.0001235555812498838, valid_loss: 0.0004645646088950646, test_loss: 0.0015028874622657895\n",
      "epoch: 1376, train_loss: 0.0001235074463614222, valid_loss: 0.00046170509691971046, test_loss: 0.0015002634609118104\n",
      "epoch: 1377, train_loss: 0.00012342127940981933, valid_loss: 0.0004617810021348608, test_loss: 0.0015000607818365097\n",
      "epoch: 1378, train_loss: 0.00012350606867466527, valid_loss: 0.0004609952447935939, test_loss: 0.0014998744009062648\n",
      "epoch: 1379, train_loss: 0.0001234465390038879, valid_loss: 0.00046226614601134014, test_loss: 0.0014981308486312628\n",
      "epoch: 1380, train_loss: 0.00012337274687445682, valid_loss: 0.0004606813211770107, test_loss: 0.0014986662426963449\n",
      "epoch: 1381, train_loss: 0.0001232642088839582, valid_loss: 0.00046179296865981695, test_loss: 0.0014977777609601617\n",
      "epoch: 1382, train_loss: 0.00012323107923451892, valid_loss: 0.00045990421979998547, test_loss: 0.0014971110504120588\n",
      "epoch: 1383, train_loss: 0.00012317834625719115, valid_loss: 0.00046065192388293025, test_loss: 0.001496803597547114\n",
      "epoch: 1384, train_loss: 0.0001231674528832588, valid_loss: 0.00046273632809364546, test_loss: 0.0014964089496061206\n",
      "epoch: 1385, train_loss: 0.0001230606756745028, valid_loss: 0.0004597754862819177, test_loss: 0.0014949903124943376\n",
      "epoch: 1386, train_loss: 0.00012315025188676688, valid_loss: 0.00046015526216554764, test_loss: 0.001496080425567925\n",
      "epoch: 1387, train_loss: 0.00012289782835464195, valid_loss: 0.00046010266184263554, test_loss: 0.0014945510774850845\n",
      "epoch: 1388, train_loss: 0.0001229573380956224, valid_loss: 0.00045997488147501525, test_loss: 0.0014953837962821126\n",
      "epoch: 1389, train_loss: 0.00012292499393325946, valid_loss: 0.00045984116150066257, test_loss: 0.0014941307017579675\n",
      "epoch: 1390, train_loss: 0.0001227613348696057, valid_loss: 0.0004606272729385334, test_loss: 0.0014944769209250808\n",
      "epoch: 1391, train_loss: 0.0001228453459233329, valid_loss: 0.0004590851264462496, test_loss: 0.0014934163773432374\n",
      "epoch: 1392, train_loss: 0.0001228702581695119, valid_loss: 0.00045876900549046695, test_loss: 0.0014911722391843796\n",
      "epoch: 1393, train_loss: 0.0001228024158753333, valid_loss: 0.0004598665376155016, test_loss: 0.0014925377909094095\n",
      "epoch: 1394, train_loss: 0.00012276729830773547, valid_loss: 0.0004584816876255597, test_loss: 0.001490986323915422\n",
      "epoch: 1395, train_loss: 0.00012274884900999376, valid_loss: 0.00045768486355276156, test_loss: 0.001490475027821958\n",
      "epoch: 1396, train_loss: 0.00012270688097012916, valid_loss: 0.00045747192537722486, test_loss: 0.0014894981868565083\n",
      "epoch: 1397, train_loss: 0.00012244529352756217, valid_loss: 0.0004579073890151146, test_loss: 0.0014893703628331423\n",
      "epoch: 1398, train_loss: 0.0001225908607644353, valid_loss: 0.00045768679895748693, test_loss: 0.0014893673360347748\n",
      "epoch: 1399, train_loss: 0.00012248460615924358, valid_loss: 0.00045694090901330736, test_loss: 0.0014887357829138637\n",
      "epoch: 1400, train_loss: 0.0001226255585409134, valid_loss: 0.00045845489754962426, test_loss: 0.0014877549838274717\n",
      "epoch: 1401, train_loss: 0.00012256410801732588, valid_loss: 0.00045872171661661315, test_loss: 0.0014878184301778674\n",
      "epoch: 1402, train_loss: 0.00012249953895503575, valid_loss: 0.00045820593610793975, test_loss: 0.001489480142481625\n",
      "epoch: 1403, train_loss: 0.0001226365550436363, valid_loss: 0.000456265236910743, test_loss: 0.0014858548529446125\n",
      "epoch: 1404, train_loss: 0.0001222871335061348, valid_loss: 0.0004568326936957116, test_loss: 0.0014860187657177448\n",
      "epoch: 1405, train_loss: 0.0001223218217922334, valid_loss: 0.0004560874367598444, test_loss: 0.0014852158492431045\n",
      "epoch: 1406, train_loss: 0.00012228664222082284, valid_loss: 0.0004559615893716303, test_loss: 0.001483903848566115\n",
      "epoch: 1407, train_loss: 0.00012218968091158035, valid_loss: 0.0004571639437926933, test_loss: 0.0014841812662780285\n",
      "epoch: 1408, train_loss: 0.00012224255276479474, valid_loss: 0.0004562303414180254, test_loss: 0.001484276494011283\n",
      "epoch: 1409, train_loss: 0.00012218597175492704, valid_loss: 0.00045590846275445074, test_loss: 0.0014846106059849262\n",
      "epoch: 1410, train_loss: 0.00012212543325680915, valid_loss: 0.00045615077154555667, test_loss: 0.0014833529712632298\n",
      "epoch: 1411, train_loss: 0.00012196971447733434, valid_loss: 0.00045606001125027734, test_loss: 0.0014824282843619585\n",
      "epoch: 1412, train_loss: 0.00012206234247929862, valid_loss: 0.0004546324319865865, test_loss: 0.001482057967223227\n",
      "epoch: 1413, train_loss: 0.00012201267046570454, valid_loss: 0.00045513209139850613, test_loss: 0.001481277635321021\n",
      "epoch: 1414, train_loss: 0.00012205254049900839, valid_loss: 0.00045438720553647727, test_loss: 0.001481123617850244\n",
      "epoch: 1415, train_loss: 0.00012180959690696277, valid_loss: 0.000454167105393329, test_loss: 0.001480990438722074\n",
      "epoch: 1416, train_loss: 0.00012185443357220325, valid_loss: 0.00045494856264364597, test_loss: 0.00148070661816746\n",
      "epoch: 1417, train_loss: 0.00012185261015395594, valid_loss: 0.0004545003345507818, test_loss: 0.0014796335017308593\n",
      "epoch: 1418, train_loss: 0.00012186523931829825, valid_loss: 0.0004531027564856534, test_loss: 0.0014786666724830866\n",
      "epoch: 1419, train_loss: 0.00012174707175607024, valid_loss: 0.0004535256157396361, test_loss: 0.001478104037232697\n",
      "epoch: 1420, train_loss: 0.00012178101061853458, valid_loss: 0.00045564747415483, test_loss: 0.0014789835549890995\n",
      "epoch: 1421, train_loss: 0.0001216778068540051, valid_loss: 0.00045381156087387353, test_loss: 0.0014780318597331643\n",
      "epoch: 1422, train_loss: 0.00012140400402764182, valid_loss: 0.0004542016055590163, test_loss: 0.0014775105519220233\n",
      "epoch: 1423, train_loss: 0.00012160616049937823, valid_loss: 0.0004528669669525698, test_loss: 0.0014773200964555144\n",
      "epoch: 1424, train_loss: 0.00012148312089266255, valid_loss: 0.00045283491878459853, test_loss: 0.0014756773598492146\n",
      "epoch: 1425, train_loss: 0.00012153880316359194, valid_loss: 0.0004530929581960663, test_loss: 0.0014765054220333695\n",
      "epoch: 1426, train_loss: 0.00012130425539394112, valid_loss: 0.0004520649187422047, test_loss: 0.0014750338159501553\n",
      "epoch: 1427, train_loss: 0.00012121541215532014, valid_loss: 0.00045271422519969445, test_loss: 0.0014752796851098537\n",
      "epoch: 1428, train_loss: 0.0001215133559763553, valid_loss: 0.0004532141101663001, test_loss: 0.0014745168155059218\n",
      "epoch: 1429, train_loss: 0.00012139505776323621, valid_loss: 0.0004514699855159658, test_loss: 0.0014737811870872974\n",
      "epoch: 1430, train_loss: 0.0001212952186545843, valid_loss: 0.0004518202622421086, test_loss: 0.001474187127314508\n",
      "epoch: 1431, train_loss: 0.00012115294457939656, valid_loss: 0.00045209254797858495, test_loss: 0.001474133227020502\n",
      "epoch: 1432, train_loss: 0.00012135991393892175, valid_loss: 0.00045075257852052647, test_loss: 0.0014720899052917957\n",
      "epoch: 1433, train_loss: 0.00012116137235273567, valid_loss: 0.000451788044301793, test_loss: 0.0014723767526447773\n",
      "epoch: 1434, train_loss: 0.00012109932456793183, valid_loss: 0.0004502323014700475, test_loss: 0.001470908522605896\n",
      "epoch: 1435, train_loss: 0.00012095995676329197, valid_loss: 0.0004519329425723602, test_loss: 0.0014709534589201212\n",
      "epoch: 1436, train_loss: 0.000121144204888918, valid_loss: 0.00044996536113709834, test_loss: 0.001470069633796811\n",
      "epoch: 1437, train_loss: 0.00012109025619397669, valid_loss: 0.0004503203332812215, test_loss: 0.0014690306270495057\n",
      "epoch: 1438, train_loss: 0.00012096050752164877, valid_loss: 0.00045027751427066204, test_loss: 0.001468841452151537\n",
      "epoch: 1439, train_loss: 0.00012110442596325731, valid_loss: 0.000451408030736881, test_loss: 0.0014689788222312927\n",
      "epoch: 1440, train_loss: 0.00012084868997234203, valid_loss: 0.0004504658330309515, test_loss: 0.001468359143473208\n",
      "epoch: 1441, train_loss: 0.0001207251198534125, valid_loss: 0.0004497689854664107, test_loss: 0.0014674916164949536\n",
      "epoch: 1442, train_loss: 0.00012088842967110078, valid_loss: 0.00044992115241863456, test_loss: 0.0014673457480967045\n",
      "epoch: 1443, train_loss: 0.00012073629003008018, valid_loss: 0.0004506057982022564, test_loss: 0.0014676378341391683\n",
      "epoch: 1444, train_loss: 0.00012075731470047131, valid_loss: 0.0004492153408743131, test_loss: 0.001465371111407876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1445, train_loss: 0.00012063163550287162, valid_loss: 0.00044839093849683803, test_loss: 0.0014655936975032091\n",
      "epoch: 1446, train_loss: 0.00012072844306796627, valid_loss: 0.00044959428244813654, test_loss: 0.0014648056821897626\n",
      "epoch: 1447, train_loss: 0.0001206982105150414, valid_loss: 0.0004493914214738955, test_loss: 0.001464498694986105\n",
      "epoch: 1448, train_loss: 0.00012064625764690825, valid_loss: 0.0004476948136774202, test_loss: 0.0014640497975051403\n",
      "epoch: 1449, train_loss: 0.0001206072591467882, valid_loss: 0.00044809244233571616, test_loss: 0.00146368402056396\n",
      "epoch: 1450, train_loss: 0.000120396876414104, valid_loss: 0.0004493905435083434, test_loss: 0.001463552238419652\n",
      "epoch: 1451, train_loss: 0.00012042518178729907, valid_loss: 0.0004502615871994446, test_loss: 0.0014636348932981491\n",
      "epoch: 1452, train_loss: 0.00012042794190600272, valid_loss: 0.00044766880697958794, test_loss: 0.0014625298790633678\n",
      "epoch: 1453, train_loss: 0.00012035899746241857, valid_loss: 0.00044884122083506856, test_loss: 0.0014621746959164739\n",
      "epoch: 1454, train_loss: 0.00012030311209736797, valid_loss: 0.00044831755561366055, test_loss: 0.0014617933193221688\n",
      "epoch: 1455, train_loss: 0.00012039835533157557, valid_loss: 0.00044776423843965557, test_loss: 0.0014616649132221937\n",
      "epoch: 1456, train_loss: 0.00012039974440678792, valid_loss: 0.00044708806429601583, test_loss: 0.001461339183151722\n",
      "epoch: 1457, train_loss: 0.00012012886271923375, valid_loss: 0.0004464070055594978, test_loss: 0.0014600650174543262\n",
      "epoch: 1458, train_loss: 0.0001202512465407262, valid_loss: 0.0004480991919990629, test_loss: 0.0014596724649891257\n",
      "epoch: 1459, train_loss: 0.00012014546740354727, valid_loss: 0.0004472345074949165, test_loss: 0.0014591981889680028\n",
      "epoch: 1460, train_loss: 0.00012028949909701781, valid_loss: 0.0004465602808825982, test_loss: 0.001457865466363728\n",
      "epoch: 1461, train_loss: 0.00012009873171336949, valid_loss: 0.00044715284457197413, test_loss: 0.0014592164661735296\n",
      "epoch: 1462, train_loss: 0.00012004366378633954, valid_loss: 0.0004457353740387286, test_loss: 0.001457696664147079\n",
      "epoch: 1463, train_loss: 0.00012001563047069003, valid_loss: 0.000445572239793061, test_loss: 0.0014564840821549296\n",
      "epoch: 1464, train_loss: 0.00011984987213509157, valid_loss: 0.0004460739025186437, test_loss: 0.0014568079495802522\n",
      "epoch: 1465, train_loss: 0.00011999583638617364, valid_loss: 0.00044714737062652904, test_loss: 0.0014573645312339067\n",
      "epoch: 1466, train_loss: 0.0001201615004506691, valid_loss: 0.00044570268558648724, test_loss: 0.0014564624289050698\n",
      "epoch: 1467, train_loss: 0.00011980506868617933, valid_loss: 0.00044453881613056484, test_loss: 0.001454705256037414\n",
      "epoch: 1468, train_loss: 0.00012002080494193765, valid_loss: 0.00044539574688921374, test_loss: 0.0014545227168127894\n",
      "epoch: 1469, train_loss: 0.00011991159566526261, valid_loss: 0.0004461761612522726, test_loss: 0.0014555825619027019\n",
      "epoch: 1470, train_loss: 0.00011970823138456225, valid_loss: 0.00044495553205100197, test_loss: 0.0014543039724230766\n",
      "epoch: 1471, train_loss: 0.00011970221638203721, valid_loss: 0.00044404233146148425, test_loss: 0.0014534196816384792\n",
      "epoch: 1472, train_loss: 0.00011973024621788089, valid_loss: 0.000443672230176162, test_loss: 0.001452529919333756\n",
      "epoch: 1473, train_loss: 0.00011968148180091268, valid_loss: 0.0004446789947299597, test_loss: 0.0014531422639265656\n",
      "epoch: 1474, train_loss: 0.00011974638176110128, valid_loss: 0.00044656660126444575, test_loss: 0.0014536863891407847\n",
      "epoch: 1475, train_loss: 0.00011956695000306986, valid_loss: 0.0004435106578360622, test_loss: 0.0014509938191622496\n",
      "epoch: 1476, train_loss: 0.00011949240655192862, valid_loss: 0.0004436594632958683, test_loss: 0.0014510692562907934\n",
      "epoch: 1477, train_loss: 0.00011959006382233423, valid_loss: 0.0004435669179656543, test_loss: 0.0014510180335491896\n",
      "epoch: 1478, train_loss: 0.00011943798333801249, valid_loss: 0.0004431432510803764, test_loss: 0.001449517672881484\n",
      "epoch: 1479, train_loss: 0.00011936150511483784, valid_loss: 0.0004426112839913306, test_loss: 0.001449923380278051\n",
      "epoch: 1480, train_loss: 0.0001195855267882428, valid_loss: 0.00044227159620883566, test_loss: 0.00144858006387949\n",
      "epoch: 1481, train_loss: 0.00011926707235903925, valid_loss: 0.0004431691922945902, test_loss: 0.001448682276532054\n",
      "epoch: 1482, train_loss: 0.00011931750423568504, valid_loss: 0.00044197935009530437, test_loss: 0.00144908984657377\n",
      "epoch: 1483, train_loss: 0.00011929381783019103, valid_loss: 0.000441719384980388, test_loss: 0.0014479553792625666\n",
      "epoch: 1484, train_loss: 0.00011918696817820486, valid_loss: 0.00044291725498624146, test_loss: 0.001448591472581029\n",
      "epoch: 1485, train_loss: 0.00011901649375668849, valid_loss: 0.0004414462261289979, test_loss: 0.0014466765569522977\n",
      "epoch: 1486, train_loss: 0.0001191705438111018, valid_loss: 0.0004409203926722209, test_loss: 0.001445953967049718\n",
      "epoch: 1487, train_loss: 0.00011951566399226937, valid_loss: 0.0004410436231410131, test_loss: 0.0014458122896030545\n",
      "epoch: 1488, train_loss: 0.00011899858667596203, valid_loss: 0.00044066084471220773, test_loss: 0.0014448771253228188\n",
      "epoch: 1489, train_loss: 0.00011903636534597077, valid_loss: 0.0004405658958906618, test_loss: 0.0014451752649620175\n",
      "epoch: 1490, train_loss: 0.00011901568296193348, valid_loss: 0.0004407373126014136, test_loss: 0.0014442899264395237\n",
      "epoch: 1491, train_loss: 0.0001190634344390634, valid_loss: 0.0004409382599988021, test_loss: 0.0014445158885791898\n",
      "epoch: 1492, train_loss: 0.00011916742242528531, valid_loss: 0.0004402447399722102, test_loss: 0.0014429151779040694\n",
      "epoch: 1493, train_loss: 0.00011872763539229155, valid_loss: 0.00044101620733272284, test_loss: 0.0014436437049880624\n",
      "epoch: 1494, train_loss: 0.00011882964083823659, valid_loss: 0.00044174885988468304, test_loss: 0.0014427942223846912\n",
      "epoch: 1495, train_loss: 0.0001186802315793972, valid_loss: 0.000439963524210422, test_loss: 0.0014415609184652567\n",
      "epoch: 1496, train_loss: 0.00011868353328242412, valid_loss: 0.0004415311899113779, test_loss: 0.0014424602268263698\n",
      "epoch: 1497, train_loss: 0.00011876034683234099, valid_loss: 0.0004424446063543049, test_loss: 0.0014425341505557299\n",
      "epoch: 1498, train_loss: 0.00011867251242106052, valid_loss: 0.00043979986852112535, test_loss: 0.0014411049196496606\n",
      "epoch: 1499, train_loss: 0.00011859234054218572, valid_loss: 0.0004399165918584913, test_loss: 0.001441105268895626\n",
      "epoch: 1500, train_loss: 0.00011870734717534936, valid_loss: 0.000439549439761322, test_loss: 0.0014402735978364944\n",
      "epoch: 1501, train_loss: 0.00011851990553702268, valid_loss: 0.00043850794463651255, test_loss: 0.0014393049059435725\n",
      "epoch: 1502, train_loss: 0.00011857058827603316, valid_loss: 0.0004388323165282297, test_loss: 0.0014389636926352978\n",
      "epoch: 1503, train_loss: 0.00011854909383185694, valid_loss: 0.0004386655612809894, test_loss: 0.0014376768376678228\n",
      "epoch: 1504, train_loss: 0.00011850463298564453, valid_loss: 0.00043850359118854004, test_loss: 0.0014375708997249603\n",
      "epoch: 1505, train_loss: 0.00011858594434275089, valid_loss: 0.00043901072300892946, test_loss: 0.001437513972632587\n",
      "epoch: 1506, train_loss: 0.00011866854386070095, valid_loss: 0.0004379038024732533, test_loss: 0.0014370194403454661\n",
      "epoch: 1507, train_loss: 0.00011827731909959212, valid_loss: 0.0004385432257549837, test_loss: 0.0014375399332493544\n",
      "epoch: 1508, train_loss: 0.00011829212946948878, valid_loss: 0.00043785030478223536, test_loss: 0.001436045509763062\n",
      "epoch: 1509, train_loss: 0.00011833141204830179, valid_loss: 0.0004379429155960679, test_loss: 0.0014369189739227295\n",
      "epoch: 1510, train_loss: 0.00011843484263290125, valid_loss: 0.0004370168414122115, test_loss: 0.001435029087588191\n",
      "epoch: 1511, train_loss: 0.00011842906836966944, valid_loss: 0.0004381971884868108, test_loss: 0.0014366749674081802\n",
      "epoch: 1512, train_loss: 0.00011815698741449286, valid_loss: 0.0004372996700112708, test_loss: 0.0014340608613565564\n",
      "epoch: 1513, train_loss: 0.00011815722941917002, valid_loss: 0.00043668093227703747, test_loss: 0.001434245496056974\n",
      "epoch: 1514, train_loss: 0.0001181219116045648, valid_loss: 0.00043917074071941897, test_loss: 0.0014356279280036688\n",
      "epoch: 1515, train_loss: 0.0001180947099625052, valid_loss: 0.000437489477917552, test_loss: 0.0014341863570734859\n",
      "epoch: 1516, train_loss: 0.00011809447365205573, valid_loss: 0.0004392836684322295, test_loss: 0.0014354240847751498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1517, train_loss: 0.00011813129980430898, valid_loss: 0.00043663294733657193, test_loss: 0.0014324960066005588\n",
      "epoch: 1518, train_loss: 0.00011805443437365086, valid_loss: 0.0004372127295937389, test_loss: 0.001433906378224492\n",
      "epoch: 1519, train_loss: 0.00011792525290967087, valid_loss: 0.0004359262966318056, test_loss: 0.0014313454739749432\n",
      "epoch: 1520, train_loss: 0.00011787025900760099, valid_loss: 0.0004361757867930767, test_loss: 0.0014310511760413647\n",
      "epoch: 1521, train_loss: 0.00011785690635999502, valid_loss: 0.00043755245860666037, test_loss: 0.0014318713219836354\n",
      "epoch: 1522, train_loss: 0.00011799396926780109, valid_loss: 0.0004349784576334059, test_loss: 0.0014307748060673475\n",
      "epoch: 1523, train_loss: 0.00011772761385857493, valid_loss: 0.00043545431496265036, test_loss: 0.001429888652637601\n",
      "epoch: 1524, train_loss: 0.00011768280028659122, valid_loss: 0.0004346198305332412, test_loss: 0.0014296836452558637\n",
      "epoch: 1525, train_loss: 0.00011765323047850119, valid_loss: 0.0004369717086471307, test_loss: 0.0014306370867416263\n",
      "epoch: 1526, train_loss: 0.00011762037383564545, valid_loss: 0.00043511503463378176, test_loss: 0.001429338357411325\n",
      "epoch: 1527, train_loss: 0.00011764374927303794, valid_loss: 0.00043448728683870286, test_loss: 0.0014287716476246715\n",
      "epoch: 1528, train_loss: 0.00011758823498182565, valid_loss: 0.0004354680471199875, test_loss: 0.001427753595635295\n",
      "epoch: 1529, train_loss: 0.0001175344928608114, valid_loss: 0.00043352711509214714, test_loss: 0.0014272318221628666\n",
      "epoch: 1530, train_loss: 0.0001175236491531984, valid_loss: 0.00043341067794244736, test_loss: 0.0014266521902754903\n",
      "epoch: 1531, train_loss: 0.00011743168959029667, valid_loss: 0.0004348595272555637, test_loss: 0.0014268087688833475\n",
      "epoch: 1532, train_loss: 0.00011757711288974743, valid_loss: 0.0004349540880260368, test_loss: 0.001426553470082581\n",
      "epoch: 1533, train_loss: 0.00011742395935985057, valid_loss: 0.00043326592034039396, test_loss: 0.0014253120170906186\n",
      "epoch: 1534, train_loss: 0.00011733263027449341, valid_loss: 0.0004348335326843274, test_loss: 0.0014261197065934539\n",
      "epoch: 1535, train_loss: 0.00011741502448390035, valid_loss: 0.00043367167866866413, test_loss: 0.0014253261033445597\n",
      "epoch: 1536, train_loss: 0.0001172253440650504, valid_loss: 0.0004324523179093376, test_loss: 0.0014237046707421541\n",
      "epoch: 1537, train_loss: 0.0001173316309375324, valid_loss: 0.0004331731906859204, test_loss: 0.001423874869942665\n",
      "epoch: 1538, train_loss: 0.00011742347629953419, valid_loss: 0.00043276332144159824, test_loss: 0.0014234448317438364\n",
      "epoch: 1539, train_loss: 0.00011706086376722416, valid_loss: 0.00043442200209635, test_loss: 0.0014248320367187262\n",
      "epoch: 1540, train_loss: 0.00011721672680046733, valid_loss: 0.00043572845849363756, test_loss: 0.0014252756955102086\n",
      "epoch: 1541, train_loss: 0.00011721034736736961, valid_loss: 0.00043370705922522273, test_loss: 0.001423342851921916\n",
      "epoch: 1542, train_loss: 0.00011712480361735368, valid_loss: 0.0004328139524053161, test_loss: 0.001422013738192618\n",
      "epoch: 1543, train_loss: 0.00011693947833365478, valid_loss: 0.0004320037502717848, test_loss: 0.0014207825297489762\n",
      "epoch: 1544, train_loss: 0.00011703536628097619, valid_loss: 0.0004334256930936438, test_loss: 0.0014218772994354367\n",
      "epoch: 1545, train_loss: 0.00011709341355447617, valid_loss: 0.0004323806448761995, test_loss: 0.001420887652784586\n",
      "epoch: 1546, train_loss: 0.00011687227474201633, valid_loss: 0.00043183569505345076, test_loss: 0.0014198662247508764\n",
      "epoch: 1547, train_loss: 0.00011688985377195817, valid_loss: 0.0004320025861185665, test_loss: 0.0014192983508110046\n",
      "epoch: 1548, train_loss: 0.00011711435028068398, valid_loss: 0.0004313786484999582, test_loss: 0.0014194929972290993\n",
      "epoch: 1549, train_loss: 0.00011683550205223425, valid_loss: 0.00043164537055417895, test_loss: 0.0014190112706273794\n",
      "epoch: 1550, train_loss: 0.00011686511456703201, valid_loss: 0.0004312275608147805, test_loss: 0.0014179765712469816\n",
      "epoch: 1551, train_loss: 0.00011678837187861538, valid_loss: 0.0004300015522555138, test_loss: 0.0014174390817061067\n",
      "epoch: 1552, train_loss: 0.00011677960561507422, valid_loss: 0.0004316645718063228, test_loss: 0.0014184104511514306\n",
      "epoch: 1553, train_loss: 0.00011684981860605109, valid_loss: 0.00043027409265050665, test_loss: 0.0014173212694004178\n",
      "epoch: 1554, train_loss: 0.00011668427601841319, valid_loss: 0.0004298457594510789, test_loss: 0.001416453393176198\n",
      "epoch: 1555, train_loss: 0.00011654650133214486, valid_loss: 0.00042944623176784563, test_loss: 0.001416312879882753\n",
      "epoch: 1556, train_loss: 0.00011663504783823358, valid_loss: 0.0004297956111258827, test_loss: 0.0014159263810142875\n",
      "epoch: 1557, train_loss: 0.00011664829704070301, valid_loss: 0.0004292220376858798, test_loss: 0.0014151505893096328\n",
      "epoch: 1558, train_loss: 0.0001166964189589793, valid_loss: 0.00043015099557427067, test_loss: 0.001415467937476933\n",
      "epoch: 1559, train_loss: 0.00011656863700962909, valid_loss: 0.0004294852430272537, test_loss: 0.0014144341694191098\n",
      "epoch: 1560, train_loss: 0.00011666882314982459, valid_loss: 0.0004311473797618722, test_loss: 0.0014152524527162313\n",
      "epoch: 1561, train_loss: 0.00011658434010789041, valid_loss: 0.0004294569856331994, test_loss: 0.001414009602740407\n",
      "epoch: 1562, train_loss: 0.0001163956827519502, valid_loss: 0.00042925858482097584, test_loss: 0.001412909128703177\n",
      "epoch: 1563, train_loss: 0.00011633244740432534, valid_loss: 0.0004312645493579718, test_loss: 0.0014142903964966536\n",
      "epoch: 1564, train_loss: 0.0001163892612448367, valid_loss: 0.0004281810251995921, test_loss: 0.0014123235596343875\n",
      "epoch: 1565, train_loss: 0.000116293793404981, valid_loss: 0.00042887040035566315, test_loss: 0.001411615521647036\n",
      "epoch: 1566, train_loss: 0.00011625404579757267, valid_loss: 0.0004288128814854038, test_loss: 0.001411683508194983\n",
      "epoch: 1567, train_loss: 0.00011621238619504173, valid_loss: 0.00042851257846147445, test_loss: 0.0014113072538748384\n",
      "epoch: 1568, train_loss: 0.00011615938685439608, valid_loss: 0.00042796948643323657, test_loss: 0.001410427037626505\n",
      "epoch: 1569, train_loss: 0.00011614400231653987, valid_loss: 0.0004286796320229769, test_loss: 0.0014105216832831502\n",
      "epoch: 1570, train_loss: 0.00011619605420096575, valid_loss: 0.0004275952451280318, test_loss: 0.001409710617735982\n",
      "epoch: 1571, train_loss: 0.00011607281371455073, valid_loss: 0.00042928828527995694, test_loss: 0.0014100116677582264\n",
      "epoch: 1572, train_loss: 0.00011615817778004819, valid_loss: 0.0004265494014058883, test_loss: 0.0014086277224123478\n",
      "epoch: 1573, train_loss: 0.00011614344839472324, valid_loss: 0.00042807279047944274, test_loss: 0.0014087777817621827\n",
      "epoch: 1574, train_loss: 0.00011605491422612786, valid_loss: 0.0004280479091297214, test_loss: 0.0014094734797254205\n",
      "epoch: 1575, train_loss: 0.00011615604402639134, valid_loss: 0.0004275674946256913, test_loss: 0.0014083414571359754\n",
      "epoch: 1576, train_loss: 0.00011588001918584189, valid_loss: 0.00042827285748595995, test_loss: 0.0014085613656789064\n",
      "epoch: 1577, train_loss: 0.00011589087301652636, valid_loss: 0.0004270512581570074, test_loss: 0.0014069220051169395\n",
      "epoch: 1578, train_loss: 0.00011595651417535365, valid_loss: 0.0004278392128374738, test_loss: 0.0014083243440836668\n",
      "epoch: 1579, train_loss: 0.00011574721619304593, valid_loss: 0.000426733919690984, test_loss: 0.0014068081509321928\n",
      "epoch: 1580, train_loss: 0.0001156820660031846, valid_loss: 0.00042610696497528505, test_loss: 0.0014057602966204286\n",
      "epoch: 1581, train_loss: 0.00011574247511579533, valid_loss: 0.00042596847682337585, test_loss: 0.0014055995270609856\n",
      "epoch: 1582, train_loss: 0.0001158602102330643, valid_loss: 0.0004253435690770857, test_loss: 0.0014057018561288714\n",
      "epoch: 1583, train_loss: 0.00011564742168917766, valid_loss: 0.0004266181034230006, test_loss: 0.0014050398021936417\n",
      "epoch: 1584, train_loss: 0.00011578651079300629, valid_loss: 0.0004269350562632705, test_loss: 0.0014059667009860277\n",
      "epoch: 1585, train_loss: 0.00011558246605209601, valid_loss: 0.0004245819848923323, test_loss: 0.0014035786734893918\n",
      "epoch: 1586, train_loss: 0.00011566729960537961, valid_loss: 0.0004270385009779905, test_loss: 0.0014044279232621193\n",
      "epoch: 1587, train_loss: 0.00011544445220826437, valid_loss: 0.00042646026122383773, test_loss: 0.0014035091735422611\n",
      "epoch: 1588, train_loss: 0.00011558912228792906, valid_loss: 0.0004255559542798437, test_loss: 0.0014028154546394944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1589, train_loss: 0.00011536886608076242, valid_loss: 0.0004251348170024964, test_loss: 0.0014016941422596574\n",
      "epoch: 1590, train_loss: 0.0001156107654147174, valid_loss: 0.0004250143368456823, test_loss: 0.0014021260431036353\n",
      "epoch: 1591, train_loss: 0.00011544117264945626, valid_loss: 0.00042446990361592424, test_loss: 0.0014015142805874348\n",
      "epoch: 1592, train_loss: 0.00011532296301321249, valid_loss: 0.0004245135035792676, test_loss: 0.001401041285134852\n",
      "epoch: 1593, train_loss: 0.00011534523440312351, valid_loss: 0.00042437949499192956, test_loss: 0.001400625566020608\n",
      "epoch: 1594, train_loss: 0.00011527875682551657, valid_loss: 0.0004236425835794459, test_loss: 0.0014003554824739695\n",
      "epoch: 1595, train_loss: 0.00011534948577679208, valid_loss: 0.0004235582406787823, test_loss: 0.0013991573359817266\n",
      "epoch: 1596, train_loss: 0.0001153782580766584, valid_loss: 0.000425220202790418, test_loss: 0.001401232206262648\n",
      "epoch: 1597, train_loss: 0.00011532441693935138, valid_loss: 0.00042483270954107866, test_loss: 0.0014000626979395747\n",
      "epoch: 1598, train_loss: 0.00011526451081685398, valid_loss: 0.00042270580646193895, test_loss: 0.0013989103026688099\n",
      "epoch: 1599, train_loss: 0.00011495284517245044, valid_loss: 0.00042416257444225874, test_loss: 0.0013985418481752276\n"
     ]
    }
   ],
   "source": [
    "seq_dim = 10 # = window_size\n",
    "\n",
    "num_epochs = 1600\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "test_loss_1 = []\n",
    "test_loss_2 = []\n",
    "test_loss_3 = []\n",
    "test_loss_4 = []\n",
    "test_loss_5 = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    train_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(train_loader):\n",
    "        # print(\"train: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(seqs.float())\n",
    "        outputs = model(seqs)\n",
    "        # print(outputs.is_cuda)\n",
    "        # print(labels.is_cuda)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs.dtype)\n",
    "        \n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_train_loss += loss.data.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch = i + 1;\n",
    "        \n",
    "        # print(\"loss: \", loss.data)\n",
    "    \n",
    "    total_valid_loss = 0.0\n",
    "    valid_batch = 0\n",
    "    # valid_seq = []\n",
    "    valid_pred = []\n",
    "    # valid_gt = []\n",
    "    for i, (seqs, labels) in enumerate(valid_loader):\n",
    "        # print(\"valid: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_valid_loss += loss.data.item()\n",
    "        # valid_gt.append(labels)\n",
    "        valid_pred.append(outputs)\n",
    "        # valid_seq.append(seqs)\n",
    "        \n",
    "        valid_batch = i + 1\n",
    "    \n",
    "    '''\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    # test_seq = []\n",
    "    test_pred = []\n",
    "    # test_gt = []\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        test_pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        test_batch = i + 1\n",
    "    '''\n",
    "    test_cur_loss_1 = get_epoch_loss(test_loader_1)\n",
    "    test_cur_loss_2 = get_epoch_loss(test_loader_2)\n",
    "    test_cur_loss_3 = get_epoch_loss(test_loader_3)\n",
    "    test_cur_loss_4 = get_epoch_loss(test_loader_4)\n",
    "    test_cur_loss_5 = get_epoch_loss(test_loader_5)\n",
    "    \n",
    "    # print(\"train batch: \", train_batch)\n",
    "    # print(\"valid batch: \", valid_batch)\n",
    "    train_loss.append(total_train_loss/train_batch)\n",
    "    valid_loss.append(total_valid_loss/valid_batch)\n",
    "    # test_loss.append(total_test_loss/test_batch)\n",
    "    test_loss_1.append(test_cur_loss_1)\n",
    "    test_loss_2.append(test_cur_loss_2)\n",
    "    test_loss_3.append(test_cur_loss_3)\n",
    "    test_loss_4.append(test_cur_loss_4)\n",
    "    test_loss_5.append(test_cur_loss_5)\n",
    "    # train_loss.append(total_train_loss)\n",
    "    # valid_loss.append(total_valid_loss)\n",
    "    # print(\"epoch: {}, train_loss: {}, valid_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_valid_loss/valid_batch, total_test_loss/test_batch))\n",
    "    print(\"epoch: {}, train_loss: {}, valid_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_valid_loss/valid_batch, test_cur_loss_1))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 1600\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "valid_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), valid_loss[starting_epoch:end_epoch])\n",
    "test_loss_curve_1, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_1[starting_epoch:end_epoch])\n",
    "test_loss_curve_2, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_2[starting_epoch:end_epoch])\n",
    "test_loss_curve_3, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_3[starting_epoch:end_epoch])\n",
    "test_loss_curve_4, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_4[starting_epoch:end_epoch])\n",
    "test_loss_curve_5, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_5[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, valid_loss_curve, test_loss_curve_1, test_loss_curve_2, test_loss_curve_3, test_loss_curve_4, test_loss_curve_5], ['Train Loss', 'Validation Loss', 'Test Loss 1', 'Test Loss 2', 'Test Loss 3', 'Test Loss 4', 'Test Loss 5'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x mean: 0.5009409709733069 ; std: 0.2436898615684393\n",
    "y mean: 0.4416590158205868 ; std: 0.23351418998386245\n",
    "z mean: 0.48521343842977466 ; std: 0.2423616199566707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch: 1599, train_loss: 0.0007271119671792763, valid_loss: 0.002470962528605014, test_loss: 0.001238117110915482\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get valid results\n",
    "seq_dim = 10 # = window_size\n",
    "input_dim = 3\n",
    "# valid_seq = []\n",
    "valid_predd = []\n",
    "# valid_gt = []\n",
    "total_valid_loss = 0.0\n",
    "valid_batch = 0\n",
    "for i, (seqs, labels) in enumerate(valid_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        \n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_valid_loss += loss.data.item()\n",
    "    valid_predd.append(outputs)\n",
    "    valid_batch = i + 1\n",
    "print(total_valid_loss/valid_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of ground true around predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore testing data\n",
    "test_seq = []\n",
    "test_gt = []\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    test_gt.append(labels)\n",
    "    test_seq.append(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = test_seq[i].numpy()\n",
    "        gt = test_gt[i].numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        test = np.append(test, test_seq[i].numpy(), axis = 0)\n",
    "        gt = np.append(gt, test_gt[i].numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test[0][-1,:].reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct analysis matrix l(0-2 col): last sequence point, g(3-5 col): next ground true point, p(6-8 col): predicted point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    last_point = test[i][-1, :].reshape(1, -1)\n",
    "    gt_point = gt[i].reshape(1, -1)\n",
    "    pred_point = pred[i].reshape(1, -1)\n",
    "    row = np.append(np.append(last_point, gt_point, axis = 1), pred_point, axis = 1)\n",
    "    if (i == 0):\n",
    "        analysis = row\n",
    "    else:\n",
    "        analysis = np.append(analysis, row, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rotation matrix\n",
    "# note: input vec_1 and vec_2 have to be normalized before passing to the function\n",
    "def get_rotatin_mat(vec_1, vec_2):\n",
    "    a,b = vec_1.reshape(3), vec_2.reshape(3)\n",
    "    v = np.cross(a,b)\n",
    "    c = np.dot(a,b)\n",
    "    s = np.linalg.norm(v)\n",
    "    # print(\"s\", s)\n",
    "    if (s == 0):\n",
    "        return np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "    I = np.identity(3)\n",
    "    vXStr = '{} {} {}; {} {} {}; {} {} {}'.format(0, -v[2], v[1], v[2], 0, -v[0], -v[1], v[0], 0)\n",
    "    k = np.matrix(vXStr)\n",
    "    r = I + k + np.matmul(k,k) * ((1 -c)/(s**2))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 1, 0])\n",
    "a = a/np.linalg.norm(a)\n",
    "b = np.array([4, 5, 6])\n",
    "b = b/np.linalg.norm(b)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "m = get_rotatin_mat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(m, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(b[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b/np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "a_n = a/np.linalg.norm(a)\n",
    "\n",
    "b = np.array([[4,5,6]])\n",
    "b_n = b/np.linalg.norm(b)\n",
    "\n",
    "c = np.array([[7,8,9]])\n",
    "c_n = c/np.linalg.norm(c)\n",
    "\n",
    "r = get_rotatin_mat(b_n, a_n) # b to a\n",
    "\n",
    "b_p = np.dot(r, b.reshape(3, -1))\n",
    "c_p = np.dot(r, c.reshape(3, -1))\n",
    "c_p = np.asarray(c_p)\n",
    "c_p_n = c_p/np.linalg.norm(c_p)\n",
    "print(b.shape)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"c\", c)\n",
    "print(\"b_p\", b_p)\n",
    "print(\"c_p\", c_p)\n",
    "\n",
    "print(get_rotatin_mat(c_n, b_n))\n",
    "\n",
    "print(get_rotatin_mat(c_p_n, a_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c_p.reshape(1, -1)\n",
    "d = np.asarray(d)\n",
    "d_n = d/np.linalg.norm(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rotatin_mat(d_n, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all point to the same starting point\n",
    "# first sequence last direction\n",
    "# direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = (analysis[0, 3:6] - analysis[0, 0:3]).reshape(1, -1)\n",
    "# normalize\n",
    "direction = direction/np.linalg.norm(direction)\n",
    "# first diff between prediction and ground true\n",
    "diff_correct = (analysis[0, 6:9] - analysis[0, 3:6]).reshape(1, -1)\n",
    "# print(type(diff_correct))\n",
    "# print(diff_correct.shape)\n",
    "for i in range(1, analysis.shape[0]):\n",
    "    # cur_direction = (analysis[i, 0:3] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    cur_direction = (analysis[i, 3:6] - analysis[i, 0:3]).reshape(1, -1)\n",
    "    # normalize\n",
    "    if (np.linalg.norm(cur_direction) == 0):\n",
    "        print(i, np.linalg.norm(cur_direction))\n",
    "    cur_direction = cur_direction/np.linalg.norm(cur_direction)\n",
    "    cur_diff = (analysis[i, 6:9] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    # get rotation matrix from cur_direction to direction\n",
    "    r = get_rotatin_mat(cur_direction, direction)\n",
    "    # apply rotation matrix to the cur_diff\n",
    "    cur_diff = np.dot(r, cur_diff.reshape(3, -1))\n",
    "    cur_diff = cur_diff.reshape(1, -1)\n",
    "    # matrix type to np array type\n",
    "    cur_diff = np.asarray(cur_diff)\n",
    "    # print(cur_diff.shape)\n",
    "    diff_correct = np.append(diff_correct, cur_diff, axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_correct[:, 0], diff_correct[:, 1], diff_correct[:, 2])\n",
    "x = [-direction[0, 0], 0]\n",
    "y = [-direction[0, 1], 0]\n",
    "z = [-direction[0, 2], 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "# ax.arrow(0, 0, 0.5, 0.5, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "# ax.quiver(0, 0, 0, -direction[0, 0], -direction[0, 1], -direction[0, 2], length=5, normalize=True, color='r')\n",
    "# x = np.zeros(10)\n",
    "# y = np.zeros(10)\n",
    "# z = np.arange(10)*10 # remove *100 and the arrow heads will reappear.\n",
    "# dx = np.zeros(10)\n",
    "# dy = np.arange(10)\n",
    "# dz = np.zeros(10)\n",
    "x = np.array([0, -direction[0, 0]])\n",
    "y = np.array([0, -direction[0, 1]])\n",
    "z = np.array([0, -direction[0, 2]])\n",
    "dx = np.array([0, 0])\n",
    "dy = np.array([0, 0])\n",
    "dz = np.array([0, 0])\n",
    "# ax.quiver(x, y, z, dx, dy, dz, length=1)\n",
    "# ax.quiver(-direction[0, 0], -direction[0, 1], -direction[0, 2], 0, 0, 0, length=100, normalize=True)\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note view_angle must be normalized\n",
    "def getProjection(view_angle, point):\n",
    "    v = point - np.array([[0, 0, 0]])\n",
    "    dist = v[0, 0]*view_angle[0, 0] + v[0, 1]*view_angle[0, 1] + v[0, 2]*view_angle[0, 2]\n",
    "    projected_point = point - dist*view_angle\n",
    "    \n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))\n",
    "# normalization\n",
    "view_angle = view_angle/np.linalg.norm(view_angle)\n",
    "print(view_angle)\n",
    "for i in range(diff_correct.shape[0]):\n",
    "    if (i == 0):\n",
    "        diff_corrrect_projection = getProjection(view_angle, diff_correct[i, :])\n",
    "    else:\n",
    "        diff_corrrect_projection = np.append(diff_corrrect_projection, getProjection(view_angle, diff_correct[i, :]), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_corrrect_projection[:, 0], diff_corrrect_projection[:, 1], diff_corrrect_projection[:, 2])\n",
    "x = [-direction[0, 0]*10, 0]\n",
    "y = [-direction[0, 1]*10, 0]\n",
    "z = [-direction[0, 2]*10, 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from 3D to 2D\n",
    "diff_corrrect_projection_2D = np.delete(diff_corrrect_projection, 1, 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from matplotlib.colors import LogNorm\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(diff_corrrect_projection_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(-5., 5.)\n",
    "y = np.linspace(-5., 5.)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -clf.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
    "                 levels=np.logspace(0, 3, 300))\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(diff_corrrect_projection_2D[:, 0], diff_corrrect_projection_2D[:, 1], .8)\n",
    "\n",
    "x = [-direction[0, 0]*5, 0]\n",
    "z = [-direction[0, 2]*5, 0]\n",
    "plt.plot(x, z, label='parametric curve', color='r')\n",
    "\n",
    "plt.title('Negative log-likelihood predicted by a GMM')\n",
    "plt.axis('tight')\n",
    "plt.xlabel(\"Direction 1\")\n",
    "plt.ylabel(\"Direction 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "C = np.array([[0., -0.7], [3.5, .7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_gaussian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(direction[0, 0], 2) + pow(direction[0, 1], 2) + pow(direction[0, 2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[2, 0:3] - analysis[2, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[3, 0:3] - analysis[3, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = direction/np.linalg.norm(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (analysis[1, 0:3] - analysis[1, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d/np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(0.84804916, 2) + pow(0.5299187, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 3:6] - analysis[0, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([13.259178  ,  0.37397736, 20.36138]) - np.asarray([12.658086  ,  1.293065  ,20.224268])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = \n",
    "        gt = test_gt[i].cpu().detach().numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        gt = np.append(gt, test_gt[i].cpu().detach().numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pred - gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff[:, 0], diff[:, 1], diff[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.from_numpy(gt), torch.from_numpy(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i in range(len(test_pred)):\n",
    "    l = criterion(test_gt[i], test_pred[i])\n",
    "    loss += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
