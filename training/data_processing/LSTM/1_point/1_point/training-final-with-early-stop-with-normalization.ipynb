{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test1 as the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, show_statistic, min_max_scaling, normalize_one, normalize_all, construct_train_valid_tensor, construct_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "PATH = \"model/with_early_stop_after_400/model_w_10.pt\"\n",
    "PATH_LOG = \"model/with_early_stop_after_400/model_w_10.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x 1:\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.49456592564496366\n",
      "std: 0.23332125084193656\n",
      "valid x 1:\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5597830145569421\n",
      "std: 0.283736478050022\n",
      "train x 1:\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: -0.026160486477819007\n",
      "std: 0.9574516122264272\n",
      "valid x 1:\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: 0.24146282986463127\n",
      "std: 1.1643343560697774\n",
      "x mean: 0.5009409709733069 ; std: 0.2436898615684393\n",
      "y mean: 0.4416590158205868 ; std: 0.23351418998386245\n",
      "z mean: 0.48521343842977466 ; std: 0.2423616199566707\n",
      "14950\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "# load each of the 5 dataset and do min_max_scaling on each of them\n",
    "train_set_x_1, valid_set_x_1, train_set_y_1, valid_set_y_1, train_set_z_1, valid_set_z_1 = load_data_2('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, valid_set_x_2, train_set_y_2, valid_set_y_2, train_set_z_2, valid_set_z_2 = load_data_2('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, valid_set_x_3, train_set_y_3, valid_set_y_3, train_set_z_3, valid_set_z_3 = load_data_2('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, valid_set_x_4, train_set_y_4, valid_set_y_4, train_set_z_4, valid_set_z_4 = load_data_2('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, valid_set_x_5, train_set_y_5, valid_set_y_5, train_set_z_5, valid_set_z_5 = load_data_2('data_preprocessing/test_5_training_xyz.txt', 400)\n",
    "\n",
    "# do min-max-scaling for each data set\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_1)\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_2)\n",
    "min_max_scaling(train_set_x_3)\n",
    "min_max_scaling(train_set_x_4)\n",
    "min_max_scaling(train_set_x_5)\n",
    "\n",
    "min_max_scaling(valid_set_x_1)\n",
    "min_max_scaling(valid_set_x_2)\n",
    "min_max_scaling(valid_set_x_3)\n",
    "min_max_scaling(valid_set_x_4)\n",
    "min_max_scaling(valid_set_x_5)\n",
    "\n",
    "min_max_scaling(train_set_y_1)\n",
    "min_max_scaling(train_set_y_2)\n",
    "min_max_scaling(train_set_y_3)\n",
    "min_max_scaling(train_set_y_4)\n",
    "min_max_scaling(train_set_y_5)\n",
    "\n",
    "min_max_scaling(valid_set_y_1)\n",
    "min_max_scaling(valid_set_y_2)\n",
    "min_max_scaling(valid_set_y_3)\n",
    "min_max_scaling(valid_set_y_4)\n",
    "min_max_scaling(valid_set_y_5)\n",
    "\n",
    "min_max_scaling(train_set_z_1)\n",
    "min_max_scaling(train_set_z_2)\n",
    "min_max_scaling(train_set_z_3)\n",
    "min_max_scaling(train_set_z_4)\n",
    "min_max_scaling(train_set_z_5)\n",
    "\n",
    "min_max_scaling(valid_set_z_1)\n",
    "min_max_scaling(valid_set_z_2)\n",
    "min_max_scaling(valid_set_z_3)\n",
    "min_max_scaling(valid_set_z_4)\n",
    "min_max_scaling(valid_set_z_5)\n",
    "\n",
    "\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "x_mean, x_std = normalize_all(train_set_x_1, train_set_x_2, train_set_x_3, train_set_x_4, train_set_x_5, valid_set_x_1, valid_set_x_2, valid_set_x_3, valid_set_x_4, valid_set_x_5)\n",
    "y_mean, y_std = normalize_all(train_set_y_1, train_set_y_2, train_set_y_3, train_set_y_4, train_set_y_5, valid_set_y_1, valid_set_y_2, valid_set_y_3, valid_set_y_4, valid_set_y_5)\n",
    "z_mean, z_std = normalize_all(train_set_z_1, train_set_z_2, train_set_z_3, train_set_z_4, train_set_z_5, valid_set_z_1, valid_set_z_2, valid_set_z_3, valid_set_z_4, valid_set_z_5)\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "\n",
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)\n",
    "\n",
    "train_dataset_1, train_label_1, valid_dataset_1, valid_label_1 = construct_train_valid_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           valid_set_x_1,\n",
    "                                                                                           valid_set_y_1,\n",
    "                                                                                           valid_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, valid_dataset_2, valid_label_2 = construct_train_valid_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           valid_set_x_2,\n",
    "                                                                                           valid_set_y_2,\n",
    "                                                                                           valid_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, valid_dataset_3, valid_label_3 = construct_train_valid_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           valid_set_x_3,\n",
    "                                                                                           valid_set_y_3,\n",
    "                                                                                           valid_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, valid_dataset_4, valid_label_4 = construct_train_valid_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           valid_set_x_4,\n",
    "                                                                                           valid_set_y_4,\n",
    "                                                                                           valid_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, valid_dataset_5, valid_label_5 = construct_train_valid_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           valid_set_x_5,\n",
    "                                                                                           valid_set_y_5,\n",
    "                                                                                           valid_set_z_5,\n",
    "                                                                                           window_size)\n",
    "\n",
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "valid_dataset = np.concatenate((valid_dataset_1,\n",
    "                               valid_dataset_2,\n",
    "                               valid_dataset_3,\n",
    "                               valid_dataset_4,\n",
    "                               valid_dataset_5), axis=0)\n",
    "valid_label = np.concatenate((valid_label_1,\n",
    "                             valid_label_2,\n",
    "                             valid_label_3,\n",
    "                             valid_label_4,\n",
    "                             valid_label_5), axis=0)\n",
    "\n",
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(valid_dataset, valid_label)\n",
    "print(len(valid_set))\n",
    "\n",
    "# batch_size = 30\n",
    "# batch_size = 50\n",
    "batch_size = 650\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# keep the valid data trajectory order\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle valid data for using continous trajectory later on\n",
    "# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "min: -1.5190000000000001\n",
      "max: 1.41144\n",
      "mean: -0.058533702725000004\n",
      "std: 0.7861266362964188\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: -0.010518088406027203\n",
      "std: 1.1008349686408698\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "390\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: -0.010518088406027203\n",
      "std: 1.1008349686408698\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: -0.003910799069243863\n",
      "std: 1.3327171046033424\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: -0.02591810651939787\n",
      "std: 1.1851356406826778\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: 0.03763954942048355\n",
      "std: 1.0437368311448298\n",
      "-----------------------------\n",
      "min: -2.0556496185321183\n",
      "max: 2.0479269257023827\n",
      "mean: -0.0646097030966006\n",
      "std: 1.0845074561144088\n"
     ]
    }
   ],
   "source": [
    "test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "# do min-max-scaling for each test data set\n",
    "show_statistic(test_set_x_1)\n",
    "min_max_scaling(test_set_x_1)\n",
    "show_statistic(test_set_x_1)\n",
    "min_max_scaling(test_set_x_2)\n",
    "min_max_scaling(test_set_x_3)\n",
    "min_max_scaling(test_set_x_4)\n",
    "min_max_scaling(test_set_x_5)\n",
    "\n",
    "min_max_scaling(test_set_y_1)\n",
    "min_max_scaling(test_set_y_2)\n",
    "min_max_scaling(test_set_y_3)\n",
    "min_max_scaling(test_set_y_4)\n",
    "min_max_scaling(test_set_y_5)\n",
    "\n",
    "min_max_scaling(test_set_z_1)\n",
    "min_max_scaling(test_set_z_2)\n",
    "min_max_scaling(test_set_z_3)\n",
    "min_max_scaling(test_set_z_4)\n",
    "min_max_scaling(test_set_z_5)\n",
    "\n",
    "\n",
    "show_statistic(test_set_x_1)\n",
    "# do normalization on x of validation test set\n",
    "normalize_one(test_set_x_1, x_mean, x_std)\n",
    "show_statistic(test_set_x_1)\n",
    "normalize_one(test_set_x_2, x_mean, x_std)\n",
    "normalize_one(test_set_x_3, x_mean, x_std)\n",
    "normalize_one(test_set_x_4, x_mean, x_std)\n",
    "normalize_one(test_set_x_5, x_mean, x_std)\n",
    "# do normalization on y of validation test set\n",
    "normalize_one(test_set_y_1, y_mean, y_std)\n",
    "normalize_one(test_set_y_2, y_mean, y_std)\n",
    "normalize_one(test_set_y_3, y_mean, y_std)\n",
    "normalize_one(test_set_y_4, y_mean, y_std)\n",
    "normalize_one(test_set_y_5, y_mean, y_std)\n",
    "# do normalization on z of validation test set\n",
    "normalize_one(test_set_z_1, z_mean, z_std)\n",
    "normalize_one(test_set_z_2, z_mean, z_std)\n",
    "normalize_one(test_set_z_3, z_mean, z_std)\n",
    "normalize_one(test_set_z_4, z_mean, z_std)\n",
    "normalize_one(test_set_z_5, z_mean, z_std)\n",
    "# show_statistic(train_set_x_1)\n",
    "\n",
    "test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                     test_set_y_1,\n",
    "                                                     test_set_z_1,\n",
    "                                                     window_size)\n",
    "test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                     test_set_y_2,\n",
    "                                                     test_set_z_2,\n",
    "                                                     window_size)\n",
    "test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                     test_set_y_3,\n",
    "                                                     test_set_z_3,\n",
    "                                                     window_size)\n",
    "test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                     test_set_y_4,\n",
    "                                                     test_set_z_4,\n",
    "                                                     window_size)\n",
    "test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                     test_set_y_5,\n",
    "                                                     test_set_z_5,\n",
    "                                                     window_size)\n",
    "\n",
    "test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "print(len(test_set_1))\n",
    "print(len(test_set_2))\n",
    "print(len(test_set_3))\n",
    "print(len(test_set_4))\n",
    "print(len(test_set_5))\n",
    "show_statistic(test_set_x_1)\n",
    "show_statistic(test_set_x_2)\n",
    "show_statistic(test_set_x_3)\n",
    "show_statistic(test_set_x_4)\n",
    "show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "batch_size = 650\n",
    "test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_loss(loader):\n",
    "    total_loss = 0.0\n",
    "    batch = 0\n",
    "    # seq = []\n",
    "    pred = []\n",
    "    # gt = []\n",
    "    for i, (seqs, labels) in enumerate(loader):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        batch = i + 1\n",
    "        \n",
    "    return total_loss/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.2659205358637416, valid_loss: 0.05760712797443072, test_loss: 0.09981565922498703\n",
      "epoch: 1, train_loss: 0.022913324444190315, valid_loss: 0.052364734932780266, test_loss: 0.09079595655202866\n",
      "epoch: 2, train_loss: 0.019976485115678413, valid_loss: 0.048177761336167656, test_loss: 0.08490238338708878\n",
      "epoch: 3, train_loss: 0.017886595719534416, valid_loss: 0.044102937603990235, test_loss: 0.07996786385774612\n",
      "epoch: 4, train_loss: 0.016238203434192616, valid_loss: 0.04116597833732764, test_loss: 0.07573536783456802\n",
      "epoch: 5, train_loss: 0.014897912376276825, valid_loss: 0.038290844609340034, test_loss: 0.0723913162946701\n",
      "epoch: 6, train_loss: 0.013744789094704649, valid_loss: 0.03578435815870762, test_loss: 0.06931673735380173\n",
      "epoch: 7, train_loss: 0.012749876341094141, valid_loss: 0.03389297239482403, test_loss: 0.06620733439922333\n",
      "epoch: 8, train_loss: 0.011861817830282709, valid_loss: 0.03193422251691421, test_loss: 0.0639079213142395\n",
      "epoch: 9, train_loss: 0.0110504819566141, valid_loss: 0.030284656832615536, test_loss: 0.06147687882184982\n",
      "epoch: 10, train_loss: 0.010312922946784807, valid_loss: 0.028578571664790314, test_loss: 0.05931733548641205\n",
      "epoch: 11, train_loss: 0.009631308322043522, valid_loss: 0.02722853235900402, test_loss: 0.05731187388300896\n",
      "epoch: 12, train_loss: 0.008995740714928379, valid_loss: 0.0261145681142807, test_loss: 0.055139996111392975\n",
      "epoch: 13, train_loss: 0.008388596910821356, valid_loss: 0.024498324220379192, test_loss: 0.05350108817219734\n",
      "epoch: 14, train_loss: 0.007825343120519234, valid_loss: 0.023527744226157665, test_loss: 0.05162624269723892\n",
      "epoch: 15, train_loss: 0.007286212346314088, valid_loss: 0.022310190834105015, test_loss: 0.04998890683054924\n",
      "epoch: 16, train_loss: 0.006790074195874774, valid_loss: 0.021594787947833538, test_loss: 0.0485118068754673\n",
      "epoch: 17, train_loss: 0.0063172431824647865, valid_loss: 0.020656559616327286, test_loss: 0.04704606160521507\n",
      "epoch: 18, train_loss: 0.00588525898511643, valid_loss: 0.01973339853187402, test_loss: 0.0458073690533638\n",
      "epoch: 19, train_loss: 0.0054893008554759235, valid_loss: 0.01905911477903525, test_loss: 0.04443473741412163\n",
      "epoch: 20, train_loss: 0.005126959502535022, valid_loss: 0.018361616414040327, test_loss: 0.043243158608675\n",
      "epoch: 21, train_loss: 0.004798967694944661, valid_loss: 0.017511891045918066, test_loss: 0.04232527315616608\n",
      "epoch: 22, train_loss: 0.004516666178065149, valid_loss: 0.017130153564115364, test_loss: 0.04131975397467613\n",
      "epoch: 23, train_loss: 0.00425396454722985, valid_loss: 0.01647316354016463, test_loss: 0.04049995169043541\n",
      "epoch: 24, train_loss: 0.004035821144023667, valid_loss: 0.016067397470275562, test_loss: 0.03973852097988129\n",
      "epoch: 25, train_loss: 0.0038463332406852555, valid_loss: 0.015562550785640875, test_loss: 0.0390540175139904\n",
      "epoch: 26, train_loss: 0.00368475839090736, valid_loss: 0.015319174466033777, test_loss: 0.038338035345077515\n",
      "epoch: 27, train_loss: 0.003552362231699669, valid_loss: 0.014937174506485462, test_loss: 0.037635453045368195\n",
      "epoch: 28, train_loss: 0.0034314063377678394, valid_loss: 0.01471250814696153, test_loss: 0.037153489887714386\n",
      "epoch: 29, train_loss: 0.003332167772738182, valid_loss: 0.014231790012369553, test_loss: 0.0367271825671196\n",
      "epoch: 30, train_loss: 0.003246510198906712, valid_loss: 0.014052463695406914, test_loss: 0.0363047756254673\n",
      "epoch: 31, train_loss: 0.0031754690242688293, valid_loss: 0.013766382820904255, test_loss: 0.03583410754799843\n",
      "epoch: 32, train_loss: 0.0031145163204358973, valid_loss: 0.013580119547744593, test_loss: 0.03545970097184181\n",
      "epoch: 33, train_loss: 0.003054246448142373, valid_loss: 0.013434125576168299, test_loss: 0.03497832268476486\n",
      "epoch: 34, train_loss: 0.0030062044970691204, valid_loss: 0.013111574420084557, test_loss: 0.03468552231788635\n",
      "epoch: 35, train_loss: 0.0029618702626422696, valid_loss: 0.013022150844335556, test_loss: 0.03436662256717682\n",
      "epoch: 36, train_loss: 0.002916990971678625, valid_loss: 0.012869662139564753, test_loss: 0.03399324044585228\n",
      "epoch: 37, train_loss: 0.002880531423927649, valid_loss: 0.012623083622505268, test_loss: 0.03370288759469986\n",
      "epoch: 38, train_loss: 0.002845599133845257, valid_loss: 0.012430627985546986, test_loss: 0.033376362174749374\n",
      "epoch: 39, train_loss: 0.0028153957454892602, valid_loss: 0.012359383205572764, test_loss: 0.03299799561500549\n",
      "epoch: 40, train_loss: 0.0027839371772563977, valid_loss: 0.012168944968531529, test_loss: 0.03271667659282684\n",
      "epoch: 41, train_loss: 0.002756811040656074, valid_loss: 0.01197035750374198, test_loss: 0.03241269662976265\n",
      "epoch: 42, train_loss: 0.0027284681311120157, valid_loss: 0.01177514453108112, test_loss: 0.03224482387304306\n",
      "epoch: 43, train_loss: 0.002703812914779005, valid_loss: 0.011632751518239578, test_loss: 0.03198733180761337\n",
      "epoch: 44, train_loss: 0.0026775731340698576, valid_loss: 0.011697343240181604, test_loss: 0.031550902873277664\n",
      "epoch: 45, train_loss: 0.0026530794819573994, valid_loss: 0.011467091894398132, test_loss: 0.0314084030687809\n",
      "epoch: 46, train_loss: 0.0026310897412021523, valid_loss: 0.011325606377795339, test_loss: 0.031060295179486275\n",
      "epoch: 47, train_loss: 0.0026043885205264974, valid_loss: 0.011254433542490005, test_loss: 0.030933232977986336\n",
      "epoch: 48, train_loss: 0.002578745535372392, valid_loss: 0.011214259934301177, test_loss: 0.030594205483794212\n",
      "epoch: 49, train_loss: 0.002559239712908216, valid_loss: 0.01095445353227357, test_loss: 0.030420981347560883\n",
      "epoch: 50, train_loss: 0.002537663266791598, valid_loss: 0.010877856131022176, test_loss: 0.030141422525048256\n",
      "epoch: 51, train_loss: 0.002519299007911721, valid_loss: 0.010789859108626842, test_loss: 0.029947154223918915\n",
      "epoch: 52, train_loss: 0.00249608942186055, valid_loss: 0.010584265614549318, test_loss: 0.029804611578583717\n",
      "epoch: 53, train_loss: 0.0024783273642558766, valid_loss: 0.010493454678605, test_loss: 0.029567783698439598\n",
      "epoch: 54, train_loss: 0.002456633809387036, valid_loss: 0.010500873361403743, test_loss: 0.0292772576212883\n",
      "epoch: 55, train_loss: 0.002442222523867436, valid_loss: 0.01036350685171783, test_loss: 0.029109137132763863\n",
      "epoch: 56, train_loss: 0.0024220484774559736, valid_loss: 0.010236908759300908, test_loss: 0.02891542762517929\n",
      "epoch: 57, train_loss: 0.0024038272120220504, valid_loss: 0.010204757641380032, test_loss: 0.02869280055165291\n",
      "epoch: 58, train_loss: 0.0023874414720289087, valid_loss: 0.01012947914811472, test_loss: 0.028518473729491234\n",
      "epoch: 59, train_loss: 0.0023713868273341136, valid_loss: 0.009974711497003833, test_loss: 0.0283057801425457\n",
      "epoch: 60, train_loss: 0.002355731304977899, valid_loss: 0.009956738445907831, test_loss: 0.028158191591501236\n",
      "epoch: 61, train_loss: 0.0023372320195093103, valid_loss: 0.009806628882264098, test_loss: 0.02806246466934681\n",
      "epoch: 62, train_loss: 0.0023200615339548044, valid_loss: 0.009726108129446706, test_loss: 0.027723655104637146\n",
      "epoch: 63, train_loss: 0.0023066035481979666, valid_loss: 0.009689720502744118, test_loss: 0.027552932500839233\n",
      "epoch: 64, train_loss: 0.0022905491312722797, valid_loss: 0.009724381923054656, test_loss: 0.0273768100887537\n",
      "epoch: 65, train_loss: 0.0022766633293307996, valid_loss: 0.009545652040590843, test_loss: 0.02719731815159321\n",
      "epoch: 66, train_loss: 0.002262227009455471, valid_loss: 0.009421067389970025, test_loss: 0.027100590988993645\n",
      "epoch: 67, train_loss: 0.0022473177295583096, valid_loss: 0.009314969182014465, test_loss: 0.026903316378593445\n",
      "epoch: 68, train_loss: 0.0022345120390958114, valid_loss: 0.00924457237124443, test_loss: 0.026839030906558037\n",
      "epoch: 69, train_loss: 0.0022221752543650245, valid_loss: 0.00924973445944488, test_loss: 0.0266552921384573\n",
      "epoch: 70, train_loss: 0.0022124931945101075, valid_loss: 0.009205231287827095, test_loss: 0.02654360607266426\n",
      "epoch: 71, train_loss: 0.002195353756415779, valid_loss: 0.009118574516226849, test_loss: 0.02629023976624012\n",
      "epoch: 72, train_loss: 0.0021837237088576608, valid_loss: 0.008959579824780425, test_loss: 0.026270179077982903\n",
      "epoch: 73, train_loss: 0.0021696589057050323, valid_loss: 0.0089350250394394, test_loss: 0.026129908859729767\n",
      "epoch: 74, train_loss: 0.0021563840210032854, valid_loss: 0.008819022914394736, test_loss: 0.026059463620185852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 75, train_loss: 0.002147041210580779, valid_loss: 0.00889305374585092, test_loss: 0.025736697018146515\n",
      "epoch: 76, train_loss: 0.002136226115829271, valid_loss: 0.00870035410237809, test_loss: 0.025655608624219894\n",
      "epoch: 77, train_loss: 0.0021236378942494807, valid_loss: 0.008615731649721662, test_loss: 0.025577809661626816\n",
      "epoch: 78, train_loss: 0.0021118300450641823, valid_loss: 0.008682424435392022, test_loss: 0.025455111637711525\n",
      "epoch: 79, train_loss: 0.0021017736593342347, valid_loss: 0.008590083879729113, test_loss: 0.02529505454003811\n",
      "epoch: 80, train_loss: 0.0020911828867848153, valid_loss: 0.008576726773753762, test_loss: 0.025127844884991646\n",
      "epoch: 81, train_loss: 0.0020800922521511498, valid_loss: 0.008464441634714603, test_loss: 0.025073794648051262\n",
      "epoch: 82, train_loss: 0.0020702297080551152, valid_loss: 0.008372440817765892, test_loss: 0.02498823031783104\n",
      "epoch: 83, train_loss: 0.0020597617536702232, valid_loss: 0.008392587536945939, test_loss: 0.02485591173171997\n",
      "epoch: 84, train_loss: 0.0020511028275865574, valid_loss: 0.008408998682474097, test_loss: 0.02468985877931118\n",
      "epoch: 85, train_loss: 0.002041878745607708, valid_loss: 0.008273914766808351, test_loss: 0.024622533470392227\n",
      "epoch: 86, train_loss: 0.0020321428461972137, valid_loss: 0.008256721853589019, test_loss: 0.02450656145811081\n",
      "epoch: 87, train_loss: 0.002020991063150375, valid_loss: 0.00816581790180256, test_loss: 0.02446720376610756\n",
      "epoch: 88, train_loss: 0.0020130895655197296, valid_loss: 0.008157407554487387, test_loss: 0.024351680651307106\n",
      "epoch: 89, train_loss: 0.0020065366482848058, valid_loss: 0.00809785300710549, test_loss: 0.024247096851468086\n",
      "epoch: 90, train_loss: 0.0019945507140263267, valid_loss: 0.008052812423557043, test_loss: 0.024185772985219955\n",
      "epoch: 91, train_loss: 0.001985631427606163, valid_loss: 0.00807321824443837, test_loss: 0.024084974080324173\n",
      "epoch: 92, train_loss: 0.0019785743712893, valid_loss: 0.007961809867992997, test_loss: 0.024054894223809242\n",
      "epoch: 93, train_loss: 0.001969635678941141, valid_loss: 0.007870928617194295, test_loss: 0.023983975872397423\n",
      "epoch: 94, train_loss: 0.0019616498915559573, valid_loss: 0.00788766413461417, test_loss: 0.023793945088982582\n",
      "epoch: 95, train_loss: 0.0019518170839823458, valid_loss: 0.007902054154934982, test_loss: 0.023708272725343704\n",
      "epoch: 96, train_loss: 0.001946441722912309, valid_loss: 0.00777914149997135, test_loss: 0.023724284023046494\n",
      "epoch: 97, train_loss: 0.0019380516048682773, valid_loss: 0.0077931681492676335, test_loss: 0.023575719445943832\n",
      "epoch: 98, train_loss: 0.001932788984445126, valid_loss: 0.0077232288507123785, test_loss: 0.023567557334899902\n",
      "epoch: 99, train_loss: 0.001923827575924604, valid_loss: 0.007755214503655831, test_loss: 0.02340882644057274\n",
      "epoch: 100, train_loss: 0.001916599414392334, valid_loss: 0.007697612978518009, test_loss: 0.023354429751634598\n",
      "epoch: 101, train_loss: 0.0019090285514602842, valid_loss: 0.007711015370053549, test_loss: 0.023313777521252632\n",
      "epoch: 102, train_loss: 0.0019018348235555966, valid_loss: 0.007651585037820041, test_loss: 0.023191018030047417\n",
      "epoch: 103, train_loss: 0.0018939168354415376, valid_loss: 0.007591054813625912, test_loss: 0.023143494501709938\n",
      "epoch: 104, train_loss: 0.001888625425003145, valid_loss: 0.00759100242673109, test_loss: 0.02308887615799904\n",
      "epoch: 105, train_loss: 0.0018825970195314806, valid_loss: 0.007543748865524928, test_loss: 0.02305494248867035\n",
      "epoch: 106, train_loss: 0.0018757928780320547, valid_loss: 0.007474281204243501, test_loss: 0.023040851578116417\n",
      "epoch: 107, train_loss: 0.0018684602666484273, valid_loss: 0.007443562770883243, test_loss: 0.02294205315411091\n",
      "epoch: 108, train_loss: 0.0018639175282062395, valid_loss: 0.007415677226769428, test_loss: 0.022829905152320862\n",
      "epoch: 109, train_loss: 0.001857311277569312, valid_loss: 0.007386166330737372, test_loss: 0.022763239219784737\n",
      "epoch: 110, train_loss: 0.0018493723549434671, valid_loss: 0.00735510482142369, test_loss: 0.022831017151474953\n",
      "epoch: 111, train_loss: 0.0018447365652283897, valid_loss: 0.007343571012218793, test_loss: 0.0227400790899992\n",
      "epoch: 112, train_loss: 0.0018406272055986135, valid_loss: 0.007293653014736871, test_loss: 0.02271169424057007\n",
      "epoch: 113, train_loss: 0.0018302731509522898, valid_loss: 0.0073410722737511, test_loss: 0.022535469383001328\n",
      "epoch: 114, train_loss: 0.0018286754771986086, valid_loss: 0.007283142108159761, test_loss: 0.02250857837498188\n",
      "epoch: 115, train_loss: 0.0018198247189107149, valid_loss: 0.007195705353903274, test_loss: 0.022494947537779808\n",
      "epoch: 116, train_loss: 0.0018159718779118164, valid_loss: 0.00718848609055082, test_loss: 0.022510677576065063\n",
      "epoch: 117, train_loss: 0.0018133939213483877, valid_loss: 0.007152230440018077, test_loss: 0.02243231050670147\n",
      "epoch: 118, train_loss: 0.0018060169759732873, valid_loss: 0.007127880739669005, test_loss: 0.02235974743962288\n",
      "epoch: 119, train_loss: 0.0017998161695330687, valid_loss: 0.00713430077303201, test_loss: 0.02238493040204048\n",
      "epoch: 120, train_loss: 0.0017954956476941056, valid_loss: 0.007092263316735625, test_loss: 0.02233930677175522\n",
      "epoch: 121, train_loss: 0.0017880078901172333, valid_loss: 0.007084069773554802, test_loss: 0.022249730303883553\n",
      "epoch: 122, train_loss: 0.0017855862894540896, valid_loss: 0.007042414237124224, test_loss: 0.02218571864068508\n",
      "epoch: 123, train_loss: 0.0017781845729231186, valid_loss: 0.007050706539303064, test_loss: 0.022169655188918114\n",
      "epoch: 124, train_loss: 0.0017748384625124543, valid_loss: 0.0070079526243110495, test_loss: 0.022112322971224785\n",
      "epoch: 125, train_loss: 0.0017677734292152784, valid_loss: 0.007027583895251155, test_loss: 0.022037459537386894\n",
      "epoch: 126, train_loss: 0.00176366388230868, valid_loss: 0.006964650548373659, test_loss: 0.022028526291251183\n",
      "epoch: 127, train_loss: 0.001758518375699287, valid_loss: 0.006981890299357474, test_loss: 0.021987957879900932\n",
      "epoch: 128, train_loss: 0.0017541856758053536, valid_loss: 0.0069464102465038495, test_loss: 0.021937677636742592\n",
      "epoch: 129, train_loss: 0.001748960875177189, valid_loss: 0.006925382263337572, test_loss: 0.02190547063946724\n",
      "epoch: 130, train_loss: 0.001745336122162964, valid_loss: 0.006908972514793277, test_loss: 0.021894996985793114\n",
      "epoch: 131, train_loss: 0.001738721530115151, valid_loss: 0.006931957051468392, test_loss: 0.02188051864504814\n",
      "epoch: 132, train_loss: 0.0017355996254912536, valid_loss: 0.006876736336077253, test_loss: 0.02176716737449169\n",
      "epoch: 133, train_loss: 0.0017298182229632916, valid_loss: 0.006818494061008096, test_loss: 0.021736733615398407\n",
      "epoch: 134, train_loss: 0.0017248939510191913, valid_loss: 0.006847940928613146, test_loss: 0.02173183299601078\n",
      "epoch: 135, train_loss: 0.0017218280775958428, valid_loss: 0.006809433301289876, test_loss: 0.021839356049895287\n",
      "epoch: 136, train_loss: 0.0017149730980315287, valid_loss: 0.006782952424449225, test_loss: 0.021646620705723763\n",
      "epoch: 137, train_loss: 0.001712614143996135, valid_loss: 0.00677878448429207, test_loss: 0.021581145003437996\n",
      "epoch: 138, train_loss: 0.0017089568301225486, valid_loss: 0.006719807667347292, test_loss: 0.02158946357667446\n",
      "epoch: 139, train_loss: 0.0017056127291415696, valid_loss: 0.006740965414792299, test_loss: 0.02160491980612278\n",
      "epoch: 140, train_loss: 0.0016992745273139167, valid_loss: 0.006690699762354295, test_loss: 0.021528145298361778\n",
      "epoch: 141, train_loss: 0.0016973041220689597, valid_loss: 0.006678946549072862, test_loss: 0.021499689668416977\n",
      "epoch: 142, train_loss: 0.0016921941423788667, valid_loss: 0.00666048606702437, test_loss: 0.021541861817240715\n",
      "epoch: 143, train_loss: 0.001687690646023206, valid_loss: 0.006655200268141925, test_loss: 0.02149485982954502\n",
      "epoch: 144, train_loss: 0.0016827835023160214, valid_loss: 0.006653695250861347, test_loss: 0.0213499516248703\n",
      "epoch: 145, train_loss: 0.0016817534241177466, valid_loss: 0.00661872160465767, test_loss: 0.021365884691476822\n",
      "epoch: 146, train_loss: 0.001676036905416328, valid_loss: 0.006606118753552437, test_loss: 0.021266844123601913\n",
      "epoch: 147, train_loss: 0.0016738752702898953, valid_loss: 0.006569461159718533, test_loss: 0.0213996060192585\n",
      "epoch: 148, train_loss: 0.0016688014350264616, valid_loss: 0.006551900606912871, test_loss: 0.02129230834543705\n",
      "epoch: 149, train_loss: 0.0016661163105669877, valid_loss: 0.006556119381760557, test_loss: 0.021287059411406517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150, train_loss: 0.0016615690724195345, valid_loss: 0.006559996438833575, test_loss: 0.021252024918794632\n",
      "epoch: 151, train_loss: 0.0016555932360579786, valid_loss: 0.006532552574450771, test_loss: 0.021264435723423958\n",
      "epoch: 152, train_loss: 0.001654020292967882, valid_loss: 0.006514209439046681, test_loss: 0.021202582865953445\n",
      "epoch: 153, train_loss: 0.001649399313306355, valid_loss: 0.006498067329327266, test_loss: 0.02125309593975544\n",
      "epoch: 154, train_loss: 0.0016471250640714297, valid_loss: 0.006461529526859522, test_loss: 0.02126031368970871\n",
      "epoch: 155, train_loss: 0.001643013907596469, valid_loss: 0.006498507728489737, test_loss: 0.021179920062422752\n",
      "epoch: 156, train_loss: 0.0016383749847907734, valid_loss: 0.006447009004962941, test_loss: 0.021169351413846016\n",
      "epoch: 157, train_loss: 0.0016345111400131946, valid_loss: 0.006426478270441294, test_loss: 0.021147748455405235\n",
      "epoch: 158, train_loss: 0.0016303573147920163, valid_loss: 0.006415533949621022, test_loss: 0.02108146995306015\n",
      "epoch: 159, train_loss: 0.0016288007052777255, valid_loss: 0.006411694339476526, test_loss: 0.02105957828462124\n",
      "epoch: 160, train_loss: 0.0016236949772776468, valid_loss: 0.006408546934835613, test_loss: 0.020960059016942978\n",
      "epoch: 161, train_loss: 0.0016206027838684943, valid_loss: 0.006375768105499446, test_loss: 0.02100015990436077\n",
      "epoch: 162, train_loss: 0.0016168368747457862, valid_loss: 0.006384659519729515, test_loss: 0.021016616374254227\n",
      "epoch: 163, train_loss: 0.001616275398587079, valid_loss: 0.006418596991958718, test_loss: 0.021004116162657738\n",
      "epoch: 164, train_loss: 0.0016101144061869254, valid_loss: 0.006315014014641444, test_loss: 0.021013017743825912\n",
      "epoch: 165, train_loss: 0.0016077684906675763, valid_loss: 0.006335372920148075, test_loss: 0.020954808220267296\n",
      "epoch: 166, train_loss: 0.0016035335138440132, valid_loss: 0.006301991137055059, test_loss: 0.020970743149518967\n",
      "epoch: 167, train_loss: 0.001600408703898606, valid_loss: 0.006312073363612096, test_loss: 0.020870842039585114\n",
      "epoch: 168, train_loss: 0.001595940273862494, valid_loss: 0.0062880174567302065, test_loss: 0.02093370445072651\n",
      "epoch: 169, train_loss: 0.0015934046671685317, valid_loss: 0.006292868289165199, test_loss: 0.02090592123568058\n",
      "epoch: 170, train_loss: 0.001590556621996929, valid_loss: 0.00624266864421467, test_loss: 0.020804526284337044\n",
      "epoch: 171, train_loss: 0.0015896579766969967, valid_loss: 0.006260796450078487, test_loss: 0.020826544612646103\n",
      "epoch: 172, train_loss: 0.0015854431346625738, valid_loss: 0.006196435967770715, test_loss: 0.020880067721009254\n",
      "epoch: 173, train_loss: 0.0015820934457461472, valid_loss: 0.006220081627058486, test_loss: 0.020725324749946594\n",
      "epoch: 174, train_loss: 0.001577398749878225, valid_loss: 0.006205488656026621, test_loss: 0.02074495330452919\n",
      "epoch: 175, train_loss: 0.0015751131273725111, valid_loss: 0.0061907521449029446, test_loss: 0.020824668928980827\n",
      "epoch: 176, train_loss: 0.0015718191305337393, valid_loss: 0.006167200510390103, test_loss: 0.020710285753011703\n",
      "epoch: 177, train_loss: 0.0015692138948234851, valid_loss: 0.006149371775488059, test_loss: 0.020738612860441208\n",
      "epoch: 178, train_loss: 0.001566842770325425, valid_loss: 0.006132538081146777, test_loss: 0.02065056934952736\n",
      "epoch: 179, train_loss: 0.0015645386732142904, valid_loss: 0.006172166399968167, test_loss: 0.020644433796405792\n",
      "epoch: 180, train_loss: 0.0015605337229435859, valid_loss: 0.006126625696197152, test_loss: 0.020629556849598885\n",
      "epoch: 181, train_loss: 0.0015569941656987953, valid_loss: 0.006130577336686353, test_loss: 0.020631534978747368\n",
      "epoch: 182, train_loss: 0.0015539409714224546, valid_loss: 0.006126840637686352, test_loss: 0.020639777183532715\n",
      "epoch: 183, train_loss: 0.001549865731605045, valid_loss: 0.006091284682042897, test_loss: 0.02061440795660019\n",
      "epoch: 184, train_loss: 0.0015468742070800583, valid_loss: 0.0061443116671095295, test_loss: 0.02048870362341404\n",
      "epoch: 185, train_loss: 0.0015454082648553278, valid_loss: 0.006081201912214358, test_loss: 0.020609760656952858\n",
      "epoch: 186, train_loss: 0.0015415224935049596, valid_loss: 0.00607478335344543, test_loss: 0.020450813695788383\n",
      "epoch: 187, train_loss: 0.0015398313331863155, valid_loss: 0.0060682146189113455, test_loss: 0.02047015354037285\n",
      "epoch: 188, train_loss: 0.0015368694338299658, valid_loss: 0.006060914252884686, test_loss: 0.020520377904176712\n",
      "epoch: 189, train_loss: 0.0015343626880127451, valid_loss: 0.00602060843569537, test_loss: 0.020460963249206543\n",
      "epoch: 190, train_loss: 0.0015317480302537265, valid_loss: 0.006018783741941054, test_loss: 0.020497968420386314\n",
      "epoch: 191, train_loss: 0.0015284560124754257, valid_loss: 0.006019427751501401, test_loss: 0.020381538197398186\n",
      "epoch: 192, train_loss: 0.0015271155862137675, valid_loss: 0.0060018802372117834, test_loss: 0.020409155637025833\n",
      "epoch: 193, train_loss: 0.0015214333644784663, valid_loss: 0.005989211533839504, test_loss: 0.02044513262808323\n",
      "epoch: 194, train_loss: 0.0015223814761671035, valid_loss: 0.005986943258903921, test_loss: 0.020383749157190323\n",
      "epoch: 195, train_loss: 0.001515487598700692, valid_loss: 0.005993711412884295, test_loss: 0.020417144522070885\n",
      "epoch: 196, train_loss: 0.0015157802718813005, valid_loss: 0.005954544331567983, test_loss: 0.020380577072501183\n",
      "epoch: 197, train_loss: 0.0015110953599619477, valid_loss: 0.00595773426660647, test_loss: 0.02044871263206005\n",
      "epoch: 198, train_loss: 0.0015081228769343832, valid_loss: 0.005903355796666195, test_loss: 0.02040279284119606\n",
      "epoch: 199, train_loss: 0.0015069127700332067, valid_loss: 0.005897289821101974, test_loss: 0.020415643230080605\n",
      "epoch: 200, train_loss: 0.001503305132602058, valid_loss: 0.005885824522313972, test_loss: 0.02034974843263626\n",
      "epoch: 201, train_loss: 0.0015009916969574988, valid_loss: 0.005902776727452874, test_loss: 0.020311512053012848\n",
      "epoch: 202, train_loss: 0.0014983452401002464, valid_loss: 0.005864970657664041, test_loss: 0.020310478284955025\n",
      "epoch: 203, train_loss: 0.0014954606607637327, valid_loss: 0.005908920352036755, test_loss: 0.020336151123046875\n",
      "epoch: 204, train_loss: 0.0014929745963815114, valid_loss: 0.005876849909933905, test_loss: 0.02024770714342594\n",
      "epoch: 205, train_loss: 0.001491538633632919, valid_loss: 0.005854658006380002, test_loss: 0.020222371444106102\n",
      "epoch: 206, train_loss: 0.0014881571835797765, valid_loss: 0.0058476184494793415, test_loss: 0.02020125836133957\n",
      "epoch: 207, train_loss: 0.0014858835872829609, valid_loss: 0.005835595346676807, test_loss: 0.02023760788142681\n",
      "epoch: 208, train_loss: 0.0014841009548905752, valid_loss: 0.005828489122601847, test_loss: 0.02014670893549919\n",
      "epoch: 209, train_loss: 0.0014813752065453193, valid_loss: 0.005838116281665862, test_loss: 0.020165154710412025\n",
      "epoch: 210, train_loss: 0.0014786927950689974, valid_loss: 0.005807803788532813, test_loss: 0.020217785611748695\n",
      "epoch: 211, train_loss: 0.0014751768245807161, valid_loss: 0.005781551628994445, test_loss: 0.020147103816270828\n",
      "epoch: 212, train_loss: 0.0014727027206074285, valid_loss: 0.00578390834076951, test_loss: 0.020083798095583916\n",
      "epoch: 213, train_loss: 0.0014731442813923502, valid_loss: 0.005743121573080619, test_loss: 0.020106187090277672\n",
      "epoch: 214, train_loss: 0.001467713423113784, valid_loss: 0.0057509853116547065, test_loss: 0.020119722932577133\n",
      "epoch: 215, train_loss: 0.0014656509687800121, valid_loss: 0.005786937351028125, test_loss: 0.020107850432395935\n",
      "epoch: 216, train_loss: 0.0014652851665311534, valid_loss: 0.005785917164757848, test_loss: 0.020113667473196983\n",
      "epoch: 217, train_loss: 0.001462262802068954, valid_loss: 0.005725999440376957, test_loss: 0.020061424002051353\n",
      "epoch: 218, train_loss: 0.0014594175002497175, valid_loss: 0.0057386714809884625, test_loss: 0.01997232809662819\n",
      "epoch: 219, train_loss: 0.0014563020136531281, valid_loss: 0.005737599509302527, test_loss: 0.020162073895335197\n",
      "epoch: 220, train_loss: 0.0014546020867545967, valid_loss: 0.005696718328787635, test_loss: 0.02009148895740509\n",
      "epoch: 221, train_loss: 0.0014506367683086705, valid_loss: 0.005697153857909143, test_loss: 0.01998165249824524\n",
      "epoch: 222, train_loss: 0.0014491244168628168, valid_loss: 0.0057101335454111295, test_loss: 0.020039411261677742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 223, train_loss: 0.0014469197409672906, valid_loss: 0.005711214888530473, test_loss: 0.019912628456950188\n",
      "epoch: 224, train_loss: 0.0014436973015899243, valid_loss: 0.005677315212475757, test_loss: 0.019999919459223747\n",
      "epoch: 225, train_loss: 0.0014426299704886649, valid_loss: 0.005665457555248092, test_loss: 0.020035238936543465\n",
      "epoch: 226, train_loss: 0.0014373642777133248, valid_loss: 0.005657260849451025, test_loss: 0.019938301295042038\n",
      "epoch: 227, train_loss: 0.001437216192362425, valid_loss: 0.0056592473411001265, test_loss: 0.019967390224337578\n",
      "epoch: 228, train_loss: 0.0014349834426589634, valid_loss: 0.005637734119469921, test_loss: 0.019879164174199104\n",
      "epoch: 229, train_loss: 0.0014325372302783248, valid_loss: 0.005615666217636317, test_loss: 0.01995096541941166\n",
      "epoch: 230, train_loss: 0.0014293002896010876, valid_loss: 0.005622889361499499, test_loss: 0.019913744181394577\n",
      "epoch: 231, train_loss: 0.0014292939221891372, valid_loss: 0.005618480364015947, test_loss: 0.01991930976510048\n",
      "epoch: 232, train_loss: 0.0014256627865783546, valid_loss: 0.005609893046009044, test_loss: 0.019994743168354034\n",
      "epoch: 233, train_loss: 0.0014232781199414444, valid_loss: 0.005620641362232466, test_loss: 0.019869575276970863\n",
      "epoch: 234, train_loss: 0.001425240932882804, valid_loss: 0.0055795190467809634, test_loss: 0.019869236275553703\n",
      "epoch: 235, train_loss: 0.0014179962403748345, valid_loss: 0.00560450895378987, test_loss: 0.019818037748336792\n",
      "epoch: 236, train_loss: 0.0014188209416992638, valid_loss: 0.0055551255742708845, test_loss: 0.01982155814766884\n",
      "epoch: 237, train_loss: 0.0014144247701472561, valid_loss: 0.00553446845151484, test_loss: 0.019797785207629204\n",
      "epoch: 238, train_loss: 0.0014110751976461513, valid_loss: 0.005519407781927536, test_loss: 0.019826391711831093\n",
      "epoch: 239, train_loss: 0.0014114990101560302, valid_loss: 0.005524628892696152, test_loss: 0.019813602790236473\n",
      "epoch: 240, train_loss: 0.0014087395190828197, valid_loss: 0.005516200365188221, test_loss: 0.019734539091587067\n",
      "epoch: 241, train_loss: 0.0014070775663561146, valid_loss: 0.005535087528793762, test_loss: 0.019796717911958694\n",
      "epoch: 242, train_loss: 0.0014051937538644543, valid_loss: 0.005542869873655339, test_loss: 0.019770614802837372\n",
      "epoch: 243, train_loss: 0.001403408699527221, valid_loss: 0.005503006143650661, test_loss: 0.019728709012269974\n",
      "epoch: 244, train_loss: 0.0013998427742120364, valid_loss: 0.005486886463283251, test_loss: 0.019762607291340828\n",
      "epoch: 245, train_loss: 0.0013994344803707106, valid_loss: 0.005465058396415164, test_loss: 0.019772954285144806\n",
      "epoch: 246, train_loss: 0.0013955023909068625, valid_loss: 0.005481002464269598, test_loss: 0.01975092850625515\n",
      "epoch: 247, train_loss: 0.0013939547099416022, valid_loss: 0.005465139188648512, test_loss: 0.019700318574905396\n",
      "epoch: 248, train_loss: 0.0013924274117807331, valid_loss: 0.005454024377589424, test_loss: 0.019706089049577713\n",
      "epoch: 249, train_loss: 0.0013894171975350575, valid_loss: 0.005434069549664855, test_loss: 0.01976446434855461\n",
      "epoch: 250, train_loss: 0.0013881052398811216, valid_loss: 0.005431068692511569, test_loss: 0.019673671573400497\n",
      "epoch: 251, train_loss: 0.0013882376167558782, valid_loss: 0.005429481447208673, test_loss: 0.019664159044623375\n",
      "epoch: 252, train_loss: 0.0013851806389815781, valid_loss: 0.005435865217198928, test_loss: 0.01969355158507824\n",
      "epoch: 253, train_loss: 0.0013809046105724638, valid_loss: 0.005421049951110035, test_loss: 0.019629253074526787\n",
      "epoch: 254, train_loss: 0.0013797998215760226, valid_loss: 0.00543422744764636, test_loss: 0.01968110352754593\n",
      "epoch: 255, train_loss: 0.0013779096517954831, valid_loss: 0.0054144488531164825, test_loss: 0.019649513065814972\n",
      "epoch: 256, train_loss: 0.0013767004104169166, valid_loss: 0.005420233937911689, test_loss: 0.019610822200775146\n",
      "epoch: 257, train_loss: 0.0013740267402128034, valid_loss: 0.005399126384872943, test_loss: 0.019641555845737457\n",
      "epoch: 258, train_loss: 0.0013720201449873655, valid_loss: 0.005381616181693971, test_loss: 0.01966169849038124\n",
      "epoch: 259, train_loss: 0.0013709568529916198, valid_loss: 0.005374337779358029, test_loss: 0.019569315016269684\n",
      "epoch: 260, train_loss: 0.0013688875428076995, valid_loss: 0.0053592363256029785, test_loss: 0.019647466018795967\n",
      "epoch: 261, train_loss: 0.0013673802149360595, valid_loss: 0.005357763768794636, test_loss: 0.01952756941318512\n",
      "epoch: 262, train_loss: 0.0013650264540363264, valid_loss: 0.005362732995611926, test_loss: 0.019522419199347496\n",
      "epoch: 263, train_loss: 0.0013651081877153206, valid_loss: 0.005329810160522659, test_loss: 0.019642526283860207\n",
      "epoch: 264, train_loss: 0.001361865481948885, valid_loss: 0.005384632172839095, test_loss: 0.019577378407120705\n",
      "epoch: 265, train_loss: 0.0013586005003636947, valid_loss: 0.005313957808539271, test_loss: 0.019560078158974648\n",
      "epoch: 266, train_loss: 0.0013585285177332876, valid_loss: 0.005330521128295611, test_loss: 0.019512038677930832\n",
      "epoch: 267, train_loss: 0.001356133449640449, valid_loss: 0.005294083074356119, test_loss: 0.019543129950761795\n",
      "epoch: 268, train_loss: 0.0013533386973785641, valid_loss: 0.005301310137535135, test_loss: 0.019571205601096153\n",
      "epoch: 269, train_loss: 0.0013515585071771689, valid_loss: 0.0052898477103250725, test_loss: 0.019560137763619423\n",
      "epoch: 270, train_loss: 0.0013502437884554915, valid_loss: 0.005292676990696539, test_loss: 0.019490189850330353\n",
      "epoch: 271, train_loss: 0.0013476667429684944, valid_loss: 0.0052985122310929, test_loss: 0.01948438212275505\n",
      "epoch: 272, train_loss: 0.0013467929649936116, valid_loss: 0.005273727603101482, test_loss: 0.019457200542092323\n",
      "epoch: 273, train_loss: 0.0013451353196338143, valid_loss: 0.005257862736470997, test_loss: 0.019432831555604935\n",
      "epoch: 274, train_loss: 0.0013412325744233701, valid_loss: 0.005262661414841811, test_loss: 0.019445544108748436\n",
      "epoch: 275, train_loss: 0.0013394136837197711, valid_loss: 0.005231522780377418, test_loss: 0.019459474831819534\n",
      "epoch: 276, train_loss: 0.0013399197993314137, valid_loss: 0.005290143075399101, test_loss: 0.01940273307263851\n",
      "epoch: 277, train_loss: 0.0013379185895799942, valid_loss: 0.005223667249083519, test_loss: 0.019475877285003662\n",
      "epoch: 278, train_loss: 0.0013339676408340101, valid_loss: 0.005219066185721506, test_loss: 0.019444191828370094\n",
      "epoch: 279, train_loss: 0.0013344474921367414, valid_loss: 0.005219621991273016, test_loss: 0.019479136914014816\n",
      "epoch: 280, train_loss: 0.0013313199919850929, valid_loss: 0.005210659854734938, test_loss: 0.019392745569348335\n",
      "epoch: 281, train_loss: 0.001330703674086734, valid_loss: 0.005217367162307103, test_loss: 0.019371556118130684\n",
      "epoch: 282, train_loss: 0.0013291480439796071, valid_loss: 0.005209768307395279, test_loss: 0.019387630745768547\n",
      "epoch: 283, train_loss: 0.0013278589037287495, valid_loss: 0.005172383273020387, test_loss: 0.019461633637547493\n",
      "epoch: 284, train_loss: 0.0013250960846957953, valid_loss: 0.0051688193731630845, test_loss: 0.019446581602096558\n",
      "epoch: 285, train_loss: 0.0013244327653766327, valid_loss: 0.005213132457962881, test_loss: 0.01937968283891678\n",
      "epoch: 286, train_loss: 0.0013219170682334705, valid_loss: 0.005145297851413488, test_loss: 0.019395485520362854\n",
      "epoch: 287, train_loss: 0.001320614213751548, valid_loss: 0.00514930880550916, test_loss: 0.0194254033267498\n",
      "epoch: 288, train_loss: 0.001317237602526565, valid_loss: 0.005160148798798521, test_loss: 0.019317660480737686\n",
      "epoch: 289, train_loss: 0.0013172927125276108, valid_loss: 0.0051752698685353, test_loss: 0.019336432218551636\n",
      "epoch: 290, train_loss: 0.0013165070457667437, valid_loss: 0.005119951325468719, test_loss: 0.019362587481737137\n",
      "epoch: 291, train_loss: 0.0013130478328093886, valid_loss: 0.005152964266017079, test_loss: 0.01927993632853031\n",
      "epoch: 292, train_loss: 0.0013111264611918318, valid_loss: 0.005111273688574632, test_loss: 0.01934690959751606\n",
      "epoch: 293, train_loss: 0.0013116875374892159, valid_loss: 0.005111476134819289, test_loss: 0.019362209364771843\n",
      "epoch: 294, train_loss: 0.0013102045100506234, valid_loss: 0.005099225731100887, test_loss: 0.019321583211421967\n",
      "epoch: 295, train_loss: 0.0013067487313452623, valid_loss: 0.005139891155219327, test_loss: 0.019274644553661346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 296, train_loss: 0.0013059724081793556, valid_loss: 0.00509687695497026, test_loss: 0.019262274727225304\n",
      "epoch: 297, train_loss: 0.0013041953890300963, valid_loss: 0.005083816863285999, test_loss: 0.019272221252322197\n",
      "epoch: 298, train_loss: 0.001301833656211586, valid_loss: 0.00509626689987878, test_loss: 0.01923360675573349\n",
      "epoch: 299, train_loss: 0.0013001621093439019, valid_loss: 0.005093471981429805, test_loss: 0.01923079788684845\n",
      "epoch: 300, train_loss: 0.0012992643346281154, valid_loss: 0.005076427148499836, test_loss: 0.019246943295001984\n",
      "epoch: 301, train_loss: 0.0012979574106714647, valid_loss: 0.005047205020673573, test_loss: 0.019245844334363937\n",
      "epoch: 302, train_loss: 0.0012957902331634061, valid_loss: 0.005098581779748201, test_loss: 0.019150985404849052\n",
      "epoch: 303, train_loss: 0.0012941700608833976, valid_loss: 0.005052300596920152, test_loss: 0.019212597981095314\n",
      "epoch: 304, train_loss: 0.0012930073147720616, valid_loss: 0.005037362493264179, test_loss: 0.01923014223575592\n",
      "epoch: 305, train_loss: 0.0012925248846168752, valid_loss: 0.005035489099100232, test_loss: 0.01925269141793251\n",
      "epoch: 306, train_loss: 0.001289219919430173, valid_loss: 0.005053379262487094, test_loss: 0.019178245216608047\n",
      "epoch: 307, train_loss: 0.001287827814888695, valid_loss: 0.005030843953136355, test_loss: 0.019202865660190582\n",
      "epoch: 308, train_loss: 0.001285367053123596, valid_loss: 0.0050045595853589475, test_loss: 0.019195105880498886\n",
      "epoch: 309, train_loss: 0.0012854651836475925, valid_loss: 0.0050100196385756135, test_loss: 0.019231829792261124\n",
      "epoch: 310, train_loss: 0.0012837305675139246, valid_loss: 0.005013095447793603, test_loss: 0.019229469820857048\n",
      "epoch: 311, train_loss: 0.0012816495196286428, valid_loss: 0.004994457956248273, test_loss: 0.019267868250608444\n",
      "epoch: 312, train_loss: 0.0012839990617383432, valid_loss: 0.004991360493780424, test_loss: 0.019182197749614716\n",
      "epoch: 313, train_loss: 0.0012801102585280719, valid_loss: 0.004993369162548333, test_loss: 0.019158318638801575\n",
      "epoch: 314, train_loss: 0.001278160522064275, valid_loss: 0.004980049251268308, test_loss: 0.019158801063895226\n",
      "epoch: 315, train_loss: 0.0012745141487001724, valid_loss: 0.004978513112291694, test_loss: 0.01917422190308571\n",
      "epoch: 316, train_loss: 0.001275101377952682, valid_loss: 0.004980441493292649, test_loss: 0.019167203456163406\n",
      "epoch: 317, train_loss: 0.0012725854783480906, valid_loss: 0.004934699099976569, test_loss: 0.019106894731521606\n",
      "epoch: 318, train_loss: 0.0012719821787196333, valid_loss: 0.00494163289355735, test_loss: 0.019126947969198227\n",
      "epoch: 319, train_loss: 0.001270841407265676, valid_loss: 0.004943403240758926, test_loss: 0.019168388098478317\n",
      "epoch: 320, train_loss: 0.0012701372135147128, valid_loss: 0.004951358229542772, test_loss: 0.019131962209939957\n",
      "epoch: 321, train_loss: 0.0012667641453647418, valid_loss: 0.004932383346992235, test_loss: 0.01917656697332859\n",
      "epoch: 322, train_loss: 0.0012652315300605867, valid_loss: 0.004943475204830368, test_loss: 0.01914220303297043\n",
      "epoch: 323, train_loss: 0.001264529948062061, valid_loss: 0.004967606762268891, test_loss: 0.01910172402858734\n",
      "epoch: 324, train_loss: 0.0012634823139271011, valid_loss: 0.004910383082460612, test_loss: 0.019100718200206757\n",
      "epoch: 325, train_loss: 0.00126084205745112, valid_loss: 0.004909329892446597, test_loss: 0.019089000299572945\n",
      "epoch: 326, train_loss: 0.0012605962262529395, valid_loss: 0.004897230712231249, test_loss: 0.019126994535326958\n",
      "epoch: 327, train_loss: 0.001259596446407554, valid_loss: 0.0049184705324781435, test_loss: 0.019025269895792007\n",
      "epoch: 328, train_loss: 0.001258596145223988, valid_loss: 0.004908623852922271, test_loss: 0.019102632999420166\n",
      "epoch: 329, train_loss: 0.0012546327602847116, valid_loss: 0.004883115761913359, test_loss: 0.019110364839434624\n",
      "epoch: 330, train_loss: 0.0012546660500052183, valid_loss: 0.004886863423356165, test_loss: 0.019027376547455788\n",
      "epoch: 331, train_loss: 0.0012517172245956633, valid_loss: 0.004891585500445217, test_loss: 0.019128039479255676\n",
      "epoch: 332, train_loss: 0.0012509441914279823, valid_loss: 0.004866391517377148, test_loss: 0.019038725644350052\n",
      "epoch: 333, train_loss: 0.0012500864990136545, valid_loss: 0.004875707595298688, test_loss: 0.018987249583005905\n",
      "epoch: 334, train_loss: 0.0012492924427573123, valid_loss: 0.004882276601468523, test_loss: 0.019006192684173584\n",
      "epoch: 335, train_loss: 0.001249521487372239, valid_loss: 0.004837813399111231, test_loss: 0.01904658041894436\n",
      "epoch: 336, train_loss: 0.0012464940588435402, valid_loss: 0.0048671103043792146, test_loss: 0.018973099067807198\n",
      "epoch: 337, train_loss: 0.0012433158674115396, valid_loss: 0.004861698272482802, test_loss: 0.01898372173309326\n",
      "epoch: 338, train_loss: 0.0012431139703435094, valid_loss: 0.004848523162460576, test_loss: 0.018991606310009956\n",
      "epoch: 339, train_loss: 0.0012414349172183354, valid_loss: 0.004829175227011244, test_loss: 0.018988115713000298\n",
      "epoch: 340, train_loss: 0.0012423052002027955, valid_loss: 0.004813844551487516, test_loss: 0.019031384959816933\n",
      "epoch: 341, train_loss: 0.0012401243540174935, valid_loss: 0.004854995505108188, test_loss: 0.01900065317749977\n",
      "epoch: 342, train_loss: 0.0012385677318493633, valid_loss: 0.004821146508523573, test_loss: 0.018976662307977676\n",
      "epoch: 343, train_loss: 0.0012366529098590431, valid_loss: 0.004827351794422914, test_loss: 0.01895824261009693\n",
      "epoch: 344, train_loss: 0.0012367884957474535, valid_loss: 0.0048181134431312484, test_loss: 0.0189445149153471\n",
      "epoch: 345, train_loss: 0.0012348147979735033, valid_loss: 0.004782839212566614, test_loss: 0.01898309215903282\n",
      "epoch: 346, train_loss: 0.0012340982009292297, valid_loss: 0.004796040089180072, test_loss: 0.018953191116452217\n",
      "epoch: 347, train_loss: 0.0012334293873130303, valid_loss: 0.004793538071680814, test_loss: 0.018958797678351402\n",
      "epoch: 348, train_loss: 0.001229808845496534, valid_loss: 0.004795987741090357, test_loss: 0.018894419074058533\n",
      "epoch: 349, train_loss: 0.0012293450468543756, valid_loss: 0.004755281105947991, test_loss: 0.018919173628091812\n",
      "epoch: 350, train_loss: 0.0012277525130902295, valid_loss: 0.004772157175466418, test_loss: 0.01896389201283455\n",
      "epoch: 351, train_loss: 0.001225233730941039, valid_loss: 0.004783822301154335, test_loss: 0.018925242125988007\n",
      "epoch: 352, train_loss: 0.0012265241998450263, valid_loss: 0.00476174559056138, test_loss: 0.01892908290028572\n",
      "epoch: 353, train_loss: 0.00122487747474857, valid_loss: 0.004733486760718127, test_loss: 0.018955539911985397\n",
      "epoch: 354, train_loss: 0.0012223105902945542, valid_loss: 0.004773315158672631, test_loss: 0.018891511484980583\n",
      "epoch: 355, train_loss: 0.0012223272705855577, valid_loss: 0.004747434887879838, test_loss: 0.018872162327170372\n",
      "epoch: 356, train_loss: 0.0012201517466050775, valid_loss: 0.004716751573141664, test_loss: 0.018936967477202415\n",
      "epoch: 357, train_loss: 0.0012197235204360406, valid_loss: 0.004741817926211904, test_loss: 0.018904590979218483\n",
      "epoch: 358, train_loss: 0.0012181040264018204, valid_loss: 0.004740825097542256, test_loss: 0.018867116421461105\n",
      "epoch: 359, train_loss: 0.001216818009118509, valid_loss: 0.004720444713408749, test_loss: 0.01884930022060871\n",
      "epoch: 360, train_loss: 0.0012158305136203442, valid_loss: 0.004705487478834887, test_loss: 0.018864566460251808\n",
      "epoch: 361, train_loss: 0.0012136347510892413, valid_loss: 0.004724361021847774, test_loss: 0.01890547387301922\n",
      "epoch: 362, train_loss: 0.0012125607919069412, valid_loss: 0.004710072903738667, test_loss: 0.018913451582193375\n",
      "epoch: 363, train_loss: 0.0012102635784844017, valid_loss: 0.004703734438711156, test_loss: 0.018883638083934784\n",
      "epoch: 364, train_loss: 0.0012106638143608427, valid_loss: 0.004680594506983955, test_loss: 0.018819119781255722\n",
      "epoch: 365, train_loss: 0.001209309992744871, valid_loss: 0.004672356105099122, test_loss: 0.018859943374991417\n",
      "epoch: 366, train_loss: 0.0012076375601084335, valid_loss: 0.004678229219280183, test_loss: 0.018857723101973534\n",
      "epoch: 367, train_loss: 0.0012072443951974096, valid_loss: 0.004678015286723773, test_loss: 0.018841253593564034\n",
      "epoch: 368, train_loss: 0.0012043724823540645, valid_loss: 0.004689582565333694, test_loss: 0.018843775615096092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 369, train_loss: 0.0012043338982671823, valid_loss: 0.00465694578209271, test_loss: 0.018844271078705788\n",
      "epoch: 370, train_loss: 0.0012028549706725323, valid_loss: 0.004640512440043191, test_loss: 0.018891585990786552\n",
      "epoch: 371, train_loss: 0.0012021417167726095, valid_loss: 0.004652723087929189, test_loss: 0.018867233768105507\n",
      "epoch: 372, train_loss: 0.0012006532254061945, valid_loss: 0.0046277765844327705, test_loss: 0.0188632532954216\n",
      "epoch: 373, train_loss: 0.0012001480740171087, valid_loss: 0.004639354767277837, test_loss: 0.018839141353964806\n",
      "epoch: 374, train_loss: 0.0011973430341838496, valid_loss: 0.0046565958570378525, test_loss: 0.018806200474500656\n",
      "epoch: 375, train_loss: 0.0011981803134245717, valid_loss: 0.004621697085288663, test_loss: 0.01881224662065506\n",
      "epoch: 376, train_loss: 0.0011954726145157347, valid_loss: 0.004620510580328603, test_loss: 0.018791571259498596\n",
      "epoch: 377, train_loss: 0.0011944186509303424, valid_loss: 0.004611973495533069, test_loss: 0.0188352782279253\n",
      "epoch: 378, train_loss: 0.0011940130143952758, valid_loss: 0.004634853374833862, test_loss: 0.018803853541612625\n",
      "epoch: 379, train_loss: 0.0011931256107662034, valid_loss: 0.004639829023896406, test_loss: 0.01871170662343502\n",
      "epoch: 380, train_loss: 0.0011919566971735785, valid_loss: 0.004638872031743328, test_loss: 0.018774215131998062\n",
      "epoch: 381, train_loss: 0.0011890770477728674, valid_loss: 0.004611772601492703, test_loss: 0.018827298656105995\n",
      "epoch: 382, train_loss: 0.0011882330493434615, valid_loss: 0.004580435013243307, test_loss: 0.01879492774605751\n",
      "epoch: 383, train_loss: 0.0011884019451985216, valid_loss: 0.004593443629952769, test_loss: 0.01879330724477768\n",
      "epoch: 384, train_loss: 0.0011861216094668794, valid_loss: 0.004601420446609457, test_loss: 0.018776262179017067\n",
      "epoch: 385, train_loss: 0.001185454359865221, valid_loss: 0.004592775580628465, test_loss: 0.018671534955501556\n",
      "epoch: 386, train_loss: 0.0011865655365197556, valid_loss: 0.0045860306126996875, test_loss: 0.018767263740301132\n",
      "epoch: 387, train_loss: 0.0011841859921570058, valid_loss: 0.004609411019676675, test_loss: 0.018705299124121666\n",
      "epoch: 388, train_loss: 0.0011821622661638844, valid_loss: 0.004570718602432559, test_loss: 0.01877555623650551\n",
      "epoch: 389, train_loss: 0.001180640892530589, valid_loss: 0.00454260620366161, test_loss: 0.018795622512698174\n",
      "epoch: 390, train_loss: 0.0011804097954629233, valid_loss: 0.004568753026736279, test_loss: 0.018781976774334908\n",
      "epoch: 391, train_loss: 0.0011790916273046446, valid_loss: 0.004535132242987554, test_loss: 0.018751803785562515\n",
      "epoch: 392, train_loss: 0.0011779713576011684, valid_loss: 0.0045494410636213916, test_loss: 0.018724538385868073\n",
      "epoch: 393, train_loss: 0.0011760464125393849, valid_loss: 0.004541770981935163, test_loss: 0.018763573840260506\n",
      "epoch: 394, train_loss: 0.001174514715903965, valid_loss: 0.004549909771109621, test_loss: 0.018758272752165794\n",
      "epoch: 395, train_loss: 0.001175764150937776, valid_loss: 0.004531635359550516, test_loss: 0.018764926120638847\n",
      "epoch: 396, train_loss: 0.0011742841629513903, valid_loss: 0.00454420749641334, test_loss: 0.018758075311779976\n",
      "epoch: 397, train_loss: 0.00117176685117833, valid_loss: 0.004524927353486419, test_loss: 0.018750779330730438\n",
      "epoch: 398, train_loss: 0.0011708171628213124, valid_loss: 0.004527357697952539, test_loss: 0.01869971863925457\n",
      "epoch: 399, train_loss: 0.0011698851139163194, valid_loss: 0.004493037490950276, test_loss: 0.018740542232990265\n",
      "epoch: 400, train_loss: 0.0011686653065819132, valid_loss: 0.004513374607389172, test_loss: 0.01873830333352089\n",
      "epoch: 401, train_loss: 0.0011680826200577228, valid_loss: 0.004518704838119447, test_loss: 0.018753306940197945\n",
      "epoch: 402, train_loss: 0.001165617573702627, valid_loss: 0.004491067724302411, test_loss: 0.018727438524365425\n",
      "epoch: 403, train_loss: 0.0011663191405165455, valid_loss: 0.004495331920528163, test_loss: 0.01870776154100895\n",
      "epoch: 404, train_loss: 0.0011653094577765012, valid_loss: 0.004487577961602558, test_loss: 0.01867653615772724\n",
      "epoch: 405, train_loss: 0.0011633567603143013, valid_loss: 0.004486220384327074, test_loss: 0.018667062744498253\n",
      "epoch: 406, train_loss: 0.0011630949852785664, valid_loss: 0.004497058126920213, test_loss: 0.018657052889466286\n",
      "epoch: 407, train_loss: 0.0011627144539607284, valid_loss: 0.004492452251724899, test_loss: 0.018685361370444298\n",
      "epoch: 408, train_loss: 0.0011610051682826293, valid_loss: 0.00447441697663938, test_loss: 0.018643906340003014\n",
      "epoch: 409, train_loss: 0.0011595193721839914, valid_loss: 0.004474109562579542, test_loss: 0.018639830872416496\n",
      "epoch: 410, train_loss: 0.0011590339025765982, valid_loss: 0.004468483210075647, test_loss: 0.018669383600354195\n",
      "epoch: 411, train_loss: 0.0011566844844745228, valid_loss: 0.004478206353572507, test_loss: 0.018653567880392075\n",
      "epoch: 412, train_loss: 0.0011569154752499383, valid_loss: 0.004467535802784066, test_loss: 0.018681544810533524\n",
      "epoch: 413, train_loss: 0.001154836472225092, valid_loss: 0.004440834280103445, test_loss: 0.01868510991334915\n",
      "epoch: 414, train_loss: 0.0011551978076686678, valid_loss: 0.004444861318916082, test_loss: 0.0187095794826746\n",
      "epoch: 415, train_loss: 0.001153187383391449, valid_loss: 0.004431179627620925, test_loss: 0.018676050007343292\n",
      "epoch: 416, train_loss: 0.0011546644888332357, valid_loss: 0.004444477439392358, test_loss: 0.018659843131899834\n",
      "epoch: 417, train_loss: 0.0011528070595965762, valid_loss: 0.004415721768358101, test_loss: 0.01866365782916546\n",
      "epoch: 418, train_loss: 0.0011517823339723375, valid_loss: 0.0044204083969816566, test_loss: 0.01867165043950081\n",
      "epoch: 419, train_loss: 0.0011496794075988557, valid_loss: 0.004415894703318675, test_loss: 0.018627746030688286\n",
      "epoch: 420, train_loss: 0.0011487043584170549, valid_loss: 0.00442664932537203, test_loss: 0.018579917028546333\n",
      "epoch: 421, train_loss: 0.0011471981592678828, valid_loss: 0.004405028691204886, test_loss: 0.018645228818058968\n",
      "epoch: 422, train_loss: 0.0011467074661556146, valid_loss: 0.004385761761416991, test_loss: 0.018636442720890045\n",
      "epoch: 423, train_loss: 0.001145625785600556, valid_loss: 0.0044138176793543, test_loss: 0.01858673058450222\n",
      "epoch: 424, train_loss: 0.0011444370434417026, valid_loss: 0.004409715824294835, test_loss: 0.018566180020570755\n",
      "epoch: 425, train_loss: 0.001143631527868464, valid_loss: 0.0044137299216041965, test_loss: 0.018603863194584846\n",
      "epoch: 426, train_loss: 0.0011427317715614386, valid_loss: 0.004393205628730357, test_loss: 0.018616361543536186\n",
      "epoch: 427, train_loss: 0.0011418669499740329, valid_loss: 0.004365574743133038, test_loss: 0.018668072298169136\n",
      "epoch: 428, train_loss: 0.0011408299956794667, valid_loss: 0.004396905074827373, test_loss: 0.01856742799282074\n",
      "epoch: 429, train_loss: 0.0011398739550951059, valid_loss: 0.0043746152077801526, test_loss: 0.01861262507736683\n",
      "epoch: 430, train_loss: 0.0011392821046604734, valid_loss: 0.004389505387128641, test_loss: 0.018593883141875267\n",
      "epoch: 431, train_loss: 0.0011378190753495564, valid_loss: 0.0043741629730599625, test_loss: 0.018599431961774826\n",
      "epoch: 432, train_loss: 0.0011361773232119563, valid_loss: 0.004358380334451795, test_loss: 0.018597451969981194\n",
      "epoch: 433, train_loss: 0.0011375143329369957, valid_loss: 0.004351221180210511, test_loss: 0.018632331863045692\n",
      "epoch: 434, train_loss: 0.0011350701122706675, valid_loss: 0.004355011643686642, test_loss: 0.018573351204395294\n",
      "epoch: 435, train_loss: 0.0011339664519966943, valid_loss: 0.0043391378518814845, test_loss: 0.01859474927186966\n",
      "epoch: 436, train_loss: 0.0011339534337267928, valid_loss: 0.004331225180067122, test_loss: 0.01858501136302948\n",
      "epoch: 437, train_loss: 0.0011317337934008758, valid_loss: 0.004334528329006086, test_loss: 0.01858700066804886\n",
      "epoch: 438, train_loss: 0.0011303476184485075, valid_loss: 0.0043322866937766475, test_loss: 0.01857193373143673\n",
      "epoch: 439, train_loss: 0.0011304175840569255, valid_loss: 0.004331344913225621, test_loss: 0.01860879920423031\n",
      "epoch: 440, train_loss: 0.001130074424588162, valid_loss: 0.004350463820931812, test_loss: 0.018592609092593193\n",
      "epoch: 441, train_loss: 0.0011278497935880137, valid_loss: 0.004307186987716705, test_loss: 0.018580090254545212\n",
      "epoch: 442, train_loss: 0.001127656222746262, valid_loss: 0.004319586829903225, test_loss: 0.01858453080058098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 443, train_loss: 0.0011262499636201107, valid_loss: 0.004317352101982881, test_loss: 0.018563224002718925\n",
      "epoch: 444, train_loss: 0.0011260651112736566, valid_loss: 0.0043130879833673435, test_loss: 0.018544785678386688\n",
      "epoch: 445, train_loss: 0.001126403862144798, valid_loss: 0.004294159162479143, test_loss: 0.01856573298573494\n",
      "epoch: 446, train_loss: 0.0011261442635694275, valid_loss: 0.00429914587099726, test_loss: 0.018557321280241013\n",
      "epoch: 447, train_loss: 0.0011230524523062227, valid_loss: 0.004286765159728627, test_loss: 0.018535921350121498\n",
      "epoch: 448, train_loss: 0.0011210087130008185, valid_loss: 0.004311979088621835, test_loss: 0.018552839756011963\n",
      "epoch: 449, train_loss: 0.0011223852153584037, valid_loss: 0.004277249604153137, test_loss: 0.018581312149763107\n",
      "epoch: 450, train_loss: 0.0011209220924090755, valid_loss: 0.004264522169250995, test_loss: 0.018556838855147362\n",
      "epoch: 451, train_loss: 0.0011198333307655284, valid_loss: 0.004275222832802683, test_loss: 0.01854407787322998\n",
      "epoch: 452, train_loss: 0.0011183275441315186, valid_loss: 0.004278411002208789, test_loss: 0.01849258318543434\n",
      "epoch: 453, train_loss: 0.0011171944484964986, valid_loss: 0.004277140814034889, test_loss: 0.01853547990322113\n",
      "epoch: 454, train_loss: 0.0011169138901016634, valid_loss: 0.004288207894812028, test_loss: 0.018508300185203552\n",
      "epoch: 455, train_loss: 0.0011160349721372452, valid_loss: 0.004245129316889991, test_loss: 0.018489107489585876\n",
      "epoch: 456, train_loss: 0.0011146810492905586, valid_loss: 0.0042751812531302376, test_loss: 0.01854998990893364\n",
      "epoch: 457, train_loss: 0.0011147580327182684, valid_loss: 0.004272660143518199, test_loss: 0.018476737663149834\n",
      "epoch: 458, train_loss: 0.0011123720627359073, valid_loss: 0.004243207513354719, test_loss: 0.018515432253479958\n",
      "epoch: 459, train_loss: 0.0011125098250846825, valid_loss: 0.0042309937804626925, test_loss: 0.018555602058768272\n",
      "epoch: 460, train_loss: 0.0011113571058756308, valid_loss: 0.00424803663433219, test_loss: 0.018484415486454964\n",
      "epoch: 461, train_loss: 0.0011100114434552581, valid_loss: 0.0042308365809731185, test_loss: 0.01853065751492977\n",
      "epoch: 462, train_loss: 0.001109627523439248, valid_loss: 0.004242424154654145, test_loss: 0.018509525805711746\n",
      "epoch: 463, train_loss: 0.0011089271258400834, valid_loss: 0.004239605274051428, test_loss: 0.018498335033655167\n",
      "epoch: 464, train_loss: 0.0011085900958910909, valid_loss: 0.004241027112584561, test_loss: 0.01844538003206253\n",
      "epoch: 465, train_loss: 0.0011065279174110165, valid_loss: 0.004228117119055241, test_loss: 0.018455591052770615\n",
      "epoch: 466, train_loss: 0.0011051087817379637, valid_loss: 0.004222637100610882, test_loss: 0.018479634076356888\n",
      "epoch: 467, train_loss: 0.0011063625064233074, valid_loss: 0.004245035408530384, test_loss: 0.018497595563530922\n",
      "epoch: 468, train_loss: 0.0011053726952725453, valid_loss: 0.004206397648279865, test_loss: 0.018520530313253403\n",
      "epoch: 469, train_loss: 0.0011056641789922571, valid_loss: 0.004200826185600211, test_loss: 0.018514595925807953\n",
      "epoch: 470, train_loss: 0.0011021473632275086, valid_loss: 0.004218973646250864, test_loss: 0.018491530790925026\n",
      "epoch: 471, train_loss: 0.0011010703519391625, valid_loss: 0.004210634564515203, test_loss: 0.018442556262016296\n",
      "epoch: 472, train_loss: 0.0011025517445017138, valid_loss: 0.004207363080543776, test_loss: 0.018457116559147835\n",
      "epoch: 473, train_loss: 0.0011000448265922782, valid_loss: 0.004183736959627519, test_loss: 0.018471885472536087\n",
      "epoch: 474, train_loss: 0.0010995634669518988, valid_loss: 0.0042059256811626256, test_loss: 0.018459662795066833\n",
      "epoch: 475, train_loss: 0.0010972956003135314, valid_loss: 0.004175087766877065, test_loss: 0.018507249653339386\n",
      "epoch: 476, train_loss: 0.0010984953639664404, valid_loss: 0.00415494543267414, test_loss: 0.01846480183303356\n",
      "epoch: 477, train_loss: 0.0010967537249519448, valid_loss: 0.004182452568784356, test_loss: 0.018468590453267097\n",
      "epoch: 478, train_loss: 0.0010969879298025498, valid_loss: 0.004164052060029159, test_loss: 0.018490523099899292\n",
      "epoch: 479, train_loss: 0.0010944328842806103, valid_loss: 0.004160956576621781, test_loss: 0.018490150570869446\n",
      "epoch: 480, train_loss: 0.0010939779003029284, valid_loss: 0.004162046844915797, test_loss: 0.018445489928126335\n",
      "epoch: 481, train_loss: 0.0010943927184638121, valid_loss: 0.004190972191281617, test_loss: 0.018430689349770546\n",
      "epoch: 482, train_loss: 0.001091603463029732, valid_loss: 0.004160792877276738, test_loss: 0.018480544909834862\n",
      "epoch: 483, train_loss: 0.001093140982962011, valid_loss: 0.0041486651947100954, test_loss: 0.018469136208295822\n",
      "epoch: 484, train_loss: 0.001091799379893295, valid_loss: 0.004139403773782154, test_loss: 0.01841546967625618\n",
      "epoch: 485, train_loss: 0.0010908028115923314, valid_loss: 0.004154171988678475, test_loss: 0.01845606602728367\n",
      "epoch: 486, train_loss: 0.0010880282371426406, valid_loss: 0.004149017913732678, test_loss: 0.018398143351078033\n",
      "epoch: 487, train_loss: 0.0010899683566885474, valid_loss: 0.004124052997212857, test_loss: 0.018442919477820396\n",
      "epoch: 488, train_loss: 0.0010871127217441149, valid_loss: 0.004124650867500653, test_loss: 0.0184338316321373\n",
      "epoch: 489, train_loss: 0.0010860714904518554, valid_loss: 0.004143511410802603, test_loss: 0.01840885542333126\n",
      "epoch: 490, train_loss: 0.0010864988485675144, valid_loss: 0.0041349765184956295, test_loss: 0.018466169014573097\n",
      "epoch: 491, train_loss: 0.0010845468824972277, valid_loss: 0.00412821463154008, test_loss: 0.018459446728229523\n",
      "epoch: 492, train_loss: 0.0010836996851772394, valid_loss: 0.004132382047828287, test_loss: 0.0184180811047554\n",
      "epoch: 493, train_loss: 0.0010841040993514268, valid_loss: 0.004130965661412726, test_loss: 0.018420696258544922\n",
      "epoch: 494, train_loss: 0.0010832831309095998, valid_loss: 0.00409852076942722, test_loss: 0.018402399495244026\n",
      "epoch: 495, train_loss: 0.0010834265697706976, valid_loss: 0.004114374712420006, test_loss: 0.018498186022043228\n",
      "epoch: 496, train_loss: 0.0010816101111350176, valid_loss: 0.0041229680452185375, test_loss: 0.018378369510173798\n",
      "epoch: 497, train_loss: 0.001080949244129917, valid_loss: 0.004114763694815338, test_loss: 0.018404167145490646\n",
      "epoch: 498, train_loss: 0.0010797595352177386, valid_loss: 0.004096570909799387, test_loss: 0.018418384715914726\n",
      "epoch: 499, train_loss: 0.00107994424837196, valid_loss: 0.00411440785198162, test_loss: 0.018437184393405914\n",
      "epoch: 500, train_loss: 0.0010783014864579816, valid_loss: 0.004117178148590028, test_loss: 0.018407700583338737\n",
      "epoch: 501, train_loss: 0.001076849779802496, valid_loss: 0.004098767201260974, test_loss: 0.0184254702180624\n",
      "epoch: 502, train_loss: 0.0010768222981942413, valid_loss: 0.004077800316736102, test_loss: 0.018433786928653717\n",
      "epoch: 503, train_loss: 0.0010757928223187184, valid_loss: 0.004078868155678113, test_loss: 0.018391208723187447\n",
      "epoch: 504, train_loss: 0.0010754941208490534, valid_loss: 0.004066001138805102, test_loss: 0.018375655636191368\n",
      "epoch: 505, train_loss: 0.0010740517527269928, valid_loss: 0.004085597426940997, test_loss: 0.01839357614517212\n",
      "epoch: 506, train_loss: 0.0010758668852403112, valid_loss: 0.00405673998951291, test_loss: 0.018358241766691208\n",
      "epoch: 507, train_loss: 0.0010724208828912158, valid_loss: 0.004078258663260688, test_loss: 0.01836376078426838\n",
      "epoch: 508, train_loss: 0.0010725138987333555, valid_loss: 0.004068234916000317, test_loss: 0.018405672162771225\n",
      "epoch: 509, train_loss: 0.0010715644963531067, valid_loss: 0.004071515752002597, test_loss: 0.01839323528110981\n",
      "epoch: 510, train_loss: 0.0010702507950001113, valid_loss: 0.0040468604808362825, test_loss: 0.018364954739809036\n",
      "epoch: 511, train_loss: 0.001070069700843938, valid_loss: 0.004060754630093773, test_loss: 0.018413592129945755\n",
      "epoch: 512, train_loss: 0.0010692681383301058, valid_loss: 0.004048750114937623, test_loss: 0.01842332072556019\n",
      "epoch: 513, train_loss: 0.0010688763957106225, valid_loss: 0.004042538581416011, test_loss: 0.018341775983572006\n",
      "epoch: 514, train_loss: 0.001067176663681217, valid_loss: 0.004037807346321642, test_loss: 0.01838468760251999\n",
      "epoch: 515, train_loss: 0.0010669457513596053, valid_loss: 0.004053883409748475, test_loss: 0.018309885635972023\n",
      "epoch: 516, train_loss: 0.001065924599179593, valid_loss: 0.004046554250332217, test_loss: 0.018394777551293373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 517, train_loss: 0.0010663350289889975, valid_loss: 0.004019305284600705, test_loss: 0.018382521346211433\n",
      "epoch: 518, train_loss: 0.0010650708168016179, valid_loss: 0.004010274404815088, test_loss: 0.018350820988416672\n",
      "epoch: 519, train_loss: 0.001065147496537184, valid_loss: 0.004012198323228707, test_loss: 0.018379757180809975\n",
      "epoch: 520, train_loss: 0.001063237229929022, valid_loss: 0.0040110395445177955, test_loss: 0.018391069024801254\n",
      "epoch: 521, train_loss: 0.0010617364618076902, valid_loss: 0.004005172542141129, test_loss: 0.018374305218458176\n",
      "epoch: 522, train_loss: 0.001061885319042789, valid_loss: 0.004014835401903838, test_loss: 0.0183683130890131\n",
      "epoch: 523, train_loss: 0.0010604518895928302, valid_loss: 0.0040100361802615225, test_loss: 0.018381386995315552\n",
      "epoch: 524, train_loss: 0.0010601976284068887, valid_loss: 0.004004684955968211, test_loss: 0.01836976781487465\n",
      "epoch: 525, train_loss: 0.0010594547746460075, valid_loss: 0.003995270351879299, test_loss: 0.018376650288701057\n",
      "epoch: 526, train_loss: 0.0010588482027346997, valid_loss: 0.003994101639060925, test_loss: 0.018312646076083183\n",
      "epoch: 527, train_loss: 0.001058457803952953, valid_loss: 0.003981150628533214, test_loss: 0.01833830215036869\n",
      "epoch: 528, train_loss: 0.001058132154867053, valid_loss: 0.004001398318602393, test_loss: 0.018357450142502785\n",
      "epoch: 529, train_loss: 0.0010561261037567062, valid_loss: 0.003997632535174489, test_loss: 0.018317947164177895\n",
      "epoch: 530, train_loss: 0.0010559535205728657, valid_loss: 0.003985344568112244, test_loss: 0.01840364933013916\n",
      "epoch: 531, train_loss: 0.0010554017979935136, valid_loss: 0.003965186498438318, test_loss: 0.01840350218117237\n",
      "epoch: 532, train_loss: 0.001053613883888592, valid_loss: 0.003995834267698228, test_loss: 0.018329517915844917\n",
      "epoch: 533, train_loss: 0.0010534661680298007, valid_loss: 0.0039553077464612825, test_loss: 0.018335958942770958\n",
      "epoch: 534, train_loss: 0.001053728218919233, valid_loss: 0.003944123726493369, test_loss: 0.018382908776402473\n",
      "epoch: 535, train_loss: 0.0010521931968548376, valid_loss: 0.003965077572502196, test_loss: 0.018358109518885612\n",
      "epoch: 536, train_loss: 0.0010521107115377874, valid_loss: 0.003957151163679858, test_loss: 0.018386952579021454\n",
      "epoch: 537, train_loss: 0.001050617553435428, valid_loss: 0.003953947181192537, test_loss: 0.018356813117861748\n",
      "epoch: 538, train_loss: 0.001050461726465627, valid_loss: 0.0039621613298853236, test_loss: 0.01834709197282791\n",
      "epoch: 539, train_loss: 0.0010510810281392996, valid_loss: 0.003938394327027102, test_loss: 0.01835015043616295\n",
      "epoch: 540, train_loss: 0.0010487742281680846, valid_loss: 0.003934131333759676, test_loss: 0.01832798682153225\n",
      "epoch: 541, train_loss: 0.001047073719192944, valid_loss: 0.003951773377290617, test_loss: 0.01831989362835884\n",
      "epoch: 542, train_loss: 0.0010472024973157954, valid_loss: 0.003951389536571999, test_loss: 0.018330201506614685\n",
      "epoch: 543, train_loss: 0.0010464542960424137, valid_loss: 0.003945729637052864, test_loss: 0.018344493582844734\n",
      "epoch: 544, train_loss: 0.0010456711980109305, valid_loss: 0.003949684071509789, test_loss: 0.018307052552700043\n",
      "epoch: 545, train_loss: 0.0010457454785785596, valid_loss: 0.003936181349369387, test_loss: 0.018326597288250923\n",
      "epoch: 546, train_loss: 0.0010453749488553276, valid_loss: 0.003943533714239796, test_loss: 0.018328076228499413\n",
      "epoch: 547, train_loss: 0.0010447070361924884, valid_loss: 0.003921089635696262, test_loss: 0.01830049604177475\n",
      "epoch: 548, train_loss: 0.001042573816527653, valid_loss: 0.003927598940208554, test_loss: 0.01832910254597664\n",
      "epoch: 549, train_loss: 0.001042862635348802, valid_loss: 0.00393804966006428, test_loss: 0.018368933349847794\n",
      "epoch: 550, train_loss: 0.0010432501060321279, valid_loss: 0.003919349401257932, test_loss: 0.018318530172109604\n",
      "epoch: 551, train_loss: 0.0010414477260580854, valid_loss: 0.003934632967381428, test_loss: 0.018303699791431427\n",
      "epoch: 552, train_loss: 0.0010403241587640798, valid_loss: 0.003912051285927494, test_loss: 0.01837299019098282\n",
      "epoch: 553, train_loss: 0.0010391380410115032, valid_loss: 0.003926301219811042, test_loss: 0.018300577998161316\n",
      "epoch: 554, train_loss: 0.0010393340717596204, valid_loss: 0.003906747791916132, test_loss: 0.01828911155462265\n",
      "epoch: 555, train_loss: 0.001039356861324252, valid_loss: 0.00390016648452729, test_loss: 0.018321722745895386\n",
      "epoch: 556, train_loss: 0.0010366809994751668, valid_loss: 0.003900563266749183, test_loss: 0.018328633159399033\n",
      "epoch: 557, train_loss: 0.0010371762327849865, valid_loss: 0.0038917718920856714, test_loss: 0.018319059163331985\n",
      "epoch: 558, train_loss: 0.0010362228570991883, valid_loss: 0.0038984421989880502, test_loss: 0.01830933801829815\n",
      "epoch: 559, train_loss: 0.0010347555431982746, valid_loss: 0.0038903161184862256, test_loss: 0.018338099122047424\n",
      "epoch: 560, train_loss: 0.0010345390334765873, valid_loss: 0.003911593191636105, test_loss: 0.018305951729416847\n",
      "epoch: 561, train_loss: 0.0010351620427251835, valid_loss: 0.0038790213293395936, test_loss: 0.01826169341802597\n",
      "epoch: 562, train_loss: 0.0010344889951343446, valid_loss: 0.0038758498849347234, test_loss: 0.01827484928071499\n",
      "epoch: 563, train_loss: 0.0010317243488869913, valid_loss: 0.003871895644503335, test_loss: 0.018301213160157204\n",
      "epoch: 564, train_loss: 0.0010332612487811434, valid_loss: 0.0038718756016654274, test_loss: 0.01833648793399334\n",
      "epoch: 565, train_loss: 0.0010325058702500942, valid_loss: 0.003874654823448509, test_loss: 0.01831810362637043\n",
      "epoch: 566, train_loss: 0.0010317632695659995, valid_loss: 0.003866891434881836, test_loss: 0.018328385427594185\n",
      "epoch: 567, train_loss: 0.0010316361263191895, valid_loss: 0.003851529249611, test_loss: 0.018312552943825722\n",
      "epoch: 568, train_loss: 0.001030267284595934, valid_loss: 0.0038515252138798437, test_loss: 0.018350085243582726\n",
      "epoch: 569, train_loss: 0.001029373969093127, valid_loss: 0.00386137169941018, test_loss: 0.018299758434295654\n",
      "epoch: 570, train_loss: 0.0010282107902204862, valid_loss: 0.0038365634391084313, test_loss: 0.01831567846238613\n",
      "epoch: 571, train_loss: 0.0010277269225891518, valid_loss: 0.003838174724175284, test_loss: 0.01833466812968254\n",
      "epoch: 572, train_loss: 0.0010280992668729436, valid_loss: 0.0038585859971741834, test_loss: 0.018292469903826714\n",
      "epoch: 573, train_loss: 0.0010255622100489943, valid_loss: 0.0038441622940202556, test_loss: 0.01831391267478466\n",
      "epoch: 574, train_loss: 0.0010260342514020917, valid_loss: 0.0038516707718372345, test_loss: 0.018313689157366753\n",
      "epoch: 575, train_loss: 0.0010253490662485685, valid_loss: 0.0038361972741161785, test_loss: 0.01828691177070141\n",
      "epoch: 576, train_loss: 0.0010240888223052025, valid_loss: 0.0038333445748624704, test_loss: 0.018325133249163628\n",
      "epoch: 577, train_loss: 0.0010233758189512985, valid_loss: 0.0038422405874977508, test_loss: 0.018267571926116943\n",
      "epoch: 578, train_loss: 0.0010229074497423742, valid_loss: 0.003832733763071398, test_loss: 0.018276218324899673\n",
      "epoch: 579, train_loss: 0.0010233357644882863, valid_loss: 0.0038295821480763457, test_loss: 0.018267115578055382\n",
      "epoch: 580, train_loss: 0.0010209821529038575, valid_loss: 0.003835689897338549, test_loss: 0.018282897770404816\n",
      "epoch: 581, train_loss: 0.0010220639448127022, valid_loss: 0.0038293604544984796, test_loss: 0.018290769308805466\n",
      "epoch: 582, train_loss: 0.001021222021612946, valid_loss: 0.0038134684048903487, test_loss: 0.018329335376620293\n",
      "epoch: 583, train_loss: 0.0010204618219691126, valid_loss: 0.0038373469336268804, test_loss: 0.018271321430802345\n",
      "epoch: 584, train_loss: 0.001021876116283238, valid_loss: 0.0038136989848377803, test_loss: 0.018278907984495163\n",
      "epoch: 585, train_loss: 0.001018380353976365, valid_loss: 0.003792865900322795, test_loss: 0.018319545313715935\n",
      "epoch: 586, train_loss: 0.001018652087583652, valid_loss: 0.0038231064681895077, test_loss: 0.0183026771992445\n",
      "epoch: 587, train_loss: 0.0010167650357091231, valid_loss: 0.00381537185360988, test_loss: 0.0182564165443182\n",
      "epoch: 588, train_loss: 0.0010165425356598976, valid_loss: 0.0037814612151123583, test_loss: 0.018298141658306122\n",
      "epoch: 589, train_loss: 0.0010173911147791407, valid_loss: 0.003783156726664553, test_loss: 0.018323156982660294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 590, train_loss: 0.001015306668072615, valid_loss: 0.0038116202728512385, test_loss: 0.01826711930334568\n",
      "epoch: 591, train_loss: 0.0010156835272462795, valid_loss: 0.003775014697263638, test_loss: 0.018263529986143112\n",
      "epoch: 592, train_loss: 0.0010144376960259092, valid_loss: 0.003776374952091525, test_loss: 0.01825537160038948\n",
      "epoch: 593, train_loss: 0.0010142911949356937, valid_loss: 0.0037889391145048044, test_loss: 0.018260447308421135\n",
      "epoch: 594, train_loss: 0.001013521664350739, valid_loss: 0.0037890770666611693, test_loss: 0.018265629187226295\n",
      "epoch: 595, train_loss: 0.0010127958553114338, valid_loss: 0.0038096366139749684, test_loss: 0.018285125494003296\n",
      "epoch: 596, train_loss: 0.0010128637279748268, valid_loss: 0.0037629810200693705, test_loss: 0.018274446949362755\n",
      "epoch: 597, train_loss: 0.001011672804294073, valid_loss: 0.003822908600947509, test_loss: 0.018220139667391777\n",
      "epoch: 598, train_loss: 0.0010136714124161265, valid_loss: 0.0037612762729016445, test_loss: 0.018273530527949333\n",
      "epoch: 599, train_loss: 0.001010064544338409, valid_loss: 0.0037606005207635462, test_loss: 0.018309473991394043\n",
      "epoch: 600, train_loss: 0.0010086075762165306, valid_loss: 0.003772583712513248, test_loss: 0.018250714987516403\n",
      "epoch: 601, train_loss: 0.0010090379358228781, valid_loss: 0.0037484995555132627, test_loss: 0.01824096404016018\n",
      "epoch: 602, train_loss: 0.0010090185855717762, valid_loss: 0.003764486697036773, test_loss: 0.018238311633467674\n",
      "epoch: 603, train_loss: 0.0010085100809152684, valid_loss: 0.0037475373634758094, test_loss: 0.018262285739183426\n",
      "epoch: 604, train_loss: 0.0010064857525993948, valid_loss: 0.003757852886337787, test_loss: 0.018305471166968346\n",
      "epoch: 605, train_loss: 0.00100696692749372, valid_loss: 0.0037340714091745517, test_loss: 0.018286552280187607\n",
      "epoch: 606, train_loss: 0.0010062150009300399, valid_loss: 0.0037263332827327154, test_loss: 0.018294863402843475\n",
      "epoch: 607, train_loss: 0.0010054715448464065, valid_loss: 0.0037488278467208147, test_loss: 0.018261663615703583\n",
      "epoch: 608, train_loss: 0.001005405509520484, valid_loss: 0.0037551757025842867, test_loss: 0.018268465995788574\n",
      "epoch: 609, train_loss: 0.0010031304782782884, valid_loss: 0.0037236959712269404, test_loss: 0.018207957968115807\n",
      "epoch: 610, train_loss: 0.0010035168354237533, valid_loss: 0.0037183816699932017, test_loss: 0.01829240284860134\n",
      "epoch: 611, train_loss: 0.001002221763053018, valid_loss: 0.0037403369400029383, test_loss: 0.018263405188918114\n",
      "epoch: 612, train_loss: 0.001003141026518753, valid_loss: 0.003756924745781968, test_loss: 0.01825037971138954\n",
      "epoch: 613, train_loss: 0.0010012350166621416, valid_loss: 0.0037222717655822635, test_loss: 0.018278416246175766\n",
      "epoch: 614, train_loss: 0.001002261875723691, valid_loss: 0.0037121670902706683, test_loss: 0.018268456682562828\n",
      "epoch: 615, train_loss: 0.0010008223192847293, valid_loss: 0.00372157166323935, test_loss: 0.018218981102108955\n",
      "epoch: 616, train_loss: 0.001000943021727321, valid_loss: 0.0036943685457420847, test_loss: 0.01826728694140911\n",
      "epoch: 617, train_loss: 0.0009993608717037284, valid_loss: 0.0037068874031926193, test_loss: 0.018275480717420578\n",
      "epoch: 618, train_loss: 0.0009980333650120251, valid_loss: 0.003719746154577782, test_loss: 0.018198534846305847\n",
      "epoch: 619, train_loss: 0.0009985987309609418, valid_loss: 0.003697441405771921, test_loss: 0.01826014369726181\n",
      "epoch: 620, train_loss: 0.000998387923059256, valid_loss: 0.0037071747550119958, test_loss: 0.018254052847623825\n",
      "epoch: 621, train_loss: 0.0009976519023716126, valid_loss: 0.0036913291357147195, test_loss: 0.018253132700920105\n",
      "epoch: 622, train_loss: 0.000997173339229725, valid_loss: 0.0037132885578709343, test_loss: 0.01821897365152836\n",
      "epoch: 623, train_loss: 0.0009954465254533875, valid_loss: 0.00370586576173082, test_loss: 0.018260184675455093\n",
      "epoch: 624, train_loss: 0.0009970299054301627, valid_loss: 0.0036951039219275117, test_loss: 0.01824789308011532\n",
      "epoch: 625, train_loss: 0.000995745986153412, valid_loss: 0.0036851926900756857, test_loss: 0.018244968727231026\n",
      "epoch: 626, train_loss: 0.0009949627413373926, valid_loss: 0.003677547734696418, test_loss: 0.018277423456311226\n",
      "epoch: 627, train_loss: 0.000994879228528589, valid_loss: 0.003703197716580083, test_loss: 0.01823168620467186\n",
      "epoch: 628, train_loss: 0.0009933926377688413, valid_loss: 0.003683085165296992, test_loss: 0.018243834376335144\n",
      "epoch: 629, train_loss: 0.0009925688323362367, valid_loss: 0.0036678308000167212, test_loss: 0.018300794064998627\n",
      "epoch: 630, train_loss: 0.0009925756856556172, valid_loss: 0.0036663583596237004, test_loss: 0.018231477588415146\n",
      "epoch: 631, train_loss: 0.0009911069494631627, valid_loss: 0.0036628501450953386, test_loss: 0.018244106322526932\n",
      "epoch: 632, train_loss: 0.0009916555870364866, valid_loss: 0.0036627016961574554, test_loss: 0.018268518149852753\n",
      "epoch: 633, train_loss: 0.00099093248338803, valid_loss: 0.0036633888958022, test_loss: 0.018279431387782097\n",
      "epoch: 634, train_loss: 0.0009903930299712913, valid_loss: 0.003654765721876174, test_loss: 0.01828804984688759\n",
      "epoch: 635, train_loss: 0.0009897038133045578, valid_loss: 0.0036621385758432248, test_loss: 0.01822602190077305\n",
      "epoch: 636, train_loss: 0.0009891186137278767, valid_loss: 0.0036618452480373285, test_loss: 0.018241088837385178\n",
      "epoch: 637, train_loss: 0.0009881184213673291, valid_loss: 0.0036627971567213535, test_loss: 0.018230672925710678\n",
      "epoch: 638, train_loss: 0.0009873338934520016, valid_loss: 0.0036487655015662313, test_loss: 0.018193695694208145\n",
      "epoch: 639, train_loss: 0.0009880424502467656, valid_loss: 0.003660926149071505, test_loss: 0.018196020275354385\n",
      "epoch: 640, train_loss: 0.000986952301742428, valid_loss: 0.0036612386466003954, test_loss: 0.01823219284415245\n",
      "epoch: 641, train_loss: 0.0009858683561501296, valid_loss: 0.003669648489449173, test_loss: 0.018226340413093567\n",
      "epoch: 642, train_loss: 0.0009865654156664791, valid_loss: 0.0036504402329834798, test_loss: 0.01821335218846798\n",
      "epoch: 643, train_loss: 0.0009849819850743465, valid_loss: 0.003643231621632973, test_loss: 0.01825573667883873\n",
      "epoch: 644, train_loss: 0.0009852171389629011, valid_loss: 0.003653872370099028, test_loss: 0.018233241513371468\n",
      "epoch: 645, train_loss: 0.000983526489859366, valid_loss: 0.003654504932152728, test_loss: 0.01823841594159603\n",
      "epoch: 646, train_loss: 0.0009839150791420884, valid_loss: 0.0036384425669287643, test_loss: 0.01827368699014187\n",
      "epoch: 647, train_loss: 0.0009813512012402973, valid_loss: 0.0036157516102927425, test_loss: 0.018286574631929398\n",
      "epoch: 648, train_loss: 0.0009830869435656655, valid_loss: 0.0036316873350491128, test_loss: 0.01821339689195156\n",
      "epoch: 649, train_loss: 0.0009818123476377325, valid_loss: 0.003624238927538196, test_loss: 0.01822870783507824\n",
      "epoch: 650, train_loss: 0.0009809501732335143, valid_loss: 0.0036200813871497908, test_loss: 0.018239758908748627\n",
      "epoch: 651, train_loss: 0.00098008102150229, valid_loss: 0.003626552934292704, test_loss: 0.018220841884613037\n",
      "epoch: 652, train_loss: 0.000980618812204541, valid_loss: 0.0036126733563529947, test_loss: 0.018248887732625008\n",
      "epoch: 653, train_loss: 0.000979317797590857, valid_loss: 0.0036276699199030795, test_loss: 0.018242521211504936\n",
      "epoch: 654, train_loss: 0.0009774984994336314, valid_loss: 0.0036226853844709694, test_loss: 0.01819777861237526\n",
      "epoch: 655, train_loss: 0.0009780647334359262, valid_loss: 0.003610338879904399, test_loss: 0.01826351322233677\n",
      "epoch: 656, train_loss: 0.0009776561713332067, valid_loss: 0.0035945658261577287, test_loss: 0.01825207844376564\n",
      "epoch: 657, train_loss: 0.0009768626591919558, valid_loss: 0.0035956212086603045, test_loss: 0.01826094649732113\n",
      "epoch: 658, train_loss: 0.0009785974474178383, valid_loss: 0.003602424946924051, test_loss: 0.018261149525642395\n",
      "epoch: 659, train_loss: 0.0009760611422319452, valid_loss: 0.0036074258775139847, test_loss: 0.018264535814523697\n",
      "epoch: 660, train_loss: 0.000975564691622782, valid_loss: 0.003608266958811631, test_loss: 0.018209952861070633\n",
      "epoch: 661, train_loss: 0.0009750706578969308, valid_loss: 0.003595464106183499, test_loss: 0.018247349187731743\n",
      "epoch: 662, train_loss: 0.0009736568418979321, valid_loss: 0.003597319203739365, test_loss: 0.018222332000732422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 663, train_loss: 0.0009740377199309676, valid_loss: 0.0035728941244694092, test_loss: 0.018237924203276634\n",
      "epoch: 664, train_loss: 0.0009737031777267871, valid_loss: 0.003585879884970685, test_loss: 0.0182344913482666\n",
      "epoch: 665, train_loss: 0.0009731785191789917, valid_loss: 0.003599199485809853, test_loss: 0.018173012882471085\n",
      "epoch: 666, train_loss: 0.0009725521465399019, valid_loss: 0.003578702744562179, test_loss: 0.01822657138109207\n",
      "epoch: 667, train_loss: 0.0009724818367470542, valid_loss: 0.0035714072679790356, test_loss: 0.018222495913505554\n",
      "epoch: 668, train_loss: 0.0009716456482672821, valid_loss: 0.0035636224977982542, test_loss: 0.018219105899333954\n",
      "epoch: 669, train_loss: 0.0009707737455913878, valid_loss: 0.0035978919089150927, test_loss: 0.01816817931830883\n",
      "epoch: 670, train_loss: 0.0009707203868817052, valid_loss: 0.0035998725215904415, test_loss: 0.01819007284939289\n",
      "epoch: 671, train_loss: 0.0009697237148192589, valid_loss: 0.0035727583648016057, test_loss: 0.018246324732899666\n",
      "epoch: 672, train_loss: 0.00097044499884562, valid_loss: 0.0035557524921993413, test_loss: 0.018270589411258698\n",
      "epoch: 673, train_loss: 0.0009693276218098143, valid_loss: 0.003554470061014096, test_loss: 0.018232012167572975\n",
      "epoch: 674, train_loss: 0.0009696045308373868, valid_loss: 0.0035518828857069216, test_loss: 0.018233580514788628\n",
      "epoch: 675, train_loss: 0.00096784537890926, valid_loss: 0.0035573249333538115, test_loss: 0.018219182267785072\n",
      "epoch: 676, train_loss: 0.0009682246726816115, valid_loss: 0.003549774002749473, test_loss: 0.018237486481666565\n",
      "epoch: 677, train_loss: 0.0009662136739200872, valid_loss: 0.0035636028042063117, test_loss: 0.01820620521903038\n",
      "epoch: 678, train_loss: 0.000966108634399817, valid_loss: 0.0035338181575449803, test_loss: 0.018232842907309532\n",
      "epoch: 679, train_loss: 0.0009656926318395721, valid_loss: 0.003547178484344234, test_loss: 0.018246492370963097\n",
      "epoch: 680, train_loss: 0.0009658688643906752, valid_loss: 0.0035576164761247733, test_loss: 0.018218878656625748\n",
      "epoch: 681, train_loss: 0.0009649096730003214, valid_loss: 0.0035456029193786285, test_loss: 0.01821293495595455\n",
      "epoch: 682, train_loss: 0.000964869664091131, valid_loss: 0.003529396820037315, test_loss: 0.018228329718112946\n",
      "epoch: 683, train_loss: 0.0009632904244505841, valid_loss: 0.003531808266416192, test_loss: 0.01822087913751602\n",
      "epoch: 684, train_loss: 0.0009637506445869803, valid_loss: 0.0035661448103686175, test_loss: 0.01820693537592888\n",
      "epoch: 685, train_loss: 0.0009627734113525113, valid_loss: 0.0035230989645545683, test_loss: 0.01823626644909382\n",
      "epoch: 686, train_loss: 0.0009617462055757642, valid_loss: 0.0035188476322218776, test_loss: 0.018213393166661263\n",
      "epoch: 687, train_loss: 0.0009632157035293462, valid_loss: 0.003532033045000086, test_loss: 0.018227433785796165\n",
      "epoch: 688, train_loss: 0.0009611490202824707, valid_loss: 0.003536097356118262, test_loss: 0.018244396895170212\n",
      "epoch: 689, train_loss: 0.0009614471801678123, valid_loss: 0.003521035502975186, test_loss: 0.018207263201475143\n",
      "epoch: 690, train_loss: 0.00096020157671655, valid_loss: 0.003512194554787129, test_loss: 0.018242226913571358\n",
      "epoch: 691, train_loss: 0.0009604423160097846, valid_loss: 0.0035122366777310767, test_loss: 0.018218830227851868\n",
      "epoch: 692, train_loss: 0.0009589096551518078, valid_loss: 0.0035222787021969757, test_loss: 0.018217377364635468\n",
      "epoch: 693, train_loss: 0.0009588679480973793, valid_loss: 0.003503503161482513, test_loss: 0.018234768882393837\n",
      "epoch: 694, train_loss: 0.000958403805270791, valid_loss: 0.003522401578569164, test_loss: 0.01819474808871746\n",
      "epoch: 695, train_loss: 0.0009566445698273246, valid_loss: 0.003520548479476323, test_loss: 0.01821533404290676\n",
      "epoch: 696, train_loss: 0.0009562810705772237, valid_loss: 0.003502786043100059, test_loss: 0.0182094294577837\n",
      "epoch: 697, train_loss: 0.0009569001950971458, valid_loss: 0.0035113474975029626, test_loss: 0.018249470740556717\n",
      "epoch: 698, train_loss: 0.0009557465319116802, valid_loss: 0.00353827290625001, test_loss: 0.018166881054639816\n",
      "epoch: 699, train_loss: 0.0009564040861416446, valid_loss: 0.003504067639975498, test_loss: 0.018215980380773544\n",
      "epoch: 700, train_loss: 0.000954973220359534, valid_loss: 0.0035005754907615483, test_loss: 0.018224382773041725\n",
      "epoch: 701, train_loss: 0.0009538401930552462, valid_loss: 0.0034913324440519014, test_loss: 0.018237978219985962\n",
      "epoch: 702, train_loss: 0.000954884699161124, valid_loss: 0.003490726366483917, test_loss: 0.018215393647551537\n",
      "epoch: 703, train_loss: 0.0009535638863504257, valid_loss: 0.003482391048843662, test_loss: 0.018241392448544502\n",
      "epoch: 704, train_loss: 0.0009537754256440246, valid_loss: 0.003483424176617215, test_loss: 0.01821305602788925\n",
      "epoch: 705, train_loss: 0.000952802765507089, valid_loss: 0.003505483347301682, test_loss: 0.018236329779028893\n",
      "epoch: 706, train_loss: 0.000954267080448082, valid_loss: 0.003487400341934214, test_loss: 0.01819133572280407\n",
      "epoch: 707, train_loss: 0.0009515660197433571, valid_loss: 0.003477606534337004, test_loss: 0.018238335847854614\n",
      "epoch: 708, train_loss: 0.0009519410365180153, valid_loss: 0.003498295166840156, test_loss: 0.01818610168993473\n",
      "epoch: 709, train_loss: 0.0009507751978083473, valid_loss: 0.003478905593510717, test_loss: 0.01822669804096222\n",
      "epoch: 710, train_loss: 0.0009492167433642823, valid_loss: 0.003473459781768421, test_loss: 0.018259607255458832\n",
      "epoch: 711, train_loss: 0.0009508174894701527, valid_loss: 0.0034693015622906387, test_loss: 0.018205683678388596\n",
      "epoch: 712, train_loss: 0.0009490761642708727, valid_loss: 0.00345745641971007, test_loss: 0.018237289041280746\n",
      "epoch: 713, train_loss: 0.000949063634439169, valid_loss: 0.0034980015479959548, test_loss: 0.018198490142822266\n",
      "epoch: 714, train_loss: 0.0009482043653564608, valid_loss: 0.00347416700484852, test_loss: 0.01820499636232853\n",
      "epoch: 715, train_loss: 0.0009477538912071158, valid_loss: 0.003454567321265737, test_loss: 0.01825772412121296\n",
      "epoch: 716, train_loss: 0.0009473906475645692, valid_loss: 0.003466099714084218, test_loss: 0.018224891275167465\n",
      "epoch: 717, train_loss: 0.0009485758137484283, valid_loss: 0.003452144757223626, test_loss: 0.018208486959338188\n",
      "epoch: 718, train_loss: 0.0009462783092875843, valid_loss: 0.003472005521568159, test_loss: 0.018210483714938164\n",
      "epoch: 719, train_loss: 0.0009452346788273882, valid_loss: 0.003437440337923666, test_loss: 0.018240144476294518\n",
      "epoch: 720, train_loss: 0.0009451562149004768, valid_loss: 0.003449901065323502, test_loss: 0.01824243552982807\n",
      "epoch: 721, train_loss: 0.0009447721050768766, valid_loss: 0.0034469496264743307, test_loss: 0.018245935440063477\n",
      "epoch: 722, train_loss: 0.0009442412992939353, valid_loss: 0.0034356819038900235, test_loss: 0.018220899626612663\n",
      "epoch: 723, train_loss: 0.0009454828003704872, valid_loss: 0.003467779897619039, test_loss: 0.01820036955177784\n",
      "epoch: 724, train_loss: 0.0009439958628955419, valid_loss: 0.0034353633721669516, test_loss: 0.018244655802845955\n",
      "epoch: 725, train_loss: 0.0009430639379981743, valid_loss: 0.0034507555149806044, test_loss: 0.018221894279122353\n",
      "epoch: 726, train_loss: 0.0009422113121811138, valid_loss: 0.003442704056700071, test_loss: 0.018228517845273018\n",
      "epoch: 727, train_loss: 0.0009415500959300476, valid_loss: 0.0034210447726460793, test_loss: 0.018241578713059425\n",
      "epoch: 728, train_loss: 0.000941855787380558, valid_loss: 0.0034364754295287034, test_loss: 0.018215361982584\n",
      "epoch: 729, train_loss: 0.0009415736700327176, valid_loss: 0.00342292672333618, test_loss: 0.018222777172923088\n",
      "epoch: 730, train_loss: 0.0009422400338656229, valid_loss: 0.0034350516313376525, test_loss: 0.018232548609375954\n",
      "epoch: 731, train_loss: 0.0009407105516520855, valid_loss: 0.003437807608861476, test_loss: 0.018203821033239365\n",
      "epoch: 732, train_loss: 0.0009411564881346472, valid_loss: 0.0034304339011820653, test_loss: 0.01821557804942131\n",
      "epoch: 733, train_loss: 0.0009395870299119016, valid_loss: 0.0034325046193165085, test_loss: 0.018198268488049507\n",
      "epoch: 734, train_loss: 0.0009386268236836338, valid_loss: 0.003437766475447764, test_loss: 0.0182375255972147\n",
      "epoch: 735, train_loss: 0.0009393149748970957, valid_loss: 0.003451881134727349, test_loss: 0.018226010724902153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 736, train_loss: 0.000938481947346388, valid_loss: 0.003418246730385969, test_loss: 0.01823294907808304\n",
      "epoch: 737, train_loss: 0.0009374896813507961, valid_loss: 0.0034132083140624068, test_loss: 0.018243713304400444\n",
      "epoch: 738, train_loss: 0.0009380900225116183, valid_loss: 0.003442191635258496, test_loss: 0.018210066482424736\n",
      "epoch: 739, train_loss: 0.0009375921217724681, valid_loss: 0.003438864756996433, test_loss: 0.018203049898147583\n",
      "epoch: 740, train_loss: 0.0009355810344340684, valid_loss: 0.0033959862388049564, test_loss: 0.01824420690536499\n",
      "epoch: 741, train_loss: 0.0009358794398038932, valid_loss: 0.0034026771512192986, test_loss: 0.01824978180229664\n",
      "epoch: 742, train_loss: 0.0009357959244643216, valid_loss: 0.003394224952595929, test_loss: 0.018244314938783646\n",
      "epoch: 743, train_loss: 0.0009353303340385141, valid_loss: 0.003414793793732921, test_loss: 0.018230484798550606\n",
      "epoch: 744, train_loss: 0.0009348227100892235, valid_loss: 0.003376911859959364, test_loss: 0.018274866044521332\n",
      "epoch: 745, train_loss: 0.0009354744891069182, valid_loss: 0.003408585791476071, test_loss: 0.018210381269454956\n",
      "epoch: 746, train_loss: 0.0009341622631915885, valid_loss: 0.003398947805787126, test_loss: 0.01822102628648281\n",
      "epoch: 747, train_loss: 0.000933520969145162, valid_loss: 0.003374761804783096, test_loss: 0.018234286457300186\n",
      "epoch: 748, train_loss: 0.0009325977298431098, valid_loss: 0.0033752693270798773, test_loss: 0.01828124187886715\n",
      "epoch: 749, train_loss: 0.0009331967929661598, valid_loss: 0.0033695655680882433, test_loss: 0.018227988854050636\n",
      "epoch: 750, train_loss: 0.0009318338428704959, valid_loss: 0.0034105755233516297, test_loss: 0.018214035779237747\n",
      "epoch: 751, train_loss: 0.0009316182999021333, valid_loss: 0.0033752206169689694, test_loss: 0.0182498712092638\n",
      "epoch: 752, train_loss: 0.0009307889748648133, valid_loss: 0.0033864091771344342, test_loss: 0.018218984827399254\n",
      "epoch: 753, train_loss: 0.0009313445189036429, valid_loss: 0.0033854166783081987, test_loss: 0.01820581592619419\n",
      "epoch: 754, train_loss: 0.000930148976571534, valid_loss: 0.00338532478781417, test_loss: 0.01819949597120285\n",
      "epoch: 755, train_loss: 0.0009291240333012589, valid_loss: 0.0033627065810530135, test_loss: 0.018267875537276268\n",
      "epoch: 756, train_loss: 0.000929778593632838, valid_loss: 0.003360914687315623, test_loss: 0.01823272928595543\n",
      "epoch: 757, train_loss: 0.0009283832193392774, valid_loss: 0.0033722064496638873, test_loss: 0.018235469236969948\n",
      "epoch: 758, train_loss: 0.0009282774458963262, valid_loss: 0.003386877360753715, test_loss: 0.018245672807097435\n",
      "epoch: 759, train_loss: 0.0009297841170337051, valid_loss: 0.0033630936522968113, test_loss: 0.018221750855445862\n",
      "epoch: 760, train_loss: 0.0009275692585936707, valid_loss: 0.0033637655821318426, test_loss: 0.018212657421827316\n",
      "epoch: 761, train_loss: 0.0009266611659372954, valid_loss: 0.0033613141470899186, test_loss: 0.018244553357362747\n",
      "epoch: 762, train_loss: 0.0009272966923637558, valid_loss: 0.003377273193715761, test_loss: 0.01821024715900421\n",
      "epoch: 763, train_loss: 0.0009263988746249157, valid_loss: 0.0033597363314280906, test_loss: 0.018202709034085274\n",
      "epoch: 764, train_loss: 0.0009251499963600351, valid_loss: 0.003357034040770183, test_loss: 0.018216822296380997\n",
      "epoch: 765, train_loss: 0.0009250492996374226, valid_loss: 0.003340653066212932, test_loss: 0.01823006570339203\n",
      "epoch: 766, train_loss: 0.00092593212004589, valid_loss: 0.003325025133866196, test_loss: 0.018258165568113327\n",
      "epoch: 767, train_loss: 0.0009242463958166215, valid_loss: 0.003343158032900343, test_loss: 0.01819499582052231\n",
      "epoch: 768, train_loss: 0.0009245783768837218, valid_loss: 0.0033522674930281937, test_loss: 0.018240759149193764\n",
      "epoch: 769, train_loss: 0.0009237433879879181, valid_loss: 0.0033343195100314915, test_loss: 0.01827302761375904\n",
      "epoch: 770, train_loss: 0.0009234162558720487, valid_loss: 0.003357822075486183, test_loss: 0.018259940668940544\n",
      "epoch: 771, train_loss: 0.0009226808779994431, valid_loss: 0.003334765807570269, test_loss: 0.018252000212669373\n",
      "epoch: 772, train_loss: 0.0009226506783465004, valid_loss: 0.003336471777098874, test_loss: 0.018224744126200676\n",
      "epoch: 773, train_loss: 0.0009221543188449805, valid_loss: 0.00335133159145092, test_loss: 0.018212372437119484\n",
      "epoch: 774, train_loss: 0.0009220677817685772, valid_loss: 0.003338604078938564, test_loss: 0.018223324790596962\n",
      "epoch: 775, train_loss: 0.0009207860414829591, valid_loss: 0.0033502288861200213, test_loss: 0.0182499922811985\n",
      "epoch: 776, train_loss: 0.0009202606768509293, valid_loss: 0.0033423895171533027, test_loss: 0.018228916451334953\n",
      "epoch: 777, train_loss: 0.0009209912766338042, valid_loss: 0.0033313376479782164, test_loss: 0.0182177796959877\n",
      "epoch: 778, train_loss: 0.0009205353841104585, valid_loss: 0.003339302163415899, test_loss: 0.01825868710875511\n",
      "epoch: 779, train_loss: 0.000919423698771583, valid_loss: 0.003312992543214932, test_loss: 0.018276821821928024\n",
      "epoch: 780, train_loss: 0.0009202816645088403, valid_loss: 0.0033275301684625447, test_loss: 0.01824752613902092\n",
      "epoch: 781, train_loss: 0.0009205236919629185, valid_loss: 0.0033056427200790495, test_loss: 0.018257809802889824\n",
      "epoch: 782, train_loss: 0.0009178568092782212, valid_loss: 0.0033199979031148055, test_loss: 0.018248775973916054\n",
      "epoch: 783, train_loss: 0.0009174523419579086, valid_loss: 0.003321312600746751, test_loss: 0.01823166012763977\n",
      "epoch: 784, train_loss: 0.0009170008909321674, valid_loss: 0.003309464421666538, test_loss: 0.018264058977365494\n",
      "epoch: 785, train_loss: 0.0009166519258819196, valid_loss: 0.0033205889048986137, test_loss: 0.01824796572327614\n",
      "epoch: 786, train_loss: 0.0009160601893318412, valid_loss: 0.003306037998603036, test_loss: 0.01822751760482788\n",
      "epoch: 787, train_loss: 0.000916519569253306, valid_loss: 0.0033114376516702273, test_loss: 0.018264925107359886\n",
      "epoch: 788, train_loss: 0.0009156955790746471, valid_loss: 0.0033187404236135385, test_loss: 0.01824084296822548\n",
      "epoch: 789, train_loss: 0.000915399841134153, valid_loss: 0.003301308451530834, test_loss: 0.018226347863674164\n",
      "epoch: 790, train_loss: 0.0009146089660530181, valid_loss: 0.003312827534197519, test_loss: 0.018245037645101547\n",
      "epoch: 791, train_loss: 0.0009147068030079422, valid_loss: 0.0033133913141985736, test_loss: 0.018236512318253517\n",
      "epoch: 792, train_loss: 0.000914383778328319, valid_loss: 0.00329576781950891, test_loss: 0.018249819055199623\n",
      "epoch: 793, train_loss: 0.0009138465114950162, valid_loss: 0.003335921404262384, test_loss: 0.018223624676465988\n",
      "epoch: 794, train_loss: 0.0009141338574092673, valid_loss: 0.0033040603545183935, test_loss: 0.018262092024087906\n",
      "epoch: 795, train_loss: 0.0009124017492665545, valid_loss: 0.0032986776010754206, test_loss: 0.01823698729276657\n",
      "epoch: 796, train_loss: 0.0009121375573475076, valid_loss: 0.003290712076704949, test_loss: 0.018237220123410225\n",
      "epoch: 797, train_loss: 0.0009125937814014437, valid_loss: 0.0033002548346606395, test_loss: 0.018251631408929825\n",
      "epoch: 798, train_loss: 0.0009115637285108476, valid_loss: 0.0032772597235937915, test_loss: 0.018261728808283806\n",
      "epoch: 799, train_loss: 0.0009110444326601598, valid_loss: 0.0032816927608413002, test_loss: 0.018257295712828636\n",
      "epoch: 800, train_loss: 0.0009113815992706171, valid_loss: 0.0032780946736844876, test_loss: 0.018260633572936058\n",
      "epoch: 801, train_loss: 0.0009120825004925871, valid_loss: 0.0032860931823961437, test_loss: 0.01826382428407669\n",
      "epoch: 802, train_loss: 0.0009102865182997092, valid_loss: 0.003281922972140213, test_loss: 0.01825167052447796\n",
      "epoch: 803, train_loss: 0.0009096600115299225, valid_loss: 0.0032824783314329884, test_loss: 0.01824461854994297\n",
      "epoch: 804, train_loss: 0.0009085674461422731, valid_loss: 0.0032734447546924153, test_loss: 0.01825653947889805\n",
      "epoch: 805, train_loss: 0.0009088277634557174, valid_loss: 0.003282587257369111, test_loss: 0.018238702788949013\n",
      "epoch: 806, train_loss: 0.0009078106782196656, valid_loss: 0.0032492559791232147, test_loss: 0.01828058995306492\n",
      "epoch: 807, train_loss: 0.0009068458330938997, valid_loss: 0.003255729602339367, test_loss: 0.018274623900651932\n",
      "epoch: 808, train_loss: 0.0009092838609712603, valid_loss: 0.0032637121718532094, test_loss: 0.018278349190950394\n",
      "epoch: 809, train_loss: 0.0009067800078217102, valid_loss: 0.0032616655710929385, test_loss: 0.018260687589645386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 810, train_loss: 0.0009068854067110173, valid_loss: 0.0032517382642254233, test_loss: 0.01826292835175991\n",
      "epoch: 811, train_loss: 0.0009064042520628351, valid_loss: 0.003273890276129047, test_loss: 0.018245983868837357\n",
      "epoch: 812, train_loss: 0.0009067626948387402, valid_loss: 0.0032769363994399705, test_loss: 0.018277879804372787\n",
      "epoch: 813, train_loss: 0.0009051459973030117, valid_loss: 0.003254804042323182, test_loss: 0.01826336607336998\n",
      "epoch: 814, train_loss: 0.0009052099751146591, valid_loss: 0.003237934327141071, test_loss: 0.018305528908967972\n",
      "epoch: 815, train_loss: 0.0009055963676908742, valid_loss: 0.003249015756106625, test_loss: 0.018276099115610123\n",
      "epoch: 816, train_loss: 0.0009042655887163204, valid_loss: 0.003258386937280496, test_loss: 0.0182668324559927\n",
      "epoch: 817, train_loss: 0.0009054935324694152, valid_loss: 0.003257393545936793, test_loss: 0.01826518028974533\n",
      "epoch: 818, train_loss: 0.0009034308301203924, valid_loss: 0.003263060934841633, test_loss: 0.018253741785883904\n",
      "epoch: 819, train_loss: 0.0009043147665974886, valid_loss: 0.0032491193269379437, test_loss: 0.018274471163749695\n",
      "epoch: 820, train_loss: 0.0009028619615887494, valid_loss: 0.0032504460541531444, test_loss: 0.018290529027581215\n",
      "epoch: 821, train_loss: 0.0009016349997735866, valid_loss: 0.003250026551540941, test_loss: 0.018290428444743156\n",
      "epoch: 822, train_loss: 0.0009021017113295586, valid_loss: 0.0032334604960245392, test_loss: 0.018266553059220314\n",
      "epoch: 823, train_loss: 0.0009034760751882973, valid_loss: 0.0032392920499357083, test_loss: 0.018245145678520203\n",
      "epoch: 824, train_loss: 0.000900426155725575, valid_loss: 0.003232568094972521, test_loss: 0.018290752544999123\n",
      "epoch: 825, train_loss: 0.0009001607085456667, valid_loss: 0.003242777253035456, test_loss: 0.018250713124871254\n",
      "epoch: 826, train_loss: 0.0009007587543000345, valid_loss: 0.0032357724267058074, test_loss: 0.018265429884195328\n",
      "epoch: 827, train_loss: 0.0008997903357300421, valid_loss: 0.0032210443944980702, test_loss: 0.018298683688044548\n",
      "epoch: 828, train_loss: 0.0008997111201650747, valid_loss: 0.0032245092637216053, test_loss: 0.018251845613121986\n",
      "epoch: 829, train_loss: 0.0008988899442002825, valid_loss: 0.003211560833733529, test_loss: 0.018282247707247734\n",
      "epoch: 830, train_loss: 0.0008989341491225945, valid_loss: 0.0032352268268975117, test_loss: 0.01827659085392952\n",
      "epoch: 831, train_loss: 0.0008991101589661254, valid_loss: 0.0032271178594479957, test_loss: 0.01824376918375492\n",
      "epoch: 832, train_loss: 0.0008990078906366682, valid_loss: 0.0032220338665259383, test_loss: 0.01825285516679287\n",
      "epoch: 833, train_loss: 0.000897059576464412, valid_loss: 0.0032112158369272947, test_loss: 0.018260685727000237\n",
      "epoch: 834, train_loss: 0.0008977259428280851, valid_loss: 0.0032176765186401704, test_loss: 0.018291791900992393\n",
      "epoch: 835, train_loss: 0.0008964224354080532, valid_loss: 0.0032196714309975505, test_loss: 0.018282882869243622\n",
      "epoch: 836, train_loss: 0.0008958833008680655, valid_loss: 0.0032402676491377256, test_loss: 0.018273087218403816\n",
      "epoch: 837, train_loss: 0.0008969683296290104, valid_loss: 0.0032269503960075476, test_loss: 0.018263133242726326\n",
      "epoch: 838, train_loss: 0.0008959294468893305, valid_loss: 0.0032063660910353065, test_loss: 0.01828838884830475\n",
      "epoch: 839, train_loss: 0.0008947743386354135, valid_loss: 0.0031960559523819634, test_loss: 0.01830301806330681\n",
      "epoch: 840, train_loss: 0.0008949826891615015, valid_loss: 0.003216901716465751, test_loss: 0.018286332488059998\n",
      "epoch: 841, train_loss: 0.0008941966756854368, valid_loss: 0.003206787630915642, test_loss: 0.018282180652022362\n",
      "epoch: 842, train_loss: 0.0008940734045134615, valid_loss: 0.0031997988116927445, test_loss: 0.01828240044414997\n",
      "epoch: 843, train_loss: 0.0008941425045992693, valid_loss: 0.0031905315020897738, test_loss: 0.018315082415938377\n",
      "epoch: 844, train_loss: 0.0008935495229114009, valid_loss: 0.0031990063143894076, test_loss: 0.018283549696207047\n",
      "epoch: 845, train_loss: 0.000893057536576753, valid_loss: 0.003189675024865816, test_loss: 0.01831717975437641\n",
      "epoch: 846, train_loss: 0.0008930996966386294, valid_loss: 0.0032105359714478254, test_loss: 0.018310287967324257\n",
      "epoch: 847, train_loss: 0.0008927764922744878, valid_loss: 0.003200459061190486, test_loss: 0.018295085057616234\n",
      "epoch: 848, train_loss: 0.0008924730734535209, valid_loss: 0.0031821099789037057, test_loss: 0.018300898373126984\n",
      "epoch: 849, train_loss: 0.0008922100051954064, valid_loss: 0.003178464403996865, test_loss: 0.018305519595742226\n",
      "epoch: 850, train_loss: 0.0008913115446415285, valid_loss: 0.0032105533755384386, test_loss: 0.01828460767865181\n",
      "epoch: 851, train_loss: 0.000890696254234923, valid_loss: 0.00317637954140082, test_loss: 0.018295040354132652\n",
      "epoch: 852, train_loss: 0.0008901388880432299, valid_loss: 0.0031799761345610023, test_loss: 0.018296895548701286\n",
      "epoch: 853, train_loss: 0.0008907171659697981, valid_loss: 0.003183530915218095, test_loss: 0.018301114439964294\n",
      "epoch: 854, train_loss: 0.0008906587811551341, valid_loss: 0.003196230022391925, test_loss: 0.01827053166925907\n",
      "epoch: 855, train_loss: 0.0008895640291840485, valid_loss: 0.0031822332142231366, test_loss: 0.01827753148972988\n",
      "epoch: 856, train_loss: 0.0008887584528723812, valid_loss: 0.003182990263060977, test_loss: 0.018294163048267365\n",
      "epoch: 857, train_loss: 0.0008881676248679667, valid_loss: 0.003166244695118318, test_loss: 0.01831677369773388\n",
      "epoch: 858, train_loss: 0.0008872836630831918, valid_loss: 0.003193447987238566, test_loss: 0.018299134448170662\n",
      "epoch: 859, train_loss: 0.0008888315262637385, valid_loss: 0.003180897697651138, test_loss: 0.018292494118213654\n",
      "epoch: 860, train_loss: 0.0008878546928905922, valid_loss: 0.003193627984728664, test_loss: 0.01826648786664009\n",
      "epoch: 861, train_loss: 0.0008879767214555455, valid_loss: 0.003160249713497857, test_loss: 0.01832776702940464\n",
      "epoch: 862, train_loss: 0.0008882324315060902, valid_loss: 0.003146093552156041, test_loss: 0.0183237474411726\n",
      "epoch: 863, train_loss: 0.0008873610413106887, valid_loss: 0.003168780579774951, test_loss: 0.018307430669665337\n",
      "epoch: 864, train_loss: 0.0008865479840492101, valid_loss: 0.0031659316058115414, test_loss: 0.01830763928592205\n",
      "epoch: 865, train_loss: 0.000885766655024465, valid_loss: 0.0031508544149498143, test_loss: 0.018315719440579414\n",
      "epoch: 866, train_loss: 0.0008849240864789033, valid_loss: 0.0031689897393031665, test_loss: 0.0183182992041111\n",
      "epoch: 867, train_loss: 0.0008845758730667117, valid_loss: 0.0031557008624076843, test_loss: 0.018291961401700974\n",
      "epoch: 868, train_loss: 0.000884556801200075, valid_loss: 0.003155242583792036, test_loss: 0.018327418714761734\n",
      "epoch: 869, train_loss: 0.0008844518603797516, valid_loss: 0.0031767868398067853, test_loss: 0.018311649560928345\n",
      "epoch: 870, train_loss: 0.0008840038233598614, valid_loss: 0.0031463423219975084, test_loss: 0.01834224909543991\n",
      "epoch: 871, train_loss: 0.0008838425172776308, valid_loss: 0.0031610455092353127, test_loss: 0.018297841772437096\n",
      "epoch: 872, train_loss: 0.0008832459241840179, valid_loss: 0.003147370987183725, test_loss: 0.018327999860048294\n",
      "epoch: 873, train_loss: 0.0008835502895121665, valid_loss: 0.0031591003644280136, test_loss: 0.018297646194696426\n",
      "epoch: 874, train_loss: 0.0008828276464634615, valid_loss: 0.0031499381293542683, test_loss: 0.018331699073314667\n",
      "epoch: 875, train_loss: 0.000881118502508363, valid_loss: 0.0031722429557703435, test_loss: 0.018289173021912575\n",
      "epoch: 876, train_loss: 0.0008828594151924809, valid_loss: 0.003153529037566235, test_loss: 0.018288427963852882\n",
      "epoch: 877, train_loss: 0.0008813092211747299, valid_loss: 0.0031404203133812794, test_loss: 0.018312351778149605\n",
      "epoch: 878, train_loss: 0.0008816259948577246, valid_loss: 0.0031239765618617335, test_loss: 0.018343031406402588\n",
      "epoch: 879, train_loss: 0.0008807737714327548, valid_loss: 0.0031361179620337984, test_loss: 0.01831074431538582\n",
      "epoch: 880, train_loss: 0.0008794446096188673, valid_loss: 0.003159087151288986, test_loss: 0.018299473449587822\n",
      "epoch: 881, train_loss: 0.0008801716612651944, valid_loss: 0.0031353171895413348, test_loss: 0.01833239756524563\n",
      "epoch: 882, train_loss: 0.0008799204926775849, valid_loss: 0.0031342065582672753, test_loss: 0.018310196697711945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 883, train_loss: 0.0008793412505284599, valid_loss: 0.0031379292389222733, test_loss: 0.018352091312408447\n",
      "epoch: 884, train_loss: 0.0008790749023951914, valid_loss: 0.00313113119530802, test_loss: 0.018331045284867287\n",
      "epoch: 885, train_loss: 0.0008799206470544248, valid_loss: 0.003129555504225815, test_loss: 0.018348850309848785\n",
      "epoch: 886, train_loss: 0.0008778671136773798, valid_loss: 0.0031494160260384283, test_loss: 0.018319198861718178\n",
      "epoch: 887, train_loss: 0.0008785777381094902, valid_loss: 0.003138079831842333, test_loss: 0.01835804618895054\n",
      "epoch: 888, train_loss: 0.0008776900434421133, valid_loss: 0.003129628060075144, test_loss: 0.018324416130781174\n",
      "epoch: 889, train_loss: 0.0008775089619397793, valid_loss: 0.0031223919650074095, test_loss: 0.018333809450268745\n",
      "epoch: 890, train_loss: 0.0008767970999621826, valid_loss: 0.003126292741702249, test_loss: 0.018327994272112846\n",
      "epoch: 891, train_loss: 0.0008765358386723244, valid_loss: 0.0031171031296253204, test_loss: 0.0183180533349514\n",
      "epoch: 892, train_loss: 0.0008774765073726683, valid_loss: 0.0031232359275842705, test_loss: 0.018323859199881554\n",
      "epoch: 893, train_loss: 0.0008781413122525681, valid_loss: 0.0031091455991069474, test_loss: 0.01834876462817192\n",
      "epoch: 894, train_loss: 0.0008773578320751371, valid_loss: 0.0031145206497361264, test_loss: 0.018343808129429817\n",
      "epoch: 895, train_loss: 0.0008745246931262639, valid_loss: 0.0031334571540355682, test_loss: 0.01831115409731865\n",
      "epoch: 896, train_loss: 0.0008757124963702391, valid_loss: 0.003126645654750367, test_loss: 0.018334750086069107\n",
      "epoch: 897, train_loss: 0.0008745829463409989, valid_loss: 0.0031373226277840636, test_loss: 0.018343018367886543\n",
      "epoch: 898, train_loss: 0.0008751455385683347, valid_loss: 0.0031183072132989764, test_loss: 0.018344402313232422\n",
      "epoch: 899, train_loss: 0.000873235479483138, valid_loss: 0.0031025196173383542, test_loss: 0.01834520511329174\n",
      "epoch: 900, train_loss: 0.0008741794989201362, valid_loss: 0.0031053529819473624, test_loss: 0.018341751769185066\n",
      "epoch: 901, train_loss: 0.0008742277328248905, valid_loss: 0.0031006458836297193, test_loss: 0.018357431516051292\n",
      "epoch: 902, train_loss: 0.0008727935871676258, valid_loss: 0.003082044296509897, test_loss: 0.018360352143645287\n",
      "epoch: 903, train_loss: 0.0008716653177069257, valid_loss: 0.0031011356719924756, test_loss: 0.01834547147154808\n",
      "epoch: 904, train_loss: 0.0008726950340053958, valid_loss: 0.0030963106837589294, test_loss: 0.018326152116060257\n",
      "epoch: 905, train_loss: 0.000871173688210547, valid_loss: 0.003076572330125297, test_loss: 0.01837012730538845\n",
      "epoch: 906, train_loss: 0.0008701219314547336, valid_loss: 0.003069913509534672, test_loss: 0.018372217193245888\n",
      "epoch: 907, train_loss: 0.0008729411638341844, valid_loss: 0.003105332318227738, test_loss: 0.018336787819862366\n",
      "epoch: 908, train_loss: 0.0008709871225342479, valid_loss: 0.003081546562801426, test_loss: 0.01834714412689209\n",
      "epoch: 909, train_loss: 0.0008699464581339904, valid_loss: 0.003079824489153301, test_loss: 0.018340865150094032\n",
      "epoch: 910, train_loss: 0.0008699702068596431, valid_loss: 0.003095397085417062, test_loss: 0.018337789922952652\n",
      "epoch: 911, train_loss: 0.0008700890213494067, valid_loss: 0.003057144106909012, test_loss: 0.018375970423221588\n",
      "epoch: 912, train_loss: 0.0008698989657442207, valid_loss: 0.0030996011725316444, test_loss: 0.01834315061569214\n",
      "epoch: 913, train_loss: 0.0008685315134125235, valid_loss: 0.0030798683874309063, test_loss: 0.018388770520687103\n",
      "epoch: 914, train_loss: 0.0008686897724502436, valid_loss: 0.0030820921335058906, test_loss: 0.01839703693985939\n",
      "epoch: 915, train_loss: 0.0008692754604651228, valid_loss: 0.003083150339080021, test_loss: 0.018376322463154793\n",
      "epoch: 916, train_loss: 0.0008688658455629711, valid_loss: 0.003086393679647396, test_loss: 0.018335191532969475\n",
      "epoch: 917, train_loss: 0.0008672831235858409, valid_loss: 0.0030601301502125957, test_loss: 0.018382754176855087\n",
      "epoch: 918, train_loss: 0.0008686151122674346, valid_loss: 0.0030957955362585685, test_loss: 0.01835981383919716\n",
      "epoch: 919, train_loss: 0.0008681163206979957, valid_loss: 0.0030608084537864975, test_loss: 0.01836601458489895\n",
      "epoch: 920, train_loss: 0.0008668413122549006, valid_loss: 0.0030652244749944657, test_loss: 0.01835467293858528\n",
      "epoch: 921, train_loss: 0.0008675219217801224, valid_loss: 0.003059202281292528, test_loss: 0.01838848926126957\n",
      "epoch: 922, train_loss: 0.0008659477487368429, valid_loss: 0.00308069126913324, test_loss: 0.01834998093545437\n",
      "epoch: 923, train_loss: 0.0008667617727516462, valid_loss: 0.003066667054857438, test_loss: 0.01835945062339306\n",
      "epoch: 924, train_loss: 0.0008667935958921747, valid_loss: 0.003074588777963072, test_loss: 0.018365314230322838\n",
      "epoch: 925, train_loss: 0.0008656962410263393, valid_loss: 0.0030541825011217347, test_loss: 0.018405983224511147\n",
      "epoch: 926, train_loss: 0.0008652501678823129, valid_loss: 0.003087763034272939, test_loss: 0.018331432715058327\n",
      "epoch: 927, train_loss: 0.0008666934369577338, valid_loss: 0.0030609221721533686, test_loss: 0.018380053341388702\n",
      "epoch: 928, train_loss: 0.0008657453556383109, valid_loss: 0.003056008727677787, test_loss: 0.018395692110061646\n",
      "epoch: 929, train_loss: 0.0008632616701778834, valid_loss: 0.0030779831189041338, test_loss: 0.018375365063548088\n",
      "epoch: 930, train_loss: 0.0008638684293660133, valid_loss: 0.003070731123443693, test_loss: 0.01835581474006176\n",
      "epoch: 931, train_loss: 0.0008643915707184731, valid_loss: 0.0030422625325930617, test_loss: 0.018366361036896706\n",
      "epoch: 932, train_loss: 0.000862636818530281, valid_loss: 0.0030599924211855978, test_loss: 0.018381835892796516\n",
      "epoch: 933, train_loss: 0.0008635426739878628, valid_loss: 0.0030513764553082487, test_loss: 0.018391592428088188\n",
      "epoch: 934, train_loss: 0.0008628217037767172, valid_loss: 0.003030832546452681, test_loss: 0.01840023882687092\n",
      "epoch: 935, train_loss: 0.0008618480692941533, valid_loss: 0.00306142670645689, test_loss: 0.0183763038367033\n",
      "epoch: 936, train_loss: 0.0008629303597642676, valid_loss: 0.00304296039394103, test_loss: 0.01836472935974598\n",
      "epoch: 937, train_loss: 0.0008609629307023209, valid_loss: 0.003031862113857642, test_loss: 0.018421253189444542\n",
      "epoch: 938, train_loss: 0.0008615555631442238, valid_loss: 0.0030465182207990438, test_loss: 0.018391013145446777\n",
      "epoch: 939, train_loss: 0.000861025200245659, valid_loss: 0.0030410319838362434, test_loss: 0.01838582754135132\n",
      "epoch: 940, train_loss: 0.000860747828087567, valid_loss: 0.003041935880901292, test_loss: 0.018402663990855217\n",
      "epoch: 941, train_loss: 0.0008597141663219942, valid_loss: 0.0030482959894773862, test_loss: 0.018382057547569275\n",
      "epoch: 942, train_loss: 0.0008604095175700343, valid_loss: 0.003020462483012428, test_loss: 0.018408412113785744\n",
      "epoch: 943, train_loss: 0.000859359648767049, valid_loss: 0.0030422129299646863, test_loss: 0.018395675346255302\n",
      "epoch: 944, train_loss: 0.0008603812716698841, valid_loss: 0.0030281910730991513, test_loss: 0.018407758325338364\n",
      "epoch: 945, train_loss: 0.0008585017159297738, valid_loss: 0.0030368421187934778, test_loss: 0.01840066909790039\n",
      "epoch: 946, train_loss: 0.0008587193644970008, valid_loss: 0.00303372775670141, test_loss: 0.018426833674311638\n",
      "epoch: 947, train_loss: 0.000858283366339848, valid_loss: 0.003033651960625624, test_loss: 0.01843709498643875\n",
      "epoch: 948, train_loss: 0.0008569661573425907, valid_loss: 0.0030258655412277826, test_loss: 0.018413253128528595\n",
      "epoch: 949, train_loss: 0.0008573950111420583, valid_loss: 0.0030366520028716573, test_loss: 0.01841168850660324\n",
      "epoch: 950, train_loss: 0.0008582142738463438, valid_loss: 0.003028567327419296, test_loss: 0.018406648188829422\n",
      "epoch: 951, train_loss: 0.0008564230975816431, valid_loss: 0.0030543411461015544, test_loss: 0.018355103209614754\n",
      "epoch: 952, train_loss: 0.000857214425670226, valid_loss: 0.003027850810516005, test_loss: 0.01841934584081173\n",
      "epoch: 953, train_loss: 0.0008565857677477534, valid_loss: 0.0030476360213166722, test_loss: 0.018412157893180847\n",
      "epoch: 954, train_loss: 0.0008551932532218811, valid_loss: 0.003022150107426569, test_loss: 0.01841215044260025\n",
      "epoch: 955, train_loss: 0.0008561384190967226, valid_loss: 0.0030228203880445412, test_loss: 0.01841449737548828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 956, train_loss: 0.0008549581297025409, valid_loss: 0.0030193731266384325, test_loss: 0.018389977514743805\n",
      "epoch: 957, train_loss: 0.000855644486601586, valid_loss: 0.0030082119919825345, test_loss: 0.018396079540252686\n",
      "epoch: 958, train_loss: 0.000855135566899148, valid_loss: 0.0030036713578738272, test_loss: 0.018435979261994362\n",
      "epoch: 959, train_loss: 0.0008541925445846889, valid_loss: 0.0030162434365289905, test_loss: 0.01841793581843376\n",
      "epoch: 960, train_loss: 0.0008539877928105061, valid_loss: 0.0030112823587842286, test_loss: 0.018407253548502922\n",
      "epoch: 961, train_loss: 0.0008538490776276296, valid_loss: 0.0030016243108548224, test_loss: 0.018438514322042465\n",
      "epoch: 962, train_loss: 0.0008538806375683001, valid_loss: 0.0030047280597500503, test_loss: 0.018429823219776154\n",
      "epoch: 963, train_loss: 0.0008539317363022786, valid_loss: 0.00299320310781089, test_loss: 0.018443673849105835\n",
      "epoch: 964, train_loss: 0.0008521897353879783, valid_loss: 0.0029993613134138286, test_loss: 0.018426967784762383\n",
      "epoch: 965, train_loss: 0.000853156111628303, valid_loss: 0.0029981933961001537, test_loss: 0.01845281384885311\n",
      "epoch: 966, train_loss: 0.0008535764620477414, valid_loss: 0.002994738722918555, test_loss: 0.01843009889125824\n",
      "epoch: 967, train_loss: 0.0008533770830937377, valid_loss: 0.0030131633684504777, test_loss: 0.01843671314418316\n",
      "epoch: 968, train_loss: 0.0008519856390826728, valid_loss: 0.0030103949441884956, test_loss: 0.018416086211800575\n",
      "epoch: 969, train_loss: 0.0008513248454698402, valid_loss: 0.002989720737483973, test_loss: 0.01846979185938835\n",
      "epoch: 970, train_loss: 0.0008532390955066228, valid_loss: 0.002995551951850454, test_loss: 0.018389252945780754\n",
      "epoch: 971, train_loss: 0.000851603541219526, valid_loss: 0.0030038119099723795, test_loss: 0.018424371257424355\n",
      "epoch: 972, train_loss: 0.0008511950297321638, valid_loss: 0.003031071314277748, test_loss: 0.018417246639728546\n",
      "epoch: 973, train_loss: 0.0008516935555709769, valid_loss: 0.002984332551325982, test_loss: 0.018432794138789177\n",
      "epoch: 974, train_loss: 0.000850344069160359, valid_loss: 0.002996957026577244, test_loss: 0.01842883974313736\n",
      "epoch: 975, train_loss: 0.0008503954564018742, valid_loss: 0.002984442369779572, test_loss: 0.018453791737556458\n",
      "epoch: 976, train_loss: 0.0008488806931342443, valid_loss: 0.0029758942934374013, test_loss: 0.018466277047991753\n",
      "epoch: 977, train_loss: 0.0008500983164159824, valid_loss: 0.0029867637106993548, test_loss: 0.01844766177237034\n",
      "epoch: 978, train_loss: 0.0008490679218717244, valid_loss: 0.002992520749103278, test_loss: 0.01844041980803013\n",
      "epoch: 979, train_loss: 0.0008493351608352816, valid_loss: 0.0030012894809866944, test_loss: 0.0184312891215086\n",
      "epoch: 980, train_loss: 0.000848569552940519, valid_loss: 0.002978955667155484, test_loss: 0.01843741163611412\n",
      "epoch: 981, train_loss: 0.0008494912737818515, valid_loss: 0.0029780959788089, test_loss: 0.018460076302289963\n",
      "epoch: 982, train_loss: 0.000849014125339201, valid_loss: 0.0029735049756709486, test_loss: 0.018473198637366295\n",
      "epoch: 983, train_loss: 0.0008474919494524922, valid_loss: 0.002988996731194978, test_loss: 0.018439142033457756\n",
      "epoch: 984, train_loss: 0.0008467646194216998, valid_loss: 0.0030092867479349175, test_loss: 0.01840689592063427\n",
      "epoch: 985, train_loss: 0.0008463084206754422, valid_loss: 0.0029773544132088623, test_loss: 0.01844014786183834\n",
      "epoch: 986, train_loss: 0.0008462763178850646, valid_loss: 0.0029933358018752187, test_loss: 0.018472639843821526\n",
      "epoch: 987, train_loss: 0.0008467041037004927, valid_loss: 0.0029745069526446364, test_loss: 0.01847551017999649\n",
      "epoch: 988, train_loss: 0.0008463660184213001, valid_loss: 0.0030048354140793285, test_loss: 0.01842961274087429\n",
      "epoch: 989, train_loss: 0.0008446654056846771, valid_loss: 0.00298076782686015, test_loss: 0.018439998850226402\n",
      "epoch: 990, train_loss: 0.0008453557510738788, valid_loss: 0.002995447750436142, test_loss: 0.018479693681001663\n",
      "epoch: 991, train_loss: 0.0008442982115134921, valid_loss: 0.003002435939076046, test_loss: 0.01845826394855976\n",
      "epoch: 992, train_loss: 0.0008448390745679321, valid_loss: 0.0029937789076939225, test_loss: 0.01847275160253048\n",
      "epoch: 993, train_loss: 0.0008446806965841223, valid_loss: 0.002970418475645905, test_loss: 0.018490154296159744\n",
      "epoch: 994, train_loss: 0.0008441142069742731, valid_loss: 0.002961250478013729, test_loss: 0.01844709925353527\n",
      "epoch: 995, train_loss: 0.0008432169687812743, valid_loss: 0.0029476163326762617, test_loss: 0.018479645252227783\n",
      "epoch: 996, train_loss: 0.0008449835232054086, valid_loss: 0.00295653449332652, test_loss: 0.018481848761439323\n",
      "epoch: 997, train_loss: 0.0008436412849139584, valid_loss: 0.0029666069409965226, test_loss: 0.018470657989382744\n",
      "epoch: 998, train_loss: 0.000842774307112331, valid_loss: 0.0029536341705049076, test_loss: 0.01846873201429844\n",
      "epoch: 999, train_loss: 0.0008434696355834603, valid_loss: 0.0029727652145083994, test_loss: 0.01849627122282982\n",
      "epoch: 1000, train_loss: 0.0008419186671507423, valid_loss: 0.0029765863437205553, test_loss: 0.01846553012728691\n",
      "epoch: 1001, train_loss: 0.0008427445554053006, valid_loss: 0.002959585671002666, test_loss: 0.018491044640541077\n",
      "epoch: 1002, train_loss: 0.0008416685589548687, valid_loss: 0.002949987664275492, test_loss: 0.018472924828529358\n",
      "epoch: 1003, train_loss: 0.0008416867336642969, valid_loss: 0.0029602856084238738, test_loss: 0.018508056178689003\n",
      "epoch: 1004, train_loss: 0.0008430078957209606, valid_loss: 0.002948166928642119, test_loss: 0.018495479598641396\n",
      "epoch: 1005, train_loss: 0.0008407241396565477, valid_loss: 0.002955190100086232, test_loss: 0.018512539565563202\n",
      "epoch: 1006, train_loss: 0.0008409618901129326, valid_loss: 0.0029390020742236325, test_loss: 0.018461469560861588\n",
      "epoch: 1007, train_loss: 0.0008407765670436556, valid_loss: 0.0029286340674540647, test_loss: 0.018517732620239258\n",
      "epoch: 1008, train_loss: 0.0008399434838160548, valid_loss: 0.0029516846601230404, test_loss: 0.018492626026272774\n",
      "epoch: 1009, train_loss: 0.0008407890918138235, valid_loss: 0.0029297797785451016, test_loss: 0.018489854410290718\n",
      "epoch: 1010, train_loss: 0.0008400984098324957, valid_loss: 0.002958424719205747, test_loss: 0.01847037859261036\n",
      "epoch: 1011, train_loss: 0.000840548747320376, valid_loss: 0.002939582356096556, test_loss: 0.018479328602552414\n",
      "epoch: 1012, train_loss: 0.0008397881629998269, valid_loss: 0.002938868283915023, test_loss: 0.018504824489355087\n",
      "epoch: 1013, train_loss: 0.000837898021057734, valid_loss: 0.0029384547766918936, test_loss: 0.01849972829222679\n",
      "epoch: 1014, train_loss: 0.00083885098422837, valid_loss: 0.0029272781393956393, test_loss: 0.01852334290742874\n",
      "epoch: 1015, train_loss: 0.000838158156155892, valid_loss: 0.0029409892837672182, test_loss: 0.018505623564124107\n",
      "epoch: 1016, train_loss: 0.0008380245088356669, valid_loss: 0.0029550116936055324, test_loss: 0.018511563539505005\n",
      "epoch: 1017, train_loss: 0.0008375597004171299, valid_loss: 0.0029157341147462526, test_loss: 0.018542027100920677\n",
      "epoch: 1018, train_loss: 0.0008369076126457556, valid_loss: 0.0029186008614487946, test_loss: 0.01852598786354065\n",
      "epoch: 1019, train_loss: 0.0008372049399084696, valid_loss: 0.002928592177340761, test_loss: 0.018504049628973007\n",
      "epoch: 1020, train_loss: 0.000838512038488103, valid_loss: 0.0029137565094667175, test_loss: 0.01853105053305626\n",
      "epoch: 1021, train_loss: 0.0008369158148644087, valid_loss: 0.0029295870335772634, test_loss: 0.018512403592467308\n",
      "epoch: 1022, train_loss: 0.000836772806233848, valid_loss: 0.002926583324248592, test_loss: 0.018512319773435593\n",
      "epoch: 1023, train_loss: 0.0008359878050649296, valid_loss: 0.002940483042038977, test_loss: 0.018509015440940857\n",
      "epoch: 1024, train_loss: 0.0008368658650990413, valid_loss: 0.0029180943577860794, test_loss: 0.018515661358833313\n",
      "epoch: 1025, train_loss: 0.0008355064656706932, valid_loss: 0.002922678084966416, test_loss: 0.01851341314613819\n",
      "epoch: 1026, train_loss: 0.0008341553798147842, valid_loss: 0.0029132602794561535, test_loss: 0.01855570822954178\n",
      "epoch: 1027, train_loss: 0.0008358789820467, valid_loss: 0.0029151853329191604, test_loss: 0.018546173349022865\n",
      "epoch: 1028, train_loss: 0.0008346216737940583, valid_loss: 0.002929510606918484, test_loss: 0.018501752987504005\n",
      "epoch: 1029, train_loss: 0.0008339677284390706, valid_loss: 0.002934352436568588, test_loss: 0.01855003647506237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1030, train_loss: 0.000833711075677496, valid_loss: 0.0028893926937598735, test_loss: 0.018558770418167114\n",
      "epoch: 1031, train_loss: 0.0008343217044096926, valid_loss: 0.0029206466861069202, test_loss: 0.018491223454475403\n",
      "epoch: 1032, train_loss: 0.0008335549323617116, valid_loss: 0.0029184058657847345, test_loss: 0.01854655146598816\n",
      "epoch: 1033, train_loss: 0.0008331502752338091, valid_loss: 0.002893888857215643, test_loss: 0.018555806949734688\n",
      "epoch: 1034, train_loss: 0.0008329291367376952, valid_loss: 0.0028949859261047095, test_loss: 0.018537338823080063\n",
      "epoch: 1035, train_loss: 0.0008325688566243195, valid_loss: 0.002913538036712756, test_loss: 0.01852213591337204\n",
      "epoch: 1036, train_loss: 0.000831959807091271, valid_loss: 0.002918329924189796, test_loss: 0.018545059487223625\n",
      "epoch: 1037, train_loss: 0.0008319448122916663, valid_loss: 0.002902084515274813, test_loss: 0.01855146698653698\n",
      "epoch: 1038, train_loss: 0.0008318875663225417, valid_loss: 0.002911931621686866, test_loss: 0.01854792982339859\n",
      "epoch: 1039, train_loss: 0.0008303994646943781, valid_loss: 0.0028875179511184492, test_loss: 0.01857662945985794\n",
      "epoch: 1040, train_loss: 0.0008311548356331238, valid_loss: 0.002889589018498858, test_loss: 0.018555739894509315\n",
      "epoch: 1041, train_loss: 0.0008309956781728113, valid_loss: 0.002892221362950901, test_loss: 0.018567655235528946\n",
      "epoch: 1042, train_loss: 0.0008315940453346981, valid_loss: 0.002905452720976124, test_loss: 0.018572885543107986\n",
      "epoch: 1043, train_loss: 0.0008314707260781332, valid_loss: 0.002888255713817974, test_loss: 0.01856146566569805\n",
      "epoch: 1044, train_loss: 0.0008314278751166295, valid_loss: 0.002903148929666107, test_loss: 0.018563659861683846\n",
      "epoch: 1045, train_loss: 0.0008304126778334055, valid_loss: 0.00289143369688342, test_loss: 0.018553461879491806\n",
      "epoch: 1046, train_loss: 0.0008290522939363575, valid_loss: 0.002887086069677025, test_loss: 0.01857789419591427\n",
      "epoch: 1047, train_loss: 0.0008308371812429117, valid_loss: 0.002890457030540953, test_loss: 0.018555762246251106\n",
      "epoch: 1048, train_loss: 0.0008291974132273184, valid_loss: 0.002900407804797093, test_loss: 0.018551556393504143\n",
      "epoch: 1049, train_loss: 0.0008295642619486898, valid_loss: 0.002898205944802612, test_loss: 0.018559332937002182\n",
      "epoch: 1050, train_loss: 0.0008292478084078301, valid_loss: 0.002877541749815767, test_loss: 0.018593696877360344\n",
      "epoch: 1051, train_loss: 0.0008289867875409191, valid_loss: 0.0028834693948738277, test_loss: 0.01856388710439205\n",
      "epoch: 1052, train_loss: 0.0008279060939853282, valid_loss: 0.0028866282179175564, test_loss: 0.018602199852466583\n",
      "epoch: 1053, train_loss: 0.0008278315616062964, valid_loss: 0.002886584843508899, test_loss: 0.018566085025668144\n",
      "epoch: 1054, train_loss: 0.0008276353246005981, valid_loss: 0.002881244814489037, test_loss: 0.018574906513094902\n",
      "epoch: 1055, train_loss: 0.0008282078095995214, valid_loss: 0.0028690441492168852, test_loss: 0.01857100985944271\n",
      "epoch: 1056, train_loss: 0.0008267211707551842, valid_loss: 0.002883040113374591, test_loss: 0.018582088872790337\n",
      "epoch: 1057, train_loss: 0.0008269638726588986, valid_loss: 0.0028653490298893303, test_loss: 0.018599417060613632\n",
      "epoch: 1058, train_loss: 0.0008272191879096563, valid_loss: 0.0029009871650487185, test_loss: 0.018567733466625214\n",
      "epoch: 1059, train_loss: 0.0008260151611782773, valid_loss: 0.0028821892628911883, test_loss: 0.01859232410788536\n",
      "epoch: 1060, train_loss: 0.0008273281931431721, valid_loss: 0.002880716279226666, test_loss: 0.018600186333060265\n",
      "epoch: 1061, train_loss: 0.0008256680954693128, valid_loss: 0.002870119681271414, test_loss: 0.018607433885335922\n",
      "epoch: 1062, train_loss: 0.0008251866928058798, valid_loss: 0.0028624655678868294, test_loss: 0.018619509413838387\n",
      "epoch: 1063, train_loss: 0.000826501942988809, valid_loss: 0.00288281895336695, test_loss: 0.018616212531924248\n",
      "epoch: 1064, train_loss: 0.0008253440205210253, valid_loss: 0.0028620878874789923, test_loss: 0.01860940083861351\n",
      "epoch: 1065, train_loss: 0.000824938829401103, valid_loss: 0.00288104706366236, test_loss: 0.01856653206050396\n",
      "epoch: 1066, train_loss: 0.000824690991304005, valid_loss: 0.002852876282607516, test_loss: 0.01857590675354004\n",
      "epoch: 1067, train_loss: 0.0008233116899171602, valid_loss: 0.00287562307009163, test_loss: 0.01861226186156273\n",
      "epoch: 1068, train_loss: 0.0008231314030759361, valid_loss: 0.0028779380566750965, test_loss: 0.018604779615998268\n",
      "epoch: 1069, train_loss: 0.0008231317725680444, valid_loss: 0.0028742538609852395, test_loss: 0.018587658181786537\n",
      "epoch: 1070, train_loss: 0.0008230744810450984, valid_loss: 0.002868580302068343, test_loss: 0.018615376204252243\n",
      "epoch: 1071, train_loss: 0.0008242204177962697, valid_loss: 0.002889883481354142, test_loss: 0.01859530806541443\n",
      "epoch: 1072, train_loss: 0.0008227408904096354, valid_loss: 0.002878654010904332, test_loss: 0.018601609393954277\n",
      "epoch: 1073, train_loss: 0.0008222951057731458, valid_loss: 0.002856302996709322, test_loss: 0.018629951402544975\n",
      "epoch: 1074, train_loss: 0.0008237115727514838, valid_loss: 0.0028558334646125636, test_loss: 0.01860712468624115\n",
      "epoch: 1075, train_loss: 0.0008227826632640284, valid_loss: 0.00286733650136739, test_loss: 0.018605824559926987\n",
      "epoch: 1076, train_loss: 0.0008215722349553329, valid_loss: 0.002846941390695671, test_loss: 0.018623653799295425\n",
      "epoch: 1077, train_loss: 0.0008232187702441993, valid_loss: 0.0028545817670722804, test_loss: 0.018628664314746857\n",
      "epoch: 1078, train_loss: 0.0008216823916882277, valid_loss: 0.0028544387702519694, test_loss: 0.018621543422341347\n",
      "epoch: 1079, train_loss: 0.0008233717855309014, valid_loss: 0.0028541116043925285, test_loss: 0.018654290586709976\n",
      "epoch: 1080, train_loss: 0.0008211008035172911, valid_loss: 0.002841308324908217, test_loss: 0.0186226274818182\n",
      "epoch: 1081, train_loss: 0.0008204635283302354, valid_loss: 0.002857538318494335, test_loss: 0.018603511154651642\n",
      "epoch: 1082, train_loss: 0.0008209156664594522, valid_loss: 0.002835173460577304, test_loss: 0.018645288422703743\n",
      "epoch: 1083, train_loss: 0.0008205540463044915, valid_loss: 0.0028419091540854424, test_loss: 0.018630467355251312\n",
      "epoch: 1084, train_loss: 0.0008203820578510995, valid_loss: 0.0028625626679665097, test_loss: 0.018607547506690025\n",
      "epoch: 1085, train_loss: 0.000818971706473309, valid_loss: 0.0028405063203535974, test_loss: 0.01864766888320446\n",
      "epoch: 1086, train_loss: 0.0008195936882301518, valid_loss: 0.0028430584934540093, test_loss: 0.01863659918308258\n",
      "epoch: 1087, train_loss: 0.0008188893350408129, valid_loss: 0.0028273005154915154, test_loss: 0.018667466938495636\n",
      "epoch: 1088, train_loss: 0.0008184029530916039, valid_loss: 0.002856828214135021, test_loss: 0.018619857728481293\n",
      "epoch: 1089, train_loss: 0.0008178598110807006, valid_loss: 0.0028439077723305672, test_loss: 0.018624741584062576\n",
      "epoch: 1090, train_loss: 0.0008195086240606463, valid_loss: 0.002846714157688742, test_loss: 0.018655255436897278\n",
      "epoch: 1091, train_loss: 0.0008178002595820504, valid_loss: 0.0028189039148855954, test_loss: 0.01866939291357994\n",
      "epoch: 1092, train_loss: 0.0008179410525253447, valid_loss: 0.0028403103545618555, test_loss: 0.0186445489525795\n",
      "epoch: 1093, train_loss: 0.0008186945696766286, valid_loss: 0.0028180048490564027, test_loss: 0.018673771992325783\n",
      "epoch: 1094, train_loss: 0.0008185325144871098, valid_loss: 0.0028337711604156843, test_loss: 0.018670029938220978\n",
      "epoch: 1095, train_loss: 0.0008164654212558399, valid_loss: 0.0028492726560216397, test_loss: 0.018644986674189568\n",
      "epoch: 1096, train_loss: 0.000817028236190748, valid_loss: 0.0028318996482994407, test_loss: 0.01863822527229786\n",
      "epoch: 1097, train_loss: 0.0008163275703301896, valid_loss: 0.0028390368970576674, test_loss: 0.01864372380077839\n",
      "epoch: 1098, train_loss: 0.0008169235421202915, valid_loss: 0.0028273416392039508, test_loss: 0.018668532371520996\n",
      "epoch: 1099, train_loss: 0.0008167284161216863, valid_loss: 0.0028406933415681124, test_loss: 0.018651597201824188\n",
      "early stop at: 1099; optimal epoch at: 698\n"
     ]
    }
   ],
   "source": [
    "seq_dim = window_size\n",
    "\n",
    "num_epochs = 1600\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "test_loss_1 = []\n",
    "test_loss_2 = []\n",
    "test_loss_3 = []\n",
    "test_loss_4 = []\n",
    "test_loss_5 = []\n",
    "\n",
    "min_loss = 100.0 # stars from a large number\n",
    "min_epoch = 0\n",
    "early_stop_epoch_size = 400\n",
    "\n",
    "f = open(PATH_LOG, \"w\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    train_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(train_loader):\n",
    "        # print(\"train: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(seqs.float())\n",
    "        outputs = model(seqs)\n",
    "        # print(outputs.is_cuda)\n",
    "        # print(labels.is_cuda)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs.dtype)\n",
    "        \n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_train_loss += loss.data.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch = i + 1;\n",
    "        \n",
    "        # print(\"loss: \", loss.data)\n",
    "    \n",
    "    total_valid_loss = 0.0\n",
    "    valid_batch = 0\n",
    "    # valid_seq = []\n",
    "    valid_pred = []\n",
    "    # valid_gt = []\n",
    "    for i, (seqs, labels) in enumerate(valid_loader):\n",
    "        # print(\"valid: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_valid_loss += loss.data.item()\n",
    "        # valid_gt.append(labels)\n",
    "        valid_pred.append(outputs)\n",
    "        # valid_seq.append(seqs)\n",
    "        \n",
    "        valid_batch = i + 1\n",
    "    \n",
    "    '''\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    # test_seq = []\n",
    "    test_pred = []\n",
    "    # test_gt = []\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        test_pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        test_batch = i + 1\n",
    "    '''\n",
    "    test_cur_loss_1 = get_epoch_loss(test_loader_1)\n",
    "    test_cur_loss_2 = get_epoch_loss(test_loader_2)\n",
    "    test_cur_loss_3 = get_epoch_loss(test_loader_3)\n",
    "    test_cur_loss_4 = get_epoch_loss(test_loader_4)\n",
    "    test_cur_loss_5 = get_epoch_loss(test_loader_5)\n",
    "    \n",
    "    # print(\"train batch: \", train_batch)\n",
    "    # print(\"valid batch: \", valid_batch)\n",
    "    train_loss.append(total_train_loss/train_batch)\n",
    "    valid_loss.append(total_valid_loss/valid_batch)\n",
    "    # test_loss.append(total_test_loss/test_batch)\n",
    "    test_loss_1.append(test_cur_loss_1)\n",
    "    test_loss_2.append(test_cur_loss_2)\n",
    "    test_loss_3.append(test_cur_loss_3)\n",
    "    test_loss_4.append(test_cur_loss_4)\n",
    "    test_loss_5.append(test_cur_loss_5)\n",
    "    # train_loss.append(total_train_loss)\n",
    "    # valid_loss.append(total_valid_loss)\n",
    "    # print(\"epoch: {}, train_loss: {}, valid_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_valid_loss/valid_batch, total_test_loss/test_batch))\n",
    "    if (epoch % 1 == 0):\n",
    "        print(\"epoch: {}, train_loss: {}, valid_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_valid_loss/valid_batch, test_cur_loss_1))\n",
    "\n",
    "    # check early stop condition\n",
    "    if (min_loss > test_cur_loss_1):\n",
    "        min_loss = test_cur_loss_1\n",
    "        min_epoch = epoch\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    if (epoch - min_epoch > early_stop_epoch_size):\n",
    "        log = \"early stop at: \" + str(epoch) + \"; optimal epoch at: \" + str(min_epoch)\n",
    "        print(log)\n",
    "        f.write(log)\n",
    "        f.close()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 1099\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "valid_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), valid_loss[starting_epoch:end_epoch])\n",
    "test_loss_curve_1, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss_1[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, valid_loss_curve, test_loss_curve_1], ['Train Loss', 'Validation Loss', 'Test Loss 1'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.018166881054639816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018166881054639816\n"
     ]
    }
   ],
   "source": [
    "# get valid results\n",
    "seq_dim = window_size\n",
    "input_dim = 3\n",
    "# valid_seq = []\n",
    "valid_predd = []\n",
    "# valid_gt = []\n",
    "total_valid_loss = 0.0\n",
    "valid_batch = 0\n",
    "for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        \n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_valid_loss += loss.data.item()\n",
    "    valid_predd.append(outputs)\n",
    "    valid_batch = i + 1\n",
    "print(total_valid_loss/valid_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('1600/with_normalization_with_earlystop/window_10/train_loss.npy', np.asarray(train_loss))\n",
    "np.save('1600/with_normalization_with_earlystop/window_10/valid_loss.npy', np.asarray(valid_loss))\n",
    "np.save('1600/with_normalization_with_earlystop/window_10/test_loss_1.npy', np.asarray(test_loss_1))\n",
    "np.save('1600/with_normalization_with_earlystop/window_10/test_loss_2.npy', np.asarray(test_loss_2))\n",
    "np.save('1600/with_normalization_with_earlystop/window_10/test_loss_3.npy', np.asarray(test_loss_3))\n",
    "np.save('1600/with_normalization_with_earlystop/window_10/test_loss_4.npy', np.asarray(test_loss_4))\n",
    "np.save('1600/with_normalization_with_earlystop/window_10/test_loss_5.npy', np.asarray(test_loss_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
