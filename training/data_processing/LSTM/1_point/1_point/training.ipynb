{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistic(l):\n",
    "    max_value = max(l)\n",
    "    min_value = min(l)\n",
    "    mean = sum(l)/len(l)\n",
    "    var = sum([((x - mean) ** 2) for x in l]) / len(l)\n",
    "    std = var**0.5\n",
    "    print(\"min:\", min_value)\n",
    "    print(\"max:\", max_value)\n",
    "    print(\"mean:\", mean)\n",
    "    print(\"std:\", std)\n",
    "\n",
    "def normalization(l):\n",
    "    max_value = max(l)\n",
    "    min_value = min(l)\n",
    "    mean = sum(l)/len(l)\n",
    "    var = sum([((x - mean) ** 2) for x in l]) / len(l)\n",
    "    std = var**0.5\n",
    "    # print(\"min:\", min_value)\n",
    "    # print(\"max:\", max_value)\n",
    "    # print(\"mean:\", mean)\n",
    "    # print(\"std:\", std)\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        l[i] = (l[i] - mean)/std\n",
    "    \n",
    "    return l\n",
    "    \n",
    "\n",
    "def load_data(file_name, test_size):\n",
    "    f = open(file_name)\n",
    "    df = pd.read_csv(f)\n",
    "    data = np.array(df[['x_0','x']])\n",
    "    x = data[:,0].tolist()\n",
    "    x.append(data[-1, -1])\n",
    "    # do normalization\n",
    "    # show_statistic(x)\n",
    "    x = normalization(x)\n",
    "    # show_statistic(x)\n",
    "    \n",
    "    data = np.array(df[['y_0','y']])\n",
    "    y = data[:,0].tolist()\n",
    "    y.append(data[-1, -1])\n",
    "    # do normalization\n",
    "    y = normalization(y)\n",
    "\n",
    "    data = np.array(df[['z_0','z']])\n",
    "    z = data[:,0].tolist()\n",
    "    z.append(data[-1, -1])\n",
    "    # do normalization\n",
    "    z = normalization(z)\n",
    "    \n",
    "    train_set_x = x[:-test_size]\n",
    "    test_set_x = x[-test_size:]\n",
    "    train_set_y = y[:-test_size]\n",
    "    test_set_y = y[-test_size:]\n",
    "    train_set_z = z[:-test_size]\n",
    "    test_set_z = z[-test_size:]\n",
    "    \n",
    "    return train_set_x, test_set_x, train_set_y, test_set_y, train_set_z, test_set_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_1, test_set_x_1, train_set_y_1, test_set_y_1, train_set_z_1, test_set_z_1 = load_data('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, test_set_x_2, train_set_y_2, test_set_y_2, train_set_z_2, test_set_z_2 = load_data('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, test_set_x_3, train_set_y_3, test_set_y_3, train_set_z_3, test_set_z_3 = load_data('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, test_set_x_4, train_set_y_4, test_set_y_4, train_set_z_4, test_set_z_4 = load_data('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, test_set_x_5, train_set_y_5, test_set_y_5, train_set_z_5, test_set_z_5 = load_data('data_preprocessing/test_5_training_xyz.txt', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of input and label pairs\n",
    "def input_data(seq, ws):\n",
    "    out = []\n",
    "    L = len(seq)\n",
    "    \n",
    "    for i in range(L - ws):\n",
    "        window = seq[i:i+ws]\n",
    "        label = seq[i+ws:i+ws+1]\n",
    "        out.append((window, label))\n",
    "        \n",
    "    return out\n",
    "\n",
    "def get_tensor(sample_size, data_x, data_y, data_z):\n",
    "    for i in range(sample_size):\n",
    "        # construct dataset\n",
    "        x = np.asarray(data_x[i][0]).reshape(1,-1) # 1 x window_size\n",
    "        y = np.asarray(data_y[i][0]).reshape(1,-1) # 1 x window_size\n",
    "        z = np.asarray(data_z[i][0]).reshape(1,-1) # 1 x window_size\n",
    "        xyz = np.append(np.append(x, y, axis = 0), z, axis = 0) # 3 x window_size\n",
    "        xyz = np.transpose(xyz) # window_size x 3\n",
    "        window_size = xyz.shape[0]\n",
    "        # print(\"window_size\", window_size)\n",
    "        xyz = xyz.reshape(1, window_size, 3) # 1 x window_size x 3\n",
    "        if (i == 0):\n",
    "            prev_xyz = xyz\n",
    "        else:\n",
    "            prev_xyz = np.append(prev_xyz, xyz, axis = 0)\n",
    "        \n",
    "        #construct label\n",
    "        x_label = np.asarray(data_x[i][1]).reshape(1,-1) # 1 x 1\n",
    "        y_label = np.asarray(data_y[i][1]).reshape(1,-1) # 1 x 1\n",
    "        z_label = np.asarray(data_z[i][1]).reshape(1,-1) # 1 x 1\n",
    "        xyz_label = np.append(np.append(x_label, y_label, axis = 0), z_label, axis = 0) # 3 x 1\n",
    "        xyz_label = xyz_label.reshape(1, 3) # 1 x 3 x 1\n",
    "        if (i == 0):\n",
    "            prev_xyz_label = xyz_label\n",
    "        else:\n",
    "            prev_xyz_label = np.append(prev_xyz_label, xyz_label, axis = 0)\n",
    "            \n",
    "    return prev_xyz, prev_xyz_label\n",
    "\n",
    "# make train_dataset\n",
    "def construct_train_test_tensor(train_set_x, train_set_y, train_set_z, test_set_x, test_set_y, test_set_z, window_size):\n",
    "    # sequence to data sample\n",
    "    train_data_x = input_data(train_set_x, window_size)\n",
    "    train_data_y = input_data(train_set_y, window_size)\n",
    "    train_data_z = input_data(train_set_z, window_size)\n",
    "    test_data_x = input_data(test_set_x, window_size)\n",
    "    test_data_y = input_data(test_set_y, window_size)\n",
    "    test_data_z = input_data(test_set_z, window_size)\n",
    "    \n",
    "    \n",
    "    # reconstruct train/test dataset and label\n",
    "    sample_size = len(train_data_x)\n",
    "    train_dataset, train_label = get_tensor(sample_size, train_data_x, train_data_y, train_data_z)\n",
    "    sample_size = len(test_data_x)\n",
    "    test_dataset, test_label = get_tensor(sample_size, test_data_x, test_data_y, test_data_z)\n",
    "    \n",
    "    train_dataset = train_dataset.astype(\"float32\")\n",
    "    train_label = train_label.astype(\"float32\")\n",
    "    test_dataset = test_dataset.astype(\"float32\")\n",
    "    test_label = test_label.astype(\"float32\")\n",
    "    \n",
    "    return train_dataset, train_label, test_dataset, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "train_dataset_1, train_label_1, test_dataset_1, test_label_1 = construct_train_test_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           test_set_x_1,\n",
    "                                                                                           test_set_y_1,\n",
    "                                                                                           test_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, test_dataset_2, test_label_2 = construct_train_test_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           test_set_x_2,\n",
    "                                                                                           test_set_y_2,\n",
    "                                                                                           test_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, test_dataset_3, test_label_3 = construct_train_test_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           test_set_x_3,\n",
    "                                                                                           test_set_y_3,\n",
    "                                                                                           test_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, test_dataset_4, test_label_4 = construct_train_test_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           test_set_x_4,\n",
    "                                                                                           test_set_y_4,\n",
    "                                                                                           test_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, test_dataset_5, test_label_5 = construct_train_test_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           test_set_x_5,\n",
    "                                                                                           test_set_y_5,\n",
    "                                                                                           test_set_z_5,\n",
    "                                                                                           window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 10, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 10, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "test_dataset = np.concatenate((test_dataset_1,\n",
    "                               test_dataset_2,\n",
    "                               test_dataset_3,\n",
    "                               test_dataset_4,\n",
    "                               test_dataset_5), axis=0)\n",
    "test_label = np.concatenate((test_label_1,\n",
    "                             test_label_2,\n",
    "                             test_label_3,\n",
    "                             test_label_4,\n",
    "                             test_label_5), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 10, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 10, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2990*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "390*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-9979ff4634ab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-9979ff4634ab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 23 = 14950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 23 = 14950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-de69bcda9ea3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-de69bcda9ea3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 3 = 1950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 3 = 1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainDataSet(Dataset):\n",
    "    def __init__(self, train_dataset, train_label):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_label = train_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.train_dataset[idx]\n",
    "        label = self.train_label[idx]\n",
    "        \n",
    "        return [seq, label]\n",
    "\n",
    "\n",
    "class MyTestDataSet(Dataset):\n",
    "    def __init__(self, test_dataset, test_label):\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_label = test_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.test_dataset[idx]\n",
    "        label = self.test_label[idx]\n",
    "\n",
    "        return [seq, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14950\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(test_dataset, test_label)\n",
    "print(len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 30\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# keep the test data trajectory order\n",
    "test_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7efff127a7f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "1 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "2 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "3 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "4 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "5 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "6 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "7 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "8 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "9 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "10 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "11 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "12 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "13 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "14 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "15 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "16 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "17 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "18 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "19 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "20 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "21 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "22 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "23 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "24 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "25 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "26 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "27 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "28 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "29 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "30 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "31 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "32 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "33 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "34 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "35 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "36 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "37 torch.Size([50, 10, 3]) torch.Size([50, 3])\n",
      "38 torch.Size([50, 10, 3]) torch.Size([50, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    print(i, seqs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[-1.5292,  0.4927,  0.3011],\n",
      "        [-1.5173,  0.4927,  0.4030],\n",
      "        [-1.4933,  0.4927,  0.5026],\n",
      "        [-1.4408,  0.4927,  0.5906],\n",
      "        [-1.3852,  0.4927,  0.6768],\n",
      "        [-1.3296,  0.4927,  0.7630],\n",
      "        [-1.2740,  0.4927,  0.8492],\n",
      "        [-1.2155,  0.4927,  0.9333],\n",
      "        [-1.1400,  0.4927,  1.0025],\n",
      "        [-1.0622,  0.4927,  1.0690]])\n",
      "tensor([-0.9840,  0.4927,  1.1351])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[0])\n",
    "        print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[ 0.1533, -0.3653,  1.0343],\n",
      "        [ 0.1890, -0.3653,  1.0158],\n",
      "        [ 0.2233, -0.3653,  0.9950],\n",
      "        [ 0.2577, -0.3653,  0.9742],\n",
      "        [ 0.2920, -0.3653,  0.9533],\n",
      "        [ 0.3263, -0.3653,  0.9325],\n",
      "        [ 0.3606, -0.3653,  0.9117],\n",
      "        [ 0.3949, -0.3653,  0.8908],\n",
      "        [ 0.4292, -0.3653,  0.8700],\n",
      "        [ 0.4635, -0.3653,  0.8491]])\n",
      "tensor([ 0.4978, -0.3653,  0.8283])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[1])\n",
    "        print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        \n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first = True)\n",
    "        # If your input data is of shape (seq_len, batch_size, features)\n",
    "        # then you don’t need batch_first=True and your LSTM will give\n",
    "        # output of shape (seq_len, batch_size, hidden_size).\n",
    "\n",
    "        # If your input data is of shape (batch_size, seq_len, features)\n",
    "        # then you need batch_first=True and your LSTM will give\n",
    "        # output of shape (batch_size, seq_len, hidden_size).\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.hidden = (torch.zeros(self.num_layers, 1, hidden_size), torch.zeros(self.num_layers, 1, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim))\n",
    "        if torch.cuda.is_available():\n",
    "            c0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            c0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim))\n",
    "        # print(\"x.size(0)\", x.size(0))\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # x is (batch_size, seq_len, features)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 0, train_loss: 0.6836076390783125, test_loss: 0.39737762396152204\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1, train_loss: 0.1502211430549223, test_loss: 0.07148451902694666\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 2, train_loss: 0.039401816328781904, test_loss: 0.04599454452415021\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 3, train_loss: 0.03342713529935549, test_loss: 0.04248273972039803\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 4, train_loss: 0.031489930093836625, test_loss: 0.04019784139325985\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 5, train_loss: 0.029852093465277382, test_loss: 0.03802041957477251\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 6, train_loss: 0.028415322908041468, test_loss: 0.03638940070493099\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 7, train_loss: 0.02713776745811653, test_loss: 0.03478986100974278\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 8, train_loss: 0.02599501683934277, test_loss: 0.033326467173771024\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 9, train_loss: 0.0249641701130564, test_loss: 0.03186329905815327\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 10, train_loss: 0.024023008875449166, test_loss: 0.03052495048644069\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 11, train_loss: 0.023172595600494773, test_loss: 0.029570904184276096\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 12, train_loss: 0.022382423381342935, test_loss: 0.02842965932825628\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 13, train_loss: 0.021655751697494834, test_loss: 0.027582070196214586\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 14, train_loss: 0.02097751357807563, test_loss: 0.026604000637188364\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 15, train_loss: 0.02034552488787517, test_loss: 0.025808796647768945\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 16, train_loss: 0.019756082040460213, test_loss: 0.025007444218947336\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 17, train_loss: 0.019200070611634184, test_loss: 0.024245148793292735\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 18, train_loss: 0.018675786396332807, test_loss: 0.023556415928909794\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 19, train_loss: 0.018180266620722103, test_loss: 0.022885532124350086\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 20, train_loss: 0.01771350592424638, test_loss: 0.022315329536258314\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 21, train_loss: 0.017268186868059397, test_loss: 0.02168141624991758\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 22, train_loss: 0.01684320527891999, test_loss: 0.021081967291255027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 23, train_loss: 0.016434657443501678, test_loss: 0.020638927477053724\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 24, train_loss: 0.01604456264513193, test_loss: 0.020139724695278954\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 25, train_loss: 0.015672951980328878, test_loss: 0.019550542433698397\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 26, train_loss: 0.015315182983601173, test_loss: 0.019161740756736927\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 27, train_loss: 0.014969526048762144, test_loss: 0.018652673804750428\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 28, train_loss: 0.014637548801953278, test_loss: 0.018226185067294117\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 29, train_loss: 0.01431248248258861, test_loss: 0.01787332270312338\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 30, train_loss: 0.014002671873315322, test_loss: 0.017443383521495912\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 31, train_loss: 0.01370111552618419, test_loss: 0.016963321078294076\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 32, train_loss: 0.013407960373684016, test_loss: 0.01661006685268755\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 33, train_loss: 0.013123278309868331, test_loss: 0.016261844200273164\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 34, train_loss: 0.012847389273632031, test_loss: 0.015933745172029983\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 35, train_loss: 0.012578852462230318, test_loss: 0.015657744218165483\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 36, train_loss: 0.012317522846311829, test_loss: 0.015291772179830914\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 37, train_loss: 0.012063019943227338, test_loss: 0.014959485401423315\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 38, train_loss: 0.01181328497409671, test_loss: 0.01464862064891256\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 39, train_loss: 0.011570763995366351, test_loss: 0.014348731551599951\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 40, train_loss: 0.01133306167340747, test_loss: 0.014068052948762974\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 41, train_loss: 0.011101853306464427, test_loss: 0.013731073582162842\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 42, train_loss: 0.010873387865161517, test_loss: 0.013478064505108751\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 43, train_loss: 0.010652983368041125, test_loss: 0.01314671226636841\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 44, train_loss: 0.010434859268753524, test_loss: 0.012854188209763752\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 45, train_loss: 0.010225685736955608, test_loss: 0.012596646927518006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 46, train_loss: 0.010018375607201188, test_loss: 0.012374101793214392\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 47, train_loss: 0.009816285098177134, test_loss: 0.012134277042395506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 48, train_loss: 0.009617069997765548, test_loss: 0.011875970889097797\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 49, train_loss: 0.009424734996152064, test_loss: 0.011634744957495386\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 50, train_loss: 0.009234774890464783, test_loss: 0.011412206350658566\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 51, train_loss: 0.009050794958096483, test_loss: 0.011173911731487188\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 52, train_loss: 0.008871561733158586, test_loss: 0.01096904733686899\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 53, train_loss: 0.008694275835198802, test_loss: 0.01069867925103515\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 54, train_loss: 0.008522870447362047, test_loss: 0.010545253146959182\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 55, train_loss: 0.008355789919879076, test_loss: 0.010314008210582707\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 56, train_loss: 0.008193562895115651, test_loss: 0.010050880309651032\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 57, train_loss: 0.0080337755523784, test_loss: 0.009902041681925766\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 58, train_loss: 0.007879442476938451, test_loss: 0.009758253573986271\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 59, train_loss: 0.007731363368080462, test_loss: 0.009515244154429708\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 60, train_loss: 0.00758455769722677, test_loss: 0.009351429718662985\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 61, train_loss: 0.007442250237317388, test_loss: 0.009152169994941244\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 62, train_loss: 0.007305663589070423, test_loss: 0.008948640765410323\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 63, train_loss: 0.007171457511120566, test_loss: 0.008834739640849128\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 64, train_loss: 0.007044081464814751, test_loss: 0.008645251161233785\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 65, train_loss: 0.006918918582794559, test_loss: 0.008457393302635934\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 66, train_loss: 0.006798071676186196, test_loss: 0.008340124943807052\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 67, train_loss: 0.006682595129071869, test_loss: 0.008216355111635027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 68, train_loss: 0.006569725328146863, test_loss: 0.008052027300343467\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 69, train_loss: 0.0064622368667442915, test_loss: 0.007871755485748274\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 70, train_loss: 0.006356774259321267, test_loss: 0.007774028997235478\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 71, train_loss: 0.006257071694745418, test_loss: 0.007648740443796552\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 72, train_loss: 0.006160461301775182, test_loss: 0.0074744860537929265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 73, train_loss: 0.006068016283282618, test_loss: 0.007396526821256162\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 74, train_loss: 0.005978912025936148, test_loss: 0.007270657324951548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 75, train_loss: 0.0058942066301532516, test_loss: 0.007144470475069307\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 76, train_loss: 0.005812933682510785, test_loss: 0.007022267689395589\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 77, train_loss: 0.005733191319912622, test_loss: 0.0069111960705739856\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 78, train_loss: 0.00565888985826394, test_loss: 0.006856131908791259\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 79, train_loss: 0.005587566971395751, test_loss: 0.006778335467037649\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 80, train_loss: 0.0055187876763781625, test_loss: 0.006666354263404337\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 81, train_loss: 0.005452191104012912, test_loss: 0.00657352866181053\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 82, train_loss: 0.005389469571763581, test_loss: 0.0064798325930053415\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 83, train_loss: 0.005329761182247695, test_loss: 0.006416450481666312\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 84, train_loss: 0.005272440723280569, test_loss: 0.0063299222835130654\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 85, train_loss: 0.005218218742203229, test_loss: 0.006245487294971752\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 86, train_loss: 0.005165073558077665, test_loss: 0.006147953244717195\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 87, train_loss: 0.005114144106383083, test_loss: 0.006117550323371035\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 88, train_loss: 0.005066825240074866, test_loss: 0.006053203121364976\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 89, train_loss: 0.005022016256178627, test_loss: 0.005990585438363875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 90, train_loss: 0.004978135005838821, test_loss: 0.005948602151758491\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 91, train_loss: 0.004936408457489541, test_loss: 0.005848970250017607\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 92, train_loss: 0.004896128496441703, test_loss: 0.005784600760447434\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 93, train_loss: 0.004857901732568134, test_loss: 0.005758804494129995\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 94, train_loss: 0.004821633295947219, test_loss: 0.005718418475365839\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 95, train_loss: 0.004786290101828458, test_loss: 0.005618606786204812\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 96, train_loss: 0.004751966575886369, test_loss: 0.0056017347180899475\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 97, train_loss: 0.004719934766149666, test_loss: 0.005537485304581097\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 98, train_loss: 0.00468970551373543, test_loss: 0.005500252643310561\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 99, train_loss: 0.004658946352792798, test_loss: 0.005470621516743006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 100, train_loss: 0.004630852514413811, test_loss: 0.005409487476497769\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 101, train_loss: 0.004603097490175163, test_loss: 0.005358404958641563\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 102, train_loss: 0.004576314653403883, test_loss: 0.005322095177222927\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 103, train_loss: 0.0045512880468865655, test_loss: 0.00527921426803984\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 104, train_loss: 0.004524495639529914, test_loss: 0.005229684124993256\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 105, train_loss: 0.004502441933032835, test_loss: 0.005232819426171959\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 106, train_loss: 0.004478790510732394, test_loss: 0.005172633199179426\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 107, train_loss: 0.0044562490750415665, test_loss: 0.005128825020316678\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 108, train_loss: 0.004434094169367117, test_loss: 0.005068521488842985\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 109, train_loss: 0.0044139409640237045, test_loss: 0.005086334330441717\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 110, train_loss: 0.004392191642322219, test_loss: 0.0050673372136840885\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 111, train_loss: 0.004373496108264803, test_loss: 0.005026881746091068\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 112, train_loss: 0.00435511584909178, test_loss: 0.004970793955577299\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 113, train_loss: 0.0043350128545721735, test_loss: 0.0049519307127085105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 114, train_loss: 0.004316458521009015, test_loss: 0.0049279126911782305\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 115, train_loss: 0.004299477737530892, test_loss: 0.004911945167128952\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 116, train_loss: 0.004280675887472828, test_loss: 0.004837150710115496\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 117, train_loss: 0.004264775055451288, test_loss: 0.004865045955506129\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 118, train_loss: 0.004248563456426944, test_loss: 0.004808329503225067\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 119, train_loss: 0.004232094931123423, test_loss: 0.004757452575200333\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 120, train_loss: 0.004216929684302779, test_loss: 0.004760659560979081\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 121, train_loss: 0.004201661509984985, test_loss: 0.004712703026598319\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 122, train_loss: 0.00418494408833405, test_loss: 0.004728190727734891\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 123, train_loss: 0.004170945104224028, test_loss: 0.00468585298035461\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 124, train_loss: 0.004155982360882329, test_loss: 0.004650726096513562\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 125, train_loss: 0.004142256880359706, test_loss: 0.004607687660982498\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 126, train_loss: 0.004127685050713677, test_loss: 0.004615073500481697\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 127, train_loss: 0.004113677479124263, test_loss: 0.004614631217032767\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 128, train_loss: 0.0041012080852720996, test_loss: 0.004551134181196968\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 129, train_loss: 0.00408755849585884, test_loss: 0.0045097422105475115\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 130, train_loss: 0.0040741078839808655, test_loss: 0.004519482583512003\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 131, train_loss: 0.0040616697482266356, test_loss: 0.004491964896143868\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 132, train_loss: 0.004049774663667677, test_loss: 0.0044509649033455225\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 133, train_loss: 0.004036672217506654, test_loss: 0.0044418302833293686\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 134, train_loss: 0.004024168769252739, test_loss: 0.004440936292760456\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 135, train_loss: 0.004011863919308278, test_loss: 0.0043882590349918855\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 136, train_loss: 0.004000773016218545, test_loss: 0.004382315697745444\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 137, train_loss: 0.003987997603668118, test_loss: 0.004364635746848459\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 138, train_loss: 0.003977328004707959, test_loss: 0.00433308067985285\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 139, train_loss: 0.003966436400755704, test_loss: 0.00432740258372341\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 140, train_loss: 0.00395508204473114, test_loss: 0.004285719309103651\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 141, train_loss: 0.003944218191037883, test_loss: 0.004290086687363397\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 142, train_loss: 0.003933297004550695, test_loss: 0.004255213708324262\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 143, train_loss: 0.003922852860992606, test_loss: 0.004249244986567646\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 144, train_loss: 0.003911395922044672, test_loss: 0.004247585680180539\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 145, train_loss: 0.0039016602013453245, test_loss: 0.004188023659830483\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 146, train_loss: 0.0038916810649683867, test_loss: 0.004207858236697622\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 147, train_loss: 0.0038810589665743246, test_loss: 0.0041591495145649575\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 148, train_loss: 0.0038712875774273505, test_loss: 0.00413742250440499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 149, train_loss: 0.003862159191568485, test_loss: 0.004118191412626169\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 150, train_loss: 0.003851302908764635, test_loss: 0.004160421557921486\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 151, train_loss: 0.003842565909271007, test_loss: 0.004106271454545025\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 152, train_loss: 0.00383275411310559, test_loss: 0.004082031274148717\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 153, train_loss: 0.003823724812648904, test_loss: 0.004059231299041317\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 154, train_loss: 0.0038140275648773266, test_loss: 0.0040657277016016916\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 155, train_loss: 0.003805832657807298, test_loss: 0.004041018850283506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 156, train_loss: 0.0037968551119428166, test_loss: 0.0040077113013607096\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 157, train_loss: 0.00378848233335748, test_loss: 0.00398939605936623\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 158, train_loss: 0.003779451702690269, test_loss: 0.003971533170746018\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 159, train_loss: 0.0037715398002942297, test_loss: 0.0039658351407314725\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 160, train_loss: 0.00376220989114228, test_loss: 0.003956474880210291\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 161, train_loss: 0.0037535418918664665, test_loss: 0.003925569587572215\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 162, train_loss: 0.0037450981042176537, test_loss: 0.003946186856736835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 163, train_loss: 0.0037372514454065433, test_loss: 0.003910582824550473\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 164, train_loss: 0.0037296753175039606, test_loss: 0.0038863352390949447\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 165, train_loss: 0.003721328672795653, test_loss: 0.0038920315437747212\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 166, train_loss: 0.0037137795680429924, test_loss: 0.0038536517180872555\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 167, train_loss: 0.003705786794959801, test_loss: 0.0038437921671758\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 168, train_loss: 0.0036982827711644828, test_loss: 0.0038210995799790206\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 169, train_loss: 0.0036909838417703814, test_loss: 0.003813544997963338\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 170, train_loss: 0.003682949675253303, test_loss: 0.003818350723938634\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 171, train_loss: 0.0036760595725245, test_loss: 0.003806760027192724\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 172, train_loss: 0.003668994144816886, test_loss: 0.0037938664198662033\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 173, train_loss: 0.0036618746244769084, test_loss: 0.0037677860527765006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 174, train_loss: 0.0036539387113234045, test_loss: 0.003741073651979558\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 175, train_loss: 0.0036475520900662453, test_loss: 0.003723811600745345\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 176, train_loss: 0.003640345659458941, test_loss: 0.0037120668895136663\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 177, train_loss: 0.0036328518813799906, test_loss: 0.0037438774146348573\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 178, train_loss: 0.0036271951347433755, test_loss: 0.003710401699675295\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 179, train_loss: 0.0036199277161346457, test_loss: 0.0036821902762405956\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 180, train_loss: 0.0036136555422352857, test_loss: 0.003694003632214541\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 181, train_loss: 0.0036064849550879394, test_loss: 0.0036629817951530316\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 182, train_loss: 0.0036004720063783527, test_loss: 0.0036375285641182787\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 183, train_loss: 0.003593762137344649, test_loss: 0.0036586742830994087\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 184, train_loss: 0.003587887377320885, test_loss: 0.0036275316684996374\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 185, train_loss: 0.0035813111955156694, test_loss: 0.003639218323932698\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 186, train_loss: 0.0035746404021218367, test_loss: 0.0036231288223718414\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 187, train_loss: 0.003568977834913805, test_loss: 0.0036038366640404537\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 188, train_loss: 0.003563578168349185, test_loss: 0.003582592514039089\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 189, train_loss: 0.0035568631314874992, test_loss: 0.0035904570358047164\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 190, train_loss: 0.003550750566887853, test_loss: 0.0035609117453882042\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 191, train_loss: 0.0035455449878309783, test_loss: 0.003544158133594558\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 192, train_loss: 0.00353855745657425, test_loss: 0.003532177877227346\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 193, train_loss: 0.0035338072561001696, test_loss: 0.0035514788678250252\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 194, train_loss: 0.003527127642785651, test_loss: 0.0035442501203551027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 195, train_loss: 0.0035226729460587, test_loss: 0.003530510321396809\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 196, train_loss: 0.0035168664248809716, test_loss: 0.0035064399394636545\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 197, train_loss: 0.0035114233673279203, test_loss: 0.0034928045799740804\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 198, train_loss: 0.0035052304545473492, test_loss: 0.0034838203622105843\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 199, train_loss: 0.0035002772403525868, test_loss: 0.003471746910527611\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 200, train_loss: 0.0034948411663561236, test_loss: 0.003455553448964985\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 201, train_loss: 0.0034895362202031384, test_loss: 0.003460276744310529\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 202, train_loss: 0.003484566956851943, test_loss: 0.0034586208703097864\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 203, train_loss: 0.0034786074116271624, test_loss: 0.0034298354290419808\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 204, train_loss: 0.0034742741656713936, test_loss: 0.0034316702203329606\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 205, train_loss: 0.0034686703589997163, test_loss: 0.0034237414996217317\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 206, train_loss: 0.0034632022046004376, test_loss: 0.0034251801191250053\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 207, train_loss: 0.003458952151992698, test_loss: 0.0033937738450894803\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 208, train_loss: 0.0034542160772914722, test_loss: 0.003400480748193028\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 209, train_loss: 0.0034488739790768676, test_loss: 0.0033811075629320187\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 210, train_loss: 0.0034431249275492085, test_loss: 0.003353844612063721\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 211, train_loss: 0.0034386748502523105, test_loss: 0.0033558856110828808\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 212, train_loss: 0.0034345929654171835, test_loss: 0.0033582743206539024\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 213, train_loss: 0.003429426747976949, test_loss: 0.0033612300959738116\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 214, train_loss: 0.003424471060637958, test_loss: 0.003346546239649447\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 215, train_loss: 0.0034209066144592535, test_loss: 0.0033348609800808704\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 216, train_loss: 0.0034153266902545772, test_loss: 0.003337635069142263\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 217, train_loss: 0.0034112144188790344, test_loss: 0.0033239327059187093\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 218, train_loss: 0.00340672690841061, test_loss: 0.003310688341839406\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 219, train_loss: 0.0034014533371602286, test_loss: 0.0033114547927069883\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 220, train_loss: 0.0033976697050701566, test_loss: 0.0033025245483702\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 221, train_loss: 0.003392149633330832, test_loss: 0.003277744577364781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 222, train_loss: 0.003388514577949974, test_loss: 0.0032634503634956977\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 223, train_loss: 0.003383715829755924, test_loss: 0.003277634471977273\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 224, train_loss: 0.003379692920377746, test_loss: 0.0032829566737816026\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 225, train_loss: 0.0033763150472729702, test_loss: 0.0032536842157312026\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 226, train_loss: 0.003371420499392128, test_loss: 0.0032590257503146017\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 227, train_loss: 0.0033674671849622125, test_loss: 0.0032392293203604193\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 228, train_loss: 0.0033625383471235335, test_loss: 0.003224626006648088\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 229, train_loss: 0.0033590708349996484, test_loss: 0.003225000124821702\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 230, train_loss: 0.0033546874891845105, test_loss: 0.003229026908416134\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 231, train_loss: 0.0033500376071805837, test_loss: 0.003225474528451928\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 232, train_loss: 0.0033457203646327515, test_loss: 0.003189911525950242\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 233, train_loss: 0.003342484637945392, test_loss: 0.0031977806200022595\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 234, train_loss: 0.003337928905567609, test_loss: 0.0031933440246249144\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 235, train_loss: 0.003334305757254885, test_loss: 0.003175863495039849\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 236, train_loss: 0.003330203373200045, test_loss: 0.003186914686618062\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 237, train_loss: 0.003326628850671746, test_loss: 0.0031799447076478735\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 238, train_loss: 0.003322184085261924, test_loss: 0.0031743680943364804\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 239, train_loss: 0.003318215874927025, test_loss: 0.003154957146109201\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 240, train_loss: 0.0033148384528829526, test_loss: 0.003143658490630631\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 241, train_loss: 0.0033112724405482885, test_loss: 0.0031485771872282317\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 242, train_loss: 0.0033063637943436894, test_loss: 0.00315290191979446\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 243, train_loss: 0.0033023621850759165, test_loss: 0.0031577026369259097\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 244, train_loss: 0.0032995729111477633, test_loss: 0.0031443466768206027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 245, train_loss: 0.0032950845092650266, test_loss: 0.003113012257530593\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 246, train_loss: 0.003291587407792604, test_loss: 0.0031190181471174583\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 247, train_loss: 0.0032883294290367727, test_loss: 0.0031042511717458135\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 248, train_loss: 0.0032850581458395965, test_loss: 0.003103985437356198\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 249, train_loss: 0.003280614920694032, test_loss: 0.0030931087415247485\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 250, train_loss: 0.003276610142320868, test_loss: 0.003117241180660084\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 251, train_loss: 0.003273914407714454, test_loss: 0.003090793121000155\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 252, train_loss: 0.003269673823877347, test_loss: 0.0030860157205144134\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 253, train_loss: 0.003266005784220196, test_loss: 0.003087474095175425\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 254, train_loss: 0.003263127161727215, test_loss: 0.003076240185645218\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 255, train_loss: 0.0032591890184719575, test_loss: 0.0030612025248746458\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 256, train_loss: 0.003255427772191705, test_loss: 0.003067437773167442\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 257, train_loss: 0.0032521527565012756, test_loss: 0.003052108386421027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 258, train_loss: 0.0032487147653951966, test_loss: 0.003035873655626813\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 259, train_loss: 0.0032453919941151394, test_loss: 0.0030310941425014813\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 260, train_loss: 0.003242414594778984, test_loss: 0.003028051560902849\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 261, train_loss: 0.003238597895675977, test_loss: 0.0030279282330481624\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 262, train_loss: 0.003235450979667937, test_loss: 0.003023512430492048\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 263, train_loss: 0.0032316980586405905, test_loss: 0.003029123295280629\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 264, train_loss: 0.003228285464763417, test_loss: 0.003028253004557859\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 265, train_loss: 0.0032251107903200522, test_loss: 0.003015777525620368\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 266, train_loss: 0.003221533128860902, test_loss: 0.0030073592202731958\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 267, train_loss: 0.0032175525347001377, test_loss: 0.002975912364304233\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 268, train_loss: 0.0032151482337821015, test_loss: 0.003006943665707532\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 269, train_loss: 0.003210968251712309, test_loss: 0.002980766807106706\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 270, train_loss: 0.003208583867742702, test_loss: 0.0029845756518905265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 271, train_loss: 0.0032046967317244305, test_loss: 0.002977004784416073\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 272, train_loss: 0.0032017408986186554, test_loss: 0.0029575587511480525\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 273, train_loss: 0.0031981690055581688, test_loss: 0.002954217866481019\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 274, train_loss: 0.0031964459692448827, test_loss: 0.002965257289291073\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 275, train_loss: 0.0031922754983847532, test_loss: 0.002957973329242892\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 276, train_loss: 0.0031889323814831253, test_loss: 0.0029399933887478444\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 277, train_loss: 0.003186369989142071, test_loss: 0.0029433554333921233\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 278, train_loss: 0.0031821120730492233, test_loss: 0.0029439070170225864\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 279, train_loss: 0.0031799091786196, test_loss: 0.0029395561689922875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 280, train_loss: 0.0031766117126821075, test_loss: 0.0029303810502498005\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 281, train_loss: 0.0031736089893146403, test_loss: 0.0029234406015931223\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 282, train_loss: 0.0031697892340174236, test_loss: 0.002938919400688834\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 283, train_loss: 0.0031674130964575702, test_loss: 0.0029279159259600327\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 284, train_loss: 0.003163827379564303, test_loss: 0.002920873595240454\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 285, train_loss: 0.003161141078881795, test_loss: 0.0029068564670011164\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 286, train_loss: 0.0031582168577921057, test_loss: 0.0029044313466725633\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 287, train_loss: 0.0031547214059909822, test_loss: 0.0028884354079566086\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 288, train_loss: 0.003152413061667346, test_loss: 0.0028849877879763832\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 289, train_loss: 0.0031490231424968775, test_loss: 0.002881648704388704\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 290, train_loss: 0.00314582129967645, test_loss: 0.002892655538371167\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 291, train_loss: 0.0031430384260602295, test_loss: 0.002888645359258263\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 292, train_loss: 0.003139814140001019, test_loss: 0.002887418742503565\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 293, train_loss: 0.003137129158627267, test_loss: 0.0028932800312013104\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 294, train_loss: 0.003133775287714279, test_loss: 0.0028783491217883495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 295, train_loss: 0.0031307184158428807, test_loss: 0.00285284624409635\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 296, train_loss: 0.0031287325513050784, test_loss: 0.002861007865972053\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 297, train_loss: 0.0031252219896517495, test_loss: 0.0028638258958474183\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 298, train_loss: 0.0031220647124853918, test_loss: 0.0028483702753407834\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 299, train_loss: 0.0031189587034197853, test_loss: 0.00285190417810391\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 300, train_loss: 0.0031165936669758382, test_loss: 0.0028504972791639515\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 301, train_loss: 0.0031137037817871565, test_loss: 0.0028353927165228063\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 302, train_loss: 0.0031104202239764824, test_loss: 0.002830306173284323\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 303, train_loss: 0.003106951834844195, test_loss: 0.002837533402131297\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 304, train_loss: 0.0031059689500157325, test_loss: 0.0028231748949157265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 305, train_loss: 0.0031012974283891577, test_loss: 0.002823339199844318\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 306, train_loss: 0.0030996083608208643, test_loss: 0.002811892538328464\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 307, train_loss: 0.0030961793710720604, test_loss: 0.0028263527235069955\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 308, train_loss: 0.0030943433521315455, test_loss: 0.0028112257598714233\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 309, train_loss: 0.0030915875879060292, test_loss: 0.0028031669080985757\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 310, train_loss: 0.003086855596696092, test_loss: 0.002811739371958165\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 311, train_loss: 0.003085445411570283, test_loss: 0.0027836505230366946\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 312, train_loss: 0.0030828523644274193, test_loss: 0.002794339624158919\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 313, train_loss: 0.0030802444900996533, test_loss: 0.00277768444786302\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 314, train_loss: 0.0030777725000962317, test_loss: 0.0027796577625663187\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 315, train_loss: 0.003074677096894727, test_loss: 0.0027836202636349183\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 316, train_loss: 0.003072014040341694, test_loss: 0.0027768048767495947\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 317, train_loss: 0.003068898738491159, test_loss: 0.0027800213802271546\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 318, train_loss: 0.003066176931719517, test_loss: 0.0027778114036221104\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 319, train_loss: 0.00306379053111727, test_loss: 0.0027616230635873927\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 320, train_loss: 0.0030607544516453077, test_loss: 0.002767007044615009\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 321, train_loss: 0.0030586092038248293, test_loss: 0.002754961585220045\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 322, train_loss: 0.0030555980459920887, test_loss: 0.0027643831910189385\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 323, train_loss: 0.0030527093804388443, test_loss: 0.002759619579778817\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 324, train_loss: 0.003050187676226141, test_loss: 0.0027590151756810835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 325, train_loss: 0.003047425136161675, test_loss: 0.0027329582379211504\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 326, train_loss: 0.003045540646981622, test_loss: 0.0027387617357397596\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 327, train_loss: 0.003042222799213111, test_loss: 0.0027250596568382415\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 328, train_loss: 0.0030404530924255883, test_loss: 0.0027261818449448747\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 329, train_loss: 0.003036580899618192, test_loss: 0.002728376077870146\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 330, train_loss: 0.0030350855689489573, test_loss: 0.0027365889086817894\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 331, train_loss: 0.0030320569597197117, test_loss: 0.0027334240779358274\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 332, train_loss: 0.0030291785945863554, test_loss: 0.0027325048905391344\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 333, train_loss: 0.003026878240675569, test_loss: 0.0027218400112663708\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 334, train_loss: 0.0030246279977205956, test_loss: 0.0027031746942203683\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 335, train_loss: 0.003022122392695659, test_loss: 0.002701790811186537\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 336, train_loss: 0.003019048885892404, test_loss: 0.0027074515170310265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 337, train_loss: 0.003017131068056954, test_loss: 0.0027012737961903885\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 338, train_loss: 0.003014310836802894, test_loss: 0.0027094478013728242\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 339, train_loss: 0.0030120040520162862, test_loss: 0.0026985494347545914\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 340, train_loss: 0.0030095445235138354, test_loss: 0.0026819650009155083\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 341, train_loss: 0.003006571173211591, test_loss: 0.002680589477770413\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 342, train_loss: 0.00300390516987139, test_loss: 0.002706828309993188\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 343, train_loss: 0.003001745163391416, test_loss: 0.0026843536735470528\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 344, train_loss: 0.002998627548613726, test_loss: 0.0026742090233780732\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 345, train_loss: 0.002997094194520028, test_loss: 0.0026825270746112205\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 346, train_loss: 0.0029936441680553515, test_loss: 0.002672907266503152\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 347, train_loss: 0.002991419579494234, test_loss: 0.002668244113476995\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 348, train_loss: 0.002989414068994838, test_loss: 0.002669634902700543\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 349, train_loss: 0.0029870100634952815, test_loss: 0.0026721121793320305\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 350, train_loss: 0.002983907717910835, test_loss: 0.0026500843863080568\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 351, train_loss: 0.002981887173871619, test_loss: 0.002660843855641687\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 352, train_loss: 0.002979237520849226, test_loss: 0.0026648641218437264\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 353, train_loss: 0.0029771523445028575, test_loss: 0.0026733016215402107\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 354, train_loss: 0.0029747373797077326, test_loss: 0.0026445254815581185\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 355, train_loss: 0.0029725823566990354, test_loss: 0.002639007638581694\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 356, train_loss: 0.0029699561080231912, test_loss: 0.002636088587072057\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 357, train_loss: 0.0029668020190403216, test_loss: 0.0026344850357884588\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 358, train_loss: 0.0029653456310483077, test_loss: 0.002635791567557205\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 359, train_loss: 0.0029629904947735767, test_loss: 0.0026300448625055975\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 360, train_loss: 0.002960633885981297, test_loss: 0.002628465399119215\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 361, train_loss: 0.0029580686286163446, test_loss: 0.00262115208114175\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 362, train_loss: 0.0029551624186122073, test_loss: 0.002625241096934471\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 363, train_loss: 0.0029531478246029178, test_loss: 0.0026291034229917643\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 364, train_loss: 0.0029510415511915964, test_loss: 0.002615156007754521\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 365, train_loss: 0.0029478713275902096, test_loss: 0.002628504367655692\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 366, train_loss: 0.002946357079431178, test_loss: 0.002618288762721185\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 367, train_loss: 0.0029434721570782813, test_loss: 0.0025950374633211116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 368, train_loss: 0.0029415885499681667, test_loss: 0.002600487513028873\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 369, train_loss: 0.0029387275795810497, test_loss: 0.00259812977254534\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 370, train_loss: 0.002937238232815096, test_loss: 0.0025973939445042885\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 371, train_loss: 0.0029346601247949443, test_loss: 0.002581061969976872\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 372, train_loss: 0.0029322379220037232, test_loss: 0.0026010745122897415\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 373, train_loss: 0.0029295397403394574, test_loss: 0.0025903186313091563\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 374, train_loss: 0.002927858412637995, test_loss: 0.002577687775975881\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 375, train_loss: 0.0029252253135628875, test_loss: 0.002585876597246776\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 376, train_loss: 0.002923540698199293, test_loss: 0.002577594441857237\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 377, train_loss: 0.0029204794290213906, test_loss: 0.0026023203345875326\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 378, train_loss: 0.002919103755392944, test_loss: 0.002582721680459471\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 379, train_loss: 0.0029158432703982277, test_loss: 0.0025785088987621027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 380, train_loss: 0.0029144768233653967, test_loss: 0.002573050295513195\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 381, train_loss: 0.0029114035270052237, test_loss: 0.0025563729137409097\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 382, train_loss: 0.002909347013746354, test_loss: 0.0025741980778268324\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 383, train_loss: 0.002907228958637245, test_loss: 0.002560658750903363\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 384, train_loss: 0.002904990234180201, test_loss: 0.002560081545081443\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 385, train_loss: 0.0029022021087339728, test_loss: 0.002540984351444655\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 386, train_loss: 0.00290100043522736, test_loss: 0.0025460402602897002\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 387, train_loss: 0.0028977984168105774, test_loss: 0.00253154900639903\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 388, train_loss: 0.0028964989253297997, test_loss: 0.0025335227565404074\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 389, train_loss: 0.0028943325417041528, test_loss: 0.002541093885576209\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 390, train_loss: 0.002892063290346414, test_loss: 0.0025402089948049532\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 391, train_loss: 0.002889884527956438, test_loss: 0.0025504898098566067\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 392, train_loss: 0.0028867177718963763, test_loss: 0.0025251836218208503\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 393, train_loss: 0.002885092403949939, test_loss: 0.002522231803502505\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 394, train_loss: 0.0028831304555578174, test_loss: 0.002549262864751001\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 395, train_loss: 0.002880740989611662, test_loss: 0.002528003627771679\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 396, train_loss: 0.002878481415326218, test_loss: 0.0025252036773450435\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 397, train_loss: 0.0028758135767899652, test_loss: 0.002522608282444521\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 398, train_loss: 0.0028749495473329016, test_loss: 0.0025231928683262174\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 399, train_loss: 0.002872137279042594, test_loss: 0.0025169133445253978\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 400, train_loss: 0.002869596594460246, test_loss: 0.002528263271625082\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 401, train_loss: 0.0028672802468752458, test_loss: 0.002505206535435998\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 402, train_loss: 0.0028656787945878035, test_loss: 0.0024944911975837434\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 403, train_loss: 0.002864033876167592, test_loss: 0.002515593073136794\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 404, train_loss: 0.002861557171927659, test_loss: 0.002508068812461809\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 405, train_loss: 0.002859085029883491, test_loss: 0.0025098304915725468\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 406, train_loss: 0.0028568020019988627, test_loss: 0.002494293023259021\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 407, train_loss: 0.002854690197886382, test_loss: 0.002502282188894848\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 408, train_loss: 0.002853083570470666, test_loss: 0.0024907432939821426\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 409, train_loss: 0.002850638345868193, test_loss: 0.002476943630310229\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 410, train_loss: 0.002849070960149655, test_loss: 0.0024944228203728413\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 411, train_loss: 0.002847059235836451, test_loss: 0.002485859139900034\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 412, train_loss: 0.0028443724949950085, test_loss: 0.002480133405650775\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 413, train_loss: 0.0028428979512743254, test_loss: 0.0024872542223117002\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 414, train_loss: 0.0028403338771406873, test_loss: 0.00247584632187425\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 415, train_loss: 0.0028382940867091888, test_loss: 0.0024904907958653686\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 416, train_loss: 0.0028366834222320406, test_loss: 0.0024766170722730937\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 417, train_loss: 0.002834375709237649, test_loss: 0.0024703614281651634\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 418, train_loss: 0.002832242391846239, test_loss: 0.0024661126741333506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 419, train_loss: 0.002830793606670243, test_loss: 0.0024577994265247327\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 420, train_loss: 0.0028280045029848975, test_loss: 0.0024756167008607196\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 421, train_loss: 0.002826456185531519, test_loss: 0.00245601302856066\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 422, train_loss: 0.002823929018169691, test_loss: 0.0024517891806914494\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 423, train_loss: 0.0028218027466306943, test_loss: 0.0024512859948654063\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 424, train_loss: 0.0028204156646900575, test_loss: 0.0024464781226691767\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 425, train_loss: 0.002818521069959242, test_loss: 0.0024518184162353952\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 426, train_loss: 0.002815332869340332, test_loss: 0.0024375876043147096\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 427, train_loss: 0.002814218741615792, test_loss: 0.0024455254307936113\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 428, train_loss: 0.002811910628370729, test_loss: 0.0024373341248764726\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 429, train_loss: 0.002809901897371887, test_loss: 0.0024464350523582348\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 430, train_loss: 0.0028079585308508083, test_loss: 0.002442834104817265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 431, train_loss: 0.0028054764483398118, test_loss: 0.002432263908886876\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 432, train_loss: 0.002803563082611724, test_loss: 0.002451649655808862\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 433, train_loss: 0.002802438097795702, test_loss: 0.0024299046628398057\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 434, train_loss: 0.002800512193767572, test_loss: 0.002432677172085581\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 435, train_loss: 0.0027980110395846134, test_loss: 0.002434660038069034\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 436, train_loss: 0.0027957342379729584, test_loss: 0.0024227527722429773\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 437, train_loss: 0.0027941411448117767, test_loss: 0.0024177722173193707\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 438, train_loss: 0.002791494486174456, test_loss: 0.0024329537155632027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 439, train_loss: 0.002790522083981168, test_loss: 0.0024294990476053688\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 440, train_loss: 0.002788319071384153, test_loss: 0.0024055157532879654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 441, train_loss: 0.0027862070472895427, test_loss: 0.0024097363821989023\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 442, train_loss: 0.002784392819331062, test_loss: 0.002417893728796536\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 443, train_loss: 0.0027818996515701273, test_loss: 0.0024119411813071333\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 444, train_loss: 0.0027802856323654985, test_loss: 0.0024110896808381835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 445, train_loss: 0.002778411283777609, test_loss: 0.002399872576158183\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 446, train_loss: 0.002776610269444842, test_loss: 0.0024117097080959818\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 447, train_loss: 0.002774389689814698, test_loss: 0.0024224411399933533\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 448, train_loss: 0.002772707616510088, test_loss: 0.002388613716063376\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 449, train_loss: 0.0027708057753148374, test_loss: 0.0024040621240438223\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 450, train_loss: 0.0027693908988267657, test_loss: 0.0023869034921517596\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 451, train_loss: 0.002766895176450345, test_loss: 0.0024005012535967696\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 452, train_loss: 0.0027651062018563505, test_loss: 0.0023958995406223363\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 453, train_loss: 0.0027635601295834178, test_loss: 0.0023765508716072268\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 454, train_loss: 0.002760594896037742, test_loss: 0.0023869868675366235\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 455, train_loss: 0.002759703306323945, test_loss: 0.0023797714574566972\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 456, train_loss: 0.0027569947471487084, test_loss: 0.0023682133021513717\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 457, train_loss: 0.002755480239488633, test_loss: 0.0023814014581521638\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 458, train_loss: 0.0027534829134925154, test_loss: 0.0023721254237431986\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 459, train_loss: 0.0027516904218125478, test_loss: 0.002378390398613798\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 460, train_loss: 0.00274931846469186, test_loss: 0.002389702592224169\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 461, train_loss: 0.002748687271913358, test_loss: 0.00237517007581818\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 462, train_loss: 0.002745885299954775, test_loss: 0.002359675296480791\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 463, train_loss: 0.002744496555883674, test_loss: 0.0023604393996393834\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 464, train_loss: 0.0027427080172631128, test_loss: 0.0023600723596665864\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 465, train_loss: 0.002740325343119211, test_loss: 0.0023510229623822784\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 466, train_loss: 0.002739210041118381, test_loss: 0.0023606063515240424\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 467, train_loss: 0.0027372484100456145, test_loss: 0.0023531151722030095\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 468, train_loss: 0.00273531391581265, test_loss: 0.0023506189721117085\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 469, train_loss: 0.0027330105917537616, test_loss: 0.0023531079734824835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 470, train_loss: 0.0027312973518482096, test_loss: 0.002343275014517041\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 471, train_loss: 0.0027295927556829657, test_loss: 0.002342376874413532\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 472, train_loss: 0.0027272942583824204, test_loss: 0.0023387374463849342\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 473, train_loss: 0.0027260102077004155, test_loss: 0.002336160123097495\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 474, train_loss: 0.002723838451449264, test_loss: 0.0023426469296025923\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 475, train_loss: 0.0027222047866723906, test_loss: 0.002340931284169738\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 476, train_loss: 0.0027201846218590053, test_loss: 0.0023382523612758047\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 477, train_loss: 0.002718702350730656, test_loss: 0.0023431530254385555\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 478, train_loss: 0.0027171091710366806, test_loss: 0.0023252785549863265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 479, train_loss: 0.002715178998381859, test_loss: 0.002330668099696199\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 480, train_loss: 0.0027130984644371282, test_loss: 0.002329615304076399\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 481, train_loss: 0.002711689741900471, test_loss: 0.002323594835336105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 482, train_loss: 0.002709101908139411, test_loss: 0.0023219899904245557\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 483, train_loss: 0.0027080899328156918, test_loss: 0.002329719066321372\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 484, train_loss: 0.0027068417733862224, test_loss: 0.002323342512961692\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 485, train_loss: 0.0027045231897704753, test_loss: 0.002318300513135723\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 486, train_loss: 0.0027018716507419996, test_loss: 0.002318067038476539\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 487, train_loss: 0.002700763080973041, test_loss: 0.00230559250989105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 488, train_loss: 0.00269896700767252, test_loss: 0.0022996205531317405\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 489, train_loss: 0.0026974547481130075, test_loss: 0.0023112707724346994\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 490, train_loss: 0.0026958782097751457, test_loss: 0.002300631896688197\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 491, train_loss: 0.002693847582378054, test_loss: 0.0023038302483305764\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 492, train_loss: 0.0026918454393573157, test_loss: 0.002310539459591755\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 493, train_loss: 0.0026895650402591394, test_loss: 0.002293623359247039\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 494, train_loss: 0.0026888052002763544, test_loss: 0.0023007542338932697\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 495, train_loss: 0.002686915852314011, test_loss: 0.002295461823963799\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 496, train_loss: 0.0026851848687082652, test_loss: 0.0022979330231184857\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 497, train_loss: 0.0026831964416706615, test_loss: 0.002298878912906497\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 498, train_loss: 0.002681941506694255, test_loss: 0.0022919319214848564\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 499, train_loss: 0.002679530773148327, test_loss: 0.002288037588303455\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 500, train_loss: 0.0026777903930935076, test_loss: 0.0022928221348989517\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 501, train_loss: 0.002676055118862857, test_loss: 0.00228206025405178\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 502, train_loss: 0.0026747174553236734, test_loss: 0.0022859562013763934\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 503, train_loss: 0.0026720356341002876, test_loss: 0.002272440899664966\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 504, train_loss: 0.002671011087965893, test_loss: 0.0022742178967186753\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 505, train_loss: 0.0026698192372454025, test_loss: 0.002277493477987054\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 506, train_loss: 0.0026674206048580367, test_loss: 0.002268336043179704\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 507, train_loss: 0.0026660506320762196, test_loss: 0.0022729184763589636\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 508, train_loss: 0.0026644531059831092, test_loss: 0.002269149450465846\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 509, train_loss: 0.0026622187011574633, test_loss: 0.0022681481499696532\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 510, train_loss: 0.0026606228987119443, test_loss: 0.0022794626207681946\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 511, train_loss: 0.002659423813694172, test_loss: 0.002271449213507227\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 512, train_loss: 0.0026575425284662375, test_loss: 0.0022593209462520927\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 513, train_loss: 0.002655743539331786, test_loss: 0.0022686629784472575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 514, train_loss: 0.002654096175897804, test_loss: 0.0022659908583871303\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 515, train_loss: 0.0026521648534956733, test_loss: 0.002249445445695295\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 516, train_loss: 0.0026509649492878107, test_loss: 0.0022552070119836107\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 517, train_loss: 0.0026496401449695017, test_loss: 0.0022558696267272658\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 518, train_loss: 0.00264752604885737, test_loss: 0.002255181025621147\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 519, train_loss: 0.0026457633195915275, test_loss: 0.0022577277010826105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 520, train_loss: 0.002644196751700359, test_loss: 0.0022556352759681595\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 521, train_loss: 0.002641999001835121, test_loss: 0.0022628382072210885\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 522, train_loss: 0.0026406567626676355, test_loss: 0.0022499062802242595\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 523, train_loss: 0.00263909445250139, test_loss: 0.002236003495337895\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 524, train_loss: 0.002636217249866519, test_loss: 0.0022408323760222024\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 525, train_loss: 0.002636391467537052, test_loss: 0.002238105792620888\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 526, train_loss: 0.0026344404927490708, test_loss: 0.002230899376370825\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 527, train_loss: 0.0026327459866579707, test_loss: 0.00222622793882739\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 528, train_loss: 0.0026310293671066086, test_loss: 0.002234412674741366\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 529, train_loss: 0.002628656966225509, test_loss: 0.0022344670474129277\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 530, train_loss: 0.002627067008199027, test_loss: 0.0022219208670624844\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 531, train_loss: 0.0026258049864885725, test_loss: 0.0022278452196564428\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 532, train_loss: 0.002624250480431727, test_loss: 0.002221087309105011\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 533, train_loss: 0.0026226664732138696, test_loss: 0.0022222779535252648\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 534, train_loss: 0.0026209582641828232, test_loss: 0.0022250380977474823\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 535, train_loss: 0.0026196734377231298, test_loss: 0.002223077455523591\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 536, train_loss: 0.0026176658170500204, test_loss: 0.0022164220617224392\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 537, train_loss: 0.002616332435862077, test_loss: 0.00220686391231795\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 538, train_loss: 0.002614538633633704, test_loss: 0.0022147583233443303\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 539, train_loss: 0.0026129523913584725, test_loss: 0.0022192712468751827\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 540, train_loss: 0.0026112177239280866, test_loss: 0.0022060858382013603\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 541, train_loss: 0.002609408690273313, test_loss: 0.0022104271640553353\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 542, train_loss: 0.002608906787124783, test_loss: 0.002211249249506718\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 543, train_loss: 0.002606468906615982, test_loss: 0.002210918721408607\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 544, train_loss: 0.0026051326470689406, test_loss: 0.0021988618611477506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 545, train_loss: 0.002603179010983874, test_loss: 0.0022028172250616197\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 546, train_loss: 0.002601747400669325, test_loss: 0.002214566272764037\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 547, train_loss: 0.002600203280494452, test_loss: 0.00219411657561954\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 548, train_loss: 0.002598561569043812, test_loss: 0.002197267884278121\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 549, train_loss: 0.0025971122856958953, test_loss: 0.0021910882384802858\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 550, train_loss: 0.0025945709806860714, test_loss: 0.002202046171311313\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 551, train_loss: 0.0025940722335024073, test_loss: 0.002187480238111069\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 552, train_loss: 0.002592543809149838, test_loss: 0.00219624926103279\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 553, train_loss: 0.00259098488748709, test_loss: 0.002198893412311848\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 554, train_loss: 0.002588837460675479, test_loss: 0.0021882044839469763\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 555, train_loss: 0.0025874725439785093, test_loss: 0.002186163742011652\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 556, train_loss: 0.002585939546820418, test_loss: 0.0021858576745636617\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 557, train_loss: 0.002584341427164906, test_loss: 0.002179752328606227\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 558, train_loss: 0.002582647262240782, test_loss: 0.0021769177856652113\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 559, train_loss: 0.002580852116661358, test_loss: 0.0021896249941514376\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 560, train_loss: 0.0025802795970103825, test_loss: 0.002177787709605092\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 561, train_loss: 0.00257806425013215, test_loss: 0.0021774060546587673\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 562, train_loss: 0.0025768651999611048, test_loss: 0.002177358828096961\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 563, train_loss: 0.002575041316831204, test_loss: 0.002171053403379539\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 564, train_loss: 0.00257357314723874, test_loss: 0.0021740312681336384\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 565, train_loss: 0.0025719443687905923, test_loss: 0.0021618369075720413\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 566, train_loss: 0.0025707175260188562, test_loss: 0.0021698305290789367\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 567, train_loss: 0.0025691079921135833, test_loss: 0.0021621068422413934\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 568, train_loss: 0.0025675267752619728, test_loss: 0.0021732507593696937\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 569, train_loss: 0.0025665075753746945, test_loss: 0.0021610880089261066\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 570, train_loss: 0.002564687904415895, test_loss: 0.0021622256194509994\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 571, train_loss: 0.0025629508028735086, test_loss: 0.002156103727592824\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 572, train_loss: 0.002561361569814683, test_loss: 0.002154749268158458\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 573, train_loss: 0.002559878256593996, test_loss: 0.002171637359051368\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 574, train_loss: 0.0025586044904659113, test_loss: 0.002153423976732227\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 575, train_loss: 0.0025571637595559853, test_loss: 0.002155296479498084\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 576, train_loss: 0.002555277904247971, test_loss: 0.002168570816791497\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 577, train_loss: 0.0025540975317277985, test_loss: 0.0021453014237639042\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 578, train_loss: 0.0025521317478596393, test_loss: 0.0021577211479710126\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 579, train_loss: 0.002551182611502028, test_loss: 0.0021493922298153243\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 580, train_loss: 0.002549197918028387, test_loss: 0.0021457777933951896\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 581, train_loss: 0.00254735490443484, test_loss: 0.0021620413858610657\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 582, train_loss: 0.00254668586489894, test_loss: 0.0021517047302600425\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 583, train_loss: 0.002544855612118333, test_loss: 0.002131073946097436\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 584, train_loss: 0.002543542148657517, test_loss: 0.0021297937185945325\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 585, train_loss: 0.0025420281777149036, test_loss: 0.0021412953165753815\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 586, train_loss: 0.0025405833269095666, test_loss: 0.002147717242433618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 587, train_loss: 0.002538636560593872, test_loss: 0.0021242920297365156\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 588, train_loss: 0.0025371459683981167, test_loss: 0.0021454491992093003\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 589, train_loss: 0.0025364437564330666, test_loss: 0.002123329809341484\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 590, train_loss: 0.0025347023583282046, test_loss: 0.0021275646860699337\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 591, train_loss: 0.0025336173280098642, test_loss: 0.002125187570644089\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 592, train_loss: 0.0025320295674685876, test_loss: 0.0021278061377905453\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 593, train_loss: 0.0025303425852530154, test_loss: 0.002114381167652587\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 594, train_loss: 0.002528546212932537, test_loss: 0.002113970792822492\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 595, train_loss: 0.002527490912323014, test_loss: 0.0021221924644823256\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 596, train_loss: 0.0025256579523994884, test_loss: 0.002119309691270479\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 597, train_loss: 0.002524263876876529, test_loss: 0.002117296891624275\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 598, train_loss: 0.0025230398618485343, test_loss: 0.0021147467505682306\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 599, train_loss: 0.00252172162652757, test_loss: 0.0021252237343921876\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 600, train_loss: 0.002519897048799228, test_loss: 0.00212247697046946\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 601, train_loss: 0.002518717988579531, test_loss: 0.002114525586456204\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 602, train_loss: 0.0025170278846617963, test_loss: 0.0021058185565589857\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 603, train_loss: 0.0025160851256445325, test_loss: 0.002110772178499386\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 604, train_loss: 0.002514053471339711, test_loss: 0.0021033683673848803\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 605, train_loss: 0.0025129101374832897, test_loss: 0.0021037453645989536\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 606, train_loss: 0.002511825977067734, test_loss: 0.002096933491441469\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 607, train_loss: 0.0025102595682255924, test_loss: 0.002102384669855476\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 608, train_loss: 0.002508768876843204, test_loss: 0.0020969793516157484\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 609, train_loss: 0.0025066587224020233, test_loss: 0.002091243104167426\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 610, train_loss: 0.002505390905135819, test_loss: 0.0020940629828822776\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 611, train_loss: 0.002504551476591818, test_loss: 0.0020983929451829633\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 612, train_loss: 0.0025028967415177293, test_loss: 0.002084127318737909\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 613, train_loss: 0.0025014011767524667, test_loss: 0.0020923407528943452\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 614, train_loss: 0.0024999181800724373, test_loss: 0.0020988334917302173\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 615, train_loss: 0.0024980968440630564, test_loss: 0.0020852447983671506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 616, train_loss: 0.0024970114715070304, test_loss: 0.002085172432438972\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 617, train_loss: 0.002495983803962485, test_loss: 0.002083398390170306\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 618, train_loss: 0.0024949109914127027, test_loss: 0.002088657159803029\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 619, train_loss: 0.002492833089271878, test_loss: 0.0020821346264804164\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 620, train_loss: 0.002491410702146141, test_loss: 0.002073172783219399\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 621, train_loss: 0.0024904334565498446, test_loss: 0.0020793078964253743\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 622, train_loss: 0.002488907692910337, test_loss: 0.0020795825707281414\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 623, train_loss: 0.0024876244907169275, test_loss: 0.002077880224626428\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 624, train_loss: 0.0024865057862933763, test_loss: 0.0020699067647980815\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 625, train_loss: 0.0024849688069720484, test_loss: 0.00207499441883276\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 626, train_loss: 0.002483335415810116, test_loss: 0.0020689653079338875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 627, train_loss: 0.0024814453494990474, test_loss: 0.0020659575899256966\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 628, train_loss: 0.0024804433743098758, test_loss: 0.002068407438622298\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 629, train_loss: 0.002479213176323973, test_loss: 0.002064853802902433\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 630, train_loss: 0.0024777446795810937, test_loss: 0.002072074649055512\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 631, train_loss: 0.0024761079752546765, test_loss: 0.0020707027019503024\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 632, train_loss: 0.0024746273318967506, test_loss: 0.002067629470846628\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 633, train_loss: 0.0024739639404507035, test_loss: 0.002056789568973144\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 634, train_loss: 0.0024723295962599955, test_loss: 0.00205282605803894\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 635, train_loss: 0.0024706926692260062, test_loss: 0.0020495836495403917\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 636, train_loss: 0.002469811328051356, test_loss: 0.0020461102002837625\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 637, train_loss: 0.002468329736628853, test_loss: 0.002051066794732693\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 638, train_loss: 0.0024669529624083966, test_loss: 0.002053572329504487\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 639, train_loss: 0.0024654951704242405, test_loss: 0.0020480836231936105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 640, train_loss: 0.0024638077866926127, test_loss: 0.0020563333882586076\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 641, train_loss: 0.002462382860135997, test_loss: 0.002067182845516632\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 642, train_loss: 0.0024611493036264622, test_loss: 0.0020549922076334506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 643, train_loss: 0.002460410649577517, test_loss: 0.0020437322647269005\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 644, train_loss: 0.002457882051845159, test_loss: 0.00202888855113624\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 645, train_loss: 0.002457934015907073, test_loss: 0.0020345089537724373\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 646, train_loss: 0.0024557604305921153, test_loss: 0.0020507070771717015\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 647, train_loss: 0.0024543199778719804, test_loss: 0.0020424552531972625\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 648, train_loss: 0.002453505473205957, test_loss: 0.002038207330863374\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 649, train_loss: 0.002451993068321966, test_loss: 0.0020313080631939764\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 650, train_loss: 0.00245022231895542, test_loss: 0.0020229113428146006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 651, train_loss: 0.0024499549525774343, test_loss: 0.0020334511392343887\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 652, train_loss: 0.002448150322750759, test_loss: 0.002029606474351544\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 653, train_loss: 0.00244672301795371, test_loss: 0.0020398905348534193\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 654, train_loss: 0.002444971967986894, test_loss: 0.0020306375773566873\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 655, train_loss: 0.002444024679479174, test_loss: 0.002026081524001291\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 656, train_loss: 0.0024424855730474544, test_loss: 0.002023839573293793\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 657, train_loss: 0.002441257001177243, test_loss: 0.0020315230867709033\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 658, train_loss: 0.0024394767644486536, test_loss: 0.002014505993336057\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 659, train_loss: 0.002438905790707495, test_loss: 0.0020244358139536464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 660, train_loss: 0.0024374442409224074, test_loss: 0.0020330522641485846\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 661, train_loss: 0.002436162618317495, test_loss: 0.0020331530820560427\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 662, train_loss: 0.0024347250582650304, test_loss: 0.0020302622750676116\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 663, train_loss: 0.0024334132055406237, test_loss: 0.00202295037273031\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 664, train_loss: 0.002432191097056867, test_loss: 0.002021049648203935\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 665, train_loss: 0.0024310286570711054, test_loss: 0.002011178386918544\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 666, train_loss: 0.002429696856639706, test_loss: 0.0020161669200229556\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 667, train_loss: 0.0024285475909554192, test_loss: 0.00201481578141027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 668, train_loss: 0.002426944019539238, test_loss: 0.0020104754051620453\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 669, train_loss: 0.002425727088227837, test_loss: 0.0020089795251782886\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 670, train_loss: 0.0024244919046292387, test_loss: 0.002011951022909787\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 671, train_loss: 0.0024229924997157515, test_loss: 0.00201500819243031\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 672, train_loss: 0.0024216988689093276, test_loss: 0.002014623130656755\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 673, train_loss: 0.0024198752892768964, test_loss: 0.001997519182809927\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 674, train_loss: 0.002418840118102966, test_loss: 0.002022383820775968\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 675, train_loss: 0.002417583766577915, test_loss: 0.001996002182400284\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 676, train_loss: 0.002416736472796736, test_loss: 0.001992163817004229\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 677, train_loss: 0.002415548408396984, test_loss: 0.0019908874625495323\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 678, train_loss: 0.0024137686203930054, test_loss: 0.0019895108948381115\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 679, train_loss: 0.0024124687214586013, test_loss: 0.001988855406988221\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 680, train_loss: 0.002411717305720681, test_loss: 0.0020009699006690285\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 681, train_loss: 0.002408294684259863, test_loss: 0.0019809999299841598\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 682, train_loss: 0.0024093810137381533, test_loss: 0.001986910659000457\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 683, train_loss: 0.0024076576909614123, test_loss: 0.001995191958252317\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 684, train_loss: 0.002406292716738142, test_loss: 0.0019858666210217425\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 685, train_loss: 0.002405541560890614, test_loss: 0.0019866012727829795\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 686, train_loss: 0.002403761068262459, test_loss: 0.0019928761073223986\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 687, train_loss: 0.0024026030750305123, test_loss: 0.001978587074494825\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 688, train_loss: 0.002401082411569615, test_loss: 0.00199335452746779\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 689, train_loss: 0.002400320350546016, test_loss: 0.0019976769020640626\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 690, train_loss: 0.002398277924319785, test_loss: 0.001972350455523552\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 691, train_loss: 0.0023978712211634003, test_loss: 0.0019717611671344806\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 692, train_loss: 0.00239662842121891, test_loss: 0.0019773347290211883\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 693, train_loss: 0.002394837700103076, test_loss: 0.0019821772399994256\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 694, train_loss: 0.0023942526541245374, test_loss: 0.0019800184988222704\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 695, train_loss: 0.002392592404020583, test_loss: 0.001976035015384416\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 696, train_loss: 0.0023907556200094744, test_loss: 0.001976848410357399\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 697, train_loss: 0.0023899947319967817, test_loss: 0.0019652706060486916\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 698, train_loss: 0.002389344905271821, test_loss: 0.0019647109511424787\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 699, train_loss: 0.002387646859609849, test_loss: 0.001966210411643633\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 700, train_loss: 0.002386314348616703, test_loss: 0.0019643268343105982\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 701, train_loss: 0.0023852743064933287, test_loss: 0.0019710608713257196\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 702, train_loss: 0.0023840900050533815, test_loss: 0.0019655885428223065\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 703, train_loss: 0.002382624682185418, test_loss: 0.001972038832602079\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 704, train_loss: 0.0023815450951594074, test_loss: 0.0019612888602587655\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 705, train_loss: 0.0023796498215014434, test_loss: 0.001972401962960318\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 706, train_loss: 0.0023790825825896313, test_loss: 0.0019522362990522734\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 707, train_loss: 0.0023774862310648225, test_loss: 0.0019702401860297704\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 708, train_loss: 0.002377041452992583, test_loss: 0.0019603711605155603\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 709, train_loss: 0.0023754066246465147, test_loss: 0.001960549742925184\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 710, train_loss: 0.0023738407593354408, test_loss: 0.0019509439852509575\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 711, train_loss: 0.0023721258048346595, test_loss: 0.0019662768230400383\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 712, train_loss: 0.0023711331540650147, test_loss: 0.0019503478126866051\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 713, train_loss: 0.0023707386118322488, test_loss: 0.0019534147252740627\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 714, train_loss: 0.002368918176289542, test_loss: 0.0019632667828819905\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 715, train_loss: 0.0023676858751382054, test_loss: 0.0019617739556288608\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 716, train_loss: 0.0023673670026699524, test_loss: 0.0019511860800542845\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 717, train_loss: 0.002365353357893287, test_loss: 0.001942519703917265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 718, train_loss: 0.0023648108524678395, test_loss: 0.001947540781074144\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 719, train_loss: 0.00236233184133566, test_loss: 0.0019418337439837602\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 720, train_loss: 0.002361897466276744, test_loss: 0.0019321730108528684\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 721, train_loss: 0.0023611346853470248, test_loss: 0.001939176711088154\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 722, train_loss: 0.0023596116785897583, test_loss: 0.0019479420680466157\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 723, train_loss: 0.0023582004292760617, test_loss: 0.0019369528023302197\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 724, train_loss: 0.002357131560343975, test_loss: 0.001938340571098543\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 725, train_loss: 0.002355918384400075, test_loss: 0.0019298899224407088\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 726, train_loss: 0.0023549371694890553, test_loss: 0.0019344152063310433\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 727, train_loss: 0.0023536892885413882, test_loss: 0.0019371156531866663\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 728, train_loss: 0.0023525474938785153, test_loss: 0.0019378298471938483\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 729, train_loss: 0.0023508982644899184, test_loss: 0.0019468937191562967\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 730, train_loss: 0.002350482119719987, test_loss: 0.0019326948977556103\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 731, train_loss: 0.002348743903179681, test_loss: 0.0019206265065944395\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 732, train_loss: 0.0023475177496528397, test_loss: 0.0019325603703365256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 733, train_loss: 0.0023469524263376278, test_loss: 0.0019242983727077118\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 734, train_loss: 0.0023449485318308514, test_loss: 0.001940298414504072\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 735, train_loss: 0.0023446340114072926, test_loss: 0.0019391829525538457\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 736, train_loss: 0.002342880122016268, test_loss: 0.0019140493485536308\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 737, train_loss: 0.0023414833292273187, test_loss: 0.001919876330924662\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 738, train_loss: 0.0023410599505494546, test_loss: 0.001929817126857905\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 739, train_loss: 0.0023398136082113125, test_loss: 0.001913200972268943\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 740, train_loss: 0.0023380496753706756, test_loss: 0.0019154696214499357\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 741, train_loss: 0.0023379018925438733, test_loss: 0.0019190598918029512\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 742, train_loss: 0.002336121185676484, test_loss: 0.0019173797541626324\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 743, train_loss: 0.002335239399378702, test_loss: 0.0019166984612895295\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 744, train_loss: 0.0023337315403378217, test_loss: 0.0019150235168173169\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 745, train_loss: 0.002333121800294495, test_loss: 0.0019112786387081426\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 746, train_loss: 0.0023315741105688842, test_loss: 0.0019162290916891578\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 747, train_loss: 0.0023303398212073776, test_loss: 0.001918402179002535\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 748, train_loss: 0.0023289900020981224, test_loss: 0.0019069705570776325\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 749, train_loss: 0.0023282354096625076, test_loss: 0.0019062887935060412\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 750, train_loss: 0.0023270261763629306, test_loss: 0.0019039169665820037\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 751, train_loss: 0.0023255437108521417, test_loss: 0.0019143719870883685\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 752, train_loss: 0.002325128124505873, test_loss: 0.0019157978352338362\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 753, train_loss: 0.0023235625737887127, test_loss: 0.0018972921454219315\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 754, train_loss: 0.00232253022451829, test_loss: 0.0019013935680283257\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 755, train_loss: 0.0023212903869349235, test_loss: 0.0019034042410427132\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 756, train_loss: 0.0023195136493822576, test_loss: 0.001897321517903131\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 757, train_loss: 0.002319112922731525, test_loss: 0.0018992960863098956\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 758, train_loss: 0.0023177430444641543, test_loss: 0.0019010080448844326\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 759, train_loss: 0.0023168233962243667, test_loss: 0.0018898554374642957\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 760, train_loss: 0.0023154834078828864, test_loss: 0.0018922684580618993\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 761, train_loss: 0.002314266326380146, test_loss: 0.0018963325960495963\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 762, train_loss: 0.0023132187961040058, test_loss: 0.0018953694351904022\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 763, train_loss: 0.002311813284041061, test_loss: 0.0018958582151800585\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 764, train_loss: 0.002310744838817917, test_loss: 0.0019081955374881195\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 765, train_loss: 0.0023099322841085342, test_loss: 0.0018980881497368384\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 766, train_loss: 0.0023088462233783235, test_loss: 0.0019013020153999184\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 767, train_loss: 0.002307631559050907, test_loss: 0.0018943870169235966\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 768, train_loss: 0.0023066091891822023, test_loss: 0.0018853754829764606\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 769, train_loss: 0.002304936495195314, test_loss: 0.0018950026103247625\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 770, train_loss: 0.002304484483215142, test_loss: 0.0018772273962475504\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 771, train_loss: 0.0023031583904439775, test_loss: 0.0018817538780590089\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 772, train_loss: 0.0023012664700534356, test_loss: 0.0018723455964041373\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 773, train_loss: 0.002300258207463513, test_loss: 0.0018973565650715802\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 774, train_loss: 0.0022997887482723365, test_loss: 0.001883806105000146\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 775, train_loss: 0.0022989751667525605, test_loss: 0.001876804115361152\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 776, train_loss: 0.0022976334436670457, test_loss: 0.0018802101026850347\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 777, train_loss: 0.0022959277355213367, test_loss: 0.0018769849826080294\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 778, train_loss: 0.0022950212095076074, test_loss: 0.0018844086514899316\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 779, train_loss: 0.0022944807674366683, test_loss: 0.001876936274362752\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 780, train_loss: 0.0022927672494386627, test_loss: 0.0018809183264891498\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 781, train_loss: 0.0022920038181016016, test_loss: 0.0018755616420560565\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 782, train_loss: 0.002291119148285848, test_loss: 0.0018686894584859076\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 783, train_loss: 0.002289931880396613, test_loss: 0.0018700921879290268\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 784, train_loss: 0.0022884776253552243, test_loss: 0.0018727401801674531\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 785, train_loss: 0.002287001222006846, test_loss: 0.0018605498635457852\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 786, train_loss: 0.0022864405773027198, test_loss: 0.0018597583748021114\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 787, train_loss: 0.002285418680492932, test_loss: 0.0018715720387915962\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 788, train_loss: 0.002284514091684667, test_loss: 0.001863028241458755\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 789, train_loss: 0.00228338861375826, test_loss: 0.0018730186362891123\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 790, train_loss: 0.0022819992908226783, test_loss: 0.0018608199752634391\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 791, train_loss: 0.0022814391799158974, test_loss: 0.0018679945269235386\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 792, train_loss: 0.002279750320746604, test_loss: 0.0018573052412904918\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 793, train_loss: 0.002278958245010791, test_loss: 0.0018653495454390605\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 794, train_loss: 0.002278038358586812, test_loss: 0.0018602272192275939\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 795, train_loss: 0.0022765016822762674, test_loss: 0.0018455817359305608\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 796, train_loss: 0.0022751234814814324, test_loss: 0.0018701382647029674\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 797, train_loss: 0.0022744416048202077, test_loss: 0.0018527880253177732\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 798, train_loss: 0.0022736487945776906, test_loss: 0.0018449741567168625\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 799, train_loss: 0.0022725329889540552, test_loss: 0.0018542490911502868\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 800, train_loss: 0.002271344434656047, test_loss: 0.0018452333511399606\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 801, train_loss: 0.0022704259216603214, test_loss: 0.0018601472844378664\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 802, train_loss: 0.002268890863386478, test_loss: 0.0018455496213461559\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 803, train_loss: 0.0022686660280225356, test_loss: 0.001850076511357493\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 804, train_loss: 0.0022668907955953896, test_loss: 0.001847988768415048\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 805, train_loss: 0.0022660899399444896, test_loss: 0.0018415807131439066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 806, train_loss: 0.0022648975280748934, test_loss: 0.0018486788814278464\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 807, train_loss: 0.0022638993465067528, test_loss: 0.001838846570022631\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 808, train_loss: 0.0022627090033452125, test_loss: 0.0018412725444366701\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 809, train_loss: 0.002261958151092491, test_loss: 0.0018486423160095748\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 810, train_loss: 0.0022611719647455137, test_loss: 0.0018448081262669382\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 811, train_loss: 0.002259769150091765, test_loss: 0.0018357586144329384\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 812, train_loss: 0.002257683970776611, test_loss: 0.0018421787373461905\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 813, train_loss: 0.002258107649885822, test_loss: 0.0018407536315349622\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 814, train_loss: 0.0022565135492843553, test_loss: 0.0018310046572691929\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 815, train_loss: 0.002255252712265274, test_loss: 0.001835312233500493\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 816, train_loss: 0.0022545176265990194, test_loss: 0.001833072358866234\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 817, train_loss: 0.0022527837663862457, test_loss: 0.0018541459249741493\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 818, train_loss: 0.0022524752127346163, test_loss: 0.0018283226637345834\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 819, train_loss: 0.002251017502708404, test_loss: 0.001842015367284871\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 820, train_loss: 0.0022506664082116716, test_loss: 0.0018293070793450356\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 821, train_loss: 0.0022493158792220307, test_loss: 0.0018359658842643674\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 822, train_loss: 0.0022483189258550025, test_loss: 0.0018229914809732388\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 823, train_loss: 0.002247192129593212, test_loss: 0.0018331959923111403\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 824, train_loss: 0.0022460583466600414, test_loss: 0.0018239553722266394\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 825, train_loss: 0.0022451512264998873, test_loss: 0.0018266930012778642\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 826, train_loss: 0.002244073910643362, test_loss: 0.0018195429322823214\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 827, train_loss: 0.002242974602706095, test_loss: 0.0018281024979212536\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 828, train_loss: 0.0022413217394268545, test_loss: 0.0018167917583830273\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 829, train_loss: 0.002240727077211786, test_loss: 0.0018254020038651875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 830, train_loss: 0.0022396200157804758, test_loss: 0.0018317275277625483\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 831, train_loss: 0.0022393764136047615, test_loss: 0.001816400852523004\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 832, train_loss: 0.002237434406106027, test_loss: 0.001818192397200992\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 833, train_loss: 0.002236875214549342, test_loss: 0.0018281596945971847\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 834, train_loss: 0.0022362053005358074, test_loss: 0.0018172912388810148\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 835, train_loss: 0.0022346807642213778, test_loss: 0.0018123426429482582\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 836, train_loss: 0.002233841454915478, test_loss: 0.0018137780485304599\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 837, train_loss: 0.002231715237398641, test_loss: 0.0018321758955911112\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 838, train_loss: 0.002231586821175595, test_loss: 0.001808309339722678\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 839, train_loss: 0.0022309534111524306, test_loss: 0.0018049630365194157\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 840, train_loss: 0.002230016518941766, test_loss: 0.0018083270176877985\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 841, train_loss: 0.0022285253587832994, test_loss: 0.0018038164435450036\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 842, train_loss: 0.002227717781899634, test_loss: 0.0018082081371222814\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 843, train_loss: 0.002226623371387536, test_loss: 0.0018097946835601202\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 844, train_loss: 0.0022254619685299136, test_loss: 0.0017989483305241149\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 845, train_loss: 0.0022249892893259765, test_loss: 0.0018019067944036438\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 846, train_loss: 0.0022228895998045953, test_loss: 0.00180506010096556\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 847, train_loss: 0.0022228797447998512, test_loss: 0.001802367996648312\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 848, train_loss: 0.0022216568662813026, test_loss: 0.0018009328487096354\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 849, train_loss: 0.002220882077150072, test_loss: 0.0017974325515425359\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 850, train_loss: 0.0022193972178248932, test_loss: 0.001806625184634378\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 851, train_loss: 0.0022182214927678945, test_loss: 0.0018127721789194485\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 852, train_loss: 0.0022171385706765234, test_loss: 0.0017964762860524037\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 853, train_loss: 0.0022166113053404893, test_loss: 0.001788663899134964\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 854, train_loss: 0.002215983025573138, test_loss: 0.0017939626578038607\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 855, train_loss: 0.002214439432576679, test_loss: 0.0017930548634374132\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 856, train_loss: 0.0022141552196557083, test_loss: 0.0017959680721665232\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 857, train_loss: 0.0022128261224737384, test_loss: 0.001786044497492138\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 858, train_loss: 0.0022118982485570264, test_loss: 0.0017935430361644532\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 859, train_loss: 0.0022109318131356684, test_loss: 0.001792011782340407\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 860, train_loss: 0.0022091924643687544, test_loss: 0.0018033720105170058\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 861, train_loss: 0.0022082505746702525, test_loss: 0.0017803595181202325\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 862, train_loss: 0.0022074877367009187, test_loss: 0.0017859228457195852\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 863, train_loss: 0.0022069355835016803, test_loss: 0.0017920297489185126\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 864, train_loss: 0.002205861375525432, test_loss: 0.00178792207887641\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 865, train_loss: 0.0022050656086646868, test_loss: 0.0017890795148932971\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 866, train_loss: 0.0022040835120730774, test_loss: 0.0017937521040902282\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 867, train_loss: 0.0022024946963962754, test_loss: 0.0017837317379113908\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 868, train_loss: 0.002202017332059658, test_loss: 0.001781188666287702\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 869, train_loss: 0.00220061405885024, test_loss: 0.0017817389743868262\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 870, train_loss: 0.002199736252087589, test_loss: 0.0017667935444012235\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 871, train_loss: 0.0021986918752455466, test_loss: 0.0017804989853548482\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 872, train_loss: 0.0021979587696131357, test_loss: 0.0017766073159201858\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 873, train_loss: 0.002197370629965555, test_loss: 0.001779389422713743\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 874, train_loss: 0.0021960583216055953, test_loss: 0.001769884859244578\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 875, train_loss: 0.0021943329070518757, test_loss: 0.0017879642794305722\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 876, train_loss: 0.002194219157991333, test_loss: 0.0017832554210774386\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 877, train_loss: 0.0021926516895361533, test_loss: 0.001778552401960135\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 878, train_loss: 0.002191893390888336, test_loss: 0.0017744565484463237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 879, train_loss: 0.002191041531601927, test_loss: 0.001766911303723016\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 880, train_loss: 0.0021905314458968636, test_loss: 0.0017727538544045857\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 881, train_loss: 0.002189528394936674, test_loss: 0.0017658613645695508\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 882, train_loss: 0.0021879393696928253, test_loss: 0.0017616430128467842\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 883, train_loss: 0.002187309337855413, test_loss: 0.001761034807341042\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 884, train_loss: 0.002185752798910611, test_loss: 0.0017806276166363834\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 885, train_loss: 0.0021849783821916054, test_loss: 0.0017636782575162868\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 886, train_loss: 0.0021843966245565587, test_loss: 0.0017557526721831006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 887, train_loss: 0.0021838154570771114, test_loss: 0.0017569192822497243\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 888, train_loss: 0.002182765197367828, test_loss: 0.0017632744297421037\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 889, train_loss: 0.0021812009935058578, test_loss: 0.0017548841204142007\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 890, train_loss: 0.0021808749128266994, test_loss: 0.0017659381874881757\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 891, train_loss: 0.002180119636110388, test_loss: 0.0017643665507934296\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 892, train_loss: 0.002178528454360306, test_loss: 0.0017576657841205955\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 893, train_loss: 0.002178261141712469, test_loss: 0.0017572455193984727\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 894, train_loss: 0.0021769923919556433, test_loss: 0.0017610706334100822\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 895, train_loss: 0.0021756198303929737, test_loss: 0.001760782015387518\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 896, train_loss: 0.002174518970316307, test_loss: 0.0017582577065928862\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 897, train_loss: 0.0021740781132795267, test_loss: 0.001748622176446355\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 898, train_loss: 0.0021732790816308975, test_loss: 0.0017472493724572137\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 899, train_loss: 0.002172484374675202, test_loss: 0.001754377657795349\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 900, train_loss: 0.0021710095200594202, test_loss: 0.0017513881496434363\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 901, train_loss: 0.0021699943969471624, test_loss: 0.0017448529985043793\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 902, train_loss: 0.0021694849234932478, test_loss: 0.0017419670458856183\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 903, train_loss: 0.0021684239199930137, test_loss: 0.0017475419997189862\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 904, train_loss: 0.0021676842714496345, test_loss: 0.001748881179828925\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 905, train_loss: 0.002166799826580209, test_loss: 0.0017431685768175297\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 906, train_loss: 0.002166022619689631, test_loss: 0.001756107127174544\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 907, train_loss: 0.002164428707338359, test_loss: 0.0017509128350260046\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 908, train_loss: 0.0021632910095019746, test_loss: 0.0017664323990734725\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 909, train_loss: 0.002163193339675067, test_loss: 0.0017470809323593783\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 910, train_loss: 0.0021618832091381927, test_loss: 0.0017434343160577835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 911, train_loss: 0.0021611058212003755, test_loss: 0.0017492302675889686\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 912, train_loss: 0.0021600587814065153, test_loss: 0.0017416086124378638\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 913, train_loss: 0.0021587503167962573, test_loss: 0.001739223227046657\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 914, train_loss: 0.0021580917348798653, test_loss: 0.0017282656544688134\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 915, train_loss: 0.002157008203749473, test_loss: 0.0017425540466389905\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 916, train_loss: 0.002156424019108044, test_loss: 0.0017314585702246032\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 917, train_loss: 0.0021553462010887718, test_loss: 0.0017367145505536778\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 918, train_loss: 0.002154395385858693, test_loss: 0.0017441142511252577\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 919, train_loss: 0.0021538646749795468, test_loss: 0.0017306524013969093\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 920, train_loss: 0.0021529235897732144, test_loss: 0.0017323777760910348\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 921, train_loss: 0.0021517388007711046, test_loss: 0.0017367158755242156\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 922, train_loss: 0.0021508553409544024, test_loss: 0.0017245303908846555\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 923, train_loss: 0.002150312048849497, test_loss: 0.0017293965035768894\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 924, train_loss: 0.002149372503128725, test_loss: 0.0017267204470371301\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 925, train_loss: 0.002147755977198525, test_loss: 0.001729503625821776\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 926, train_loss: 0.0021473789920704906, test_loss: 0.0017307547022937797\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 927, train_loss: 0.002146244252507067, test_loss: 0.0017291148656229966\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 928, train_loss: 0.002145779135140176, test_loss: 0.00173082296495499\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 929, train_loss: 0.0021445460215150174, test_loss: 0.0017262159802694209\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 930, train_loss: 0.0021438646748549862, test_loss: 0.0017231361241264937\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 931, train_loss: 0.0021426858059029378, test_loss: 0.0017305603198623524\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 932, train_loss: 0.002141847542680202, test_loss: 0.0017339273407019507\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 933, train_loss: 0.002141057818465754, test_loss: 0.0017236640030601753\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 934, train_loss: 0.002139718563900949, test_loss: 0.0017230353338303617\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 935, train_loss: 0.0021390563835246633, test_loss: 0.0017113964793866333\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 936, train_loss: 0.0021384753086129173, test_loss: 0.0017126856576233434\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 937, train_loss: 0.0021374005823159422, test_loss: 0.0017209063291188497\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 938, train_loss: 0.0021365263739158476, test_loss: 0.0017186860547950252\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 939, train_loss: 0.002136029095940267, test_loss: 0.0017201228706070627\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 940, train_loss: 0.002134265146357627, test_loss: 0.0017127439352453877\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 941, train_loss: 0.0021337487649781485, test_loss: 0.0017141333603201648\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 942, train_loss: 0.0021329231636737314, test_loss: 0.0017175540286608827\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 943, train_loss: 0.0021321663225535303, test_loss: 0.0017174619062690255\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 944, train_loss: 0.0021310130751926925, test_loss: 0.0017229538871339546\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 945, train_loss: 0.0021304476042169093, test_loss: 0.001709500304572523\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 946, train_loss: 0.0021292616387970907, test_loss: 0.0017083005838615533\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 947, train_loss: 0.002127977566798233, test_loss: 0.0017142377344918293\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 948, train_loss: 0.002127720087156927, test_loss: 0.0017027589468246636\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 949, train_loss: 0.0021265947824907817, test_loss: 0.0017026332751788103\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 950, train_loss: 0.00212527397542298, test_loss: 0.0017241038300338774\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 951, train_loss: 0.002125345668767152, test_loss: 0.0017105769285803828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 952, train_loss: 0.002123935678997155, test_loss: 0.0016964914957031559\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 953, train_loss: 0.002123076347910634, test_loss: 0.0017010953021011888\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 954, train_loss: 0.002122489331631974, test_loss: 0.0017035742273900467\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 955, train_loss: 0.0021213481602193568, test_loss: 0.0017080819823791105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 956, train_loss: 0.0021202820315248455, test_loss: 0.0017127436697662163\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 957, train_loss: 0.0021194620520292307, test_loss: 0.0017154841090161472\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 958, train_loss: 0.002118031232384965, test_loss: 0.0016944446891638786\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 959, train_loss: 0.002117452870943057, test_loss: 0.0017053687859935542\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 960, train_loss: 0.002116657759064503, test_loss: 0.0016989062772690654\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 961, train_loss: 0.002115712561652401, test_loss: 0.001687337341793001\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 962, train_loss: 0.002115567027899533, test_loss: 0.0016950758098317191\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 963, train_loss: 0.00211389824492237, test_loss: 0.0016878820569194758\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 964, train_loss: 0.0021140306404858742, test_loss: 0.0016962877050854075\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 965, train_loss: 0.0021127677616238107, test_loss: 0.001695503716734656\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 966, train_loss: 0.0021120554421612536, test_loss: 0.0017012932814675598\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 967, train_loss: 0.0021109284398365567, test_loss: 0.0016930652386807383\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 968, train_loss: 0.002109935183337357, test_loss: 0.0016865135923860809\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 969, train_loss: 0.0021091651342370014, test_loss: 0.001694967164299809\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 970, train_loss: 0.0021083102604167155, test_loss: 0.0016874148036899546\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 971, train_loss: 0.0021077017827398677, test_loss: 0.0016895045387522818\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 972, train_loss: 0.002106592891076704, test_loss: 0.0016890332751641742\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 973, train_loss: 0.0021058425580207443, test_loss: 0.0016963067352592659\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 974, train_loss: 0.0021043541693451027, test_loss: 0.0016939624168653376\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 975, train_loss: 0.002104206190091779, test_loss: 0.0016760463934191742\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 976, train_loss: 0.0021031784478895816, test_loss: 0.0016841964950286544\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 977, train_loss: 0.002102472983595812, test_loss: 0.0016737021882689963\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 978, train_loss: 0.0021015046390994484, test_loss: 0.0016878889114312374\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 979, train_loss: 0.0021008085079368883, test_loss: 0.0016801523427905825\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 980, train_loss: 0.0020994892441021295, test_loss: 0.001680421289981892\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 981, train_loss: 0.002099260139332798, test_loss: 0.00168403503126823\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 982, train_loss: 0.0020980648708103043, test_loss: 0.0016756187352047374\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 983, train_loss: 0.0020963664112704492, test_loss: 0.001692406390830934\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 984, train_loss: 0.0020966875160706344, test_loss: 0.0016910068430334856\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 985, train_loss: 0.002095659987912996, test_loss: 0.0016799687435149024\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 986, train_loss: 0.002094778993842893, test_loss: 0.0016794786684047908\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 987, train_loss: 0.002094342708735599, test_loss: 0.0016811903363141494\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 988, train_loss: 0.00209285574332487, test_loss: 0.0016828747057633547\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 989, train_loss: 0.002092547675580671, test_loss: 0.0016759801551532478\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 990, train_loss: 0.002091494714374028, test_loss: 0.001672670871462637\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 991, train_loss: 0.002090363308513244, test_loss: 0.001670282710760008\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 992, train_loss: 0.0020898112409707646, test_loss: 0.0016790498173064313\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 993, train_loss: 0.002088972948741535, test_loss: 0.0016777468546747398\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 994, train_loss: 0.0020877704636371357, test_loss: 0.0016637514869589955\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 995, train_loss: 0.0020870838843225297, test_loss: 0.001664237864390135\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 996, train_loss: 0.0020864175142600422, test_loss: 0.0016687130872667648\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 997, train_loss: 0.0020856337236741986, test_loss: 0.0016735399885174746\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 998, train_loss: 0.0020847531446500254, test_loss: 0.001681707148143174\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 999, train_loss: 0.002083841641783951, test_loss: 0.0016754252572308104\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1000, train_loss: 0.002082991494269992, test_loss: 0.0016746175223931621\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1001, train_loss: 0.0020823166776724146, test_loss: 0.001668987742726667\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1002, train_loss: 0.00208088774831707, test_loss: 0.0016707397703729116\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1003, train_loss: 0.002080950061175474, test_loss: 0.0016657831509245369\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1004, train_loss: 0.0020793943205905958, test_loss: 0.0016657008921193222\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1005, train_loss: 0.0020787472963990352, test_loss: 0.0016618529929800364\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1006, train_loss: 0.002077775182129261, test_loss: 0.0016739897749712095\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1007, train_loss: 0.0020766753879925518, test_loss: 0.0016580392497976741\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1008, train_loss: 0.0020764056563199835, test_loss: 0.0016620237988834747\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1009, train_loss: 0.0020758038025386755, test_loss: 0.0016612692725641702\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1010, train_loss: 0.002074732336357398, test_loss: 0.0016615185451929648\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1011, train_loss: 0.0020742207259620326, test_loss: 0.0016663033671554727\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1012, train_loss: 0.0020734546987385017, test_loss: 0.001662403094180776\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1013, train_loss: 0.0020722015767652593, test_loss: 0.001656600472648652\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1014, train_loss: 0.002071817688870533, test_loss: 0.001652008310394684\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1015, train_loss: 0.0020709942428069656, test_loss: 0.0016559752525958733\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1016, train_loss: 0.0020692996978783126, test_loss: 0.0016526423429380744\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1017, train_loss: 0.0020694002233360765, test_loss: 0.0016485153563478843\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1018, train_loss: 0.0020681776031018563, test_loss: 0.0016608521301606789\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1019, train_loss: 0.0020673398295827025, test_loss: 0.0016540745782549493\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1020, train_loss: 0.002066982758190969, test_loss: 0.0016563760690564194\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1021, train_loss: 0.0020652246018279855, test_loss: 0.0016540054223300198\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1022, train_loss: 0.0020651572676341247, test_loss: 0.0016546918671408023\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1023, train_loss: 0.0020644412088647087, test_loss: 0.001648506990116006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1024, train_loss: 0.0020637130466006374, test_loss: 0.0016526073144259266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1025, train_loss: 0.0020624608917700683, test_loss: 0.0016557042558232728\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1026, train_loss: 0.0020616929386321915, test_loss: 0.001646647588551367\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1027, train_loss: 0.0020612141078129335, test_loss: 0.0016536276490138008\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1028, train_loss: 0.0020599831062108965, test_loss: 0.001648179989178271\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1029, train_loss: 0.0020593508370773538, test_loss: 0.0016402456512412008\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1030, train_loss: 0.0020587789069979536, test_loss: 0.001639746937890334\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1031, train_loss: 0.002057558939057125, test_loss: 0.0016361754734624321\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1032, train_loss: 0.002056797028808889, test_loss: 0.001643106870268149\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1033, train_loss: 0.002056070841809137, test_loss: 0.0016377914474175598\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1034, train_loss: 0.002055377892065127, test_loss: 0.001649094994205277\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1035, train_loss: 0.002054701017466589, test_loss: 0.0016336225709006286\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1036, train_loss: 0.002053172280661525, test_loss: 0.001653042453630732\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1037, train_loss: 0.0020526898155622743, test_loss: 0.0016309229436163337\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1038, train_loss: 0.0020522463795757014, test_loss: 0.0016329305780788835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1039, train_loss: 0.002051283270654029, test_loss: 0.0016412321800509323\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1040, train_loss: 0.002050991419388953, test_loss: 0.0016375049961378607\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1041, train_loss: 0.0020498416525152915, test_loss: 0.0016346529392206159\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1042, train_loss: 0.002049168332477558, test_loss: 0.001641472045426221\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1043, train_loss: 0.0020477632921108084, test_loss: 0.0016236109882951356\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1044, train_loss: 0.0020475757815820606, test_loss: 0.0016299609687382307\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1045, train_loss: 0.0020466434575047306, test_loss: 0.001636293296775339\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1046, train_loss: 0.002045659455926279, test_loss: 0.0016278593445745988\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1047, train_loss: 0.0020449487274406745, test_loss: 0.001620961450941813\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1048, train_loss: 0.0020443500077233566, test_loss: 0.0016231681835891393\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1049, train_loss: 0.002043680753558874, test_loss: 0.001634522073100715\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1050, train_loss: 0.0020427596286160367, test_loss: 0.0016317491552819354\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1051, train_loss: 0.0020421314595211112, test_loss: 0.0016182827789751359\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1052, train_loss: 0.002041578563054818, test_loss: 0.0016251383905257217\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1053, train_loss: 0.0020404231293052184, test_loss: 0.0016367418481819092\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1054, train_loss: 0.0020397577698951443, test_loss: 0.0016312354189442256\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1055, train_loss: 0.0020388771115411324, test_loss: 0.0016259247985987875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1056, train_loss: 0.002038100042188054, test_loss: 0.0016249649402780984\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1057, train_loss: 0.0020372228641265437, test_loss: 0.0016156884299468799\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1058, train_loss: 0.0020366943393441473, test_loss: 0.0016185059006928871\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1059, train_loss: 0.002036168496494328, test_loss: 0.00162115284724561\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1060, train_loss: 0.0020351812451179247, test_loss: 0.0016180444333421735\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1061, train_loss: 0.0020338644353006296, test_loss: 0.0016133813722915829\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1062, train_loss: 0.002033799692905075, test_loss: 0.001614487097518017\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1063, train_loss: 0.0020329167484889822, test_loss: 0.001620522371339552\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1064, train_loss: 0.0020321330078371357, test_loss: 0.0016146601692292816\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1065, train_loss: 0.002030444300903263, test_loss: 0.0016121723224970107\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1066, train_loss: 0.002030365450644903, test_loss: 0.0016233926793435016\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1067, train_loss: 0.0020294903971630157, test_loss: 0.0016164103855352062\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1068, train_loss: 0.002029038857852411, test_loss: 0.0016292389624047643\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1069, train_loss: 0.0020282449544641815, test_loss: 0.001626658553738171\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1070, train_loss: 0.002027263452943701, test_loss: 0.0016096022815103284\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1071, train_loss: 0.0020266095660883858, test_loss: 0.001615877064371792\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1072, train_loss: 0.002026070086293259, test_loss: 0.0016160916116533396\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1073, train_loss: 0.002024772708879684, test_loss: 0.001612267347063141\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1074, train_loss: 0.002024594376861962, test_loss: 0.001614974819322248\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1075, train_loss: 0.0020238735107043103, test_loss: 0.0016138481027715339\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1076, train_loss: 0.002022663857783954, test_loss: 0.0016116845878199316\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1077, train_loss: 0.0020221205519545, test_loss: 0.0016092560498434931\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1078, train_loss: 0.002021543261611423, test_loss: 0.0016125728854184183\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1079, train_loss: 0.0020202662844256026, test_loss: 0.001621951800562107\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1080, train_loss: 0.002020083955157374, test_loss: 0.0016139910003403202\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1081, train_loss: 0.0020192271479277757, test_loss: 0.0016092897031996518\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1082, train_loss: 0.0020181011612196904, test_loss: 0.0016111135227196754\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1083, train_loss: 0.0020174841318835496, test_loss: 0.001603030718494362\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1084, train_loss: 0.0020167963314734485, test_loss: 0.0016028644591316176\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1085, train_loss: 0.002015962908344256, test_loss: 0.0016124222928714843\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1086, train_loss: 0.0020147929543626953, test_loss: 0.001599717332469598\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1087, train_loss: 0.002014851087706637, test_loss: 0.0015984518818769199\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1088, train_loss: 0.002013455504819402, test_loss: 0.001609134740227511\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1089, train_loss: 0.002013219064716748, test_loss: 0.001603036905110527\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1090, train_loss: 0.002012589025579044, test_loss: 0.0015937694132354898\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1091, train_loss: 0.0020115562413085602, test_loss: 0.0016097021777964889\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1092, train_loss: 0.0020110684912846453, test_loss: 0.0015992024073523518\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1093, train_loss: 0.002010136845258429, test_loss: 0.0015965058344777506\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1094, train_loss: 0.002009526910637988, test_loss: 0.001598601910070731\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1095, train_loss: 0.002008344202900436, test_loss: 0.0016000108343387882\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1096, train_loss: 0.002007886511877173, test_loss: 0.0016049812592637653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1097, train_loss: 0.002007213912524258, test_loss: 0.0015989059230867918\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1098, train_loss: 0.002006172890312359, test_loss: 0.0015875877264382627\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1099, train_loss: 0.002005808808352957, test_loss: 0.0015939117805921854\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1100, train_loss: 0.0020043327728906228, test_loss: 0.0016015893300228084\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1101, train_loss: 0.0020044089852128315, test_loss: 0.0015911034116960275\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1102, train_loss: 0.002003169424456535, test_loss: 0.0015908185769848598\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1103, train_loss: 0.0020027889365511, test_loss: 0.0015859665460690546\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1104, train_loss: 0.002001777402457024, test_loss: 0.001593592570712551\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1105, train_loss: 0.0020006060841482407, test_loss: 0.001604084422302922\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1106, train_loss: 0.002000671227086479, test_loss: 0.0015892385033508523\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1107, train_loss: 0.0019995197246900844, test_loss: 0.0015786816722352225\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1108, train_loss: 0.001999047012002142, test_loss: 0.0015925838787762376\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1109, train_loss: 0.001998333957740869, test_loss: 0.0015875498963093075\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1110, train_loss: 0.0019971823405838854, test_loss: 0.001585758099771248\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1111, train_loss: 0.001996651696964762, test_loss: 0.0015772160856077112\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1112, train_loss: 0.00199638294216859, test_loss: 0.0015876110970006825\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1113, train_loss: 0.0019952089540943825, test_loss: 0.0015786704084931468\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1114, train_loss: 0.001994773300608346, test_loss: 0.0015776375316468498\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1115, train_loss: 0.0019940050031887024, test_loss: 0.0015885620299601355\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1116, train_loss: 0.001993356567510033, test_loss: 0.0015797268501139628\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1117, train_loss: 0.0019925932659234937, test_loss: 0.0015783971966578602\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1118, train_loss: 0.0019918068518074085, test_loss: 0.0015902300814886051\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1119, train_loss: 0.001991167825913013, test_loss: 0.001581820542364417\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1120, train_loss: 0.0019903214948943327, test_loss: 0.0015712794082878062\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1121, train_loss: 0.001989608677649204, test_loss: 0.0015881907018364216\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1122, train_loss: 0.0019890562522950827, test_loss: 0.0015861154653323002\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1123, train_loss: 0.0019879806612541954, test_loss: 0.0015894799162663568\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1124, train_loss: 0.0019869665658283657, test_loss: 0.0015830337689317858\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1125, train_loss: 0.001986456257902416, test_loss: 0.0015819047035527034\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1126, train_loss: 0.001986285193373969, test_loss: 0.0015771508342601382\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1127, train_loss: 0.001985075208338999, test_loss: 0.0015727683475676875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1128, train_loss: 0.00198467181377225, test_loss: 0.0015732830058998214\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1129, train_loss: 0.0019839111947023112, test_loss: 0.0015763072145861\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1130, train_loss: 0.001983251024534409, test_loss: 0.001577773842235239\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1131, train_loss: 0.0019823465035701897, test_loss: 0.0015674557595421225\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1132, train_loss: 0.0019818224276830273, test_loss: 0.0015757898744106148\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1133, train_loss: 0.0019813373117991114, test_loss: 0.001571449949644888\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1134, train_loss: 0.001980250600466885, test_loss: 0.0015619435681191345\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1135, train_loss: 0.0019790064259260945, test_loss: 0.0015709893113802164\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1136, train_loss: 0.0019789150546937182, test_loss: 0.001576905169013816\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1137, train_loss: 0.001978215544226028, test_loss: 0.0015692294923027451\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1138, train_loss: 0.0019775392605617865, test_loss: 0.001568041587560378\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1139, train_loss: 0.0019767353866781954, test_loss: 0.001574070656669135\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1140, train_loss: 0.0019760687274268103, test_loss: 0.001565179639631727\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1141, train_loss: 0.0019754210534119313, test_loss: 0.0015635755581360382\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1142, train_loss: 0.001974743834823638, test_loss: 0.0015553147690624404\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1143, train_loss: 0.0019736413037588193, test_loss: 0.0015566323901368424\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1144, train_loss: 0.001973583236847534, test_loss: 0.0015716628380080995\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1145, train_loss: 0.0019726316422382898, test_loss: 0.0015633675461569324\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1146, train_loss: 0.0019717922446475266, test_loss: 0.001558888087926719\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1147, train_loss: 0.001971367627207461, test_loss: 0.0015574188985808108\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1148, train_loss: 0.001970611978906291, test_loss: 0.0015587637289382445\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1149, train_loss: 0.0019697379236176525, test_loss: 0.0015609323792234182\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1150, train_loss: 0.0019693597001654334, test_loss: 0.001567221674448751\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1151, train_loss: 0.0019682032899287867, test_loss: 0.001554257280636543\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1152, train_loss: 0.0019676694285356592, test_loss: 0.0015633306818647096\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1153, train_loss: 0.001966708211654978, test_loss: 0.001558469654320596\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1154, train_loss: 0.001966310749208106, test_loss: 0.001562611578078642\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1155, train_loss: 0.0019658550850378915, test_loss: 0.0015574490369031897\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1156, train_loss: 0.0019645223627256284, test_loss: 0.0015669344375521925\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1157, train_loss: 0.00196459960536237, test_loss: 0.0015531028994024755\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1158, train_loss: 0.001963757875766356, test_loss: 0.0015604080856577733\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1159, train_loss: 0.0019626086929160477, test_loss: 0.0015501726856410431\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1160, train_loss: 0.0019616354408227748, test_loss: 0.0015421474221138619\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1161, train_loss: 0.001961010216519604, test_loss: 0.001560019351391253\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1162, train_loss: 0.0019608378379386117, test_loss: 0.0015477907483074933\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1163, train_loss: 0.0019602816048495623, test_loss: 0.0015551631562023733\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1164, train_loss: 0.0019587852392909445, test_loss: 0.001570868815738672\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1165, train_loss: 0.0019591208938162705, test_loss: 0.0015486902117024725\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1166, train_loss: 0.001957867990121756, test_loss: 0.0015431287161519346\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1167, train_loss: 0.0019577668746543994, test_loss: 0.0015572912060843708\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1168, train_loss: 0.001956898868484983, test_loss: 0.001545970177409263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1169, train_loss: 0.0019558664722980177, test_loss: 0.0015514133679411875\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1170, train_loss: 0.0019552485653694506, test_loss: 0.0015478596663645778\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1171, train_loss: 0.001954758671979828, test_loss: 0.0015539902856406302\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1172, train_loss: 0.001953925857500307, test_loss: 0.0015456609904410867\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1173, train_loss: 0.001953222281062029, test_loss: 0.0015472264841754134\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1174, train_loss: 0.00195276025522276, test_loss: 0.0015530838160581577\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1175, train_loss: 0.001952132663629594, test_loss: 0.0015523140016459646\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1176, train_loss: 0.0019513062360294686, test_loss: 0.0015399232836968552\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1177, train_loss: 0.0019506655163213417, test_loss: 0.0015398562853732028\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1178, train_loss: 0.0019501738367449604, test_loss: 0.0015424056354020603\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1179, train_loss: 0.0019490026282325673, test_loss: 0.0015422441319037133\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1180, train_loss: 0.0019480932153427325, test_loss: 0.0015399602852994576\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1181, train_loss: 0.0019478951592502936, test_loss: 0.0015521547175012529\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1182, train_loss: 0.0019470639087853137, test_loss: 0.0015352231874623192\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1183, train_loss: 0.0019469495737546724, test_loss: 0.0015374308191568186\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1184, train_loss: 0.0019459508594598957, test_loss: 0.0015402688179911377\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1185, train_loss: 0.0019451825406260627, test_loss: 0.0015389795653157844\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1186, train_loss: 0.0019446762133059543, test_loss: 0.0015428385310000381\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1187, train_loss: 0.001943981437028906, test_loss: 0.0015371887109209544\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1188, train_loss: 0.0019433198677357225, test_loss: 0.0015363290216415548\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1189, train_loss: 0.001942521224754635, test_loss: 0.0015410288501581631\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1190, train_loss: 0.0019419056059322305, test_loss: 0.0015303130682859307\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1191, train_loss: 0.001941301671178376, test_loss: 0.0015422567963609113\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1192, train_loss: 0.0019402939199982367, test_loss: 0.0015321704870588386\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1193, train_loss: 0.0019399947956956637, test_loss: 0.0015297758365751435\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1194, train_loss: 0.0019395700998504624, test_loss: 0.0015264587786171036\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1195, train_loss: 0.0019387754905240042, test_loss: 0.001528437857931026\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1196, train_loss: 0.0019378580861992786, test_loss: 0.0015389588937605135\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1197, train_loss: 0.0019375104615799647, test_loss: 0.0015348938697850546\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1198, train_loss: 0.0019367356412034844, test_loss: 0.0015351202595271529\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1199, train_loss: 0.0019358215817751794, test_loss: 0.0015304608071649757\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1200, train_loss: 0.0019353599457714993, test_loss: 0.0015332649461178777\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1201, train_loss: 0.0019348383067217796, test_loss: 0.0015296777656215292\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1202, train_loss: 0.00193323211060731, test_loss: 0.0015441548075147856\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1203, train_loss: 0.0019329767094077822, test_loss: 0.0015164200410184164\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1204, train_loss: 0.0019322793829927602, test_loss: 0.0015393867948799967\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1205, train_loss: 0.0019320779050588234, test_loss: 0.0015323602412349712\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1206, train_loss: 0.0019315060111888712, test_loss: 0.0015340572202918478\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1207, train_loss: 0.0019308657498904084, test_loss: 0.0015262797120764542\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1208, train_loss: 0.0019298238839107692, test_loss: 0.0015297693449284476\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1209, train_loss: 0.001929379737411118, test_loss: 0.0015282691535666788\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1210, train_loss: 0.001928583100355576, test_loss: 0.001526846380072992\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1211, train_loss: 0.0019280837037866114, test_loss: 0.0015175710891176446\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1212, train_loss: 0.0019272353686431243, test_loss: 0.001516800843558322\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1213, train_loss: 0.0019267707612746268, test_loss: 0.001526188751225933\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1214, train_loss: 0.0019261333349715284, test_loss: 0.001517149226669855\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1215, train_loss: 0.0019256851905886782, test_loss: 0.0015191890553792175\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1216, train_loss: 0.0019246380373482814, test_loss: 0.0015217882261286154\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1217, train_loss: 0.0019240250389413274, test_loss: 0.001520249731234239\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1218, train_loss: 0.001923650428792929, test_loss: 0.0015179390092947497\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1219, train_loss: 0.0019229921683813927, test_loss: 0.0015223382172571758\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1220, train_loss: 0.0019217580214242473, test_loss: 0.0015129813296087564\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1221, train_loss: 0.0019215270991742386, test_loss: 0.0015204940011757666\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1222, train_loss: 0.0019209139144638347, test_loss: 0.0015197100906217925\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1223, train_loss: 0.0019206422463645006, test_loss: 0.0015138950777607253\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1224, train_loss: 0.0019193287498576964, test_loss: 0.0015085393711043976\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1225, train_loss: 0.0019193292445741457, test_loss: 0.0015144542522671728\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1226, train_loss: 0.0019183864603465404, test_loss: 0.0015284777732748084\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1227, train_loss: 0.0019173958115116148, test_loss: 0.0015152851171875133\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1228, train_loss: 0.001917063082505273, test_loss: 0.001506602702041467\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1229, train_loss: 0.0019163450946093827, test_loss: 0.0015074447618621306\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1230, train_loss: 0.0019159607065959918, test_loss: 0.0015116592388576232\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1231, train_loss: 0.0019153067467377574, test_loss: 0.0015161894459593801\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1232, train_loss: 0.0019146611867900812, test_loss: 0.0015114791687945118\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1233, train_loss: 0.0019138749465669497, test_loss: 0.0015130936623721097\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1234, train_loss: 0.0019132226828585401, test_loss: 0.0015050797054964977\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1235, train_loss: 0.0019126754430964445, test_loss: 0.001515163721006292\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1236, train_loss: 0.0019117941250393858, test_loss: 0.0015195409921430743\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1237, train_loss: 0.0019113223753086165, test_loss: 0.0015039848304019333\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1238, train_loss: 0.0019102200049475571, test_loss: 0.0015093763289862694\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1239, train_loss: 0.0019099952896004138, test_loss: 0.0015104279215544618\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1240, train_loss: 0.0019095408481804273, test_loss: 0.0015076097727451737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1241, train_loss: 0.0019085105953591697, test_loss: 0.0015102211525514482\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1242, train_loss: 0.0019081673725238723, test_loss: 0.0014989465969101263\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1243, train_loss: 0.0019071242432530508, test_loss: 0.0014976886704038733\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1244, train_loss: 0.0019070617786945813, test_loss: 0.0015152266466593107\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1245, train_loss: 0.001906479172473602, test_loss: 0.0015059366823362114\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1246, train_loss: 0.0019058363051169293, test_loss: 0.001511075580007552\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1247, train_loss: 0.0019052630840539061, test_loss: 0.0015031336969024357\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1248, train_loss: 0.0019042688888345025, test_loss: 0.0015018301406272282\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1249, train_loss: 0.0019035318205062998, test_loss: 0.001506800656221831\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1250, train_loss: 0.0019032194883695781, test_loss: 0.001506494790821587\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1251, train_loss: 0.0019023634617358607, test_loss: 0.0015020576310911193\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1252, train_loss: 0.0019019633253380644, test_loss: 0.001497507475365777\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1253, train_loss: 0.00190136616603381, test_loss: 0.0015000543123908127\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1254, train_loss: 0.0019003218516822668, test_loss: 0.0014871544786169229\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1255, train_loss: 0.0019005389619259675, test_loss: 0.001497848334764011\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1256, train_loss: 0.0018992203565775401, test_loss: 0.0015008292973037057\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1257, train_loss: 0.001899005001470023, test_loss: 0.0014883069510314949\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1258, train_loss: 0.0018983994824554847, test_loss: 0.0014959365088053835\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1259, train_loss: 0.0018977149866441932, test_loss: 0.0014920858319293563\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1260, train_loss: 0.0018970419650423936, test_loss: 0.0015022165794224024\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1261, train_loss: 0.0018962339659585122, test_loss: 0.001502130272572872\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1262, train_loss: 0.0018958892442759463, test_loss: 0.001489873883353236\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1263, train_loss: 0.001895252792951168, test_loss: 0.0014935448233676979\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1264, train_loss: 0.0018942245415779434, test_loss: 0.0014861274017349412\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1265, train_loss: 0.0018942920226536779, test_loss: 0.0014920874121180999\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1266, train_loss: 0.0018932374348446578, test_loss: 0.0014921761815994382\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1267, train_loss: 0.0018928213186918705, test_loss: 0.001496723256236575\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1268, train_loss: 0.0018919797670283046, test_loss: 0.00148640503567339\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1269, train_loss: 0.0018914001159168264, test_loss: 0.0014849718958990148\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1270, train_loss: 0.0018908720712502952, test_loss: 0.0014800040294992356\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1271, train_loss: 0.0018902713085219489, test_loss: 0.0014900099327534032\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1272, train_loss: 0.0018896821988115778, test_loss: 0.0014961661499677203\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1273, train_loss: 0.001888916661826984, test_loss: 0.0014898213018731882\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1274, train_loss: 0.0018884492641895773, test_loss: 0.0014942278548215444\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1275, train_loss: 0.0018879335541449437, test_loss: 0.0014941718895547988\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1276, train_loss: 0.001887241969166174, test_loss: 0.001490980994556314\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1277, train_loss: 0.0018866501284652642, test_loss: 0.001485091123285178\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1278, train_loss: 0.001885503198728386, test_loss: 0.0014846055773653998\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1279, train_loss: 0.00188526904162169, test_loss: 0.0014838655182617144\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1280, train_loss: 0.0018850324549291744, test_loss: 0.0014829705107378033\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1281, train_loss: 0.0018840656906053313, test_loss: 0.0014886669574853163\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1282, train_loss: 0.001883496929825997, test_loss: 0.0014888274982252207\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1283, train_loss: 0.0018829662549616239, test_loss: 0.001485823766415706\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1284, train_loss: 0.0018824276009268505, test_loss: 0.0014848446981676926\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1285, train_loss: 0.0018818096104342753, test_loss: 0.0014861679524190736\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1286, train_loss: 0.0018811229003957756, test_loss: 0.0014851881634781305\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1287, train_loss: 0.001880641716887106, test_loss: 0.0014762299221892578\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1288, train_loss: 0.0018802047205853734, test_loss: 0.0014800329543224787\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1289, train_loss: 0.0018794131049626262, test_loss: 0.0014760057295997962\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1290, train_loss: 0.0018782895587907986, test_loss: 0.0014829452101809855\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1291, train_loss: 0.0018781017060243651, test_loss: 0.0014823942722451205\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1292, train_loss: 0.001877794976569681, test_loss: 0.0014797347942546296\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1293, train_loss: 0.001876888560011662, test_loss: 0.0014737827071094683\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1294, train_loss: 0.0018762987836873015, test_loss: 0.0014754153488842675\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1295, train_loss: 0.001876073642246453, test_loss: 0.0014749976449260989\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1296, train_loss: 0.0018753782397503637, test_loss: 0.001468217850346357\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1297, train_loss: 0.001874225272818073, test_loss: 0.001484702722033641\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1298, train_loss: 0.001874188520228428, test_loss: 0.0014725305337402134\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1299, train_loss: 0.001873384547352878, test_loss: 0.001476655450973433\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1300, train_loss: 0.0018728444828530369, test_loss: 0.0014750584211866132\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1301, train_loss: 0.0018721005734699439, test_loss: 0.0014748714731048006\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1302, train_loss: 0.0018719288911421264, test_loss: 0.0014778547828870777\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1303, train_loss: 0.0018708592347518843, test_loss: 0.0014830752987062368\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1304, train_loss: 0.0018706081353710422, test_loss: 0.0014800475279722984\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1305, train_loss: 0.0018702292497336777, test_loss: 0.001470330126698499\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1306, train_loss: 0.001869031790539585, test_loss: 0.0014702507650303559\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1307, train_loss: 0.0018686248576673477, test_loss: 0.0014699816623387728\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1308, train_loss: 0.0018678909839468893, test_loss: 0.0014787947413704107\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1309, train_loss: 0.0018676695409799338, test_loss: 0.001468644799434771\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1310, train_loss: 0.0018671030117538337, test_loss: 0.001465906154641207\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1311, train_loss: 0.0018663705402734475, test_loss: 0.0014621291665184216\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1312, train_loss: 0.0018652150923126132, test_loss: 0.001482018790668987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1313, train_loss: 0.0018652834996493865, test_loss: 0.0014661474887338371\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1314, train_loss: 0.0018647168204266216, test_loss: 0.0014684184518559144\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1315, train_loss: 0.0018634972673370487, test_loss: 0.0014788434332914245\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1316, train_loss: 0.001863409384865513, test_loss: 0.0014814845837043743\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1317, train_loss: 0.0018630221657502468, test_loss: 0.001462935513233951\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1318, train_loss: 0.0018622754317382038, test_loss: 0.0014712514545326717\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1319, train_loss: 0.0018614566422388407, test_loss: 0.0014615834044003023\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1320, train_loss: 0.001861246014216348, test_loss: 0.0014653242609571093\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1321, train_loss: 0.0018603936732080096, test_loss: 0.001453361382001999\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1322, train_loss: 0.0018600382379577013, test_loss: 0.0014663724686192253\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1323, train_loss: 0.0018590525006618426, test_loss: 0.0014622896352450016\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1324, train_loss: 0.0018587059069410852, test_loss: 0.0014605589796048708\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1325, train_loss: 0.0018584452574156897, test_loss: 0.0014652132130532453\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1326, train_loss: 0.00185752850894919, test_loss: 0.0014508205367280587\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1327, train_loss: 0.0018571803426482157, test_loss: 0.0014503979552664065\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1328, train_loss: 0.001856799784270629, test_loss: 0.0014658109548924048\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1329, train_loss: 0.0018561782131795413, test_loss: 0.0014629097303184562\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1330, train_loss: 0.0018555025071630652, test_loss: 0.0014603003719224571\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1331, train_loss: 0.0018550656472307858, test_loss: 0.0014614759849605318\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1332, train_loss: 0.0018542641986988447, test_loss: 0.0014631856848166373\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1333, train_loss: 0.001853783512340055, test_loss: 0.001454245975578371\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1334, train_loss: 0.001853204503605597, test_loss: 0.0014572020901631243\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1335, train_loss: 0.001852564265084045, test_loss: 0.001463292218828335\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1336, train_loss: 0.0018520948133878348, test_loss: 0.0014514806679448483\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1337, train_loss: 0.0018512699142741631, test_loss: 0.0014648428105619342\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1338, train_loss: 0.001850877408141482, test_loss: 0.0014574782318772916\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1339, train_loss: 0.0018502997224145946, test_loss: 0.0014634614193570227\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1340, train_loss: 0.001849780818832926, test_loss: 0.0014473388521135068\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1341, train_loss: 0.001849203742291065, test_loss: 0.0014555169471303741\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1342, train_loss: 0.0018484352907379249, test_loss: 0.001448173905746677\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1343, train_loss: 0.0018482318718997094, test_loss: 0.0014546345718815408\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1344, train_loss: 0.0018474354422211266, test_loss: 0.0014557072859017167\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1345, train_loss: 0.0018464354790410768, test_loss: 0.001471813421327776\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1346, train_loss: 0.001846360864704101, test_loss: 0.0014586211164453581\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1347, train_loss: 0.0018459019167652434, test_loss: 0.0014507597807096317\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1348, train_loss: 0.0018451537317471785, test_loss: 0.0014490279403010693\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1349, train_loss: 0.0018447356262161743, test_loss: 0.0014475073504190033\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1350, train_loss: 0.001844030084149962, test_loss: 0.0014520982062654535\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1351, train_loss: 0.001843459888133236, test_loss: 0.001454471634459855\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1352, train_loss: 0.001843209653788318, test_loss: 0.0014485136419422745\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1353, train_loss: 0.0018422278648796788, test_loss: 0.0014407648111675652\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1354, train_loss: 0.0018420516953057606, test_loss: 0.001448934330908695\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1355, train_loss: 0.0018408423499223685, test_loss: 0.0014519557165234194\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1356, train_loss: 0.0018407412955341207, test_loss: 0.0014398164206888909\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1357, train_loss: 0.0018402080784160892, test_loss: 0.0014469018332863178\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1358, train_loss: 0.001839376640479613, test_loss: 0.0014458351091516074\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1359, train_loss: 0.0018389198507017216, test_loss: 0.0014347978938484373\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1360, train_loss: 0.0018385703829047037, test_loss: 0.0014441940643920158\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1361, train_loss: 0.0018373337036222967, test_loss: 0.0014487491645036826\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1362, train_loss: 0.0018378190396830165, test_loss: 0.0014434988319967838\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1363, train_loss: 0.0018369230607417054, test_loss: 0.0014425083830316837\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1364, train_loss: 0.0018361670320953265, test_loss: 0.0014447642968615326\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1365, train_loss: 0.0018357308588285049, test_loss: 0.0014377304389013145\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1366, train_loss: 0.0018352654970478404, test_loss: 0.0014325048949290365\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1367, train_loss: 0.0018348204807914095, test_loss: 0.0014452241213383297\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1368, train_loss: 0.0018339544088586722, test_loss: 0.0014422578635089733\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1369, train_loss: 0.001833521925694742, test_loss: 0.0014441997287250184\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1370, train_loss: 0.0018330708790565047, test_loss: 0.0014382038388225154\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1371, train_loss: 0.001831655536225785, test_loss: 0.001460574216019804\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1372, train_loss: 0.0018323893982171077, test_loss: 0.001447019494289401\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1373, train_loss: 0.0018311982494926885, test_loss: 0.0014458445248004491\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1374, train_loss: 0.0018311891187742402, test_loss: 0.0014352404216016368\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1375, train_loss: 0.0018303242004336323, test_loss: 0.001439756447862004\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1376, train_loss: 0.0018298441390771681, test_loss: 0.0014453157851582943\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1377, train_loss: 0.0018290031735783748, test_loss: 0.001427314444579399\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1378, train_loss: 0.0018286732794213915, test_loss: 0.001441365556484715\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1379, train_loss: 0.0018280462048348993, test_loss: 0.0014293216406188619\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1380, train_loss: 0.001827808432526307, test_loss: 0.001433840781921903\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1381, train_loss: 0.0018270751933958912, test_loss: 0.0014295041395458751\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1382, train_loss: 0.0018264060646017552, test_loss: 0.0014292402018513829\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1383, train_loss: 0.0018259811383106723, test_loss: 0.001443005728502519\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1384, train_loss: 0.001825213085986163, test_loss: 0.0014295407552428795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1385, train_loss: 0.0018250896583932661, test_loss: 0.0014338378530691187\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1386, train_loss: 0.0018240748598519869, test_loss: 0.0014344147630953768\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1387, train_loss: 0.0018237841495252012, test_loss: 0.0014372548450923299\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1388, train_loss: 0.0018230796224915774, test_loss: 0.0014277953350989637\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1389, train_loss: 0.0018227484796926642, test_loss: 0.001427414762497975\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1390, train_loss: 0.0018222223576796817, test_loss: 0.0014318224226978298\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1391, train_loss: 0.0018212285770195223, test_loss: 0.0014212073551905819\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1392, train_loss: 0.0018213824721374067, test_loss: 0.0014261525791833918\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1393, train_loss: 0.0018206969528251878, test_loss: 0.0014291252258632225\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1394, train_loss: 0.0018200902513580328, test_loss: 0.0014314307180682544\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1395, train_loss: 0.001819293514726509, test_loss: 0.0014377527957735881\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1396, train_loss: 0.0018191136556823917, test_loss: 0.0014282324375996653\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1397, train_loss: 0.0018184365304404976, test_loss: 0.0014260679105437025\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1398, train_loss: 0.0018179323646881583, test_loss: 0.0014225639086082363\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1399, train_loss: 0.0018174163072469667, test_loss: 0.0014235582142972793\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1400, train_loss: 0.001816756018911672, test_loss: 0.001423889392380937\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1401, train_loss: 0.0018162113026171506, test_loss: 0.001430773882860264\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1402, train_loss: 0.0018160953522265817, test_loss: 0.0014305113124861142\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1403, train_loss: 0.001815358485873122, test_loss: 0.0014216224537962356\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1404, train_loss: 0.0018144199284172017, test_loss: 0.0014358841387244563\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1405, train_loss: 0.0018142441625494266, test_loss: 0.001419020091140201\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1406, train_loss: 0.0018131089942155245, test_loss: 0.0014252025297863814\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1407, train_loss: 0.0018131686430027316, test_loss: 0.0014260421701645753\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1408, train_loss: 0.0018123556244814688, test_loss: 0.0014317344278261329\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1409, train_loss: 0.0018122221342635257, test_loss: 0.0014248402593903637\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1410, train_loss: 0.0018114797108305187, test_loss: 0.0014332402580550525\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1411, train_loss: 0.0018113838688020335, test_loss: 0.0014189124553806137\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1412, train_loss: 0.001810671163786726, test_loss: 0.0014158737476697215\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1413, train_loss: 0.0018097729425850858, test_loss: 0.0014253285646704265\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1414, train_loss: 0.001809567521228128, test_loss: 0.0014202620281699079\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1415, train_loss: 0.0018087783242407625, test_loss: 0.0014175544497564447\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1416, train_loss: 0.001808668621488688, test_loss: 0.0014110506068764385\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1417, train_loss: 0.0018077912802899874, test_loss: 0.001420679179621705\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1418, train_loss: 0.0018074220455412046, test_loss: 0.001416478351949571\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1419, train_loss: 0.0018070568214754658, test_loss: 0.0014209274056319816\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1420, train_loss: 0.0018062038968331227, test_loss: 0.001425070921332241\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1421, train_loss: 0.0018057030618633067, test_loss: 0.0014174476065165482\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1422, train_loss: 0.0018049339107475259, test_loss: 0.0014261676375239263\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1423, train_loss: 0.001804498826538253, test_loss: 0.0014104178374579463\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1424, train_loss: 0.0018040845458735292, test_loss: 0.0014159952464606017\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1425, train_loss: 0.001803597329841634, test_loss: 0.0014059805388313134\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1426, train_loss: 0.0018033863422554299, test_loss: 0.0014128319928250932\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1427, train_loss: 0.001802186761693842, test_loss: 0.0014197471701220658\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1428, train_loss: 0.0018021002561060314, test_loss: 0.001423531685969255\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1429, train_loss: 0.0018017384944714132, test_loss: 0.0014227957851686275\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1430, train_loss: 0.0018009041901483173, test_loss: 0.0014148266060286774\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1431, train_loss: 0.001800600412250325, test_loss: 0.001412281316637139\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1432, train_loss: 0.0018000050476985052, test_loss: 0.0014084628003333004\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1433, train_loss: 0.0017992085113869918, test_loss: 0.00141367843706319\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1434, train_loss: 0.0017991189849383843, test_loss: 0.0014157398459829916\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1435, train_loss: 0.0017988156565437, test_loss: 0.0014087220798291338\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1436, train_loss: 0.0017979607431071632, test_loss: 0.0014028437291973461\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1437, train_loss: 0.0017975116055926003, test_loss: 0.0014085381717916627\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1438, train_loss: 0.0017966397570362035, test_loss: 0.0014109049153399284\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1439, train_loss: 0.0017965554936600532, test_loss: 0.0014034319166780617\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1440, train_loss: 0.0017960201445161006, test_loss: 0.0014076318005278993\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1441, train_loss: 0.0017953359738598113, test_loss: 0.0014057925594291494\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1442, train_loss: 0.00179441780173905, test_loss: 0.001416563257897416\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1443, train_loss: 0.0017943800100745217, test_loss: 0.0014092343464234057\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1444, train_loss: 0.0017939246124622995, test_loss: 0.0014053295624482697\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1445, train_loss: 0.0017935359401509234, test_loss: 0.0014043194037535263\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1446, train_loss: 0.001793018220725273, test_loss: 0.001405464794515865\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1447, train_loss: 0.0017923758600575281, test_loss: 0.0013964380210755058\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1448, train_loss: 0.0017921716722552306, test_loss: 0.0014065176361234477\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1449, train_loss: 0.0017914529823632693, test_loss: 0.001401420168967422\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1450, train_loss: 0.0017907851366684414, test_loss: 0.0014050068421056494\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1451, train_loss: 0.0017900888948821241, test_loss: 0.0014104881298314864\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1452, train_loss: 0.0017899814992656396, test_loss: 0.0014052372800786262\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1453, train_loss: 0.0017895568566639007, test_loss: 0.0013997883599166437\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1454, train_loss: 0.0017890309082518603, test_loss: 0.0013979244670339634\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1455, train_loss: 0.0017884066257350159, test_loss: 0.0013965338393739867\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1456, train_loss: 0.0017879975574787233, test_loss: 0.0013937267460091673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1457, train_loss: 0.001787257316784332, test_loss: 0.0014009507648528924\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1458, train_loss: 0.0017870914518644716, test_loss: 0.00140434873257898\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1459, train_loss: 0.0017861414605940078, test_loss: 0.001412845810894699\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1460, train_loss: 0.001786011343256938, test_loss: 0.0014008502878893453\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1461, train_loss: 0.0017852512612938102, test_loss: 0.0014024039523158139\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1462, train_loss: 0.001784850267501062, test_loss: 0.0013877852304572931\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1463, train_loss: 0.001784350293771018, test_loss: 0.0013970660285663027\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1464, train_loss: 0.0017841363412929218, test_loss: 0.0013983411714807749\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1465, train_loss: 0.001783614287099935, test_loss: 0.0014026008085728302\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1466, train_loss: 0.0017826598737993118, test_loss: 0.0014008822615253918\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1467, train_loss: 0.0017827465393608528, test_loss: 0.0013962853100122467\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1468, train_loss: 0.0017819944126718369, test_loss: 0.001389472668215137\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1469, train_loss: 0.0017813403708556586, test_loss: 0.001389718941936735\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1470, train_loss: 0.0017807236591308848, test_loss: 0.0014035290462346987\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1471, train_loss: 0.0017807679736549127, test_loss: 0.0013949026144520105\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1472, train_loss: 0.0017798815331290193, test_loss: 0.0013939207466393828\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1473, train_loss: 0.001779420335094186, test_loss: 0.0013941971296728111\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1474, train_loss: 0.0017780919475005909, test_loss: 0.001412549390900098\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1475, train_loss: 0.0017791123233188916, test_loss: 0.001392065650202895\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1476, train_loss: 0.0017778431673251547, test_loss: 0.001391951632309102\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1477, train_loss: 0.0017773969924150589, test_loss: 0.0013985666054605434\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1478, train_loss: 0.001777151429624278, test_loss: 0.0013948415602126028\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1479, train_loss: 0.001776774465277741, test_loss: 0.0013914243791077752\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1480, train_loss: 0.0017754329645104496, test_loss: 0.0014035260097350543\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1481, train_loss: 0.001775524459945125, test_loss: 0.0013908544370235924\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1482, train_loss: 0.0017749917179787607, test_loss: 0.0013950506491267171\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1483, train_loss: 0.001774169277846751, test_loss: 0.0013796647620508268\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1484, train_loss: 0.0017742812505515953, test_loss: 0.0013827987236137956\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1485, train_loss: 0.0017737034484580545, test_loss: 0.0013941530702980414\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1486, train_loss: 0.0017733404049895124, test_loss: 0.0013958253346059697\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1487, train_loss: 0.0017722930662730323, test_loss: 0.0013785237074793817\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1488, train_loss: 0.001771990697185375, test_loss: 0.0013811330876099721\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1489, train_loss: 0.001771848312778442, test_loss: 0.0013834008332662799\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1490, train_loss: 0.0017706738095956366, test_loss: 0.001395752203489582\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1491, train_loss: 0.0017704319686396826, test_loss: 0.0013815530855469822\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1492, train_loss: 0.0017699072513002031, test_loss: 0.0013804900953777505\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1493, train_loss: 0.0017699099118185905, test_loss: 0.0013823995151398822\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1494, train_loss: 0.0017695289418505455, test_loss: 0.0013814730187639212\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1495, train_loss: 0.0017683174079369728, test_loss: 0.0013791454303151486\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1496, train_loss: 0.001768352052104974, test_loss: 0.0013769698049052237\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1497, train_loss: 0.0017678306288518833, test_loss: 0.0013757705506950747\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1498, train_loss: 0.0017674702743728672, test_loss: 0.0013861941005392836\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1499, train_loss: 0.0017668886581688097, test_loss: 0.0013798473937174257\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1500, train_loss: 0.0017664572547180547, test_loss: 0.0013758190386094309\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1501, train_loss: 0.0017657470527295035, test_loss: 0.0013758760956429446\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1502, train_loss: 0.0017654845887408856, test_loss: 0.001380625360373718\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1503, train_loss: 0.0017649459724242871, test_loss: 0.00138426998884638\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1504, train_loss: 0.0017644101803472895, test_loss: 0.0013803599042428406\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1505, train_loss: 0.0017633977815093018, test_loss: 0.0013774243531000908\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1506, train_loss: 0.001763521749810487, test_loss: 0.0013775206489066104\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1507, train_loss: 0.001762929130703784, test_loss: 0.0013883188921914925\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1508, train_loss: 0.0017625797217957874, test_loss: 0.0013837032195461246\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1509, train_loss: 0.0017621083476893715, test_loss: 0.0013783873722646744\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1510, train_loss: 0.0017613657684194665, test_loss: 0.0013718998871366077\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1511, train_loss: 0.0017612169734801922, test_loss: 0.0013749679445680112\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1512, train_loss: 0.0017604390482408895, test_loss: 0.0013821035801158405\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1513, train_loss: 0.001759317518215016, test_loss: 0.0013647020134204724\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1514, train_loss: 0.0017593444811131954, test_loss: 0.0013703951013807314\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1515, train_loss: 0.0017592008779468607, test_loss: 0.001376383132316196\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1516, train_loss: 0.001758492603916737, test_loss: 0.0013755387303834327\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1517, train_loss: 0.0017579963042776109, test_loss: 0.0013684690096012603\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1518, train_loss: 0.0017580383440306765, test_loss: 0.0013748018914506806\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1519, train_loss: 0.0017571605434976083, test_loss: 0.001382745744101554\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1520, train_loss: 0.0017565776876066453, test_loss: 0.0013783166965974185\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1521, train_loss: 0.0017564602333072722, test_loss: 0.0013696629001247063\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1522, train_loss: 0.001755677137806557, test_loss: 0.0013630780881496135\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1523, train_loss: 0.0017554498324131773, test_loss: 0.0013680591283236237\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1524, train_loss: 0.0017544878975731199, test_loss: 0.0013821466950153431\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1525, train_loss: 0.0017545291546375544, test_loss: 0.001367239398976567\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1526, train_loss: 0.0017535279675878982, test_loss: 0.0013636802844606184\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1527, train_loss: 0.0017534402015316846, test_loss: 0.0013753012380897044\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1528, train_loss: 0.0017531322172054877, test_loss: 0.0013709070341204866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1529, train_loss: 0.0017519428247852987, test_loss: 0.0013913299714122564\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1530, train_loss: 0.001752370964275536, test_loss: 0.0013699705093294585\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1531, train_loss: 0.0017518198547260306, test_loss: 0.001373364487312173\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1532, train_loss: 0.0017512039536256717, test_loss: 0.0013687804485715998\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1533, train_loss: 0.0017508189463528515, test_loss: 0.0013709352370381404\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1534, train_loss: 0.0017500508430236359, test_loss: 0.0013568921130149064\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1535, train_loss: 0.0017496696356987661, test_loss: 0.001365598034727811\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1536, train_loss: 0.0017491513033406061, test_loss: 0.001360122258912993\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1537, train_loss: 0.001748902594562025, test_loss: 0.0013721962784005043\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1538, train_loss: 0.0017480173543014605, test_loss: 0.0013795284097656035\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1539, train_loss: 0.0017480736926009561, test_loss: 0.00137128596888476\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1540, train_loss: 0.0017472810000878133, test_loss: 0.0013644222865248313\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1541, train_loss: 0.001747112681199158, test_loss: 0.0013614907011521072\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1542, train_loss: 0.00174648164581495, test_loss: 0.0013619713090287032\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1543, train_loss: 0.0017461737345890096, test_loss: 0.0013652567082690671\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1544, train_loss: 0.0017453919403896633, test_loss: 0.0013814045495774525\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1545, train_loss: 0.0017451920026226473, test_loss: 0.0013707729031478104\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1546, train_loss: 0.0017448231970336923, test_loss: 0.001370715466924967\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1547, train_loss: 0.0017441271395067432, test_loss: 0.0013663332748155182\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1548, train_loss: 0.0017438098400080558, test_loss: 0.0013674689042277467\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1549, train_loss: 0.0017433111683580788, test_loss: 0.0013543816144542338\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1550, train_loss: 0.0017425967592112938, test_loss: 0.0013671353023173884\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1551, train_loss: 0.001742464218469318, test_loss: 0.0013781905352236489\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1552, train_loss: 0.0017422934499433745, test_loss: 0.0013653806135362873\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1553, train_loss: 0.0017414360572303642, test_loss: 0.0013558469411061825\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1554, train_loss: 0.0017411959506290961, test_loss: 0.0013630520773473948\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1555, train_loss: 0.001740428316465308, test_loss: 0.0013644683360605715\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1556, train_loss: 0.0017399707323183184, test_loss: 0.0013633818840063238\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1557, train_loss: 0.0017398276168114838, test_loss: 0.0013614089072619278\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1558, train_loss: 0.0017393909252726046, test_loss: 0.001353950915542485\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1559, train_loss: 0.0017391271572085067, test_loss: 0.0013609125848093894\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1560, train_loss: 0.0017374142710264323, test_loss: 0.0013722986446877117\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1561, train_loss: 0.0017382025554286392, test_loss: 0.0013671999397793582\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1562, train_loss: 0.0017373604936693425, test_loss: 0.0013558609513356946\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1563, train_loss: 0.0017371805102769193, test_loss: 0.0013503531382668417\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1564, train_loss: 0.001736155581850571, test_loss: 0.0013597057473723395\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1565, train_loss: 0.0017363751147691485, test_loss: 0.0013541846160361997\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1566, train_loss: 0.001735297241364995, test_loss: 0.0013675230987316284\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1567, train_loss: 0.0017353447371615677, test_loss: 0.0013553799098866503\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1568, train_loss: 0.0017347505394884083, test_loss: 0.001366461629050975\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1569, train_loss: 0.0017340131612871477, test_loss: 0.0013704761602126288\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1570, train_loss: 0.0017333712881815661, test_loss: 0.0013606710408333666\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1571, train_loss: 0.0017337405205942553, test_loss: 0.0013609361029431545\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1572, train_loss: 0.0017327345414383081, test_loss: 0.0013669789480515576\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1573, train_loss: 0.0017325048049990504, test_loss: 0.0013580657603317657\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1574, train_loss: 0.0017318564753692884, test_loss: 0.0013488577395223845\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1575, train_loss: 0.0017315842018810795, test_loss: 0.0013463532571525623\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1576, train_loss: 0.0017312040271094992, test_loss: 0.001351907635044205\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1577, train_loss: 0.0017307645588315848, test_loss: 0.0013525768289326106\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1578, train_loss: 0.001730216814327306, test_loss: 0.0013481109190018824\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1579, train_loss: 0.001729338139170597, test_loss: 0.001361408334979877\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1580, train_loss: 0.001729495350567758, test_loss: 0.0013480194445433787\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1581, train_loss: 0.001728805826510066, test_loss: 0.0013443904075150688\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1582, train_loss: 0.0017285799408774593, test_loss: 0.0013541601874749176\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1583, train_loss: 0.0017281285195882405, test_loss: 0.00135262843991185\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1584, train_loss: 0.0017277149726515231, test_loss: 0.0013483090187013985\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1585, train_loss: 0.0017271006343976586, test_loss: 0.001346158565399655\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1586, train_loss: 0.0017267528660112625, test_loss: 0.0013437831524546102\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1587, train_loss: 0.001726237429094884, test_loss: 0.0013468562344944356\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1588, train_loss: 0.0017257208419445105, test_loss: 0.0013445749048118254\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1589, train_loss: 0.0017253358801281747, test_loss: 0.0013375659827710106\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1590, train_loss: 0.0017253072176246988, test_loss: 0.001344328453295673\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1591, train_loss: 0.0017242934229222272, test_loss: 0.0013459946421789937\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1592, train_loss: 0.001724151518436485, test_loss: 0.0013428537969071514\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1593, train_loss: 0.001723752343438466, test_loss: 0.00133999532996453\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1594, train_loss: 0.0017232998260347497, test_loss: 0.0013392081500802678\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1595, train_loss: 0.0017226099946834365, test_loss: 0.00133581266242963\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1596, train_loss: 0.00172246130297029, test_loss: 0.0013494101870328917\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1597, train_loss: 0.0017216968433627778, test_loss: 0.0013369046484093326\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1598, train_loss: 0.001721565550240691, test_loss: 0.001350943265816358\n",
      "train batch:  299\n",
      "test batch:  39\n",
      "epoch: 1599, train_loss: 0.0017213411129373328, test_loss: 0.0013484270857588854\n"
     ]
    }
   ],
   "source": [
    "seq_dim = 10 # = window_size\n",
    "\n",
    "num_epochs = 1600 # 400 # 200 will overfit, 100 is good, see the plot: plt.plot(train_loss[20:]) and plt.plot(test_loss[20:])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    train_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(train_loader):\n",
    "        # print(\"train: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(seqs.float())\n",
    "        outputs = model(seqs)\n",
    "        # print(outputs.is_cuda)\n",
    "        # print(labels.is_cuda)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs.dtype)\n",
    "        \n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_train_loss += loss.data.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch = i + 1;\n",
    "        \n",
    "        # print(\"loss: \", loss.data)\n",
    "    \n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    # test_seq = []\n",
    "    test_pred = []\n",
    "    # test_gt = []\n",
    "    for i, (seqs, labels) in enumerate(test_loader):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        test_pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        test_batch = i + 1\n",
    "    \n",
    "    print(\"train batch: \", train_batch)\n",
    "    print(\"test batch: \", test_batch)\n",
    "    train_loss.append(total_train_loss/train_batch)\n",
    "    test_loss.append(total_test_loss/test_batch)\n",
    "    # train_loss.append(total_train_loss)\n",
    "    # test_loss.append(total_test_loss)\n",
    "    print(\"epoch: {}, train_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_test_loss/test_batch))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "124/(199/39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 1600\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "test_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, test_loss_curve], ['Train Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(50, len(train_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013484270857588854\n"
     ]
    }
   ],
   "source": [
    "# get test results\n",
    "seq_dim = 10 # = window_size\n",
    "input_dim = 3\n",
    "# test_seq = []\n",
    "test_predd = []\n",
    "# test_gt = []\n",
    "total_test_loss = 0.0\n",
    "test_batch = 0\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        \n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_test_loss += loss.data.item()\n",
    "    test_predd.append(outputs)\n",
    "    test_batch = i + 1\n",
    "print(total_test_loss/test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3591, -0.2594, -1.2198],\n",
       "        [ 0.1797, -0.2594, -1.2168],\n",
       "        [ 0.0073, -0.2596, -1.1807],\n",
       "        [-0.1624, -0.2591, -1.1296],\n",
       "        [-0.3344, -0.2583, -1.0978],\n",
       "        [-0.5071, -0.2583, -1.0742],\n",
       "        [-0.6797, -0.2585, -1.0518],\n",
       "        [-0.8495, -0.2588, -1.0161],\n",
       "        [-0.9931, -0.2594, -0.9066],\n",
       "        [-1.1044, -0.2589, -0.7588],\n",
       "        [-1.1153, -0.2605, -0.5499],\n",
       "        [-1.1272, -0.2581, -0.3526],\n",
       "        [-1.1241, -0.2575, -0.1668],\n",
       "        [-1.0960, -0.2581,  0.0111],\n",
       "        [-1.0665, -0.2580,  0.1842],\n",
       "        [-1.0241, -0.2583,  0.3504],\n",
       "        [-0.8737, -0.2622,  0.4689],\n",
       "        [-0.7243, -0.2626,  0.5871],\n",
       "        [-0.5762, -0.2622,  0.6942],\n",
       "        [-0.4162, -0.2623,  0.7744],\n",
       "        [-0.2332, -0.2630,  0.7990],\n",
       "        [-0.0428, -0.2637,  0.7634],\n",
       "        [ 0.1369, -0.2643,  0.6991],\n",
       "        [ 0.3093, -0.2647,  0.6592],\n",
       "        [ 0.4671, -0.2642,  0.7472],\n",
       "        [ 0.6353, -0.2610,  0.7547],\n",
       "        [ 0.8032, -0.2604,  0.6757],\n",
       "        [ 0.9305, -0.2603,  0.5323],\n",
       "        [ 1.0075, -0.2603,  0.3410],\n",
       "        [ 1.0915, -0.2619,  0.1749],\n",
       "        [ 1.1356, -0.2610,  0.0015],\n",
       "        [ 1.1582, -0.2603, -0.1726],\n",
       "        [ 1.0869, -0.2584, -0.3467],\n",
       "        [ 0.9815, -0.2580, -0.5064],\n",
       "        [ 0.8589, -0.2580, -0.6469],\n",
       "        [ 0.7464, -0.2583, -0.7840],\n",
       "        [ 0.6549, -0.2589, -0.9291],\n",
       "        [ 0.5482, -0.2591, -1.0597],\n",
       "        [ 0.3796, -0.2586, -1.1317],\n",
       "        [ 0.2128, -0.2587, -1.2063],\n",
       "        [ 0.0419, -0.2595, -1.2506],\n",
       "        [-0.1428, -0.2599, -1.1752],\n",
       "        [-0.2961, -0.2591, -1.0288],\n",
       "        [-0.4375, -0.2570, -0.8973],\n",
       "        [-0.5793, -0.2556, -0.7912],\n",
       "        [-0.7215, -0.2551, -0.6949],\n",
       "        [-0.8661, -0.2548, -0.6027],\n",
       "        [-1.0119, -0.2547, -0.5108],\n",
       "        [-1.0930, -0.2570, -0.3508],\n",
       "        [-1.0184, -0.2616, -0.1436]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predd[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4635323 , -0.36533475,  0.8491472 ],\n",
       "       [ 0.4978454 , -0.36533475,  0.82831156],\n",
       "       [ 0.5321585 , -0.36533475,  0.80747604],\n",
       "       ...,\n",
       "       [-0.86375517, -1.7444978 , -0.03705572],\n",
       "       [-0.8341637 , -1.7444978 , -0.01024055],\n",
       "       [-0.8022799 , -1.7444978 ,  0.0139663 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of ground true around predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore testing data\n",
    "test_seq = []\n",
    "test_gt = []\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    test_gt.append(labels)\n",
    "    test_seq.append(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = test_seq[i].numpy()\n",
    "        gt = test_gt[i].numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        test = np.append(test, test_seq[i].numpy(), axis = 0)\n",
    "        gt = np.append(gt, test_gt[i].numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test[0][-1,:].reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct analysis matrix l(0-2 col): last sequence point, g(3-5 col): next ground true point, p(6-8 col): predicted point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    last_point = test[i][-1, :].reshape(1, -1)\n",
    "    gt_point = gt[i].reshape(1, -1)\n",
    "    pred_point = pred[i].reshape(1, -1)\n",
    "    row = np.append(np.append(last_point, gt_point, axis = 1), pred_point, axis = 1)\n",
    "    if (i == 0):\n",
    "        analysis = row\n",
    "    else:\n",
    "        analysis = np.append(analysis, row, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rotation matrix\n",
    "# note: input vec_1 and vec_2 have to be normalized before passing to the function\n",
    "def get_rotatin_mat(vec_1, vec_2):\n",
    "    a,b = vec_1.reshape(3), vec_2.reshape(3)\n",
    "    v = np.cross(a,b)\n",
    "    c = np.dot(a,b)\n",
    "    s = np.linalg.norm(v)\n",
    "    # print(\"s\", s)\n",
    "    if (s == 0):\n",
    "        return np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "    I = np.identity(3)\n",
    "    vXStr = '{} {} {}; {} {} {}; {} {} {}'.format(0, -v[2], v[1], v[2], 0, -v[0], -v[1], v[0], 0)\n",
    "    k = np.matrix(vXStr)\n",
    "    r = I + k + np.matmul(k,k) * ((1 -c)/(s**2))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 1, 0])\n",
    "a = a/np.linalg.norm(a)\n",
    "b = np.array([4, 5, 6])\n",
    "b = b/np.linalg.norm(b)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "m = get_rotatin_mat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(m, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(b[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b/np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "a_n = a/np.linalg.norm(a)\n",
    "\n",
    "b = np.array([[4,5,6]])\n",
    "b_n = b/np.linalg.norm(b)\n",
    "\n",
    "c = np.array([[7,8,9]])\n",
    "c_n = c/np.linalg.norm(c)\n",
    "\n",
    "r = get_rotatin_mat(b_n, a_n) # b to a\n",
    "\n",
    "b_p = np.dot(r, b.reshape(3, -1))\n",
    "c_p = np.dot(r, c.reshape(3, -1))\n",
    "c_p = np.asarray(c_p)\n",
    "c_p_n = c_p/np.linalg.norm(c_p)\n",
    "print(b.shape)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"c\", c)\n",
    "print(\"b_p\", b_p)\n",
    "print(\"c_p\", c_p)\n",
    "\n",
    "print(get_rotatin_mat(c_n, b_n))\n",
    "\n",
    "print(get_rotatin_mat(c_p_n, a_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c_p.reshape(1, -1)\n",
    "d = np.asarray(d)\n",
    "d_n = d/np.linalg.norm(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rotatin_mat(d_n, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all point to the same starting point\n",
    "# first sequence last direction\n",
    "# direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = (analysis[0, 3:6] - analysis[0, 0:3]).reshape(1, -1)\n",
    "# normalize\n",
    "direction = direction/np.linalg.norm(direction)\n",
    "# first diff between prediction and ground true\n",
    "diff_correct = (analysis[0, 6:9] - analysis[0, 3:6]).reshape(1, -1)\n",
    "# print(type(diff_correct))\n",
    "# print(diff_correct.shape)\n",
    "for i in range(1, analysis.shape[0]):\n",
    "    # cur_direction = (analysis[i, 0:3] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    cur_direction = (analysis[i, 3:6] - analysis[i, 0:3]).reshape(1, -1)\n",
    "    # normalize\n",
    "    if (np.linalg.norm(cur_direction) == 0):\n",
    "        print(i, np.linalg.norm(cur_direction))\n",
    "    cur_direction = cur_direction/np.linalg.norm(cur_direction)\n",
    "    cur_diff = (analysis[i, 6:9] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    # get rotation matrix from cur_direction to direction\n",
    "    r = get_rotatin_mat(cur_direction, direction)\n",
    "    # apply rotation matrix to the cur_diff\n",
    "    cur_diff = np.dot(r, cur_diff.reshape(3, -1))\n",
    "    cur_diff = cur_diff.reshape(1, -1)\n",
    "    # matrix type to np array type\n",
    "    cur_diff = np.asarray(cur_diff)\n",
    "    # print(cur_diff.shape)\n",
    "    diff_correct = np.append(diff_correct, cur_diff, axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_correct[:, 0], diff_correct[:, 1], diff_correct[:, 2])\n",
    "x = [-direction[0, 0], 0]\n",
    "y = [-direction[0, 1], 0]\n",
    "z = [-direction[0, 2], 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "# ax.arrow(0, 0, 0.5, 0.5, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "# ax.quiver(0, 0, 0, -direction[0, 0], -direction[0, 1], -direction[0, 2], length=5, normalize=True, color='r')\n",
    "# x = np.zeros(10)\n",
    "# y = np.zeros(10)\n",
    "# z = np.arange(10)*10 # remove *100 and the arrow heads will reappear.\n",
    "# dx = np.zeros(10)\n",
    "# dy = np.arange(10)\n",
    "# dz = np.zeros(10)\n",
    "x = np.array([0, -direction[0, 0]])\n",
    "y = np.array([0, -direction[0, 1]])\n",
    "z = np.array([0, -direction[0, 2]])\n",
    "dx = np.array([0, 0])\n",
    "dy = np.array([0, 0])\n",
    "dz = np.array([0, 0])\n",
    "# ax.quiver(x, y, z, dx, dy, dz, length=1)\n",
    "# ax.quiver(-direction[0, 0], -direction[0, 1], -direction[0, 2], 0, 0, 0, length=100, normalize=True)\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note view_angle must be normalized\n",
    "def getProjection(view_angle, point):\n",
    "    v = point - np.array([[0, 0, 0]])\n",
    "    dist = v[0, 0]*view_angle[0, 0] + v[0, 1]*view_angle[0, 1] + v[0, 2]*view_angle[0, 2]\n",
    "    projected_point = point - dist*view_angle\n",
    "    \n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))\n",
    "# normalization\n",
    "view_angle = view_angle/np.linalg.norm(view_angle)\n",
    "print(view_angle)\n",
    "for i in range(diff_correct.shape[0]):\n",
    "    if (i == 0):\n",
    "        diff_corrrect_projection = getProjection(view_angle, diff_correct[i, :])\n",
    "    else:\n",
    "        diff_corrrect_projection = np.append(diff_corrrect_projection, getProjection(view_angle, diff_correct[i, :]), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_corrrect_projection[:, 0], diff_corrrect_projection[:, 1], diff_corrrect_projection[:, 2])\n",
    "x = [-direction[0, 0]*10, 0]\n",
    "y = [-direction[0, 1]*10, 0]\n",
    "z = [-direction[0, 2]*10, 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from 3D to 2D\n",
    "diff_corrrect_projection_2D = np.delete(diff_corrrect_projection, 1, 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from matplotlib.colors import LogNorm\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(diff_corrrect_projection_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(-5., 5.)\n",
    "y = np.linspace(-5., 5.)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -clf.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
    "                 levels=np.logspace(0, 3, 300))\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(diff_corrrect_projection_2D[:, 0], diff_corrrect_projection_2D[:, 1], .8)\n",
    "\n",
    "x = [-direction[0, 0]*5, 0]\n",
    "z = [-direction[0, 2]*5, 0]\n",
    "plt.plot(x, z, label='parametric curve', color='r')\n",
    "\n",
    "plt.title('Negative log-likelihood predicted by a GMM')\n",
    "plt.axis('tight')\n",
    "plt.xlabel(\"Direction 1\")\n",
    "plt.ylabel(\"Direction 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "C = np.array([[0., -0.7], [3.5, .7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_gaussian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(direction[0, 0], 2) + pow(direction[0, 1], 2) + pow(direction[0, 2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[2, 0:3] - analysis[2, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[3, 0:3] - analysis[3, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = direction/np.linalg.norm(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (analysis[1, 0:3] - analysis[1, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d/np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(0.84804916, 2) + pow(0.5299187, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 3:6] - analysis[0, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([13.259178  ,  0.37397736, 20.36138]) - np.asarray([12.658086  ,  1.293065  ,20.224268])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = \n",
    "        gt = test_gt[i].cpu().detach().numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        gt = np.append(gt, test_gt[i].cpu().detach().numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pred - gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff[:, 0], diff[:, 1], diff[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.from_numpy(gt), torch.from_numpy(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i in range(len(test_pred)):\n",
    "    l = criterion(test_gt[i], test_pred[i])\n",
    "    loss += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
