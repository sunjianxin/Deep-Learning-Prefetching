{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data, normalize_all, construct_train_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load each of the 5 dataset and do min_max_scaling on each of them\n",
    "train_set_x_1, test_set_x_1, train_set_y_1, test_set_y_1, train_set_z_1, test_set_z_1 = load_data('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, test_set_x_2, train_set_y_2, test_set_y_2, train_set_z_2, test_set_z_2 = load_data('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, test_set_x_3, train_set_y_3, test_set_y_3, train_set_z_3, test_set_z_3 = load_data('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, test_set_x_4, train_set_y_4, test_set_y_4, train_set_z_4, test_set_z_4 = load_data('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, test_set_x_5, train_set_y_5, test_set_y_5, train_set_z_5, test_set_z_5 = load_data('data_preprocessing/test_5_training_xyz.txt', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_statistic(train_set_x_1)\n",
    "x_mean, x_std = normalize_all(train_set_x_1, train_set_x_2, train_set_x_3, train_set_x_4, train_set_x_5, test_set_x_1, test_set_x_2, test_set_x_3, test_set_x_4, test_set_x_5)\n",
    "y_mean, y_std = normalize_all(train_set_y_1, train_set_y_2, train_set_y_3, train_set_y_4, train_set_y_5, test_set_y_1, test_set_y_2, test_set_y_3, test_set_y_4, test_set_y_5)\n",
    "z_mean, z_std = normalize_all(train_set_z_1, train_set_z_2, train_set_z_3, train_set_z_4, train_set_z_5, test_set_z_1, test_set_z_2, test_set_z_3, test_set_z_4, test_set_z_5)\n",
    "# show_statistic(train_set_x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "train_dataset_1, train_label_1, test_dataset_1, test_label_1 = construct_train_test_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           test_set_x_1,\n",
    "                                                                                           test_set_y_1,\n",
    "                                                                                           test_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, test_dataset_2, test_label_2 = construct_train_test_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           test_set_x_2,\n",
    "                                                                                           test_set_y_2,\n",
    "                                                                                           test_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, test_dataset_3, test_label_3 = construct_train_test_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           test_set_x_3,\n",
    "                                                                                           test_set_y_3,\n",
    "                                                                                           test_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, test_dataset_4, test_label_4 = construct_train_test_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           test_set_x_4,\n",
    "                                                                                           test_set_y_4,\n",
    "                                                                                           test_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, test_dataset_5, test_label_5 = construct_train_test_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           test_set_x_5,\n",
    "                                                                                           test_set_y_5,\n",
    "                                                                                           test_set_z_5,\n",
    "                                                                                           window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 10, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2990, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 10, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "test_dataset = np.concatenate((test_dataset_1,\n",
    "                               test_dataset_2,\n",
    "                               test_dataset_3,\n",
    "                               test_dataset_4,\n",
    "                               test_dataset_5), axis=0)\n",
    "test_label = np.concatenate((test_label_1,\n",
    "                             test_label_2,\n",
    "                             test_label_3,\n",
    "                             test_label_4,\n",
    "                             test_label_5), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 10, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14950, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 10, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2990*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "390*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-9979ff4634ab>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-9979ff4634ab>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 23 = 14950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 23 = 14950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-de69bcda9ea3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-de69bcda9ea3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    2 x 5 x 5 x 13 x 3 = 1950\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 x 5 x 5 x 13 x 3 = 1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14950"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 5 * 5 * 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainDataSet(Dataset):\n",
    "    def __init__(self, train_dataset, train_label):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_label = train_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.train_dataset[idx]\n",
    "        label = self.train_label[idx]\n",
    "        \n",
    "        return [seq, label]\n",
    "\n",
    "\n",
    "class MyTestDataSet(Dataset):\n",
    "    def __init__(self, test_dataset, test_label):\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_label = test_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.test_dataset[idx]\n",
    "        label = self.test_label[idx]\n",
    "\n",
    "        return [seq, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14950\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(test_dataset, test_label)\n",
    "print(len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 30\n",
    "# batch_size = 50\n",
    "batch_size = 650\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# keep the test data trajectory order\n",
    "test_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "# test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9bedbb4898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "1 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "2 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "3 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "4 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "5 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "6 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "7 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "8 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "9 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "10 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "11 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "12 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "13 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "14 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "15 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "16 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "17 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "18 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "19 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "20 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "21 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "22 torch.Size([650, 10, 3]) torch.Size([650, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    print(i, seqs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "1 torch.Size([650, 10, 3]) torch.Size([650, 3])\n",
      "2 torch.Size([650, 10, 3]) torch.Size([650, 3])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    print(i, seqs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[ 1.0846,  0.4371, -0.9165],\n",
      "        [ 1.0257,  0.4371, -0.9544],\n",
      "        [ 0.9650,  0.4371, -0.9890],\n",
      "        [ 0.9043,  0.4371, -1.0236],\n",
      "        [ 0.8435,  0.4371, -1.0582],\n",
      "        [ 0.7828,  0.4371, -1.0928],\n",
      "        [ 0.7221,  0.4371, -1.1274],\n",
      "        [ 0.6611,  0.4371, -1.1614],\n",
      "        [ 0.5964,  0.4371, -1.1859],\n",
      "        [ 0.5278,  0.4371, -1.1911]])\n",
      "tensor([ 0.4591,  0.4371, -1.1936])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[0])\n",
    "        print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.float32 torch.float32\n",
      "tensor([[1.3742e-01, 2.5346e-04, 1.0000e+00],\n",
      "        [1.7205e-01, 2.5346e-04, 9.8299e-01],\n",
      "        [2.0532e-01, 2.5346e-04, 9.6373e-01],\n",
      "        [2.3859e-01, 2.5346e-04, 9.4447e-01],\n",
      "        [2.7186e-01, 2.5346e-04, 9.2522e-01],\n",
      "        [3.0513e-01, 2.5346e-04, 9.0596e-01],\n",
      "        [3.3839e-01, 2.5346e-04, 8.8670e-01],\n",
      "        [3.7166e-01, 2.5346e-04, 8.6744e-01],\n",
      "        [4.0493e-01, 2.5346e-04, 8.4819e-01],\n",
      "        [4.3820e-01, 2.5346e-04, 8.2893e-01]])\n",
      "tensor([4.7147e-01, 2.5346e-04, 8.0967e-01])\n"
     ]
    }
   ],
   "source": [
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    if (i == 0):\n",
    "        print(i, seqs.dtype, labels.dtype)\n",
    "        print(seqs[1])\n",
    "        print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        \n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first = True)\n",
    "        # If your input data is of shape (seq_len, batch_size, features)\n",
    "        # then you don’t need batch_first=True and your LSTM will give\n",
    "        # output of shape (seq_len, batch_size, hidden_size).\n",
    "\n",
    "        # If your input data is of shape (batch_size, seq_len, features)\n",
    "        # then you need batch_first=True and your LSTM will give\n",
    "        # output of shape (batch_size, seq_len, hidden_size).\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # self.hidden = (torch.zeros(self.num_layers, 1, hidden_size), torch.zeros(self.num_layers, 1, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim))\n",
    "        if torch.cuda.is_available():\n",
    "            c0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            c0 = Variable(torch.zeros(self.layer_dim,\n",
    "                                      x.size(0), # batch_size, retrieved from batch data x\n",
    "                                      self.hidden_dim))\n",
    "        # print(\"x.size(0)\", x.size(0))\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # x is (batch_size, seq_len, features)\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 3])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 0.23804715478225894, test_loss: 0.024995891532550257\n",
      "epoch: 1, train_loss: 0.023748560525152996, test_loss: 0.022051654600848753\n",
      "epoch: 2, train_loss: 0.020665235817432404, test_loss: 0.01992081229885419\n",
      "epoch: 3, train_loss: 0.018422964798367542, test_loss: 0.01824365199233095\n",
      "epoch: 4, train_loss: 0.016702389466049877, test_loss: 0.016589487126717966\n",
      "epoch: 5, train_loss: 0.01524590660372506, test_loss: 0.015361862101902565\n",
      "epoch: 6, train_loss: 0.014012482665155245, test_loss: 0.014366083468000094\n",
      "epoch: 7, train_loss: 0.012932147506786429, test_loss: 0.013281046723326048\n",
      "epoch: 8, train_loss: 0.01199318598146024, test_loss: 0.012453692654768625\n",
      "epoch: 9, train_loss: 0.011130765728328539, test_loss: 0.011634066778545579\n",
      "epoch: 10, train_loss: 0.010340465759129627, test_loss: 0.010903502038369576\n",
      "epoch: 11, train_loss: 0.009606663218658903, test_loss: 0.01009085075929761\n",
      "epoch: 12, train_loss: 0.008932312100153902, test_loss: 0.009448806134363016\n",
      "epoch: 13, train_loss: 0.008306349000043196, test_loss: 0.008829104636485377\n",
      "epoch: 14, train_loss: 0.007697385573840659, test_loss: 0.00820608320645988\n",
      "epoch: 15, train_loss: 0.007130781370822502, test_loss: 0.007714997511357069\n",
      "epoch: 16, train_loss: 0.006601356172367283, test_loss: 0.007166093525787194\n",
      "epoch: 17, train_loss: 0.0061174396263516465, test_loss: 0.0066358439313868684\n",
      "epoch: 18, train_loss: 0.0056649335336102094, test_loss: 0.006164712908988197\n",
      "epoch: 19, train_loss: 0.005264041661892248, test_loss: 0.005807679767409961\n",
      "epoch: 20, train_loss: 0.0048974072763129425, test_loss: 0.005425237972910206\n",
      "epoch: 21, train_loss: 0.00458380626514554, test_loss: 0.0051198861716936035\n",
      "epoch: 22, train_loss: 0.00429557756844746, test_loss: 0.004839296219870448\n",
      "epoch: 23, train_loss: 0.004060573592458082, test_loss: 0.004598801994385819\n",
      "epoch: 24, train_loss: 0.003854206297546625, test_loss: 0.004357121807212631\n",
      "epoch: 25, train_loss: 0.003688020854378524, test_loss: 0.004236618018088241\n",
      "epoch: 26, train_loss: 0.0035433467761006045, test_loss: 0.004064345305475096\n",
      "epoch: 27, train_loss: 0.0034238501735355544, test_loss: 0.00394551952679952\n",
      "epoch: 28, train_loss: 0.003321360928289916, test_loss: 0.0038665628526359797\n",
      "epoch: 29, train_loss: 0.0032380821750215864, test_loss: 0.0037667916622012854\n",
      "epoch: 30, train_loss: 0.0031681532829837956, test_loss: 0.003681746583121518\n",
      "epoch: 31, train_loss: 0.00310852674200483, test_loss: 0.0036134676774963737\n",
      "epoch: 32, train_loss: 0.0030552808539536986, test_loss: 0.003528562063972155\n",
      "epoch: 33, train_loss: 0.0030065113374882417, test_loss: 0.003521501513508459\n",
      "epoch: 34, train_loss: 0.0029633523207967696, test_loss: 0.0034761669424672923\n",
      "epoch: 35, train_loss: 0.002925561204471666, test_loss: 0.003431205482532581\n",
      "epoch: 36, train_loss: 0.002885066600435454, test_loss: 0.0033421832292030254\n",
      "epoch: 37, train_loss: 0.0028498199788610573, test_loss: 0.003323618322610855\n",
      "epoch: 38, train_loss: 0.0028166942521119895, test_loss: 0.0033215127574900785\n",
      "epoch: 39, train_loss: 0.0027859439344509788, test_loss: 0.0032293424398327866\n",
      "epoch: 40, train_loss: 0.0027562792250967545, test_loss: 0.003227727875734369\n",
      "epoch: 41, train_loss: 0.0027286320742543626, test_loss: 0.0031907816883176565\n",
      "epoch: 42, train_loss: 0.0027017434350336375, test_loss: 0.003129343929079672\n",
      "epoch: 43, train_loss: 0.002677807594528017, test_loss: 0.003086481631423036\n",
      "epoch: 44, train_loss: 0.0026491875760257244, test_loss: 0.003090087207965553\n",
      "epoch: 45, train_loss: 0.002626238471787909, test_loss: 0.0030519949505105615\n",
      "epoch: 46, train_loss: 0.0026048926697315082, test_loss: 0.003007318281258146\n",
      "epoch: 47, train_loss: 0.002577575110911351, test_loss: 0.002967708627693355\n",
      "epoch: 48, train_loss: 0.0025563345229982033, test_loss: 0.002935022426148256\n",
      "epoch: 49, train_loss: 0.0025338656066552453, test_loss: 0.0029093349973360696\n",
      "epoch: 50, train_loss: 0.0025181209202855825, test_loss: 0.0028799683786928654\n",
      "epoch: 51, train_loss: 0.002490826497745255, test_loss: 0.002865016693249345\n",
      "epoch: 52, train_loss: 0.0024745778443858676, test_loss: 0.002858637676884731\n",
      "epoch: 53, train_loss: 0.0024530878189303303, test_loss: 0.0028073775271574655\n",
      "epoch: 54, train_loss: 0.002437533770242463, test_loss: 0.002757379513544341\n",
      "epoch: 55, train_loss: 0.0024178033876840186, test_loss: 0.0027667368218923607\n",
      "epoch: 56, train_loss: 0.00239814188250381, test_loss: 0.0027463321457616985\n",
      "epoch: 57, train_loss: 0.0023829992151940646, test_loss: 0.0026916980665797987\n",
      "epoch: 58, train_loss: 0.002364097557880956, test_loss: 0.002657562513680508\n",
      "epoch: 59, train_loss: 0.0023523056120409265, test_loss: 0.0026842683049229286\n",
      "epoch: 60, train_loss: 0.0023323902202284207, test_loss: 0.0026193331771840653\n",
      "epoch: 61, train_loss: 0.0023182309840036473, test_loss: 0.0026113353475617864\n",
      "epoch: 62, train_loss: 0.0022992584904979753, test_loss: 0.002593288450346639\n",
      "epoch: 63, train_loss: 0.002286993108851754, test_loss: 0.0025954810553230345\n",
      "epoch: 64, train_loss: 0.0022718129963006663, test_loss: 0.0025313178775832057\n",
      "epoch: 65, train_loss: 0.0022580383912376737, test_loss: 0.0025220487732440233\n",
      "epoch: 66, train_loss: 0.0022443473470680738, test_loss: 0.0025328437138038376\n",
      "epoch: 67, train_loss: 0.002230985302721029, test_loss: 0.002479104655018697\n",
      "epoch: 68, train_loss: 0.002215869689319769, test_loss: 0.0024829465352619686\n",
      "epoch: 69, train_loss: 0.0022031261241468396, test_loss: 0.002493230567779392\n",
      "epoch: 70, train_loss: 0.0021885107979988275, test_loss: 0.0024631239163378873\n",
      "epoch: 71, train_loss: 0.00217971736666463, test_loss: 0.002406876941677183\n",
      "epoch: 72, train_loss: 0.00216291812957143, test_loss: 0.0024040016578510404\n",
      "epoch: 73, train_loss: 0.0021522617190266433, test_loss: 0.0023916124288613596\n",
      "epoch: 74, train_loss: 0.0021407691832712812, test_loss: 0.002361831138841808\n",
      "epoch: 75, train_loss: 0.002130859804785122, test_loss: 0.0023776347128053508\n",
      "epoch: 76, train_loss: 0.0021179365804014, test_loss: 0.002333724347408861\n",
      "epoch: 77, train_loss: 0.0021081858861219625, test_loss: 0.0023179022634091475\n",
      "epoch: 78, train_loss: 0.0020974735253611984, test_loss: 0.002290592276646445\n",
      "epoch: 79, train_loss: 0.0020886335987597704, test_loss: 0.002287763597754141\n",
      "epoch: 80, train_loss: 0.0020746284002519173, test_loss: 0.002278343716170639\n",
      "epoch: 81, train_loss: 0.002066426859844638, test_loss: 0.0022781901449585953\n",
      "epoch: 82, train_loss: 0.0020566403045845423, test_loss: 0.00224824317653353\n",
      "epoch: 83, train_loss: 0.0020475380436476807, test_loss: 0.002228970406576991\n",
      "epoch: 84, train_loss: 0.0020363702762710013, test_loss: 0.0022285208300066492\n",
      "epoch: 85, train_loss: 0.002031470244021519, test_loss: 0.0022028600214980543\n",
      "epoch: 86, train_loss: 0.0020160093225295777, test_loss: 0.0022084551358905933\n",
      "epoch: 87, train_loss: 0.0020099587930852304, test_loss: 0.0021898496391562126\n",
      "epoch: 88, train_loss: 0.002000705774306603, test_loss: 0.002165393224762132\n",
      "epoch: 89, train_loss: 0.0019960356521946583, test_loss: 0.0021498671655232706\n",
      "epoch: 90, train_loss: 0.0019885414057289777, test_loss: 0.0021555874263867736\n",
      "epoch: 91, train_loss: 0.0019759618444368243, test_loss: 0.002126211766153574\n",
      "epoch: 92, train_loss: 0.0019707564750443335, test_loss: 0.0021366581204347312\n",
      "epoch: 93, train_loss: 0.0019600306910908093, test_loss: 0.0021252724109217525\n",
      "epoch: 94, train_loss: 0.0019512976185702112, test_loss: 0.0021107576709861555\n",
      "epoch: 95, train_loss: 0.0019435670945550437, test_loss: 0.0020826278681245944\n",
      "epoch: 96, train_loss: 0.001934538519455363, test_loss: 0.002097915819225212\n",
      "epoch: 97, train_loss: 0.001928539997290658, test_loss: 0.002067706761105607\n",
      "epoch: 98, train_loss: 0.0019231195328757167, test_loss: 0.00205912854289636\n",
      "epoch: 99, train_loss: 0.001916884232069487, test_loss: 0.002056988033776482\n",
      "epoch: 100, train_loss: 0.0019086321066741061, test_loss: 0.0020424280276832483\n",
      "epoch: 101, train_loss: 0.001902540335836618, test_loss: 0.002028617813872794\n",
      "epoch: 102, train_loss: 0.0018969505579899187, test_loss: 0.0020299148939860365\n",
      "epoch: 103, train_loss: 0.001891846510419703, test_loss: 0.002000395111584415\n",
      "epoch: 104, train_loss: 0.0018809618296749566, test_loss: 0.001996087589456389\n",
      "epoch: 105, train_loss: 0.0018754918178867388, test_loss: 0.0019952351964699724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 106, train_loss: 0.0018699766217690446, test_loss: 0.001970970925564567\n",
      "epoch: 107, train_loss: 0.0018612342360227005, test_loss: 0.0019720573521529636\n",
      "epoch: 108, train_loss: 0.0018580058164408674, test_loss: 0.0020039990389098725\n",
      "epoch: 109, train_loss: 0.0018508315096487818, test_loss: 0.0019596288717972734\n",
      "epoch: 110, train_loss: 0.001846139921563799, test_loss: 0.0019525010138750076\n",
      "epoch: 111, train_loss: 0.0018366802453427858, test_loss: 0.0019424258692500491\n",
      "epoch: 112, train_loss: 0.0018331787660313041, test_loss: 0.0019400242987709742\n",
      "epoch: 113, train_loss: 0.0018281877071232252, test_loss: 0.001921082866222908\n",
      "epoch: 114, train_loss: 0.0018213437870144844, test_loss: 0.0019406944047659636\n",
      "epoch: 115, train_loss: 0.0018181298130556293, test_loss: 0.0019192040781490505\n",
      "epoch: 116, train_loss: 0.0018090907150231626, test_loss: 0.0019186626692923407\n",
      "epoch: 117, train_loss: 0.0018032818232946422, test_loss: 0.0018876696703955531\n",
      "epoch: 118, train_loss: 0.001802161852221774, test_loss: 0.0019010407850146294\n",
      "epoch: 119, train_loss: 0.0017981902929022908, test_loss: 0.0018912639546518524\n",
      "epoch: 120, train_loss: 0.0017888436053434143, test_loss: 0.0018823907400170963\n",
      "epoch: 121, train_loss: 0.0017860228975262978, test_loss: 0.0018732888080800574\n",
      "epoch: 122, train_loss: 0.0017801092674388833, test_loss: 0.0019075560267083347\n",
      "epoch: 123, train_loss: 0.0017762754679374073, test_loss: 0.0018663270748220384\n",
      "epoch: 124, train_loss: 0.00177162304095438, test_loss: 0.0018610107557227213\n",
      "epoch: 125, train_loss: 0.0017656262802041094, test_loss: 0.0018539569185425837\n",
      "epoch: 126, train_loss: 0.0017603955879483533, test_loss: 0.0018388367413232725\n",
      "epoch: 127, train_loss: 0.0017581662800891893, test_loss: 0.0018349382056233783\n",
      "epoch: 128, train_loss: 0.0017508064427048616, test_loss: 0.0018386405621034403\n",
      "epoch: 129, train_loss: 0.0017449710639598577, test_loss: 0.0018257304715613525\n",
      "epoch: 130, train_loss: 0.001742214855292569, test_loss: 0.001816377373567472\n",
      "epoch: 131, train_loss: 0.0017357331994192107, test_loss: 0.0018185042038870354\n",
      "epoch: 132, train_loss: 0.0017338403723324122, test_loss: 0.0018120072588014107\n",
      "epoch: 133, train_loss: 0.0017276973028302841, test_loss: 0.001799795621385177\n",
      "epoch: 134, train_loss: 0.0017239742092383296, test_loss: 0.0017886100298104186\n",
      "epoch: 135, train_loss: 0.0017192415265447419, test_loss: 0.001794799657848974\n",
      "epoch: 136, train_loss: 0.0017133394114511168, test_loss: 0.0017978916293941438\n",
      "epoch: 137, train_loss: 0.0017082446973527904, test_loss: 0.001790039687572668\n",
      "epoch: 138, train_loss: 0.0017065063989518778, test_loss: 0.00177969119977206\n",
      "epoch: 139, train_loss: 0.0017027317637415683, test_loss: 0.001775649783667177\n",
      "epoch: 140, train_loss: 0.0016988956005029056, test_loss: 0.0017604318951877456\n",
      "epoch: 141, train_loss: 0.0016953776813233676, test_loss: 0.0017599494312889874\n",
      "epoch: 142, train_loss: 0.0016912857333765082, test_loss: 0.0017454876409222682\n",
      "epoch: 143, train_loss: 0.0016855859329276111, test_loss: 0.0017541239115719993\n",
      "epoch: 144, train_loss: 0.0016816223708345838, test_loss: 0.0017296809043424826\n",
      "epoch: 145, train_loss: 0.001677894043614683, test_loss: 0.0017423543225352962\n",
      "epoch: 146, train_loss: 0.0016749630499955106, test_loss: 0.0017319968901574612\n",
      "epoch: 147, train_loss: 0.001669884553057668, test_loss: 0.0017336771318999429\n",
      "epoch: 148, train_loss: 0.001666850582494036, test_loss: 0.0017194006553230186\n",
      "epoch: 149, train_loss: 0.0016619348797299292, test_loss: 0.001720305144165953\n",
      "epoch: 150, train_loss: 0.0016586486928407912, test_loss: 0.0017156956406931083\n",
      "epoch: 151, train_loss: 0.0016541054229373517, test_loss: 0.0017071764644545813\n",
      "epoch: 152, train_loss: 0.001651629587918844, test_loss: 0.0017214458785019815\n",
      "epoch: 153, train_loss: 0.0016474241552793462, test_loss: 0.0017017494731893141\n",
      "epoch: 154, train_loss: 0.0016424330103251598, test_loss: 0.0017020642020118733\n",
      "epoch: 155, train_loss: 0.0016396139728148346, test_loss: 0.0016875226089420419\n",
      "epoch: 156, train_loss: 0.001636619502476052, test_loss: 0.0016875186896262069\n",
      "epoch: 157, train_loss: 0.0016342108998366673, test_loss: 0.0016732589186479647\n",
      "epoch: 158, train_loss: 0.001631717516473778, test_loss: 0.0016822752659209073\n",
      "epoch: 159, train_loss: 0.0016242037628493879, test_loss: 0.0016694636627410848\n",
      "epoch: 160, train_loss: 0.0016226730734595785, test_loss: 0.0016823015563810866\n",
      "epoch: 161, train_loss: 0.0016177359755839343, test_loss: 0.0016621060591811936\n",
      "epoch: 162, train_loss: 0.0016154150134357421, test_loss: 0.0016569983563385904\n",
      "epoch: 163, train_loss: 0.0016134549337236779, test_loss: 0.001672171560736994\n",
      "epoch: 164, train_loss: 0.0016080601110487528, test_loss: 0.0016446684797604878\n",
      "epoch: 165, train_loss: 0.0016042535173018341, test_loss: 0.0016442241030745208\n",
      "epoch: 166, train_loss: 0.0016034404575095875, test_loss: 0.0016745940859739978\n",
      "epoch: 167, train_loss: 0.0015996400185901186, test_loss: 0.0016436912798477958\n",
      "epoch: 168, train_loss: 0.0015959902059124863, test_loss: 0.0016349734311612945\n",
      "epoch: 169, train_loss: 0.0015911476980165942, test_loss: 0.001622804954725628\n",
      "epoch: 170, train_loss: 0.0015899145510047674, test_loss: 0.0016461611570169528\n",
      "epoch: 171, train_loss: 0.001586420324873989, test_loss: 0.001636844497018804\n",
      "epoch: 172, train_loss: 0.0015808636545325103, test_loss: 0.0016172499163076282\n",
      "epoch: 173, train_loss: 0.0015780209087888184, test_loss: 0.001621987185596178\n",
      "epoch: 174, train_loss: 0.0015752049385691466, test_loss: 0.0016310165422813345\n",
      "epoch: 175, train_loss: 0.0015734915581086407, test_loss: 0.0016089842732374866\n",
      "epoch: 176, train_loss: 0.001571093969371008, test_loss: 0.0016061412073516597\n",
      "epoch: 177, train_loss: 0.001567344616531678, test_loss: 0.0016144920761386554\n",
      "epoch: 178, train_loss: 0.0015652414320198739, test_loss: 0.0015984103083610535\n",
      "epoch: 179, train_loss: 0.0015616185922661553, test_loss: 0.0015982940191558253\n",
      "epoch: 180, train_loss: 0.0015577508247983844, test_loss: 0.0015870671001418184\n",
      "epoch: 181, train_loss: 0.0015543394610690682, test_loss: 0.0015855019446462393\n",
      "epoch: 182, train_loss: 0.001553909073624274, test_loss: 0.0015860263083595783\n",
      "epoch: 183, train_loss: 0.0015481317304479687, test_loss: 0.0015921602219653626\n",
      "epoch: 184, train_loss: 0.0015476473009861681, test_loss: 0.001580031025999536\n",
      "epoch: 185, train_loss: 0.001542572660938553, test_loss: 0.0015917651180643588\n",
      "epoch: 186, train_loss: 0.0015418164675002513, test_loss: 0.0015680164894244324\n",
      "epoch: 187, train_loss: 0.001535021912549501, test_loss: 0.0015611534666580458\n",
      "epoch: 188, train_loss: 0.0015329381871118169, test_loss: 0.0015827015255733083\n",
      "epoch: 189, train_loss: 0.001533491835605515, test_loss: 0.0015656556352041662\n",
      "epoch: 190, train_loss: 0.0015285119171375814, test_loss: 0.001561914502720659\n",
      "epoch: 191, train_loss: 0.0015257352750505443, test_loss: 0.0015497052615197997\n",
      "epoch: 192, train_loss: 0.0015239381818505733, test_loss: 0.0015542889887001365\n",
      "epoch: 193, train_loss: 0.0015211929136927686, test_loss: 0.0015499996467648696\n",
      "epoch: 194, train_loss: 0.0015170876260684884, test_loss: 0.001550849595029528\n",
      "epoch: 195, train_loss: 0.0015147458002700107, test_loss: 0.0015375515407261748\n",
      "epoch: 196, train_loss: 0.0015103599340047526, test_loss: 0.0015415219628872971\n",
      "epoch: 197, train_loss: 0.0015069168648156135, test_loss: 0.0015352756114831816\n",
      "epoch: 198, train_loss: 0.0015057083877289424, test_loss: 0.0015263463428709656\n",
      "epoch: 199, train_loss: 0.0015029829380142949, test_loss: 0.001526186194193239\n",
      "epoch: 200, train_loss: 0.001500714150176424, test_loss: 0.001531160397765537\n",
      "epoch: 201, train_loss: 0.0014999969609324698, test_loss: 0.0015193739964161068\n",
      "epoch: 202, train_loss: 0.0014961516464372044, test_loss: 0.0015081234935981531\n",
      "epoch: 203, train_loss: 0.0014935850682061005, test_loss: 0.0015194931474979967\n",
      "epoch: 204, train_loss: 0.001490113396278542, test_loss: 0.0015154332213569432\n",
      "epoch: 205, train_loss: 0.0014878163043328602, test_loss: 0.0015197322354651988\n",
      "epoch: 206, train_loss: 0.0014849470361419346, test_loss: 0.0015085231376967083\n",
      "epoch: 207, train_loss: 0.0014812819781186788, test_loss: 0.001504619528229038\n",
      "epoch: 208, train_loss: 0.001480570665317709, test_loss: 0.0015007247081181656\n",
      "epoch: 209, train_loss: 0.0014795149630947929, test_loss: 0.0015007720212452114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 210, train_loss: 0.0014743207467962866, test_loss: 0.0015014134114608169\n",
      "epoch: 211, train_loss: 0.0014713693754342587, test_loss: 0.001490829299048831\n",
      "epoch: 212, train_loss: 0.0014704375062137842, test_loss: 0.0014918277447577566\n",
      "epoch: 213, train_loss: 0.0014675130369141698, test_loss: 0.0014922593157583226\n",
      "epoch: 214, train_loss: 0.001466051963886813, test_loss: 0.0014777527151939769\n",
      "epoch: 215, train_loss: 0.0014619271868191984, test_loss: 0.0014725600194651634\n",
      "epoch: 216, train_loss: 0.0014606473522017832, test_loss: 0.001466964711047088\n",
      "epoch: 217, train_loss: 0.001460150392908279, test_loss: 0.0014790022978559136\n",
      "epoch: 218, train_loss: 0.0014563182966135766, test_loss: 0.0014728151727467775\n",
      "epoch: 219, train_loss: 0.0014542776423142009, test_loss: 0.001465167433101063\n",
      "epoch: 220, train_loss: 0.0014508099538152633, test_loss: 0.0014702469925396144\n",
      "epoch: 221, train_loss: 0.001448249039442643, test_loss: 0.001474979236566772\n",
      "epoch: 222, train_loss: 0.0014472349759434228, test_loss: 0.0014831720327492803\n",
      "epoch: 223, train_loss: 0.0014446944671521044, test_loss: 0.0014556866857067992\n",
      "epoch: 224, train_loss: 0.0014411720052442472, test_loss: 0.001458115322748199\n",
      "epoch: 225, train_loss: 0.0014397011609221606, test_loss: 0.0014593633241020143\n",
      "epoch: 226, train_loss: 0.0014373949607429297, test_loss: 0.0014517503635336955\n",
      "epoch: 227, train_loss: 0.0014348009262112496, test_loss: 0.0014564144366886467\n",
      "epoch: 228, train_loss: 0.0014333144974206452, test_loss: 0.0014385026782595862\n",
      "epoch: 229, train_loss: 0.00142995516627865, test_loss: 0.001444110074468578\n",
      "epoch: 230, train_loss: 0.001428851862842946, test_loss: 0.0014374438421024631\n",
      "epoch: 231, train_loss: 0.0014258411500360007, test_loss: 0.001439921169852217\n",
      "epoch: 232, train_loss: 0.0014233033542278345, test_loss: 0.0014329968447176118\n",
      "epoch: 233, train_loss: 0.0014207513709350126, test_loss: 0.0014262542439003785\n",
      "epoch: 234, train_loss: 0.0014185608395010881, test_loss: 0.0014265057106968015\n",
      "epoch: 235, train_loss: 0.0014162485831943543, test_loss: 0.0014307770179584622\n",
      "epoch: 236, train_loss: 0.0014138552056544502, test_loss: 0.0014244704313265781\n",
      "epoch: 237, train_loss: 0.0014121850203398776, test_loss: 0.001426070889768501\n",
      "epoch: 238, train_loss: 0.0014092488740773304, test_loss: 0.001412805247431\n",
      "epoch: 239, train_loss: 0.001408487578611011, test_loss: 0.0014101318762792896\n",
      "epoch: 240, train_loss: 0.0014061644754332044, test_loss: 0.0014039322074192266\n",
      "epoch: 241, train_loss: 0.001403478513289567, test_loss: 0.0014348793386792142\n",
      "epoch: 242, train_loss: 0.001404272081107711, test_loss: 0.0014157290182386835\n",
      "epoch: 243, train_loss: 0.0014005871970489945, test_loss: 0.0014146016328595579\n",
      "epoch: 244, train_loss: 0.0013979854335522523, test_loss: 0.0013945560882954549\n",
      "epoch: 245, train_loss: 0.0013953309111378114, test_loss: 0.0014004508460250993\n",
      "epoch: 246, train_loss: 0.0013946558085634656, test_loss: 0.0013959169349012275\n",
      "epoch: 247, train_loss: 0.0013916417931044555, test_loss: 0.0013923223596066236\n",
      "epoch: 248, train_loss: 0.00138995001780922, test_loss: 0.0013970097061246634\n",
      "epoch: 249, train_loss: 0.0013870190115362082, test_loss: 0.0013847159959065418\n",
      "epoch: 250, train_loss: 0.0013852941591049666, test_loss: 0.001384900911943987\n",
      "epoch: 251, train_loss: 0.0013847285476715667, test_loss: 0.0013925273087807\n",
      "epoch: 252, train_loss: 0.0013831157348645122, test_loss: 0.001388153264997527\n",
      "epoch: 253, train_loss: 0.0013806204686103308, test_loss: 0.0013746477100842942\n",
      "epoch: 254, train_loss: 0.0013768501964438221, test_loss: 0.0013839614985045046\n",
      "epoch: 255, train_loss: 0.0013774604101300888, test_loss: 0.0013837031729053706\n",
      "epoch: 256, train_loss: 0.001372511098768724, test_loss: 0.0013820733098934095\n",
      "epoch: 257, train_loss: 0.001372581686946037, test_loss: 0.0013765320569897692\n",
      "epoch: 258, train_loss: 0.0013697899235210018, test_loss: 0.001367877838977923\n",
      "epoch: 259, train_loss: 0.0013672598997009513, test_loss: 0.0013605239995134373\n",
      "epoch: 260, train_loss: 0.0013670595109705691, test_loss: 0.0013628418188697349\n",
      "epoch: 261, train_loss: 0.0013643862254431715, test_loss: 0.0013640567291683208\n",
      "epoch: 262, train_loss: 0.0013635561766062417, test_loss: 0.0013638381885054212\n",
      "epoch: 263, train_loss: 0.0013597018196773918, test_loss: 0.0013644172674200188\n",
      "epoch: 264, train_loss: 0.0013568957194523966, test_loss: 0.0013626083285392572\n",
      "epoch: 265, train_loss: 0.001358005240984747, test_loss: 0.0013851479743607342\n",
      "epoch: 266, train_loss: 0.0013568825518671908, test_loss: 0.0013539108622353524\n",
      "epoch: 267, train_loss: 0.001353071400207346, test_loss: 0.001355635739552478\n",
      "epoch: 268, train_loss: 0.0013530974848317387, test_loss: 0.001342889746107782\n",
      "epoch: 269, train_loss: 0.001349394877805658, test_loss: 0.0013428192566304158\n",
      "epoch: 270, train_loss: 0.0013484681257978082, test_loss: 0.0013481430457128833\n",
      "epoch: 271, train_loss: 0.001346198950752454, test_loss: 0.0013417891265513997\n",
      "epoch: 272, train_loss: 0.0013436845949162607, test_loss: 0.0013423555647023022\n",
      "epoch: 273, train_loss: 0.001343387454930369, test_loss: 0.0013313307814920943\n",
      "epoch: 274, train_loss: 0.001339854900320263, test_loss: 0.0013761009807543207\n",
      "epoch: 275, train_loss: 0.0013398942309836655, test_loss: 0.0013373785574610035\n",
      "epoch: 276, train_loss: 0.001338251165377543, test_loss: 0.0013287640370739002\n",
      "epoch: 277, train_loss: 0.001335220740419691, test_loss: 0.0013318423589225858\n",
      "epoch: 278, train_loss: 0.0013333420577944944, test_loss: 0.001342867666001742\n",
      "epoch: 279, train_loss: 0.0013328870510399017, test_loss: 0.0013168188161216676\n",
      "epoch: 280, train_loss: 0.0013293225404239543, test_loss: 0.001324719477755328\n",
      "epoch: 281, train_loss: 0.0013282924116103222, test_loss: 0.0013229000323917717\n",
      "epoch: 282, train_loss: 0.001326235451573587, test_loss: 0.001337741588940844\n",
      "epoch: 283, train_loss: 0.0013246880591158633, test_loss: 0.0013144718056234221\n",
      "epoch: 284, train_loss: 0.0013229501099609163, test_loss: 0.0013241766622134794\n",
      "epoch: 285, train_loss: 0.0013229635179690692, test_loss: 0.0013106795474110793\n",
      "epoch: 286, train_loss: 0.0013192033916509347, test_loss: 0.0013375892497909565\n",
      "epoch: 287, train_loss: 0.001318147074451427, test_loss: 0.001336452968340988\n",
      "epoch: 288, train_loss: 0.0013183047665971453, test_loss: 0.0013041861820966005\n",
      "epoch: 289, train_loss: 0.0013156385419120932, test_loss: 0.0013079374524143834\n",
      "epoch: 290, train_loss: 0.0013135183784786773, test_loss: 0.001299134853373592\n",
      "epoch: 291, train_loss: 0.0013114549295531342, test_loss: 0.0013057177420705557\n",
      "epoch: 292, train_loss: 0.0013104776887263617, test_loss: 0.0012947852762105565\n",
      "epoch: 293, train_loss: 0.0013092085617877867, test_loss: 0.0013048359639166545\n",
      "epoch: 294, train_loss: 0.0013088424786236947, test_loss: 0.0013133028890782346\n",
      "epoch: 295, train_loss: 0.0013062183709774652, test_loss: 0.001296753587666899\n",
      "epoch: 296, train_loss: 0.0013030151325358968, test_loss: 0.0012921894861695666\n",
      "epoch: 297, train_loss: 0.0013032112164301393, test_loss: 0.001295827649300918\n",
      "epoch: 298, train_loss: 0.0013012578609682944, test_loss: 0.0012915388312345992\n",
      "epoch: 299, train_loss: 0.001301907564756339, test_loss: 0.0012933269026689231\n",
      "epoch: 300, train_loss: 0.0012992562058017306, test_loss: 0.0012836484529543668\n",
      "epoch: 301, train_loss: 0.0012961108232443423, test_loss: 0.0012865158496424556\n",
      "epoch: 302, train_loss: 0.0012933597039512317, test_loss: 0.001283888394633929\n",
      "epoch: 303, train_loss: 0.0012944310817503087, test_loss: 0.0012877444387413561\n",
      "epoch: 304, train_loss: 0.00129238271113971, test_loss: 0.0012892648325456928\n",
      "epoch: 305, train_loss: 0.0012909387562261975, test_loss: 0.0012758755668376882\n",
      "epoch: 306, train_loss: 0.0012873204946315484, test_loss: 0.0012682493737277885\n",
      "epoch: 307, train_loss: 0.0012884743881168897, test_loss: 0.0012890713308782626\n",
      "epoch: 308, train_loss: 0.001286666048184523, test_loss: 0.0012810280119689803\n",
      "epoch: 309, train_loss: 0.0012825631597281797, test_loss: 0.0012645090658528109\n",
      "epoch: 310, train_loss: 0.0012809678016270957, test_loss: 0.001274572025674085\n",
      "epoch: 311, train_loss: 0.0012795593306098294, test_loss: 0.0012705010012723505\n",
      "epoch: 312, train_loss: 0.001279236629868493, test_loss: 0.0012586451193783432\n",
      "epoch: 313, train_loss: 0.0012785860251270883, test_loss: 0.001274579835201924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 314, train_loss: 0.0012772964571764612, test_loss: 0.0012662122899200767\n",
      "epoch: 315, train_loss: 0.001277238980907461, test_loss: 0.0012632554959661018\n",
      "epoch: 316, train_loss: 0.0012741392458100681, test_loss: 0.0012565497405982267\n",
      "epoch: 317, train_loss: 0.001272014618102137, test_loss: 0.001250633446034044\n",
      "epoch: 318, train_loss: 0.0012708308919251938, test_loss: 0.0012517021386884153\n",
      "epoch: 319, train_loss: 0.0012692324969026706, test_loss: 0.001250490895472467\n",
      "epoch: 320, train_loss: 0.0012665619876276216, test_loss: 0.0012467822040586423\n",
      "epoch: 321, train_loss: 0.0012689086320081158, test_loss: 0.0012574023373114567\n",
      "epoch: 322, train_loss: 0.0012639886978001375, test_loss: 0.0012566608493216336\n",
      "epoch: 323, train_loss: 0.0012644578034625106, test_loss: 0.001253624747429664\n",
      "epoch: 324, train_loss: 0.0012619931012918444, test_loss: 0.0012462917754116158\n",
      "epoch: 325, train_loss: 0.0012603597361189516, test_loss: 0.0012353502679616213\n",
      "epoch: 326, train_loss: 0.0012596420154137456, test_loss: 0.0012332301994320005\n",
      "epoch: 327, train_loss: 0.0012582006595988312, test_loss: 0.0012353078248755385\n",
      "epoch: 328, train_loss: 0.0012576009561915112, test_loss: 0.001233975092569987\n",
      "epoch: 329, train_loss: 0.0012567882911990519, test_loss: 0.0012264133741458256\n",
      "epoch: 330, train_loss: 0.0012559515763195636, test_loss: 0.001241073526519661\n",
      "epoch: 331, train_loss: 0.001254149479791522, test_loss: 0.001229429297382012\n",
      "epoch: 332, train_loss: 0.0012513893484340413, test_loss: 0.001242901082150638\n",
      "epoch: 333, train_loss: 0.0012504065500410354, test_loss: 0.0012270285030050825\n",
      "epoch: 334, train_loss: 0.0012496559521304848, test_loss: 0.0012230929957392316\n",
      "epoch: 335, train_loss: 0.0012482041290596776, test_loss: 0.001231852191267535\n",
      "epoch: 336, train_loss: 0.0012456728499787657, test_loss: 0.0012213371131413926\n",
      "epoch: 337, train_loss: 0.0012444578662879117, test_loss: 0.0012192095746286213\n",
      "epoch: 338, train_loss: 0.0012430229234387693, test_loss: 0.001229160251871993\n",
      "epoch: 339, train_loss: 0.001243327136920846, test_loss: 0.0012212796912839015\n",
      "epoch: 340, train_loss: 0.0012420311737198222, test_loss: 0.0012126991641707718\n",
      "epoch: 341, train_loss: 0.0012374971638960035, test_loss: 0.0012135904398746789\n",
      "epoch: 342, train_loss: 0.0012402614507743199, test_loss: 0.0012271973149230082\n",
      "epoch: 343, train_loss: 0.0012371714135793889, test_loss: 0.0012133117803993325\n",
      "epoch: 344, train_loss: 0.001236354605719933, test_loss: 0.0012195469656338294\n",
      "epoch: 345, train_loss: 0.0012351565933583872, test_loss: 0.001206392499928673\n",
      "epoch: 346, train_loss: 0.001233591857548479, test_loss: 0.0012162006848181288\n",
      "epoch: 347, train_loss: 0.0012317758974740686, test_loss: 0.0012153718465318282\n",
      "epoch: 348, train_loss: 0.0012290670091043348, test_loss: 0.001197582809254527\n",
      "epoch: 349, train_loss: 0.001229264942989887, test_loss: 0.001203224897229423\n",
      "epoch: 350, train_loss: 0.001228442863540967, test_loss: 0.0012227789654086034\n",
      "epoch: 351, train_loss: 0.00122536281781757, test_loss: 0.0012091142416466027\n",
      "epoch: 352, train_loss: 0.001226777610692965, test_loss: 0.0012014922588908423\n",
      "epoch: 353, train_loss: 0.001221908491772964, test_loss: 0.0012002546767083306\n",
      "epoch: 354, train_loss: 0.0012220617044595597, test_loss: 0.0012023913635251422\n",
      "epoch: 355, train_loss: 0.0012214666956266308, test_loss: 0.0011995279928669333\n",
      "epoch: 356, train_loss: 0.001219600163217958, test_loss: 0.0012153356122629095\n",
      "epoch: 357, train_loss: 0.0012195070537374072, test_loss: 0.0011900743217362713\n",
      "epoch: 358, train_loss: 0.0012192882916323192, test_loss: 0.0011890190265451868\n",
      "epoch: 359, train_loss: 0.0012176047033711295, test_loss: 0.001191832930392896\n",
      "epoch: 360, train_loss: 0.001215837911054816, test_loss: 0.0011898061202373356\n",
      "epoch: 361, train_loss: 0.0012151987048918786, test_loss: 0.0011888928420376033\n",
      "epoch: 362, train_loss: 0.001213042176854999, test_loss: 0.001192449068184942\n",
      "epoch: 363, train_loss: 0.0012110733659937978, test_loss: 0.0011853439985619236\n",
      "epoch: 364, train_loss: 0.0012106595803862033, test_loss: 0.0011877278060031433\n",
      "epoch: 365, train_loss: 0.0012081337677638817, test_loss: 0.0011850363322688888\n",
      "epoch: 366, train_loss: 0.001208097231068203, test_loss: 0.0011954036114426951\n",
      "epoch: 367, train_loss: 0.0012086191108328817, test_loss: 0.0011862092457401256\n",
      "epoch: 368, train_loss: 0.0012056988729771388, test_loss: 0.001191507598074774\n",
      "epoch: 369, train_loss: 0.0012045603336604393, test_loss: 0.0011795159371104091\n",
      "epoch: 370, train_loss: 0.0012050164438298216, test_loss: 0.0011772739429337282\n",
      "epoch: 371, train_loss: 0.0012030646017671604, test_loss: 0.0011737160384654999\n",
      "epoch: 372, train_loss: 0.0012025126640725396, test_loss: 0.0011810575379058719\n",
      "epoch: 373, train_loss: 0.0011997326054488835, test_loss: 0.0011655109992716461\n",
      "epoch: 374, train_loss: 0.0011987089452779164, test_loss: 0.0011884061580834289\n",
      "epoch: 375, train_loss: 0.0012003556754359085, test_loss: 0.0011649081910339494\n",
      "epoch: 376, train_loss: 0.0011954927366510358, test_loss: 0.001185203727800399\n",
      "epoch: 377, train_loss: 0.0011947200982831419, test_loss: 0.0011736309873716284\n",
      "epoch: 378, train_loss: 0.0011948784231208265, test_loss: 0.0011607480410020798\n",
      "epoch: 379, train_loss: 0.001195144885138649, test_loss: 0.001173654804006219\n",
      "epoch: 380, train_loss: 0.0011919281425197487, test_loss: 0.0011599764984566718\n",
      "epoch: 381, train_loss: 0.001190653711091727, test_loss: 0.0011609110321539144\n",
      "epoch: 382, train_loss: 0.0011902011743665714, test_loss: 0.001168770967827489\n",
      "epoch: 383, train_loss: 0.0011891508070023162, test_loss: 0.0011583685603303213\n",
      "epoch: 384, train_loss: 0.001187382802448195, test_loss: 0.0011655739702594776\n",
      "epoch: 385, train_loss: 0.0011870650797272506, test_loss: 0.0011518284251602988\n",
      "epoch: 386, train_loss: 0.0011852322748619254, test_loss: 0.0011482520979673911\n",
      "epoch: 387, train_loss: 0.001183315947064725, test_loss: 0.0011605274921748787\n",
      "epoch: 388, train_loss: 0.0011832512201457891, test_loss: 0.0011470949975773692\n",
      "epoch: 389, train_loss: 0.0011820240685230365, test_loss: 0.001147953337446476\n",
      "epoch: 390, train_loss: 0.0011820294160355368, test_loss: 0.0011448953688765566\n",
      "epoch: 391, train_loss: 0.0011804768279113848, test_loss: 0.0011451807416354616\n",
      "epoch: 392, train_loss: 0.0011787210874583411, test_loss: 0.001137203701849406\n",
      "epoch: 393, train_loss: 0.0011779851249783583, test_loss: 0.0011413423053454608\n",
      "epoch: 394, train_loss: 0.00117886228405911, test_loss: 0.0011551877056869368\n",
      "epoch: 395, train_loss: 0.0011763261281885207, test_loss: 0.0011380253223857533\n",
      "epoch: 396, train_loss: 0.001174871480780775, test_loss: 0.0011372758211412777\n",
      "epoch: 397, train_loss: 0.0011743653803537397, test_loss: 0.0011383483263974388\n",
      "epoch: 398, train_loss: 0.0011725974264149756, test_loss: 0.0011476170523868252\n",
      "epoch: 399, train_loss: 0.0011699977001863654, test_loss: 0.0011403112342425932\n",
      "epoch: 400, train_loss: 0.0011710775813654714, test_loss: 0.001147444195036466\n",
      "epoch: 401, train_loss: 0.0011707876236987827, test_loss: 0.0011441652022767812\n",
      "epoch: 402, train_loss: 0.0011681427030176249, test_loss: 0.0011343893614442397\n",
      "epoch: 403, train_loss: 0.0011682570937251592, test_loss: 0.0011413432269667585\n",
      "epoch: 404, train_loss: 0.0011656708995123272, test_loss: 0.0011298906368513901\n",
      "epoch: 405, train_loss: 0.0011661254507287042, test_loss: 0.0011314005338742088\n",
      "epoch: 406, train_loss: 0.0011627762401274042, test_loss: 0.0011229792629213382\n",
      "epoch: 407, train_loss: 0.001163381888308441, test_loss: 0.0011272991832811385\n",
      "epoch: 408, train_loss: 0.0011618305680989895, test_loss: 0.0011351389402989298\n",
      "epoch: 409, train_loss: 0.0011615911523973489, test_loss: 0.0011233661110357691\n",
      "epoch: 410, train_loss: 0.0011599459331077726, test_loss: 0.0011290457235494007\n",
      "epoch: 411, train_loss: 0.0011588012819866772, test_loss: 0.0011229246738366783\n",
      "epoch: 412, train_loss: 0.0011584737373556457, test_loss: 0.0011252390298371513\n",
      "epoch: 413, train_loss: 0.0011576054789854782, test_loss: 0.0011309644226760913\n",
      "epoch: 414, train_loss: 0.0011572770131549435, test_loss: 0.001124052583084752\n",
      "epoch: 415, train_loss: 0.0011543448528517847, test_loss: 0.0011354485662498821\n",
      "epoch: 416, train_loss: 0.0011540982776078517, test_loss: 0.001115755798916022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 417, train_loss: 0.0011526696819245167, test_loss: 0.0011161762425520767\n",
      "epoch: 418, train_loss: 0.0011522754363756142, test_loss: 0.0011201004866355409\n",
      "epoch: 419, train_loss: 0.0011514750278919287, test_loss: 0.001123049665087213\n",
      "epoch: 420, train_loss: 0.0011494047737315946, test_loss: 0.0011114662629552186\n",
      "epoch: 421, train_loss: 0.001148651758937732, test_loss: 0.0011208129581063986\n",
      "epoch: 422, train_loss: 0.001148820561154381, test_loss: 0.0011128002952318639\n",
      "epoch: 423, train_loss: 0.0011480519340535545, test_loss: 0.001109523562869678\n",
      "epoch: 424, train_loss: 0.001146485343190801, test_loss: 0.0011192875584432234\n",
      "epoch: 425, train_loss: 0.0011457382478629765, test_loss: 0.0011139411168793838\n",
      "epoch: 426, train_loss: 0.0011452540487010517, test_loss: 0.0011041806816744308\n",
      "epoch: 427, train_loss: 0.0011433903988369782, test_loss: 0.0011112468879825126\n",
      "epoch: 428, train_loss: 0.0011433173558148353, test_loss: 0.0011010533392739792\n",
      "epoch: 429, train_loss: 0.0011422612739767394, test_loss: 0.0011032999027520418\n",
      "epoch: 430, train_loss: 0.0011391920498167367, test_loss: 0.0011057003090778987\n",
      "epoch: 431, train_loss: 0.0011417823261581361, test_loss: 0.0011028373943797003\n",
      "epoch: 432, train_loss: 0.0011383015940816183, test_loss: 0.0010974663018714637\n",
      "epoch: 433, train_loss: 0.0011368850119533422, test_loss: 0.001110316040770461\n",
      "epoch: 434, train_loss: 0.001134832991975481, test_loss: 0.001101181871490553\n",
      "epoch: 435, train_loss: 0.0011354201174665081, test_loss: 0.0011001284195420642\n",
      "epoch: 436, train_loss: 0.001134334372498257, test_loss: 0.001088455591040353\n",
      "epoch: 437, train_loss: 0.0011332342451762247, test_loss: 0.001094343945927297\n",
      "epoch: 438, train_loss: 0.0011327446605402815, test_loss: 0.0010941623477265239\n",
      "epoch: 439, train_loss: 0.0011324223419448929, test_loss: 0.0010912936316647877\n",
      "epoch: 440, train_loss: 0.0011305299526809351, test_loss: 0.0010882661445066333\n",
      "epoch: 441, train_loss: 0.0011316352857924674, test_loss: 0.0010939664401424427\n",
      "epoch: 442, train_loss: 0.0011298058443176358, test_loss: 0.0010964484924140077\n",
      "epoch: 443, train_loss: 0.0011284377643555079, test_loss: 0.0010938908574947466\n",
      "epoch: 444, train_loss: 0.0011271284007386346, test_loss: 0.0010864377157607426\n",
      "epoch: 445, train_loss: 0.001127894013694933, test_loss: 0.0010866931697819382\n",
      "epoch: 446, train_loss: 0.0011256750617378755, test_loss: 0.0010863493856353064\n",
      "epoch: 447, train_loss: 0.001127015164061247, test_loss: 0.001085455296561122\n",
      "epoch: 448, train_loss: 0.0011237400796507363, test_loss: 0.001081012247595936\n",
      "epoch: 449, train_loss: 0.001125045062508434, test_loss: 0.0010799001223252465\n",
      "epoch: 450, train_loss: 0.001124094253021252, test_loss: 0.0010821514394289504\n",
      "epoch: 451, train_loss: 0.0011222607395409243, test_loss: 0.001081223813040803\n",
      "epoch: 452, train_loss: 0.0011190510552336016, test_loss: 0.0010782446479424834\n",
      "epoch: 453, train_loss: 0.0011210661740852115, test_loss: 0.0010705702491880704\n",
      "epoch: 454, train_loss: 0.0011187446222681067, test_loss: 0.00108826572735173\n",
      "epoch: 455, train_loss: 0.001118857727345565, test_loss: 0.001085452609307443\n",
      "epoch: 456, train_loss: 0.0011154359419141774, test_loss: 0.0010757121975378443\n",
      "epoch: 457, train_loss: 0.0011169162740849931, test_loss: 0.0010813388604825984\n",
      "epoch: 458, train_loss: 0.001114077979961977, test_loss: 0.0010719012255625178\n",
      "epoch: 459, train_loss: 0.0011135093316071384, test_loss: 0.0010638022019217412\n",
      "epoch: 460, train_loss: 0.0011162619313994503, test_loss: 0.001077944247905786\n",
      "epoch: 461, train_loss: 0.001111242120437648, test_loss: 0.0010641954528788726\n",
      "epoch: 462, train_loss: 0.0011112699538226361, test_loss: 0.0010684900626074523\n",
      "epoch: 463, train_loss: 0.0011110431970218601, test_loss: 0.0010611865533671032\n",
      "epoch: 464, train_loss: 0.0011112158282902901, test_loss: 0.0010624523759664346\n",
      "epoch: 465, train_loss: 0.0011082526230577218, test_loss: 0.0010754678611798834\n",
      "epoch: 466, train_loss: 0.001109282617740657, test_loss: 0.001069731767832612\n",
      "epoch: 467, train_loss: 0.0011069766807612841, test_loss: 0.0010584980870286624\n",
      "epoch: 468, train_loss: 0.0011073033370927949, test_loss: 0.0010656043887138367\n",
      "epoch: 469, train_loss: 0.0011052011041497083, test_loss: 0.0010748457474013169\n",
      "epoch: 470, train_loss: 0.0011066898587159812, test_loss: 0.0010595351050142199\n",
      "epoch: 471, train_loss: 0.0011032316808426833, test_loss: 0.0010702795213243614\n",
      "epoch: 472, train_loss: 0.0011032438993899395, test_loss: 0.0010641042802793284\n",
      "epoch: 473, train_loss: 0.001103499139983045, test_loss: 0.0010592209388657163\n",
      "epoch: 474, train_loss: 0.001102800015360117, test_loss: 0.0010644392653678854\n",
      "epoch: 475, train_loss: 0.0011011223187265189, test_loss: 0.0010550740601805348\n",
      "epoch: 476, train_loss: 0.0011015666253945749, test_loss: 0.001061323913745582\n",
      "epoch: 477, train_loss: 0.0011017860708069866, test_loss: 0.0010607610553658258\n",
      "epoch: 478, train_loss: 0.0010987813306122046, test_loss: 0.0010615355082942794\n",
      "epoch: 479, train_loss: 0.0010982635532222364, test_loss: 0.0010607431759126484\n",
      "epoch: 480, train_loss: 0.0010973019398870351, test_loss: 0.0010559089423622936\n",
      "epoch: 481, train_loss: 0.0010968639348309648, test_loss: 0.0010560586233623326\n",
      "epoch: 482, train_loss: 0.0010969045814936576, test_loss: 0.0010607100654548656\n",
      "epoch: 483, train_loss: 0.0010946480729414718, test_loss: 0.001059397104351471\n",
      "epoch: 484, train_loss: 0.0010933485565423641, test_loss: 0.0010546230090161164\n",
      "epoch: 485, train_loss: 0.0010928393103708715, test_loss: 0.001058319185782845\n",
      "epoch: 486, train_loss: 0.0010931587413601253, test_loss: 0.0010427021770738065\n",
      "epoch: 487, train_loss: 0.0010909479789678817, test_loss: 0.0010531478037592024\n",
      "epoch: 488, train_loss: 0.0010913110631720526, test_loss: 0.0010423422888076554\n",
      "epoch: 489, train_loss: 0.0010899970758422885, test_loss: 0.0010405316425021738\n",
      "epoch: 490, train_loss: 0.001088440382812658, test_loss: 0.0010408362043866266\n",
      "epoch: 491, train_loss: 0.0010881423190964954, test_loss: 0.0010444312744463484\n",
      "epoch: 492, train_loss: 0.0010873014259192607, test_loss: 0.001042481269299363\n",
      "epoch: 493, train_loss: 0.0010868062153863518, test_loss: 0.001046100592551132\n",
      "epoch: 494, train_loss: 0.001084429257468361, test_loss: 0.0010514771662807714\n",
      "epoch: 495, train_loss: 0.0010854830236538596, test_loss: 0.0010364277695771307\n",
      "epoch: 496, train_loss: 0.00108552360198582, test_loss: 0.0010479201446287334\n",
      "epoch: 497, train_loss: 0.001084611842246807, test_loss: 0.0010387645258257787\n",
      "epoch: 498, train_loss: 0.0010828775475206583, test_loss: 0.0010465544376832743\n",
      "epoch: 499, train_loss: 0.0010819291397321808, test_loss: 0.0010354310700980325\n",
      "epoch: 500, train_loss: 0.0010823349078671763, test_loss: 0.0010349007595020037\n",
      "epoch: 501, train_loss: 0.0010804652221217427, test_loss: 0.0010433764546178281\n",
      "epoch: 502, train_loss: 0.001078740693628788, test_loss: 0.0010323923973677058\n",
      "epoch: 503, train_loss: 0.0010783556550133812, test_loss: 0.0010326334449928254\n",
      "epoch: 504, train_loss: 0.0010782691027523706, test_loss: 0.0010347660378708194\n",
      "epoch: 505, train_loss: 0.0010769109687079554, test_loss: 0.001029488902228574\n",
      "epoch: 506, train_loss: 0.0010770672512159724, test_loss: 0.0010285902729568381\n",
      "epoch: 507, train_loss: 0.0010757955960402994, test_loss: 0.0010247247410006821\n",
      "epoch: 508, train_loss: 0.0010749386400794206, test_loss: 0.00102910185038733\n",
      "epoch: 509, train_loss: 0.0010740675016954217, test_loss: 0.0010318158601876348\n",
      "epoch: 510, train_loss: 0.0010734426272709084, test_loss: 0.0010215319440855335\n",
      "epoch: 511, train_loss: 0.0010729775278140669, test_loss: 0.0010393563037117322\n",
      "epoch: 512, train_loss: 0.0010734938980971017, test_loss: 0.0010259094027181466\n",
      "epoch: 513, train_loss: 0.0010715309586173491, test_loss: 0.0010254654431870829\n",
      "epoch: 514, train_loss: 0.0010705425874735026, test_loss: 0.001015960908262059\n",
      "epoch: 515, train_loss: 0.001068514017590686, test_loss: 0.0010292839045481135\n",
      "epoch: 516, train_loss: 0.001070028532843065, test_loss: 0.0010241019966391225\n",
      "epoch: 517, train_loss: 0.001068392414194734, test_loss: 0.0010312653515332688\n",
      "epoch: 518, train_loss: 0.001066943458483919, test_loss: 0.0010237574654941757\n",
      "epoch: 519, train_loss: 0.0010674150569526398, test_loss: 0.0010383801321343829\n",
      "epoch: 520, train_loss: 0.0010684368797861364, test_loss: 0.001014689701454093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 521, train_loss: 0.0010669788259648435, test_loss: 0.0010113111250878621\n",
      "epoch: 522, train_loss: 0.0010664125033856733, test_loss: 0.0010164558286002527\n",
      "epoch: 523, train_loss: 0.0010645985729870913, test_loss: 0.001016558613628149\n",
      "epoch: 524, train_loss: 0.001062959915978591, test_loss: 0.0010327241907361895\n",
      "epoch: 525, train_loss: 0.0010633244579050529, test_loss: 0.0010175405089588214\n",
      "epoch: 526, train_loss: 0.0010617034504716487, test_loss: 0.0010159766437330593\n",
      "epoch: 527, train_loss: 0.0010604378666080859, test_loss: 0.001009353776074325\n",
      "epoch: 528, train_loss: 0.0010601800243856142, test_loss: 0.001013024748923878\n",
      "epoch: 529, train_loss: 0.001061162220455868, test_loss: 0.0010128073239078124\n",
      "epoch: 530, train_loss: 0.0010613275049051838, test_loss: 0.0010110605023025225\n",
      "epoch: 531, train_loss: 0.0010577733223528965, test_loss: 0.0010109826010496665\n",
      "epoch: 532, train_loss: 0.0010573255814328465, test_loss: 0.0010036731061215203\n",
      "epoch: 533, train_loss: 0.0010567998371856368, test_loss: 0.0010120303971537699\n",
      "epoch: 534, train_loss: 0.0010559765910527305, test_loss: 0.0010019029141403735\n",
      "epoch: 535, train_loss: 0.0010547027239859428, test_loss: 0.0010126436148614932\n",
      "epoch: 536, train_loss: 0.0010541064598921525, test_loss: 0.0010058378587321688\n",
      "epoch: 537, train_loss: 0.001054503684154833, test_loss: 0.0010019671462941915\n",
      "epoch: 538, train_loss: 0.0010537074564996622, test_loss: 0.0010267043350419651\n",
      "epoch: 539, train_loss: 0.0010524047563945794, test_loss: 0.0010086182252659153\n",
      "epoch: 540, train_loss: 0.0010528370520383444, test_loss: 0.001003699318971485\n",
      "epoch: 541, train_loss: 0.0010513978549684196, test_loss: 0.000999904111570989\n",
      "epoch: 542, train_loss: 0.0010519790179703546, test_loss: 0.0010056209672863285\n",
      "epoch: 543, train_loss: 0.001053162650508887, test_loss: 0.0010010464272151391\n",
      "epoch: 544, train_loss: 0.0010498075584030669, test_loss: 0.0010014990790902327\n",
      "epoch: 545, train_loss: 0.0010507011877210891, test_loss: 0.0010002104196852695\n",
      "epoch: 546, train_loss: 0.0010470084228512385, test_loss: 0.0009924160161366065\n",
      "epoch: 547, train_loss: 0.0010467735195087025, test_loss: 0.0009952926872453343\n",
      "epoch: 548, train_loss: 0.001046650706405711, test_loss: 0.0009944155560030292\n",
      "epoch: 549, train_loss: 0.0010468448972855897, test_loss: 0.0009967620377816881\n",
      "epoch: 550, train_loss: 0.0010461333541847441, test_loss: 0.0009937631257344037\n",
      "epoch: 551, train_loss: 0.001045586589379641, test_loss: 0.0009984701270392786\n",
      "epoch: 552, train_loss: 0.0010432917295712168, test_loss: 0.000995898755112042\n",
      "epoch: 553, train_loss: 0.0010450371849836538, test_loss: 0.0009871575069458534\n",
      "epoch: 554, train_loss: 0.0010432653083546977, test_loss: 0.0009986737956448148\n",
      "epoch: 555, train_loss: 0.001041796202670135, test_loss: 0.0009875325971127797\n",
      "epoch: 556, train_loss: 0.001041143960521921, test_loss: 0.0009850755838366847\n",
      "epoch: 557, train_loss: 0.001039620278828332, test_loss: 0.001001387572614476\n",
      "epoch: 558, train_loss: 0.0010399531102334352, test_loss: 0.0009896488530406107\n",
      "epoch: 559, train_loss: 0.0010388384082192636, test_loss: 0.0009874038078123704\n",
      "epoch: 560, train_loss: 0.0010387925431132317, test_loss: 0.0009853581141214818\n",
      "epoch: 561, train_loss: 0.0010395045088523109, test_loss: 0.000984649076902618\n",
      "epoch: 562, train_loss: 0.001037167283989813, test_loss: 0.0010025222630550463\n",
      "epoch: 563, train_loss: 0.0010361362972458744, test_loss: 0.0009900370884376268\n",
      "epoch: 564, train_loss: 0.0010382867387860365, test_loss: 0.0009840618586167693\n",
      "epoch: 565, train_loss: 0.0010350209650705042, test_loss: 0.0009999608397871877\n",
      "epoch: 566, train_loss: 0.0010334907210958393, test_loss: 0.0009896851455171902\n",
      "epoch: 567, train_loss: 0.0010354722566578698, test_loss: 0.00098363688448444\n",
      "epoch: 568, train_loss: 0.0010323294402990975, test_loss: 0.0009956236172001809\n",
      "epoch: 569, train_loss: 0.001034292141886671, test_loss: 0.0009759079063466439\n",
      "epoch: 570, train_loss: 0.0010308123790942457, test_loss: 0.0009901618177536875\n",
      "epoch: 571, train_loss: 0.0010326108895242214, test_loss: 0.000975387345533818\n",
      "epoch: 572, train_loss: 0.001029952796196322, test_loss: 0.0009776428875435765\n",
      "epoch: 573, train_loss: 0.0010283560360498402, test_loss: 0.0009865046595223248\n",
      "epoch: 574, train_loss: 0.0010297994898713152, test_loss: 0.0009766284444291766\n",
      "epoch: 575, train_loss: 0.001028007081122664, test_loss: 0.0009744529670570046\n",
      "epoch: 576, train_loss: 0.0010282102941899843, test_loss: 0.0009746394450000176\n",
      "epoch: 577, train_loss: 0.0010278391520209286, test_loss: 0.0009828155549863975\n",
      "epoch: 578, train_loss: 0.001025944743204214, test_loss: 0.0009774173183056216\n",
      "epoch: 579, train_loss: 0.001024812943322341, test_loss: 0.0009762258462918302\n",
      "epoch: 580, train_loss: 0.001025298592614253, test_loss: 0.0009822823388579611\n",
      "epoch: 581, train_loss: 0.0010246836489228451, test_loss: 0.0009775867608065407\n",
      "epoch: 582, train_loss: 0.0010243419977917295, test_loss: 0.0009806772189525266\n",
      "epoch: 583, train_loss: 0.001026130235834938, test_loss: 0.0009749743136732528\n",
      "epoch: 584, train_loss: 0.0010234020250525487, test_loss: 0.0009745056837952385\n",
      "epoch: 585, train_loss: 0.0010231152814610498, test_loss: 0.0009639904019422829\n",
      "epoch: 586, train_loss: 0.0010216981704022896, test_loss: 0.0009635267391179999\n",
      "epoch: 587, train_loss: 0.0010220962374106698, test_loss: 0.0009623860920934627\n",
      "epoch: 588, train_loss: 0.001019863221976582, test_loss: 0.0009673927270341665\n",
      "epoch: 589, train_loss: 0.0010201824859351568, test_loss: 0.0009750047029228881\n",
      "epoch: 590, train_loss: 0.0010189364927456431, test_loss: 0.000969928689301014\n",
      "epoch: 591, train_loss: 0.0010194390045438447, test_loss: 0.0009675530600361526\n",
      "epoch: 592, train_loss: 0.0010196727412023945, test_loss: 0.000976096610732687\n",
      "epoch: 593, train_loss: 0.0010175133204978445, test_loss: 0.0009633981680963188\n",
      "epoch: 594, train_loss: 0.0010174151849123123, test_loss: 0.0009597702446626499\n",
      "epoch: 595, train_loss: 0.0010163201165952437, test_loss: 0.0009564668968475113\n",
      "epoch: 596, train_loss: 0.0010150919400115051, test_loss: 0.0009615149950453391\n",
      "epoch: 597, train_loss: 0.0010156271594536045, test_loss: 0.0009608202914629752\n",
      "epoch: 598, train_loss: 0.0010145036377134206, test_loss: 0.0009547534282319248\n",
      "epoch: 599, train_loss: 0.0010130714356858769, test_loss: 0.000958876897736142\n",
      "epoch: 600, train_loss: 0.0010145145326690829, test_loss: 0.0009555544626588622\n",
      "epoch: 601, train_loss: 0.0010133020822768626, test_loss: 0.0009569024211183811\n",
      "epoch: 602, train_loss: 0.0010127294403703315, test_loss: 0.0009893588915777702\n",
      "epoch: 603, train_loss: 0.0010135526080494342, test_loss: 0.000962143904568317\n",
      "epoch: 604, train_loss: 0.0010115666740128527, test_loss: 0.0009618257560456792\n",
      "epoch: 605, train_loss: 0.0010097277016662385, test_loss: 0.0009549596870783716\n",
      "epoch: 606, train_loss: 0.0010109426246186638, test_loss: 0.0009663472495352229\n",
      "epoch: 607, train_loss: 0.0010081418415369546, test_loss: 0.0009519123220040152\n",
      "epoch: 608, train_loss: 0.0010086519684156647, test_loss: 0.0009459738163665558\n",
      "epoch: 609, train_loss: 0.0010094293241348603, test_loss: 0.0009525500160331527\n",
      "epoch: 610, train_loss: 0.0010074478164112763, test_loss: 0.0009547130466671661\n",
      "epoch: 611, train_loss: 0.0010082663679697914, test_loss: 0.000952786571967105\n",
      "epoch: 612, train_loss: 0.0010063049064584725, test_loss: 0.0009504744278577467\n",
      "epoch: 613, train_loss: 0.0010050064302049577, test_loss: 0.0009456659948530918\n",
      "epoch: 614, train_loss: 0.001005932174967197, test_loss: 0.0009438464427754903\n",
      "epoch: 615, train_loss: 0.0010048141652394247, test_loss: 0.0009652930311858654\n",
      "epoch: 616, train_loss: 0.0010049304565536263, test_loss: 0.0009636526131847253\n",
      "epoch: 617, train_loss: 0.001004325461310699, test_loss: 0.0009488779614912346\n",
      "epoch: 618, train_loss: 0.0010030994535950215, test_loss: 0.0009412002982571721\n",
      "epoch: 619, train_loss: 0.0010033358121290803, test_loss: 0.0009477596031501889\n",
      "epoch: 620, train_loss: 0.0010015773257929022, test_loss: 0.0009428676712559536\n",
      "epoch: 621, train_loss: 0.0010009665603992407, test_loss: 0.0009496396814938635\n",
      "epoch: 622, train_loss: 0.0009996318157114413, test_loss: 0.0009459956927457824\n",
      "epoch: 623, train_loss: 0.0010004495928549895, test_loss: 0.0009426141429382066\n",
      "epoch: 624, train_loss: 0.0009997744548498936, test_loss: 0.0009397411971197774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 625, train_loss: 0.0009983525657014031, test_loss: 0.0009516207016228387\n",
      "epoch: 626, train_loss: 0.000998632010558377, test_loss: 0.0009370092205548038\n",
      "epoch: 627, train_loss: 0.000997810082955529, test_loss: 0.000936959205622164\n",
      "epoch: 628, train_loss: 0.0009984991325915832, test_loss: 0.0009452625187501932\n",
      "epoch: 629, train_loss: 0.0009968898653903086, test_loss: 0.0009381488974516591\n",
      "epoch: 630, train_loss: 0.0009958008785084214, test_loss: 0.0009402077121194452\n",
      "epoch: 631, train_loss: 0.0009958230910579796, test_loss: 0.0009722306858748198\n",
      "epoch: 632, train_loss: 0.0009961036266759038, test_loss: 0.0009398267381281281\n",
      "epoch: 633, train_loss: 0.0009947026290161455, test_loss: 0.0009391518591049438\n",
      "epoch: 634, train_loss: 0.0009941999375334253, test_loss: 0.000934455378834779\n",
      "epoch: 635, train_loss: 0.0009927757504477124, test_loss: 0.0009346557538568353\n",
      "epoch: 636, train_loss: 0.0009932803298832605, test_loss: 0.0009353359103746092\n",
      "epoch: 637, train_loss: 0.0009937060910843961, test_loss: 0.0009320923030221214\n",
      "epoch: 638, train_loss: 0.0009913643563935614, test_loss: 0.000934785518135565\n",
      "epoch: 639, train_loss: 0.0009923486454088402, test_loss: 0.0009340458151806766\n",
      "epoch: 640, train_loss: 0.0009913024386269567, test_loss: 0.0009285889148789769\n",
      "epoch: 641, train_loss: 0.0009884338790510335, test_loss: 0.0009414410257401565\n",
      "epoch: 642, train_loss: 0.000990366707454719, test_loss: 0.0009369872762666395\n",
      "epoch: 643, train_loss: 0.0009902315821660602, test_loss: 0.000941055579460226\n",
      "epoch: 644, train_loss: 0.0009901910696340644, test_loss: 0.0009386803236945221\n",
      "epoch: 645, train_loss: 0.0009864182894766006, test_loss: 0.0009269793954445049\n",
      "epoch: 646, train_loss: 0.0009867953966655161, test_loss: 0.0009232315981838232\n",
      "epoch: 647, train_loss: 0.0009875391513797576, test_loss: 0.0009278968985502919\n",
      "epoch: 648, train_loss: 0.0009867258207953494, test_loss: 0.0009255224043348184\n",
      "epoch: 649, train_loss: 0.000986839869849222, test_loss: 0.0009271704329876229\n",
      "epoch: 650, train_loss: 0.0009856139583027234, test_loss: 0.0009314095029064143\n",
      "epoch: 651, train_loss: 0.0009856659529285262, test_loss: 0.0009216981998179108\n",
      "epoch: 652, train_loss: 0.0009849440539255738, test_loss: 0.0009249180487434691\n",
      "epoch: 653, train_loss: 0.0009825385717229674, test_loss: 0.0009286318915352846\n",
      "epoch: 654, train_loss: 0.0009842446939412343, test_loss: 0.0009201094120120009\n",
      "epoch: 655, train_loss: 0.0009828519491154862, test_loss: 0.0009324519487563521\n",
      "epoch: 656, train_loss: 0.0009826264146755896, test_loss: 0.0009351569509211307\n",
      "epoch: 657, train_loss: 0.000983126780382641, test_loss: 0.000922659300461722\n",
      "epoch: 658, train_loss: 0.000979515794745606, test_loss: 0.0009279843022037918\n",
      "epoch: 659, train_loss: 0.0009813113618925538, test_loss: 0.0009229640224172423\n",
      "epoch: 660, train_loss: 0.0009796881526911063, test_loss: 0.0009219662218432253\n",
      "epoch: 661, train_loss: 0.0009794295208690608, test_loss: 0.0009179221378872171\n",
      "epoch: 662, train_loss: 0.0009788666755649383, test_loss: 0.0009265379097390299\n",
      "epoch: 663, train_loss: 0.0009781415802021713, test_loss: 0.0009188318654196337\n",
      "epoch: 664, train_loss: 0.00097765041636708, test_loss: 0.0009169612700740496\n",
      "epoch: 665, train_loss: 0.000978795411672605, test_loss: 0.0009181568554292122\n",
      "epoch: 666, train_loss: 0.0009780011959778874, test_loss: 0.0009201752642790476\n",
      "epoch: 667, train_loss: 0.0009745729393730669, test_loss: 0.0009157191816484556\n",
      "epoch: 668, train_loss: 0.000975179959230287, test_loss: 0.0009200112156880399\n",
      "epoch: 669, train_loss: 0.000977646663238335, test_loss: 0.0009285710451270764\n",
      "epoch: 670, train_loss: 0.0009753794444765409, test_loss: 0.0009189643897116184\n",
      "epoch: 671, train_loss: 0.0009742807647537278, test_loss: 0.000915593933314085\n",
      "epoch: 672, train_loss: 0.000973081034000801, test_loss: 0.000911045756462651\n",
      "epoch: 673, train_loss: 0.0009728649469173473, test_loss: 0.0009214514720952138\n",
      "epoch: 674, train_loss: 0.0009716362312800535, test_loss: 0.0009245503606507555\n",
      "epoch: 675, train_loss: 0.0009721468567200329, test_loss: 0.0009141333818358058\n",
      "epoch: 676, train_loss: 0.000971333313551124, test_loss: 0.0009060005686478689\n",
      "epoch: 677, train_loss: 0.000971068142224913, test_loss: 0.0009087302945166206\n",
      "epoch: 678, train_loss: 0.0009705946887033465, test_loss: 0.0009097761067096144\n",
      "epoch: 679, train_loss: 0.0009707966237328947, test_loss: 0.0009125441865762696\n",
      "epoch: 680, train_loss: 0.0009689604073924863, test_loss: 0.0009093608484060193\n",
      "epoch: 681, train_loss: 0.000969940171394821, test_loss: 0.0009087414412836855\n",
      "epoch: 682, train_loss: 0.0009687516519435397, test_loss: 0.0009131173671145613\n",
      "epoch: 683, train_loss: 0.0009681052027229706, test_loss: 0.000931409687230674\n",
      "epoch: 684, train_loss: 0.0009681658019595172, test_loss: 0.0009129916919240108\n",
      "epoch: 685, train_loss: 0.0009682455059626828, test_loss: 0.000906592695779788\n",
      "epoch: 686, train_loss: 0.000967215878241088, test_loss: 0.000908622566688185\n",
      "epoch: 687, train_loss: 0.0009649798638470795, test_loss: 0.000902513701779147\n",
      "epoch: 688, train_loss: 0.0009655552364521376, test_loss: 0.0009071815729839727\n",
      "epoch: 689, train_loss: 0.0009644145636981273, test_loss: 0.0009057053684955463\n",
      "epoch: 690, train_loss: 0.000964454962345569, test_loss: 0.0009028459317050874\n",
      "epoch: 691, train_loss: 0.0009635439947368982, test_loss: 0.0009201946522807702\n",
      "epoch: 692, train_loss: 0.0009636576363371443, test_loss: 0.0009280788120425617\n",
      "epoch: 693, train_loss: 0.0009636065553185408, test_loss: 0.0009081289608730003\n",
      "epoch: 694, train_loss: 0.0009632257861085236, test_loss: 0.0009050391963683069\n",
      "epoch: 695, train_loss: 0.0009630250835629261, test_loss: 0.0009098625838911781\n",
      "epoch: 696, train_loss: 0.0009625231158798156, test_loss: 0.0009007522991547982\n",
      "epoch: 697, train_loss: 0.0009611691297639323, test_loss: 0.0008974411709156508\n",
      "epoch: 698, train_loss: 0.0009611646882663279, test_loss: 0.0009025378725103413\n",
      "epoch: 699, train_loss: 0.0009615980265864536, test_loss: 0.0009001012804219499\n",
      "epoch: 700, train_loss: 0.0009594161225401837, test_loss: 0.0009029863819402332\n",
      "epoch: 701, train_loss: 0.0009588438551872969, test_loss: 0.0009110315586440265\n",
      "epoch: 702, train_loss: 0.0009570634777090796, test_loss: 0.0009019643039209768\n",
      "epoch: 703, train_loss: 0.0009580460635175848, test_loss: 0.0009038655892557775\n",
      "epoch: 704, train_loss: 0.0009576951852068305, test_loss: 0.0009098058653762564\n",
      "epoch: 705, train_loss: 0.0009594045215002868, test_loss: 0.0008975155409037446\n",
      "epoch: 706, train_loss: 0.0009556588838281839, test_loss: 0.0009058776437692965\n",
      "epoch: 707, train_loss: 0.0009554854427140368, test_loss: 0.0008931926762064298\n",
      "epoch: 708, train_loss: 0.0009563825695532496, test_loss: 0.0009030604123836383\n",
      "epoch: 709, train_loss: 0.0009534055564512054, test_loss: 0.0008896055175379539\n",
      "epoch: 710, train_loss: 0.00095408762116795, test_loss: 0.0008913233129230017\n",
      "epoch: 711, train_loss: 0.0009554322965888549, test_loss: 0.0008971449860837311\n",
      "epoch: 712, train_loss: 0.0009521789514743116, test_loss: 0.0008931449022687351\n",
      "epoch: 713, train_loss: 0.0009525547477254725, test_loss: 0.0008909090878053879\n",
      "epoch: 714, train_loss: 0.0009528206580359002, test_loss: 0.0008933165615114073\n",
      "epoch: 715, train_loss: 0.0009515850055638863, test_loss: 0.0009064735301459829\n",
      "epoch: 716, train_loss: 0.0009512026824623994, test_loss: 0.0008951959268112356\n",
      "epoch: 717, train_loss: 0.0009499006783184798, test_loss: 0.0008937473418579126\n",
      "epoch: 718, train_loss: 0.0009505201495535996, test_loss: 0.0008971848292276263\n",
      "epoch: 719, train_loss: 0.0009502423395726668, test_loss: 0.0008959027375870695\n",
      "epoch: 720, train_loss: 0.0009497845464426538, test_loss: 0.0008866770561629286\n",
      "epoch: 721, train_loss: 0.0009494109873902862, test_loss: 0.000893766880229426\n",
      "epoch: 722, train_loss: 0.0009489995882967892, test_loss: 0.000902937133408462\n",
      "epoch: 723, train_loss: 0.0009491614612710217, test_loss: 0.000891456506603087\n",
      "epoch: 724, train_loss: 0.0009479092120233437, test_loss: 0.0008862329559633508\n",
      "epoch: 725, train_loss: 0.0009480254729683309, test_loss: 0.0008864896711505329\n",
      "epoch: 726, train_loss: 0.0009462244975704538, test_loss: 0.0008947639144025743\n",
      "epoch: 727, train_loss: 0.0009458714807608529, test_loss: 0.0008893525421929857\n",
      "epoch: 728, train_loss: 0.0009467631031799575, test_loss: 0.0008850385784171522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 729, train_loss: 0.0009441914785977291, test_loss: 0.0009064057667274028\n",
      "epoch: 730, train_loss: 0.0009457365731182306, test_loss: 0.0008779878747494271\n",
      "epoch: 731, train_loss: 0.0009461009910370668, test_loss: 0.0008881325193215162\n",
      "epoch: 732, train_loss: 0.0009436943370889386, test_loss: 0.0008830028479375566\n",
      "epoch: 733, train_loss: 0.0009434549416334409, test_loss: 0.0008813486056169495\n",
      "epoch: 734, train_loss: 0.0009427251150001488, test_loss: 0.0008815311642441278\n",
      "epoch: 735, train_loss: 0.0009435563317864486, test_loss: 0.0008920915715862066\n",
      "epoch: 736, train_loss: 0.0009418351059455587, test_loss: 0.0008822547000211974\n",
      "epoch: 737, train_loss: 0.0009416380932595094, test_loss: 0.0008800416496039057\n",
      "epoch: 738, train_loss: 0.0009408118228590035, test_loss: 0.0008770259446464479\n",
      "epoch: 739, train_loss: 0.0009419628312689779, test_loss: 0.0008830911780629928\n",
      "epoch: 740, train_loss: 0.0009410939655145226, test_loss: 0.0008716985466890037\n",
      "epoch: 741, train_loss: 0.0009398646121237265, test_loss: 0.0008742436912143603\n",
      "epoch: 742, train_loss: 0.0009404417739816657, test_loss: 0.0008772590517764911\n",
      "epoch: 743, train_loss: 0.0009403162377724505, test_loss: 0.0008800914220046252\n",
      "epoch: 744, train_loss: 0.0009390095061541575, test_loss: 0.0008880669338395819\n",
      "epoch: 745, train_loss: 0.0009401977725286523, test_loss: 0.000873097994675239\n",
      "epoch: 746, train_loss: 0.0009377740485512692, test_loss: 0.0008854036083600173\n",
      "epoch: 747, train_loss: 0.0009359243027258502, test_loss: 0.0008862406684784219\n",
      "epoch: 748, train_loss: 0.0009373287877837277, test_loss: 0.0008733346670245131\n",
      "epoch: 749, train_loss: 0.0009376149492986176, test_loss: 0.0008745843175953875\n",
      "epoch: 750, train_loss: 0.0009371301478139409, test_loss: 0.0008774460099327067\n",
      "epoch: 751, train_loss: 0.000935167474064814, test_loss: 0.000876512184428672\n",
      "epoch: 752, train_loss: 0.0009353520328421955, test_loss: 0.0008783834491623566\n",
      "epoch: 753, train_loss: 0.0009361556225010883, test_loss: 0.0008721213477353255\n",
      "epoch: 754, train_loss: 0.0009342304775086434, test_loss: 0.0008735018636798486\n",
      "epoch: 755, train_loss: 0.0009342201089526972, test_loss: 0.000876418833892482\n",
      "epoch: 756, train_loss: 0.0009341631666757166, test_loss: 0.0008696974303650981\n",
      "epoch: 757, train_loss: 0.0009313269072900648, test_loss: 0.0008825364542038491\n",
      "epoch: 758, train_loss: 0.0009334045968463887, test_loss: 0.000867277131571124\n",
      "epoch: 759, train_loss: 0.000932370932550048, test_loss: 0.0008772188108802462\n",
      "epoch: 760, train_loss: 0.0009298767494645132, test_loss: 0.0008649493247503415\n",
      "epoch: 761, train_loss: 0.0009325392893515527, test_loss: 0.0008684897620696574\n",
      "epoch: 762, train_loss: 0.0009297545817073272, test_loss: 0.0008681120816618204\n",
      "epoch: 763, train_loss: 0.0009298526110006092, test_loss: 0.0008692495806220298\n",
      "epoch: 764, train_loss: 0.0009297760249034542, test_loss: 0.0008694779195745165\n",
      "epoch: 765, train_loss: 0.0009322112790592339, test_loss: 0.0008661652391310781\n",
      "epoch: 766, train_loss: 0.0009297352491716003, test_loss: 0.0008664135433112582\n",
      "epoch: 767, train_loss: 0.0009274918246892807, test_loss: 0.0008755618813059604\n",
      "epoch: 768, train_loss: 0.0009277347834659336, test_loss: 0.0008658144312600294\n",
      "epoch: 769, train_loss: 0.0009267673114031229, test_loss: 0.0008692361443536356\n",
      "epoch: 770, train_loss: 0.0009281399189089627, test_loss: 0.000872827586135827\n",
      "epoch: 771, train_loss: 0.0009262549675717626, test_loss: 0.0008684144995640963\n",
      "epoch: 772, train_loss: 0.0009259413548178323, test_loss: 0.000860107810391734\n",
      "epoch: 773, train_loss: 0.0009243433318181854, test_loss: 0.0008665572628766919\n",
      "epoch: 774, train_loss: 0.0009259651845280567, test_loss: 0.0008707257317534337\n",
      "epoch: 775, train_loss: 0.0009253952100508562, test_loss: 0.000860727586162587\n",
      "epoch: 776, train_loss: 0.0009262898187760426, test_loss: 0.0008669772748059282\n",
      "epoch: 777, train_loss: 0.0009253069140907863, test_loss: 0.0008736357946569721\n",
      "epoch: 778, train_loss: 0.0009238971802203552, test_loss: 0.0008579587301937863\n",
      "epoch: 779, train_loss: 0.0009246476971463341, test_loss: 0.0008619350119261071\n",
      "epoch: 780, train_loss: 0.0009242831299121937, test_loss: 0.0008640995074529201\n",
      "epoch: 781, train_loss: 0.0009217668076693687, test_loss: 0.0008588718337705359\n",
      "epoch: 782, train_loss: 0.0009222414025672427, test_loss: 0.0008601431084874397\n",
      "epoch: 783, train_loss: 0.0009216551628449689, test_loss: 0.0008544317630973334\n",
      "epoch: 784, train_loss: 0.0009210204538565291, test_loss: 0.0008597291016485542\n",
      "epoch: 785, train_loss: 0.000921272313343766, test_loss: 0.0008553137983350704\n",
      "epoch: 786, train_loss: 0.0009205633895876615, test_loss: 0.0008553472968439261\n",
      "epoch: 787, train_loss: 0.0009197342543579314, test_loss: 0.0008542975265299901\n",
      "epoch: 788, train_loss: 0.0009204825644543314, test_loss: 0.000870138086611405\n",
      "epoch: 789, train_loss: 0.000919649134511533, test_loss: 0.0008604489266872406\n",
      "epoch: 790, train_loss: 0.0009179768258827212, test_loss: 0.0008636289664233724\n",
      "epoch: 791, train_loss: 0.0009188861484922793, test_loss: 0.0008522527787135914\n",
      "epoch: 792, train_loss: 0.0009187039433290129, test_loss: 0.0008539785194443539\n",
      "epoch: 793, train_loss: 0.0009179477271138002, test_loss: 0.0008539205882698298\n",
      "epoch: 794, train_loss: 0.000918378443558417, test_loss: 0.0008555479580536485\n",
      "epoch: 795, train_loss: 0.0009154749517936422, test_loss: 0.0008517866372130811\n",
      "epoch: 796, train_loss: 0.0009163716813023, test_loss: 0.0008499245450366288\n",
      "epoch: 797, train_loss: 0.0009152585660795803, test_loss: 0.0008514983007141078\n",
      "epoch: 798, train_loss: 0.0009155339946079513, test_loss: 0.0008597383324134474\n",
      "epoch: 799, train_loss: 0.0009140312145261661, test_loss: 0.0008519660089708244\n",
      "epoch: 800, train_loss: 0.0009160550619961451, test_loss: 0.0008483478838267425\n",
      "epoch: 801, train_loss: 0.0009163331883230611, test_loss: 0.0008521064446540549\n",
      "epoch: 802, train_loss: 0.0009128527928386693, test_loss: 0.0008560455898987129\n",
      "epoch: 803, train_loss: 0.0009133397809067822, test_loss: 0.0008512882244152328\n",
      "epoch: 804, train_loss: 0.0009137102093992997, test_loss: 0.0008523651340510696\n",
      "epoch: 805, train_loss: 0.0009142794739961138, test_loss: 0.0008469134772894904\n",
      "epoch: 806, train_loss: 0.0009126821709999248, test_loss: 0.0008680535732613256\n",
      "epoch: 807, train_loss: 0.0009118871657056329, test_loss: 0.0008533157718678316\n",
      "epoch: 808, train_loss: 0.0009124953775545177, test_loss: 0.0008479607980310296\n",
      "epoch: 809, train_loss: 0.0009107204715964263, test_loss: 0.0008549679429658378\n",
      "epoch: 810, train_loss: 0.0009101541793864707, test_loss: 0.0008521507794891173\n",
      "epoch: 811, train_loss: 0.0009110828674317378, test_loss: 0.0008499387380046149\n",
      "epoch: 812, train_loss: 0.0009088552450639722, test_loss: 0.0008472147601423785\n",
      "epoch: 813, train_loss: 0.000909657303608306, test_loss: 0.0008428183064097539\n",
      "epoch: 814, train_loss: 0.000908537651412189, test_loss: 0.000882987214329963\n",
      "epoch: 815, train_loss: 0.0009101891723137511, test_loss: 0.0008435921287552143\n",
      "epoch: 816, train_loss: 0.0009092834965406877, test_loss: 0.0008394311735173687\n",
      "epoch: 817, train_loss: 0.0009076632508684112, test_loss: 0.0008406748723549148\n",
      "epoch: 818, train_loss: 0.0009063144072728312, test_loss: 0.0008390337219073748\n",
      "epoch: 819, train_loss: 0.0009087797876892854, test_loss: 0.0008367449530245116\n",
      "epoch: 820, train_loss: 0.0009068919031921288, test_loss: 0.0008462380937999114\n",
      "epoch: 821, train_loss: 0.0009051616602253331, test_loss: 0.0008404169493587688\n",
      "epoch: 822, train_loss: 0.0009057438355344145, test_loss: 0.0008395302235536898\n",
      "epoch: 823, train_loss: 0.0009051810256610422, test_loss: 0.0008376427722396329\n",
      "epoch: 824, train_loss: 0.0009057849225507158, test_loss: 0.0008382885619842758\n",
      "epoch: 825, train_loss: 0.0009040245204232633, test_loss: 0.000841953781976675\n",
      "epoch: 826, train_loss: 0.0009018242113676894, test_loss: 0.0008528898082052668\n",
      "epoch: 827, train_loss: 0.000903891442525808, test_loss: 0.0008378014366220062\n",
      "epoch: 828, train_loss: 0.0009024581598603855, test_loss: 0.0008373224360790724\n",
      "epoch: 829, train_loss: 0.0009023991575383621, test_loss: 0.0008368724132499968\n",
      "epoch: 830, train_loss: 0.000902977308926537, test_loss: 0.0008445577889991304\n",
      "epoch: 831, train_loss: 0.0009022876721523378, test_loss: 0.00083348931123813\n",
      "epoch: 832, train_loss: 0.0009014358334040836, test_loss: 0.0008337354035271952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 833, train_loss: 0.0009005299323926802, test_loss: 0.0008309972375476112\n",
      "epoch: 834, train_loss: 0.0008999001103169892, test_loss: 0.0008372959564439952\n",
      "epoch: 835, train_loss: 0.0009006903020908003, test_loss: 0.0008434549866554638\n",
      "epoch: 836, train_loss: 0.000899047950161216, test_loss: 0.0008319180342368782\n",
      "epoch: 837, train_loss: 0.0008988115005195141, test_loss: 0.0008357801076878483\n",
      "epoch: 838, train_loss: 0.0008988616299693999, test_loss: 0.0008359620163294798\n",
      "epoch: 839, train_loss: 0.0008995612582151333, test_loss: 0.0008348416983305166\n",
      "epoch: 840, train_loss: 0.0008980007260106504, test_loss: 0.0008312067172179619\n",
      "epoch: 841, train_loss: 0.0008998227877663853, test_loss: 0.0008333895530086011\n",
      "epoch: 842, train_loss: 0.0008982521729827251, test_loss: 0.0008296309145710742\n",
      "epoch: 843, train_loss: 0.0008969467421791152, test_loss: 0.0008340836260079717\n",
      "epoch: 844, train_loss: 0.0008971230936763079, test_loss: 0.0008343336424635103\n",
      "epoch: 845, train_loss: 0.0008955522992081293, test_loss: 0.0008352124665786201\n",
      "epoch: 846, train_loss: 0.0008958673494382073, test_loss: 0.0008259267099977782\n",
      "epoch: 847, train_loss: 0.0008965932445236198, test_loss: 0.00082816165619685\n",
      "epoch: 848, train_loss: 0.0008969357232158275, test_loss: 0.0008299438292548681\n",
      "epoch: 849, train_loss: 0.0008959961199687551, test_loss: 0.000835140070800359\n",
      "epoch: 850, train_loss: 0.0008942916174419224, test_loss: 0.0008257631270680577\n",
      "epoch: 851, train_loss: 0.0008939084895562543, test_loss: 0.0008360343296468878\n",
      "epoch: 852, train_loss: 0.0008941193835040474, test_loss: 0.0008259208164721107\n",
      "epoch: 853, train_loss: 0.0008935343484272776, test_loss: 0.0008437960495939478\n",
      "epoch: 854, train_loss: 0.0008924727849459842, test_loss: 0.0008295855416993921\n",
      "epoch: 855, train_loss: 0.0008942326429583456, test_loss: 0.0008372759281580026\n",
      "epoch: 856, train_loss: 0.0008932543771705874, test_loss: 0.0008380806684726849\n",
      "epoch: 857, train_loss: 0.0008917271928942722, test_loss: 0.0008254530306051796\n",
      "epoch: 858, train_loss: 0.0008904112492809477, test_loss: 0.0008246641324755425\n",
      "epoch: 859, train_loss: 0.0008910832364800508, test_loss: 0.0008261479864207407\n",
      "epoch: 860, train_loss: 0.0008904861751943827, test_loss: 0.0008301826649888729\n",
      "epoch: 861, train_loss: 0.0008909188706994704, test_loss: 0.0008202754688682035\n",
      "epoch: 862, train_loss: 0.0008907578227555622, test_loss: 0.0008184648322639987\n",
      "epoch: 863, train_loss: 0.0008895675520129178, test_loss: 0.0008258228578294317\n",
      "epoch: 864, train_loss: 0.0008907296300015372, test_loss: 0.0008217178595562776\n",
      "epoch: 865, train_loss: 0.0008889067255001029, test_loss: 0.0008310778842618068\n",
      "epoch: 866, train_loss: 0.0008872800744543581, test_loss: 0.0008245678230499228\n",
      "epoch: 867, train_loss: 0.000888412891704913, test_loss: 0.0008220312496026357\n",
      "epoch: 868, train_loss: 0.0008879884566261393, test_loss: 0.0008221925333297501\n",
      "epoch: 869, train_loss: 0.000887426608444556, test_loss: 0.0008278608972129101\n",
      "epoch: 870, train_loss: 0.000887664131131833, test_loss: 0.0008235074831948926\n",
      "epoch: 871, train_loss: 0.0008874465989799279, test_loss: 0.0008190728646392623\n",
      "epoch: 872, train_loss: 0.0008852061177806362, test_loss: 0.000820540304024083\n",
      "epoch: 873, train_loss: 0.0008865619285801506, test_loss: 0.0008177286169181267\n",
      "epoch: 874, train_loss: 0.0008840271671626555, test_loss: 0.0008219487014381835\n",
      "epoch: 875, train_loss: 0.0008860860328675936, test_loss: 0.0008241147006629035\n",
      "epoch: 876, train_loss: 0.000886001254674857, test_loss: 0.0008307604924387609\n",
      "epoch: 877, train_loss: 0.0008837377207110758, test_loss: 0.0008141121118872737\n",
      "epoch: 878, train_loss: 0.0008844257276707693, test_loss: 0.0008181126468116418\n",
      "epoch: 879, train_loss: 0.000884690395374175, test_loss: 0.0008254998829215765\n",
      "epoch: 880, train_loss: 0.0008844092220027486, test_loss: 0.0008181387432462847\n",
      "epoch: 881, train_loss: 0.0008840876322685052, test_loss: 0.0008332169527420774\n",
      "epoch: 882, train_loss: 0.0008826839646486485, test_loss: 0.0008223160063304628\n",
      "epoch: 883, train_loss: 0.0008821853578252637, test_loss: 0.0008137971599353477\n",
      "epoch: 884, train_loss: 0.0008830296042699205, test_loss: 0.0008220799936680123\n",
      "epoch: 885, train_loss: 0.0008832442538772265, test_loss: 0.0008161356527125463\n",
      "epoch: 886, train_loss: 0.0008808293066028019, test_loss: 0.0008148125731774295\n",
      "epoch: 887, train_loss: 0.0008808993177650416, test_loss: 0.0008188759772262225\n",
      "epoch: 888, train_loss: 0.0008819519881280544, test_loss: 0.0008244819085424145\n",
      "epoch: 889, train_loss: 0.0008804808477061274, test_loss: 0.0008166071929736063\n",
      "epoch: 890, train_loss: 0.000880849767860997, test_loss: 0.000813802044528226\n",
      "epoch: 891, train_loss: 0.0008790457631339846, test_loss: 0.0008166015225773057\n",
      "epoch: 892, train_loss: 0.000880417963186198, test_loss: 0.0008179991661260525\n",
      "epoch: 893, train_loss: 0.0008806342679156882, test_loss: 0.000812408106867224\n",
      "epoch: 894, train_loss: 0.0008797964319060354, test_loss: 0.0008135179377859458\n",
      "epoch: 895, train_loss: 0.000877957575974743, test_loss: 0.0008103712413382406\n",
      "epoch: 896, train_loss: 0.0008773150038905442, test_loss: 0.0008109384798444808\n",
      "epoch: 897, train_loss: 0.0008785254347300076, test_loss: 0.0008070421220812326\n",
      "epoch: 898, train_loss: 0.0008767751151817324, test_loss: 0.0008136983475803087\n",
      "epoch: 899, train_loss: 0.0008780854253057877, test_loss: 0.0008061926879842455\n",
      "epoch: 900, train_loss: 0.0008775742886506993, test_loss: 0.0008135268338567888\n",
      "epoch: 901, train_loss: 0.0008754691174861206, test_loss: 0.0008193516502312074\n",
      "epoch: 902, train_loss: 0.0008763373859793595, test_loss: 0.0008142633499422421\n",
      "epoch: 903, train_loss: 0.000874707956150498, test_loss: 0.000814844529183271\n",
      "epoch: 904, train_loss: 0.0008761820487131405, test_loss: 0.0008081902875953043\n",
      "epoch: 905, train_loss: 0.0008746288038547273, test_loss: 0.0008046430205771079\n",
      "epoch: 906, train_loss: 0.0008733367456285202, test_loss: 0.000817073528499653\n",
      "epoch: 907, train_loss: 0.0008752012331767575, test_loss: 0.0008213494826729099\n",
      "epoch: 908, train_loss: 0.0008749507858579898, test_loss: 0.0008156800031429157\n",
      "epoch: 909, train_loss: 0.0008735948358662426, test_loss: 0.0008051229863970851\n",
      "epoch: 910, train_loss: 0.0008720706176498662, test_loss: 0.0008018519292818382\n",
      "epoch: 911, train_loss: 0.0008712824960441693, test_loss: 0.0008022737262460092\n",
      "epoch: 912, train_loss: 0.000871623747313962, test_loss: 0.0008152964922677105\n",
      "epoch: 913, train_loss: 0.0008717127113967487, test_loss: 0.0008097917937751239\n",
      "epoch: 914, train_loss: 0.0008711330896324438, test_loss: 0.0008016859889418507\n",
      "epoch: 915, train_loss: 0.0008730634454759243, test_loss: 0.0008034376175298045\n",
      "epoch: 916, train_loss: 0.0008706367807462811, test_loss: 0.0008064276577594379\n",
      "epoch: 917, train_loss: 0.0008706878301302862, test_loss: 0.0008020587605036175\n",
      "epoch: 918, train_loss: 0.0008714021634027038, test_loss: 0.000815472289104946\n",
      "epoch: 919, train_loss: 0.0008708955542913274, test_loss: 0.0008075577740479881\n",
      "epoch: 920, train_loss: 0.0008702687337063253, test_loss: 0.0008090043459863713\n",
      "epoch: 921, train_loss: 0.0008701876023500834, test_loss: 0.0008131592573287586\n",
      "epoch: 922, train_loss: 0.0008686591121975495, test_loss: 0.0008052302049084877\n",
      "epoch: 923, train_loss: 0.0008673685623089905, test_loss: 0.0008078392856987193\n",
      "epoch: 924, train_loss: 0.0008681931117873477, test_loss: 0.0008053905670143043\n",
      "epoch: 925, train_loss: 0.0008674761908047873, test_loss: 0.000803967957229664\n",
      "epoch: 926, train_loss: 0.000866840661847559, test_loss: 0.0008108381443889812\n",
      "epoch: 927, train_loss: 0.0008672790920726308, test_loss: 0.0008062291114280621\n",
      "epoch: 928, train_loss: 0.0008669290868767902, test_loss: 0.0007994477831137677\n",
      "epoch: 929, train_loss: 0.0008654164772926141, test_loss: 0.0008062713168328628\n",
      "epoch: 930, train_loss: 0.0008675385792942151, test_loss: 0.0008065329990737761\n",
      "epoch: 931, train_loss: 0.0008641751761467236, test_loss: 0.0008092357165878639\n",
      "epoch: 932, train_loss: 0.0008653747550535785, test_loss: 0.0008068655151873827\n",
      "epoch: 933, train_loss: 0.0008655765584831977, test_loss: 0.0007995028475609919\n",
      "epoch: 934, train_loss: 0.0008643816134123051, test_loss: 0.000800611887825653\n",
      "epoch: 935, train_loss: 0.0008645237792971666, test_loss: 0.000791269660112448\n",
      "epoch: 936, train_loss: 0.0008648789877517391, test_loss: 0.0008037831818607325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 937, train_loss: 0.000864763933983024, test_loss: 0.0007972880315113192\n",
      "epoch: 938, train_loss: 0.0008634533935590931, test_loss: 0.0007933042506920174\n",
      "epoch: 939, train_loss: 0.0008629466376631805, test_loss: 0.0008084554040882116\n",
      "epoch: 940, train_loss: 0.0008624906540321915, test_loss: 0.0007941893757864212\n",
      "epoch: 941, train_loss: 0.000862085599573734, test_loss: 0.0007943944462264577\n",
      "epoch: 942, train_loss: 0.0008617420250590404, test_loss: 0.0008029361439791197\n",
      "epoch: 943, train_loss: 0.0008613575394164123, test_loss: 0.0007923025162502503\n",
      "epoch: 944, train_loss: 0.0008613615848488458, test_loss: 0.0007930232435076808\n",
      "epoch: 945, train_loss: 0.0008615313791264982, test_loss: 0.0008107640751404688\n",
      "epoch: 946, train_loss: 0.0008614424150437117, test_loss: 0.0008087731500078613\n",
      "epoch: 947, train_loss: 0.0008646006943941441, test_loss: 0.0007934353828507786\n",
      "epoch: 948, train_loss: 0.0008609854545363265, test_loss: 0.0008040020863215128\n",
      "epoch: 949, train_loss: 0.0008600578142289559, test_loss: 0.0008057849724233771\n",
      "epoch: 950, train_loss: 0.0008594815103013231, test_loss: 0.000789509984315373\n",
      "epoch: 951, train_loss: 0.0008585937966531872, test_loss: 0.00079461368053065\n",
      "epoch: 952, train_loss: 0.0008611248922534287, test_loss: 0.0007909011716643969\n",
      "epoch: 953, train_loss: 0.0008594788934873498, test_loss: 0.0007887863030191511\n",
      "epoch: 954, train_loss: 0.0008582393056713045, test_loss: 0.0007900710043031722\n",
      "epoch: 955, train_loss: 0.0008593425686147226, test_loss: 0.0007954487906924138\n",
      "epoch: 956, train_loss: 0.0008571390771185575, test_loss: 0.0007967706284640977\n",
      "epoch: 957, train_loss: 0.0008577490351972697, test_loss: 0.0007928353588795289\n",
      "epoch: 958, train_loss: 0.0008572563149399408, test_loss: 0.0007906153283935661\n",
      "epoch: 959, train_loss: 0.0008565400747339363, test_loss: 0.0007961695664562285\n",
      "epoch: 960, train_loss: 0.0008561520548739835, test_loss: 0.000803393234188358\n",
      "epoch: 961, train_loss: 0.0008564014468625512, test_loss: 0.0007920775533420965\n",
      "epoch: 962, train_loss: 0.000858175024167513, test_loss: 0.0007961980591062456\n",
      "epoch: 963, train_loss: 0.0008559536452040724, test_loss: 0.0007861460811303308\n",
      "epoch: 964, train_loss: 0.0008561381052815071, test_loss: 0.0007829027260110403\n",
      "epoch: 965, train_loss: 0.000854708947767706, test_loss: 0.0007869932887842879\n",
      "epoch: 966, train_loss: 0.0008540940875916377, test_loss: 0.0007906445922950903\n",
      "epoch: 967, train_loss: 0.0008549977501388639, test_loss: 0.0008042235858738422\n",
      "epoch: 968, train_loss: 0.0008537799712149022, test_loss: 0.000789964726815621\n",
      "epoch: 969, train_loss: 0.000854588121317489, test_loss: 0.0007844001908476154\n",
      "epoch: 970, train_loss: 0.0008536348974777628, test_loss: 0.0007830955194852626\n",
      "epoch: 971, train_loss: 0.0008528072579318415, test_loss: 0.0007835053111193702\n",
      "epoch: 972, train_loss: 0.0008516282035523783, test_loss: 0.0007917855352085704\n",
      "epoch: 973, train_loss: 0.0008526589169733874, test_loss: 0.0007857864499480153\n",
      "epoch: 974, train_loss: 0.000851499478575652, test_loss: 0.000786699815459239\n",
      "epoch: 975, train_loss: 0.0008524357386783737, test_loss: 0.0007848501021120077\n",
      "epoch: 976, train_loss: 0.0008516164962202311, test_loss: 0.0007820691098459065\n",
      "epoch: 977, train_loss: 0.0008510558653379912, test_loss: 0.0007906292545764396\n",
      "epoch: 978, train_loss: 0.0008512656381556197, test_loss: 0.0007810055491669724\n",
      "epoch: 979, train_loss: 0.0008507687876851338, test_loss: 0.0007950139988679439\n",
      "epoch: 980, train_loss: 0.0008518355544251592, test_loss: 0.0007856088632252067\n",
      "epoch: 981, train_loss: 0.0008506762982427102, test_loss: 0.0007816089443319166\n",
      "epoch: 982, train_loss: 0.0008491221132040348, test_loss: 0.0007795589675273126\n",
      "epoch: 983, train_loss: 0.0008481029679229402, test_loss: 0.0007934069096033151\n",
      "epoch: 984, train_loss: 0.0008487411415325882, test_loss: 0.0007977167745896926\n",
      "epoch: 985, train_loss: 0.0008483879931230584, test_loss: 0.0007836123598584285\n",
      "epoch: 986, train_loss: 0.0008473884030852629, test_loss: 0.0007811245838335404\n",
      "epoch: 987, train_loss: 0.0008472722281863832, test_loss: 0.0007913693310304856\n",
      "epoch: 988, train_loss: 0.0008485687684024806, test_loss: 0.000785456165128077\n",
      "epoch: 989, train_loss: 0.0008494328003903122, test_loss: 0.000783937105249303\n",
      "epoch: 990, train_loss: 0.0008457203233695549, test_loss: 0.0007767150964355096\n",
      "epoch: 991, train_loss: 0.0008459007291568686, test_loss: 0.0007769470345616961\n",
      "epoch: 992, train_loss: 0.0008453623032318833, test_loss: 0.0007780741289025173\n",
      "epoch: 993, train_loss: 0.0008448038589330795, test_loss: 0.0007889121334301308\n",
      "epoch: 994, train_loss: 0.0008457372592681128, test_loss: 0.0007846111887677883\n",
      "epoch: 995, train_loss: 0.000845578296676926, test_loss: 0.0007836081106991818\n",
      "epoch: 996, train_loss: 0.0008453458000946304, test_loss: 0.0007839255607298886\n",
      "epoch: 997, train_loss: 0.0008444659432153339, test_loss: 0.0007771004505533105\n",
      "epoch: 998, train_loss: 0.0008440468176875426, test_loss: 0.0007822290062904358\n",
      "epoch: 999, train_loss: 0.0008433376028931335, test_loss: 0.0007808592636138201\n",
      "epoch: 1000, train_loss: 0.0008445792608772931, test_loss: 0.0007785145135130733\n",
      "epoch: 1001, train_loss: 0.0008433513576164842, test_loss: 0.0007857105180543537\n",
      "epoch: 1002, train_loss: 0.0008430095976623504, test_loss: 0.0007795166845122973\n",
      "epoch: 1003, train_loss: 0.0008428031616621529, test_loss: 0.0007768638024572283\n",
      "epoch: 1004, train_loss: 0.0008427889387467471, test_loss: 0.0007791673754885172\n",
      "epoch: 1005, train_loss: 0.0008429174043197671, test_loss: 0.0007833638568020737\n",
      "epoch: 1006, train_loss: 0.0008427285963831389, test_loss: 0.0007829359189296762\n",
      "epoch: 1007, train_loss: 0.0008422854589298368, test_loss: 0.0007746093469904736\n",
      "epoch: 1008, train_loss: 0.0008411677187288423, test_loss: 0.000773292820667848\n",
      "epoch: 1009, train_loss: 0.0008404832966018306, test_loss: 0.0007798251366087546\n",
      "epoch: 1010, train_loss: 0.0008416037130898432, test_loss: 0.0007744042086414993\n",
      "epoch: 1011, train_loss: 0.000841446661228395, test_loss: 0.0007748967085111266\n",
      "epoch: 1012, train_loss: 0.0008419916646190635, test_loss: 0.0007772094152945405\n",
      "epoch: 1013, train_loss: 0.0008399158275848174, test_loss: 0.0007690383693746602\n",
      "epoch: 1014, train_loss: 0.0008402605004527646, test_loss: 0.0007772305834805593\n",
      "epoch: 1015, train_loss: 0.0008393605644612209, test_loss: 0.0007740321743767709\n",
      "epoch: 1016, train_loss: 0.0008410129002700357, test_loss: 0.0007682805201814821\n",
      "epoch: 1017, train_loss: 0.0008394948697810912, test_loss: 0.000773738298448734\n",
      "epoch: 1018, train_loss: 0.0008381816492739903, test_loss: 0.0007686413009651005\n",
      "epoch: 1019, train_loss: 0.0008374629921846739, test_loss: 0.0007694532978348434\n",
      "epoch: 1020, train_loss: 0.0008380990741146809, test_loss: 0.0007690753312393402\n",
      "epoch: 1021, train_loss: 0.0008383032223007277, test_loss: 0.0007691020825101683\n",
      "epoch: 1022, train_loss: 0.0008366966453056945, test_loss: 0.0007834804952532674\n",
      "epoch: 1023, train_loss: 0.0008375835706196401, test_loss: 0.0007690915420729046\n",
      "epoch: 1024, train_loss: 0.0008361105700833318, test_loss: 0.0007682719248502204\n",
      "epoch: 1025, train_loss: 0.0008354825296682184, test_loss: 0.0007693291602966686\n",
      "epoch: 1026, train_loss: 0.0008349064611019971, test_loss: 0.0007770184747641906\n",
      "epoch: 1027, train_loss: 0.0008349670147847222, test_loss: 0.0007673740183236077\n",
      "epoch: 1028, train_loss: 0.0008352791242625402, test_loss: 0.0007673612077875683\n",
      "epoch: 1029, train_loss: 0.0008355505769545941, test_loss: 0.0007615869011109074\n",
      "epoch: 1030, train_loss: 0.0008353003649972379, test_loss: 0.0007732937083346769\n",
      "epoch: 1031, train_loss: 0.0008353914017789066, test_loss: 0.0007681831339141354\n",
      "epoch: 1032, train_loss: 0.0008345869162281895, test_loss: 0.000766227594188725\n",
      "epoch: 1033, train_loss: 0.0008350915462790948, test_loss: 0.0007676882135759419\n",
      "epoch: 1034, train_loss: 0.0008337142366065603, test_loss: 0.00076321706486245\n",
      "epoch: 1035, train_loss: 0.0008344166259200352, test_loss: 0.0007719472390211498\n",
      "epoch: 1036, train_loss: 0.0008331599048055384, test_loss: 0.0007693813774191464\n",
      "epoch: 1037, train_loss: 0.000832268788540007, test_loss: 0.0007656360782372454\n",
      "epoch: 1038, train_loss: 0.0008345333019109523, test_loss: 0.0007706363297378024\n",
      "epoch: 1039, train_loss: 0.0008323630368660973, test_loss: 0.000783939947723411\n",
      "epoch: 1040, train_loss: 0.0008321693572013274, test_loss: 0.0007987901093050217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1041, train_loss: 0.0008318753907983394, test_loss: 0.0007692342430042723\n",
      "epoch: 1042, train_loss: 0.0008330107059167779, test_loss: 0.0007621572343244528\n",
      "epoch: 1043, train_loss: 0.0008313641301356256, test_loss: 0.0007878970112263536\n",
      "epoch: 1044, train_loss: 0.0008318632279279764, test_loss: 0.0007650631741853431\n",
      "epoch: 1045, train_loss: 0.0008296392549274732, test_loss: 0.0007776836767637482\n",
      "epoch: 1046, train_loss: 0.0008317819118256803, test_loss: 0.0007658730755792931\n",
      "epoch: 1047, train_loss: 0.0008312043576987218, test_loss: 0.0007611763430759311\n",
      "epoch: 1048, train_loss: 0.000828847010700923, test_loss: 0.0007621140345387781\n",
      "epoch: 1049, train_loss: 0.0008322081437496388, test_loss: 0.0007676534102453539\n",
      "epoch: 1050, train_loss: 0.0008303524784581816, test_loss: 0.0007644933939445764\n",
      "epoch: 1051, train_loss: 0.0008296517746361053, test_loss: 0.0007596872019348666\n",
      "epoch: 1052, train_loss: 0.0008280386049907817, test_loss: 0.0007607496906227121\n",
      "epoch: 1053, train_loss: 0.0008284622884314994, test_loss: 0.0007632432437579458\n",
      "epoch: 1054, train_loss: 0.0008271698632439518, test_loss: 0.0007682294478096688\n",
      "epoch: 1055, train_loss: 0.000828717828920597, test_loss: 0.0007624422966424996\n",
      "epoch: 1056, train_loss: 0.0008273817391296768, test_loss: 0.0007572181542248776\n",
      "epoch: 1057, train_loss: 0.0008278077154460808, test_loss: 0.0007626784936292097\n",
      "epoch: 1058, train_loss: 0.0008283250626055119, test_loss: 0.0007739679422229528\n",
      "epoch: 1059, train_loss: 0.0008273875421803931, test_loss: 0.0007708573490769292\n",
      "epoch: 1060, train_loss: 0.0008268502576317152, test_loss: 0.0007696968726425742\n",
      "epoch: 1061, train_loss: 0.0008263368230394047, test_loss: 0.0007645876030437648\n",
      "epoch: 1062, train_loss: 0.0008269066735089797, test_loss: 0.0007579965410210813\n",
      "epoch: 1063, train_loss: 0.00082632831459784, test_loss: 0.0007757094911842918\n",
      "epoch: 1064, train_loss: 0.0008250094352937911, test_loss: 0.0007560969970654696\n",
      "epoch: 1065, train_loss: 0.0008249282685067991, test_loss: 0.0007709607743890956\n",
      "epoch: 1066, train_loss: 0.0008255416203452194, test_loss: 0.0007557360707626989\n",
      "epoch: 1067, train_loss: 0.000824374204967171, test_loss: 0.0007605650413703794\n",
      "epoch: 1068, train_loss: 0.0008247419457842151, test_loss: 0.0007527407433371991\n",
      "epoch: 1069, train_loss: 0.0008234286417617747, test_loss: 0.0007579086814075708\n",
      "epoch: 1070, train_loss: 0.0008254547365536184, test_loss: 0.0007558995857834816\n",
      "epoch: 1071, train_loss: 0.0008233846873854814, test_loss: 0.0007545488673107078\n",
      "epoch: 1072, train_loss: 0.0008231336428054973, test_loss: 0.0007560206286143512\n",
      "epoch: 1073, train_loss: 0.0008219800808507463, test_loss: 0.0007598208612762392\n",
      "epoch: 1074, train_loss: 0.0008235616563900334, test_loss: 0.0007536884659202769\n",
      "epoch: 1075, train_loss: 0.0008232398274982268, test_loss: 0.0007513407714820156\n",
      "epoch: 1076, train_loss: 0.000822985005747203, test_loss: 0.0007523534635159498\n",
      "epoch: 1077, train_loss: 0.0008217061100446659, test_loss: 0.000754576874896884\n",
      "epoch: 1078, train_loss: 0.0008211841442338798, test_loss: 0.0007514862081734464\n",
      "epoch: 1079, train_loss: 0.0008223197174906406, test_loss: 0.0007646971450109655\n",
      "epoch: 1080, train_loss: 0.000822220513921069, test_loss: 0.0007562413229607046\n",
      "epoch: 1081, train_loss: 0.0008202462112935989, test_loss: 0.0007574101618956774\n",
      "epoch: 1082, train_loss: 0.000819956352326857, test_loss: 0.0007470205115775267\n",
      "epoch: 1083, train_loss: 0.0008202493266688417, test_loss: 0.0007612293071967239\n",
      "epoch: 1084, train_loss: 0.0008198697089582034, test_loss: 0.0007517150612936044\n",
      "epoch: 1085, train_loss: 0.0008210062590913604, test_loss: 0.0007551620025575782\n",
      "epoch: 1086, train_loss: 0.000818824685310297, test_loss: 0.0007526094996137545\n",
      "epoch: 1087, train_loss: 0.0008198404735278176, test_loss: 0.0007512078009312972\n",
      "epoch: 1088, train_loss: 0.0008185542385984698, test_loss: 0.0007496383526207259\n",
      "epoch: 1089, train_loss: 0.0008186204409550713, test_loss: 0.0007479344155096138\n",
      "epoch: 1090, train_loss: 0.0008169796834092425, test_loss: 0.0007462383509846404\n",
      "epoch: 1091, train_loss: 0.000819273671368137, test_loss: 0.00075619632358818\n",
      "epoch: 1092, train_loss: 0.0008193344034699966, test_loss: 0.0007529933791374788\n",
      "epoch: 1093, train_loss: 0.0008177565405671687, test_loss: 0.0007522842158020163\n",
      "epoch: 1094, train_loss: 0.0008177265534987268, test_loss: 0.0007652071362826973\n",
      "epoch: 1095, train_loss: 0.0008171548441801544, test_loss: 0.0007516513433074579\n",
      "epoch: 1096, train_loss: 0.0008175540183994757, test_loss: 0.0007513059730020663\n",
      "epoch: 1097, train_loss: 0.0008161666868861927, test_loss: 0.0007564469609254351\n",
      "epoch: 1098, train_loss: 0.000816221577975818, test_loss: 0.0007499393298833942\n",
      "epoch: 1099, train_loss: 0.0008163580331829903, test_loss: 0.0007492091196278731\n",
      "epoch: 1100, train_loss: 0.0008168451730971751, test_loss: 0.000749187272352477\n",
      "epoch: 1101, train_loss: 0.0008142195469902261, test_loss: 0.0007550689964167153\n",
      "epoch: 1102, train_loss: 0.0008146154704382238, test_loss: 0.0007566691201645881\n",
      "epoch: 1103, train_loss: 0.0008148411137011388, test_loss: 0.0007548647117801011\n",
      "epoch: 1104, train_loss: 0.0008158664543019688, test_loss: 0.0007467410808506733\n",
      "epoch: 1105, train_loss: 0.0008157491608036925, test_loss: 0.0007542080469041442\n",
      "epoch: 1106, train_loss: 0.000813994736349939, test_loss: 0.0007451037963619456\n",
      "epoch: 1107, train_loss: 0.0008137279681092048, test_loss: 0.0007476288931987559\n",
      "epoch: 1108, train_loss: 0.0008144217528119359, test_loss: 0.0007537643638594697\n",
      "epoch: 1109, train_loss: 0.0008139145666854861, test_loss: 0.0007422241275586808\n",
      "epoch: 1110, train_loss: 0.0008127333687456405, test_loss: 0.000745555167668499\n",
      "epoch: 1111, train_loss: 0.0008134639685285156, test_loss: 0.000739840033929795\n",
      "epoch: 1112, train_loss: 0.0008123986190184951, test_loss: 0.0007511987156855563\n",
      "epoch: 1113, train_loss: 0.0008137407965715165, test_loss: 0.0007579949209078526\n",
      "epoch: 1114, train_loss: 0.000811736684029355, test_loss: 0.0007581748019826288\n",
      "epoch: 1115, train_loss: 0.0008138483997596347, test_loss: 0.0007421773383005833\n",
      "epoch: 1116, train_loss: 0.0008124450560780647, test_loss: 0.0007464710458104188\n",
      "epoch: 1117, train_loss: 0.0008132334763143698, test_loss: 0.0007514902148007726\n",
      "epoch: 1118, train_loss: 0.0008107491885311902, test_loss: 0.0007505128451157361\n",
      "epoch: 1119, train_loss: 0.0008130401307109581, test_loss: 0.000751903309719637\n",
      "epoch: 1120, train_loss: 0.0008124142768792808, test_loss: 0.0007423561910400167\n",
      "epoch: 1121, train_loss: 0.0008100416769435549, test_loss: 0.0007432720449287444\n",
      "epoch: 1122, train_loss: 0.0008112632785923779, test_loss: 0.0007442317485886937\n",
      "epoch: 1123, train_loss: 0.0008130540398111486, test_loss: 0.0007412930038602402\n",
      "epoch: 1124, train_loss: 0.000808890903894992, test_loss: 0.0007407662730353574\n",
      "epoch: 1125, train_loss: 0.0008090586330660659, test_loss: 0.0007414938202903917\n",
      "epoch: 1126, train_loss: 0.0008087561683445845, test_loss: 0.0007461361965397373\n",
      "epoch: 1127, train_loss: 0.0008082946625781124, test_loss: 0.0007404148830876996\n",
      "epoch: 1128, train_loss: 0.0008080931603098693, test_loss: 0.0007381466324053084\n",
      "epoch: 1129, train_loss: 0.0008076824723621426, test_loss: 0.0007469387928722426\n",
      "epoch: 1130, train_loss: 0.0008082418429219853, test_loss: 0.0007497461338061839\n",
      "epoch: 1131, train_loss: 0.0008069017709678282, test_loss: 0.0007440602542677274\n",
      "epoch: 1132, train_loss: 0.0008080998643139458, test_loss: 0.0007508767982168744\n",
      "epoch: 1133, train_loss: 0.0008079822899004364, test_loss: 0.0007494638654558609\n",
      "epoch: 1134, train_loss: 0.0008085410631990627, test_loss: 0.0007457979809259996\n",
      "epoch: 1135, train_loss: 0.0008071359631645938, test_loss: 0.0007393305568257347\n",
      "epoch: 1136, train_loss: 0.0008064181893132627, test_loss: 0.0007414017697252954\n",
      "epoch: 1137, train_loss: 0.0008076230145019034, test_loss: 0.0007429634279105812\n",
      "epoch: 1138, train_loss: 0.0008068934649876926, test_loss: 0.0007381727045867592\n",
      "epoch: 1139, train_loss: 0.0008084530051311721, test_loss: 0.000736999490375941\n",
      "epoch: 1140, train_loss: 0.0008057848140394882, test_loss: 0.0007414613355649635\n",
      "epoch: 1141, train_loss: 0.0008060356106042215, test_loss: 0.0007436278683599085\n",
      "epoch: 1142, train_loss: 0.0008060386146256781, test_loss: 0.0007406550284940749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1143, train_loss: 0.0008037959797429326, test_loss: 0.0007440220603408912\n",
      "epoch: 1144, train_loss: 0.0008068710449151695, test_loss: 0.0007395393768092617\n",
      "epoch: 1145, train_loss: 0.0008036221514213021, test_loss: 0.0007494894816773012\n",
      "epoch: 1146, train_loss: 0.0008062169755525563, test_loss: 0.0007453313252578179\n",
      "epoch: 1147, train_loss: 0.0008039614641229095, test_loss: 0.0007408711729416003\n",
      "epoch: 1148, train_loss: 0.0008045647586898311, test_loss: 0.0007352214403605709\n",
      "epoch: 1149, train_loss: 0.0008046253731109849, test_loss: 0.0007572810233493025\n",
      "epoch: 1150, train_loss: 0.0008042131970717531, test_loss: 0.0007425956428050995\n",
      "epoch: 1151, train_loss: 0.0008041479133838868, test_loss: 0.0007373175418858106\n",
      "epoch: 1152, train_loss: 0.0008029553042117344, test_loss: 0.0007305643327223758\n",
      "epoch: 1153, train_loss: 0.0008019258713592653, test_loss: 0.0007340900289515654\n",
      "epoch: 1154, train_loss: 0.0008031799326367352, test_loss: 0.000741342994539688\n",
      "epoch: 1155, train_loss: 0.000802497030235827, test_loss: 0.0007341445355753725\n",
      "epoch: 1156, train_loss: 0.0008004122595910145, test_loss: 0.0007306779734790325\n",
      "epoch: 1157, train_loss: 0.0008015119592132776, test_loss: 0.0007346896745730191\n",
      "epoch: 1158, train_loss: 0.0008019905046397901, test_loss: 0.0007372119289357215\n",
      "epoch: 1159, train_loss: 0.0008014451444108525, test_loss: 0.0007371989583286146\n",
      "epoch: 1160, train_loss: 0.0008002157530585384, test_loss: 0.0007311033502143497\n",
      "epoch: 1161, train_loss: 0.0007998273940756917, test_loss: 0.000732803096373876\n",
      "epoch: 1162, train_loss: 0.0008008545062140278, test_loss: 0.0007317553245229647\n",
      "epoch: 1163, train_loss: 0.0007995393294233667, test_loss: 0.0007288166185996184\n",
      "epoch: 1164, train_loss: 0.0008011999762738528, test_loss: 0.0007276761558993409\n",
      "epoch: 1165, train_loss: 0.0008006085611312934, test_loss: 0.0007330449977113555\n",
      "epoch: 1166, train_loss: 0.0007989977780000671, test_loss: 0.0007284682991060739\n",
      "epoch: 1167, train_loss: 0.000800064327094056, test_loss: 0.0007278961226499329\n",
      "epoch: 1168, train_loss: 0.0007992489086261586, test_loss: 0.0007370106128898138\n",
      "epoch: 1169, train_loss: 0.0007975281206081095, test_loss: 0.0007248888481020307\n",
      "epoch: 1170, train_loss: 0.0007980806783408574, test_loss: 0.0007301347262303656\n",
      "epoch: 1171, train_loss: 0.000797329435084501, test_loss: 0.0007304374739760533\n",
      "epoch: 1172, train_loss: 0.0007971401538942819, test_loss: 0.0007267220935318619\n",
      "epoch: 1173, train_loss: 0.0007988943809481418, test_loss: 0.000738469990513598\n",
      "epoch: 1174, train_loss: 0.0007981202924502609, test_loss: 0.0007290204375749454\n",
      "epoch: 1175, train_loss: 0.0007961121737025678, test_loss: 0.0007335396124593293\n",
      "epoch: 1176, train_loss: 0.0007987250320856338, test_loss: 0.0007260934604952732\n",
      "epoch: 1177, train_loss: 0.0007973691124631011, test_loss: 0.0007478872139472514\n",
      "epoch: 1178, train_loss: 0.0007972661785416952, test_loss: 0.000730764601030387\n",
      "epoch: 1179, train_loss: 0.0007983811285711178, test_loss: 0.0007236215848630915\n",
      "epoch: 1180, train_loss: 0.0007960797824046534, test_loss: 0.0007326795748667791\n",
      "epoch: 1181, train_loss: 0.0007950182417002709, test_loss: 0.0007319208283054953\n",
      "epoch: 1182, train_loss: 0.0007965919111207452, test_loss: 0.0007375924663695818\n",
      "epoch: 1183, train_loss: 0.0007957586431470902, test_loss: 0.0007305046844218547\n",
      "epoch: 1184, train_loss: 0.0007960008275087761, test_loss: 0.0007341481347490723\n",
      "epoch: 1185, train_loss: 0.0007960181911071033, test_loss: 0.0007338730453435952\n",
      "epoch: 1186, train_loss: 0.000794166058767587, test_loss: 0.0007201186866344264\n",
      "epoch: 1187, train_loss: 0.0007950600803546283, test_loss: 0.0007263346875940139\n",
      "epoch: 1188, train_loss: 0.0007933934836688896, test_loss: 0.0007337967205482224\n",
      "epoch: 1189, train_loss: 0.0007936243010867063, test_loss: 0.0007333924004342407\n",
      "epoch: 1190, train_loss: 0.0007933505669074214, test_loss: 0.00073042961594183\n",
      "epoch: 1191, train_loss: 0.000793979923321825, test_loss: 0.0007309889673100164\n",
      "epoch: 1192, train_loss: 0.0007937207195462416, test_loss: 0.0007254255518394833\n",
      "epoch: 1193, train_loss: 0.0007924979562749681, test_loss: 0.0007227392294832194\n",
      "epoch: 1194, train_loss: 0.0007934730181106082, test_loss: 0.0007341601012740284\n",
      "epoch: 1195, train_loss: 0.0007929786579157023, test_loss: 0.0007299817370949313\n",
      "epoch: 1196, train_loss: 0.0007940474897623062, test_loss: 0.0007282334457462033\n",
      "epoch: 1197, train_loss: 0.0007922009681351483, test_loss: 0.0007294314515699322\n",
      "epoch: 1198, train_loss: 0.000791512842229365, test_loss: 0.0007255414141885316\n",
      "epoch: 1199, train_loss: 0.0007922219659161309, test_loss: 0.0007250428316183388\n",
      "epoch: 1200, train_loss: 0.0007912417919294018, test_loss: 0.0007294774453233307\n",
      "epoch: 1201, train_loss: 0.0007915057636716443, test_loss: 0.0007253345102071762\n",
      "epoch: 1202, train_loss: 0.0007917338314101747, test_loss: 0.000724467730227237\n",
      "epoch: 1203, train_loss: 0.0007924484418016737, test_loss: 0.0007306670934970801\n",
      "epoch: 1204, train_loss: 0.0007906798475786396, test_loss: 0.000723599805496633\n",
      "epoch: 1205, train_loss: 0.0007911657955011596, test_loss: 0.000726911734091118\n",
      "epoch: 1206, train_loss: 0.0007899314818053466, test_loss: 0.0007258027762873098\n",
      "epoch: 1207, train_loss: 0.0007903697399382034, test_loss: 0.0007235290007277703\n",
      "epoch: 1208, train_loss: 0.0007906772750531039, test_loss: 0.0007182740567562481\n",
      "epoch: 1209, train_loss: 0.0007895463267746179, test_loss: 0.0007280458909614632\n",
      "epoch: 1210, train_loss: 0.000789812213042751, test_loss: 0.0007221377018140629\n",
      "epoch: 1211, train_loss: 0.0007903201153765068, test_loss: 0.0007252896466525272\n",
      "epoch: 1212, train_loss: 0.0007891731549297338, test_loss: 0.0007157019329800581\n",
      "epoch: 1213, train_loss: 0.000787681945518631, test_loss: 0.0007192581397248432\n",
      "epoch: 1214, train_loss: 0.0007883755011868703, test_loss: 0.0007254070175501207\n",
      "epoch: 1215, train_loss: 0.0007884262974940888, test_loss: 0.0007321371861811107\n",
      "epoch: 1216, train_loss: 0.0007892808947793168, test_loss: 0.0007159745049042007\n",
      "epoch: 1217, train_loss: 0.0007870634992444968, test_loss: 0.0007216928497655317\n",
      "epoch: 1218, train_loss: 0.0007860827912657481, test_loss: 0.0007269488657281423\n",
      "epoch: 1219, train_loss: 0.0007883574555466032, test_loss: 0.0007173355746393403\n",
      "epoch: 1220, train_loss: 0.0007876846863402296, test_loss: 0.000714393681846559\n",
      "epoch: 1221, train_loss: 0.00078685256227365, test_loss: 0.0007323116830472524\n",
      "epoch: 1222, train_loss: 0.000786883698310703, test_loss: 0.0007179595510630558\n",
      "epoch: 1223, train_loss: 0.0007868210289060422, test_loss: 0.0007161518539457271\n",
      "epoch: 1224, train_loss: 0.0007864425158755773, test_loss: 0.0007231975614558905\n",
      "epoch: 1225, train_loss: 0.0007858693260578034, test_loss: 0.00072189611576808\n",
      "epoch: 1226, train_loss: 0.0007873657501160936, test_loss: 0.0007164518028730527\n",
      "epoch: 1227, train_loss: 0.0007855290339490318, test_loss: 0.0007237123257558172\n",
      "epoch: 1228, train_loss: 0.0007857871748021115, test_loss: 0.0007386643652959416\n",
      "epoch: 1229, train_loss: 0.0007873356048746601, test_loss: 0.0007212917068197081\n",
      "epoch: 1230, train_loss: 0.0007851994166191181, test_loss: 0.000716523359490869\n",
      "epoch: 1231, train_loss: 0.0007856590053289319, test_loss: 0.0007224052969831973\n",
      "epoch: 1232, train_loss: 0.0007839859513889836, test_loss: 0.0007223277401256686\n",
      "epoch: 1233, train_loss: 0.0007830746952727761, test_loss: 0.0007099941625104597\n",
      "epoch: 1234, train_loss: 0.0007832200499251485, test_loss: 0.0007199093524832278\n",
      "epoch: 1235, train_loss: 0.000783852105403481, test_loss: 0.0007217255527696883\n",
      "epoch: 1236, train_loss: 0.0007842401265288177, test_loss: 0.0007159167580539361\n",
      "epoch: 1237, train_loss: 0.0007849880747254128, test_loss: 0.0007146371547908833\n",
      "epoch: 1238, train_loss: 0.0007826256370880519, test_loss: 0.0007140077359508723\n",
      "epoch: 1239, train_loss: 0.0007831289093819973, test_loss: 0.0007203455509928366\n",
      "epoch: 1240, train_loss: 0.0007831100500998614, test_loss: 0.0007136660860851407\n",
      "epoch: 1241, train_loss: 0.0007827599512656097, test_loss: 0.0007225510635180399\n",
      "epoch: 1242, train_loss: 0.0007822802391551111, test_loss: 0.0007246809739929935\n",
      "epoch: 1243, train_loss: 0.0007839467422024387, test_loss: 0.0007123388301503534\n",
      "epoch: 1244, train_loss: 0.000781949819571784, test_loss: 0.0007180730705537522\n",
      "epoch: 1245, train_loss: 0.000782399878675199, test_loss: 0.0007138683722587302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1246, train_loss: 0.000781120494534464, test_loss: 0.0007165808347053826\n",
      "epoch: 1247, train_loss: 0.000783487870965315, test_loss: 0.0007111610563394303\n",
      "epoch: 1248, train_loss: 0.0007816465576584249, test_loss: 0.0007141528913052753\n",
      "epoch: 1249, train_loss: 0.0007820524801702603, test_loss: 0.0007173158325410137\n",
      "epoch: 1250, train_loss: 0.0007809903978815545, test_loss: 0.000707632532188048\n",
      "epoch: 1251, train_loss: 0.0007795404880712538, test_loss: 0.0007098726055119187\n",
      "epoch: 1252, train_loss: 0.0007803615058630543, test_loss: 0.0007122554331241796\n",
      "epoch: 1253, train_loss: 0.0007796184205369133, test_loss: 0.0007125200694038843\n",
      "epoch: 1254, train_loss: 0.0007798327816366825, test_loss: 0.0007095348749620219\n",
      "epoch: 1255, train_loss: 0.0007789312727729101, test_loss: 0.0007117244579906886\n",
      "epoch: 1256, train_loss: 0.0007790495292283595, test_loss: 0.0007186698300453523\n",
      "epoch: 1257, train_loss: 0.000779238332103452, test_loss: 0.0007284706807695329\n",
      "epoch: 1258, train_loss: 0.0007811292180912974, test_loss: 0.0007149030134314671\n",
      "epoch: 1259, train_loss: 0.0007786065942364867, test_loss: 0.0007081064347100133\n",
      "epoch: 1260, train_loss: 0.0007790843450018893, test_loss: 0.0007276725809788331\n",
      "epoch: 1261, train_loss: 0.0007775286819947803, test_loss: 0.0007182146558382859\n",
      "epoch: 1262, train_loss: 0.0007777705980950724, test_loss: 0.0007188125309767202\n",
      "epoch: 1263, train_loss: 0.0007768801752599361, test_loss: 0.0007178257316506157\n",
      "epoch: 1264, train_loss: 0.0007786095729502647, test_loss: 0.0007094901811797172\n",
      "epoch: 1265, train_loss: 0.0007782756609072828, test_loss: 0.0007140523133178552\n",
      "epoch: 1266, train_loss: 0.0007759567992964193, test_loss: 0.0007129342702683061\n",
      "epoch: 1267, train_loss: 0.000778097517626441, test_loss: 0.0007133470644475892\n",
      "epoch: 1268, train_loss: 0.0007773112561351255, test_loss: 0.000714037193877933\n",
      "epoch: 1269, train_loss: 0.0007768772016076938, test_loss: 0.0007137907135377949\n",
      "epoch: 1270, train_loss: 0.0007771619484233468, test_loss: 0.0007089433444586272\n",
      "epoch: 1271, train_loss: 0.0007774779527529103, test_loss: 0.000717571199250718\n",
      "epoch: 1272, train_loss: 0.0007769865662102466, test_loss: 0.0007170965448797991\n",
      "epoch: 1273, train_loss: 0.0007776776480528971, test_loss: 0.0007178911813146746\n",
      "epoch: 1274, train_loss: 0.00077662999873333, test_loss: 0.0007132889198449751\n",
      "epoch: 1275, train_loss: 0.0007776357005755215, test_loss: 0.0007047029891206572\n",
      "epoch: 1276, train_loss: 0.000775521302231304, test_loss: 0.000709681868708382\n",
      "epoch: 1277, train_loss: 0.0007743824478340051, test_loss: 0.0007105455345784625\n",
      "epoch: 1278, train_loss: 0.0007754096953684221, test_loss: 0.0007040071553395441\n",
      "epoch: 1279, train_loss: 0.0007734922464410572, test_loss: 0.0007087661603388066\n",
      "epoch: 1280, train_loss: 0.000774894651207749, test_loss: 0.0007099885454711815\n",
      "epoch: 1281, train_loss: 0.0007740082067396978, test_loss: 0.0007097185249828423\n",
      "epoch: 1282, train_loss: 0.0007742976304143667, test_loss: 0.000726435479009524\n",
      "epoch: 1283, train_loss: 0.000774074641926943, test_loss: 0.0007055421301629394\n",
      "epoch: 1284, train_loss: 0.0007730433945139141, test_loss: 0.0007058478028435881\n",
      "epoch: 1285, train_loss: 0.0007743774280559434, test_loss: 0.0007088557419289524\n",
      "epoch: 1286, train_loss: 0.0007751004481890603, test_loss: 0.0007057025019700328\n",
      "epoch: 1287, train_loss: 0.0007720411851313775, test_loss: 0.0007105049589881673\n",
      "epoch: 1288, train_loss: 0.0007732689441384181, test_loss: 0.0007044361991574988\n",
      "epoch: 1289, train_loss: 0.0007728046216273113, test_loss: 0.0007044105683841432\n",
      "epoch: 1290, train_loss: 0.0007739051943644881, test_loss: 0.0007014381505238513\n",
      "epoch: 1291, train_loss: 0.0007730139249875008, test_loss: 0.0007117370647999147\n",
      "epoch: 1292, train_loss: 0.0007713630684125035, test_loss: 0.0007066069229040295\n",
      "epoch: 1293, train_loss: 0.0007724641194648069, test_loss: 0.0007089586967291931\n",
      "epoch: 1294, train_loss: 0.0007726007201911315, test_loss: 0.0007024238293524832\n",
      "epoch: 1295, train_loss: 0.0007713457326526227, test_loss: 0.0007056040485622361\n",
      "epoch: 1296, train_loss: 0.0007713909220436345, test_loss: 0.0007020168607899299\n",
      "epoch: 1297, train_loss: 0.0007706374466500205, test_loss: 0.0007019684126134962\n",
      "epoch: 1298, train_loss: 0.0007716962389405007, test_loss: 0.0007056101118602479\n",
      "epoch: 1299, train_loss: 0.0007715086558955195, test_loss: 0.0007252526508333782\n",
      "epoch: 1300, train_loss: 0.000771559170022121, test_loss: 0.0007064841241420557\n",
      "epoch: 1301, train_loss: 0.0007698905664374647, test_loss: 0.0007029419066384435\n",
      "epoch: 1302, train_loss: 0.0007689848476413475, test_loss: 0.0007036392441174636\n",
      "epoch: 1303, train_loss: 0.0007705004827585071, test_loss: 0.0007015458638003717\n",
      "epoch: 1304, train_loss: 0.0007715917916199112, test_loss: 0.0007026820336856568\n",
      "epoch: 1305, train_loss: 0.0007702021267386558, test_loss: 0.0007034711015876383\n",
      "epoch: 1306, train_loss: 0.00076937362673166, test_loss: 0.0006975842746517932\n",
      "epoch: 1307, train_loss: 0.0007694395507037964, test_loss: 0.0007038396676459039\n",
      "epoch: 1308, train_loss: 0.0007703079786354109, test_loss: 0.0007088420000703385\n",
      "epoch: 1309, train_loss: 0.000770723166288403, test_loss: 0.0007096160697983578\n",
      "epoch: 1310, train_loss: 0.0007691544597037137, test_loss: 0.0006965784026154628\n",
      "epoch: 1311, train_loss: 0.0007682794732871749, test_loss: 0.0007083434708571682\n",
      "epoch: 1312, train_loss: 0.0007701544521335999, test_loss: 0.0007030322934345653\n",
      "epoch: 1313, train_loss: 0.0007678021059331039, test_loss: 0.0007052660269740348\n",
      "epoch: 1314, train_loss: 0.0007683000522260757, test_loss: 0.0006983024407721435\n",
      "epoch: 1315, train_loss: 0.0007701651952431902, test_loss: 0.0007035123417153955\n",
      "epoch: 1316, train_loss: 0.0007696709185636238, test_loss: 0.0007062376971589401\n",
      "epoch: 1317, train_loss: 0.0007672750267058449, test_loss: 0.000702300516422838\n",
      "epoch: 1318, train_loss: 0.0007671874305030898, test_loss: 0.0007026968378340825\n",
      "epoch: 1319, train_loss: 0.0007670654538188777, test_loss: 0.0007058937529412409\n",
      "epoch: 1320, train_loss: 0.0007674366655840498, test_loss: 0.0007081584675082316\n",
      "epoch: 1321, train_loss: 0.0007678155746796857, test_loss: 0.0006998381286393851\n",
      "epoch: 1322, train_loss: 0.0007661648054161798, test_loss: 0.0007067160622682422\n",
      "epoch: 1323, train_loss: 0.0007682011903101659, test_loss: 0.0006968944071559235\n",
      "epoch: 1324, train_loss: 0.0007651636285869324, test_loss: 0.0007051202216340849\n",
      "epoch: 1325, train_loss: 0.0007669572166734091, test_loss: 0.0006956239133918037\n",
      "epoch: 1326, train_loss: 0.000765320576686898, test_loss: 0.0006974950277556976\n",
      "epoch: 1327, train_loss: 0.0007656457246807606, test_loss: 0.0006953071230479205\n",
      "epoch: 1328, train_loss: 0.0007657833199988564, test_loss: 0.0007026501746925836\n",
      "epoch: 1329, train_loss: 0.0007666359660620599, test_loss: 0.000692813356484597\n",
      "epoch: 1330, train_loss: 0.000764785293975602, test_loss: 0.0006937757182943945\n",
      "epoch: 1331, train_loss: 0.0007668565174200288, test_loss: 0.0007002546578102434\n",
      "epoch: 1332, train_loss: 0.0007661582254197286, test_loss: 0.0007062899579371636\n",
      "epoch: 1333, train_loss: 0.0007649224894323751, test_loss: 0.0007023157571287205\n",
      "epoch: 1334, train_loss: 0.0007660519357001328, test_loss: 0.000694989925250411\n",
      "epoch: 1335, train_loss: 0.0007638271629769841, test_loss: 0.0006972289653883005\n",
      "epoch: 1336, train_loss: 0.0007641986113689516, test_loss: 0.000705119843284289\n",
      "epoch: 1337, train_loss: 0.000763657682514547, test_loss: 0.0006939458350340525\n",
      "epoch: 1338, train_loss: 0.0007656529158576513, test_loss: 0.0006920506226985405\n",
      "epoch: 1339, train_loss: 0.0007642565153377211, test_loss: 0.0006960030489911636\n",
      "epoch: 1340, train_loss: 0.0007634163889831499, test_loss: 0.0006997277572130164\n",
      "epoch: 1341, train_loss: 0.0007625328726135194, test_loss: 0.000697588514109763\n",
      "epoch: 1342, train_loss: 0.0007622632066435788, test_loss: 0.0007050504112460961\n",
      "epoch: 1343, train_loss: 0.000763992515757032, test_loss: 0.0006947829630613948\n",
      "epoch: 1344, train_loss: 0.0007621559122091402, test_loss: 0.0006955741797961915\n",
      "epoch: 1345, train_loss: 0.0007619378574536709, test_loss: 0.0006989850905180598\n",
      "epoch: 1346, train_loss: 0.0007611780855096067, test_loss: 0.0006946880651715522\n",
      "epoch: 1347, train_loss: 0.0007610831045262191, test_loss: 0.0006921400393669804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1348, train_loss: 0.0007627377610491669, test_loss: 0.0006909776226772616\n",
      "epoch: 1349, train_loss: 0.0007621121704173477, test_loss: 0.0007016064967804899\n",
      "epoch: 1350, train_loss: 0.000762355648001413, test_loss: 0.0006951887383668994\n",
      "epoch: 1351, train_loss: 0.000761692974028056, test_loss: 0.0007067428523441777\n",
      "epoch: 1352, train_loss: 0.0007609463987730281, test_loss: 0.0007056273267759631\n",
      "epoch: 1353, train_loss: 0.0007608150342054179, test_loss: 0.0006945034401724115\n",
      "epoch: 1354, train_loss: 0.0007595867475332773, test_loss: 0.0006898523812803129\n",
      "epoch: 1355, train_loss: 0.0007595970793930894, test_loss: 0.000689285239786841\n",
      "epoch: 1356, train_loss: 0.0007610243223810002, test_loss: 0.0007002793572610244\n",
      "epoch: 1357, train_loss: 0.0007603016666784558, test_loss: 0.0006913068985644107\n",
      "epoch: 1358, train_loss: 0.0007613890465227482, test_loss: 0.0006944313742375622\n",
      "epoch: 1359, train_loss: 0.0007607352454215288, test_loss: 0.0006902639336961632\n",
      "epoch: 1360, train_loss: 0.0007599296792329329, test_loss: 0.0006920973343464235\n",
      "epoch: 1361, train_loss: 0.000759323865286601, test_loss: 0.0007022258165913323\n",
      "epoch: 1362, train_loss: 0.0007596753016316696, test_loss: 0.0006942109272737677\n",
      "epoch: 1363, train_loss: 0.0007600445495209779, test_loss: 0.0006900064375561973\n",
      "epoch: 1364, train_loss: 0.0007600128453265389, test_loss: 0.0006975597255708029\n",
      "epoch: 1365, train_loss: 0.0007601828521886921, test_loss: 0.0007072603330016136\n",
      "epoch: 1366, train_loss: 0.0007588707401579165, test_loss: 0.0006892578336798275\n",
      "epoch: 1367, train_loss: 0.0007580851113585675, test_loss: 0.0006925646788052594\n",
      "epoch: 1368, train_loss: 0.0007573878746884673, test_loss: 0.000693994695514751\n",
      "epoch: 1369, train_loss: 0.0007571643724551667, test_loss: 0.000698717893101275\n",
      "epoch: 1370, train_loss: 0.0007576234942383093, test_loss: 0.0006918205375162264\n",
      "epoch: 1371, train_loss: 0.0007583569512581048, test_loss: 0.0006971626280574128\n",
      "epoch: 1372, train_loss: 0.0007584419217891991, test_loss: 0.0006951712760686254\n",
      "epoch: 1373, train_loss: 0.0007579615744559661, test_loss: 0.0006961849236783261\n",
      "epoch: 1374, train_loss: 0.0007569097063463668, test_loss: 0.0006990369874984026\n",
      "epoch: 1375, train_loss: 0.0007575021413883761, test_loss: 0.0006897056834228957\n",
      "epoch: 1376, train_loss: 0.0007576264856059266, test_loss: 0.0006920679831334079\n",
      "epoch: 1377, train_loss: 0.0007576455473494918, test_loss: 0.000718200666597113\n",
      "epoch: 1378, train_loss: 0.0007577415216592667, test_loss: 0.0006870373181300238\n",
      "epoch: 1379, train_loss: 0.0007567629369947573, test_loss: 0.0006918343133293092\n",
      "epoch: 1380, train_loss: 0.0007558804784860948, test_loss: 0.0006847641586015621\n",
      "epoch: 1381, train_loss: 0.0007565684854457884, test_loss: 0.000685635008267127\n",
      "epoch: 1382, train_loss: 0.0007560363839096997, test_loss: 0.0006844522237467269\n",
      "epoch: 1383, train_loss: 0.0007575106321145659, test_loss: 0.000690472525699685\n",
      "epoch: 1384, train_loss: 0.0007558211674103918, test_loss: 0.0006900631997268647\n",
      "epoch: 1385, train_loss: 0.0007549478209314301, test_loss: 0.0006925807538209483\n",
      "epoch: 1386, train_loss: 0.0007540200654741215, test_loss: 0.0006903793255332857\n",
      "epoch: 1387, train_loss: 0.0007565577144977515, test_loss: 0.0006892867046796406\n",
      "epoch: 1388, train_loss: 0.0007551059888615071, test_loss: 0.0006889534512689958\n",
      "epoch: 1389, train_loss: 0.0007543488286698804, test_loss: 0.0006986277342851585\n",
      "epoch: 1390, train_loss: 0.000754746960212841, test_loss: 0.0006911605014465749\n",
      "epoch: 1391, train_loss: 0.0007545812214906935, test_loss: 0.0006891698916054642\n",
      "epoch: 1392, train_loss: 0.0007548052437967905, test_loss: 0.0006893277992882455\n",
      "epoch: 1393, train_loss: 0.000753951033719046, test_loss: 0.0006872174805418277\n",
      "epoch: 1394, train_loss: 0.0007540069687504159, test_loss: 0.0006842652122334888\n",
      "epoch: 1395, train_loss: 0.000754658269452984, test_loss: 0.0006851285046044117\n",
      "epoch: 1396, train_loss: 0.0007535994138933071, test_loss: 0.000685033582461377\n",
      "epoch: 1397, train_loss: 0.0007545735975524977, test_loss: 0.0006972266807376096\n",
      "epoch: 1398, train_loss: 0.0007531566838936313, test_loss: 0.00068388253566809\n",
      "epoch: 1399, train_loss: 0.0007542891708789798, test_loss: 0.000692262445227243\n",
      "epoch: 1400, train_loss: 0.0007536754368946118, test_loss: 0.0006868272951881712\n",
      "epoch: 1401, train_loss: 0.000753486809907886, test_loss: 0.0006821631832281128\n",
      "epoch: 1402, train_loss: 0.0007518860562097119, test_loss: 0.0006912131163214023\n",
      "epoch: 1403, train_loss: 0.0007515856338178982, test_loss: 0.0006850674884238591\n",
      "epoch: 1404, train_loss: 0.0007525975006106107, test_loss: 0.0006834917730884627\n",
      "epoch: 1405, train_loss: 0.0007524200570870838, test_loss: 0.000698921998264268\n",
      "epoch: 1406, train_loss: 0.000751796532827227, test_loss: 0.0006809486173248539\n",
      "epoch: 1407, train_loss: 0.000751006303091898, test_loss: 0.0006841415452072397\n",
      "epoch: 1408, train_loss: 0.0007517910144878956, test_loss: 0.000689357562805526\n",
      "epoch: 1409, train_loss: 0.0007500767672393957, test_loss: 0.0006857493378144378\n",
      "epoch: 1410, train_loss: 0.0007516955678432208, test_loss: 0.0006902191477517287\n",
      "epoch: 1411, train_loss: 0.0007505261455662549, test_loss: 0.0006778349197702482\n",
      "epoch: 1412, train_loss: 0.0007507828729854816, test_loss: 0.0006958565015035371\n",
      "epoch: 1413, train_loss: 0.0007522122089184173, test_loss: 0.0006853201775811613\n",
      "epoch: 1414, train_loss: 0.0007498619468052588, test_loss: 0.000680395751260221\n",
      "epoch: 1415, train_loss: 0.0007512590913709415, test_loss: 0.0006932984203255425\n",
      "epoch: 1416, train_loss: 0.000751131900039542, test_loss: 0.0006833905354142189\n",
      "epoch: 1417, train_loss: 0.0007496452029875439, test_loss: 0.0006792458249644066\n",
      "epoch: 1418, train_loss: 0.0007504302927333375, test_loss: 0.0006830261700088158\n",
      "epoch: 1419, train_loss: 0.0007518924185601265, test_loss: 0.0006826872219486783\n",
      "epoch: 1420, train_loss: 0.0007486874617271773, test_loss: 0.000677763896722657\n",
      "epoch: 1421, train_loss: 0.0007499741648485804, test_loss: 0.0006777350790798664\n",
      "epoch: 1422, train_loss: 0.0007485369202660639, test_loss: 0.0006926093628862873\n",
      "epoch: 1423, train_loss: 0.0007482669063155417, test_loss: 0.0006841708721670633\n",
      "epoch: 1424, train_loss: 0.0007494341951551969, test_loss: 0.0006804720566530401\n",
      "epoch: 1425, train_loss: 0.000748412734221505, test_loss: 0.0006798244042632481\n",
      "epoch: 1426, train_loss: 0.0007481222666050915, test_loss: 0.0006842820585006848\n",
      "epoch: 1427, train_loss: 0.0007501767604085414, test_loss: 0.0006880762957734987\n",
      "epoch: 1428, train_loss: 0.0007485449010425288, test_loss: 0.0006800135209535559\n",
      "epoch: 1429, train_loss: 0.0007475093260164494, test_loss: 0.0006958680654255053\n",
      "epoch: 1430, train_loss: 0.0007464489469345173, test_loss: 0.0006901448020168269\n",
      "epoch: 1431, train_loss: 0.0007484367334931765, test_loss: 0.0006854236174452429\n",
      "epoch: 1432, train_loss: 0.0007457212031231788, test_loss: 0.0006821935773283864\n",
      "epoch: 1433, train_loss: 0.0007465444112945672, test_loss: 0.0006816485256422311\n",
      "epoch: 1434, train_loss: 0.000747392957513828, test_loss: 0.00067880126395418\n",
      "epoch: 1435, train_loss: 0.0007458090599950241, test_loss: 0.0006781092330735797\n",
      "epoch: 1436, train_loss: 0.000747392561448657, test_loss: 0.0006785692870228862\n",
      "epoch: 1437, train_loss: 0.0007470383855473736, test_loss: 0.0006776926214418685\n",
      "epoch: 1438, train_loss: 0.0007452913939588419, test_loss: 0.0006767428518893818\n",
      "epoch: 1439, train_loss: 0.0007461632725924416, test_loss: 0.0006830303027527407\n",
      "epoch: 1440, train_loss: 0.0007460401950242078, test_loss: 0.0006821331286725277\n",
      "epoch: 1441, train_loss: 0.0007466493128879887, test_loss: 0.0006762275734217837\n",
      "epoch: 1442, train_loss: 0.000747780973577629, test_loss: 0.0006780630986516675\n",
      "epoch: 1443, train_loss: 0.000746785296106954, test_loss: 0.000677950088478004\n",
      "epoch: 1444, train_loss: 0.0007465794839410354, test_loss: 0.0006735855567967519\n",
      "epoch: 1445, train_loss: 0.0007450642107236806, test_loss: 0.00068570242729038\n",
      "epoch: 1446, train_loss: 0.000746003757028476, test_loss: 0.000679677376562419\n",
      "epoch: 1447, train_loss: 0.0007444988270593888, test_loss: 0.0006784781580790877\n",
      "epoch: 1448, train_loss: 0.0007454842511240555, test_loss: 0.0006787451032626753\n",
      "epoch: 1449, train_loss: 0.0007444905463869319, test_loss: 0.0006794270593672991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1450, train_loss: 0.0007437263595183258, test_loss: 0.0006752887566108257\n",
      "epoch: 1451, train_loss: 0.0007446233319806988, test_loss: 0.000677557754291532\n",
      "epoch: 1452, train_loss: 0.0007437172158540267, test_loss: 0.0006922710793636119\n",
      "epoch: 1453, train_loss: 0.000744418956025544, test_loss: 0.000679556493802617\n",
      "epoch: 1454, train_loss: 0.0007438211317133645, test_loss: 0.0006737747850517432\n",
      "epoch: 1455, train_loss: 0.0007428899483309816, test_loss: 0.0006793031934648752\n",
      "epoch: 1456, train_loss: 0.0007429276972644678, test_loss: 0.0006842441410602381\n",
      "epoch: 1457, train_loss: 0.0007435490349408886, test_loss: 0.0006788156557983408\n",
      "epoch: 1458, train_loss: 0.0007435148328785663, test_loss: 0.0006861859340763962\n",
      "epoch: 1459, train_loss: 0.0007426986475880055, test_loss: 0.0006753161093608165\n",
      "epoch: 1460, train_loss: 0.0007416165102293472, test_loss: 0.0006742898888963585\n",
      "epoch: 1461, train_loss: 0.0007423939633831058, test_loss: 0.0006730446378545215\n",
      "epoch: 1462, train_loss: 0.0007422020071712525, test_loss: 0.0006735703257921463\n",
      "epoch: 1463, train_loss: 0.0007422875116943666, test_loss: 0.0006869953358545899\n",
      "epoch: 1464, train_loss: 0.000743737651804543, test_loss: 0.0006832385503609354\n",
      "epoch: 1465, train_loss: 0.0007424394235662792, test_loss: 0.0006735533194538826\n",
      "epoch: 1466, train_loss: 0.000742860162458585, test_loss: 0.0006750845398831492\n",
      "epoch: 1467, train_loss: 0.0007425466724468962, test_loss: 0.0006784974296654885\n",
      "epoch: 1468, train_loss: 0.0007416499581228455, test_loss: 0.000675890221221683\n",
      "epoch: 1469, train_loss: 0.0007413301614376352, test_loss: 0.0006742417366088679\n",
      "epoch: 1470, train_loss: 0.0007437736861428003, test_loss: 0.0006732632900821045\n",
      "epoch: 1471, train_loss: 0.0007400502648164073, test_loss: 0.0006831025869663184\n",
      "epoch: 1472, train_loss: 0.0007421826645124542, test_loss: 0.0006802325757841269\n",
      "epoch: 1473, train_loss: 0.0007413175949097975, test_loss: 0.0006818959906619663\n",
      "epoch: 1474, train_loss: 0.0007412878862258209, test_loss: 0.0006781742752840122\n",
      "epoch: 1475, train_loss: 0.0007412436028497051, test_loss: 0.0006871417135698721\n",
      "epoch: 1476, train_loss: 0.0007427274148263361, test_loss: 0.0006718732911394909\n",
      "epoch: 1477, train_loss: 0.0007401490470131292, test_loss: 0.0006701249464337403\n",
      "epoch: 1478, train_loss: 0.0007409240238105311, test_loss: 0.0006770971231162548\n",
      "epoch: 1479, train_loss: 0.0007402285080625797, test_loss: 0.0006753470176287616\n",
      "epoch: 1480, train_loss: 0.0007400838278598436, test_loss: 0.0006698022916680202\n",
      "epoch: 1481, train_loss: 0.0007402190240100026, test_loss: 0.0006789762798386315\n",
      "epoch: 1482, train_loss: 0.0007410476809246061, test_loss: 0.0006701822712784633\n",
      "epoch: 1483, train_loss: 0.0007390382714853015, test_loss: 0.0006728493268989647\n",
      "epoch: 1484, train_loss: 0.0007385596956895745, test_loss: 0.0006750337100432565\n",
      "epoch: 1485, train_loss: 0.0007398941189698551, test_loss: 0.0006769579097939035\n",
      "epoch: 1486, train_loss: 0.0007396099820692579, test_loss: 0.0006686109506214658\n",
      "epoch: 1487, train_loss: 0.0007391945464010148, test_loss: 0.0006707202264806256\n",
      "epoch: 1488, train_loss: 0.0007407020767340841, test_loss: 0.0006681805437741181\n",
      "epoch: 1489, train_loss: 0.0007415336199894385, test_loss: 0.0006705366443687429\n",
      "epoch: 1490, train_loss: 0.000739286233590025, test_loss: 0.0006733452367673939\n",
      "epoch: 1491, train_loss: 0.0007376161279946404, test_loss: 0.00067153465837085\n",
      "epoch: 1492, train_loss: 0.0007380051588963555, test_loss: 0.000668328910251148\n",
      "epoch: 1493, train_loss: 0.0007391282516713861, test_loss: 0.0006700269150314853\n",
      "epoch: 1494, train_loss: 0.0007383489257494068, test_loss: 0.000668776638728256\n",
      "epoch: 1495, train_loss: 0.0007375533333169701, test_loss: 0.0006779416483671715\n",
      "epoch: 1496, train_loss: 0.0007366404068939712, test_loss: 0.0006740572328756874\n",
      "epoch: 1497, train_loss: 0.0007382723320599483, test_loss: 0.0006675560192282622\n",
      "epoch: 1498, train_loss: 0.0007379794614794462, test_loss: 0.000665793830800491\n",
      "epoch: 1499, train_loss: 0.0007371463225749524, test_loss: 0.0006743264724112427\n",
      "epoch: 1500, train_loss: 0.0007370932928650924, test_loss: 0.0006693880811023215\n",
      "epoch: 1501, train_loss: 0.0007382385387166363, test_loss: 0.000672041904181242\n",
      "epoch: 1502, train_loss: 0.0007362495880047588, test_loss: 0.0006739640327092881\n",
      "epoch: 1503, train_loss: 0.0007370928018961264, test_loss: 0.0006703196316569423\n",
      "epoch: 1504, train_loss: 0.0007366170162719715, test_loss: 0.0006732769931356112\n",
      "epoch: 1505, train_loss: 0.0007356669900574437, test_loss: 0.0006789515561346585\n",
      "epoch: 1506, train_loss: 0.0007375250950091234, test_loss: 0.0006721871808016052\n",
      "epoch: 1507, train_loss: 0.0007356379988461571, test_loss: 0.0006675338566613694\n",
      "epoch: 1508, train_loss: 0.0007375144721110067, test_loss: 0.0006773382240983968\n",
      "epoch: 1509, train_loss: 0.0007365185504212328, test_loss: 0.0006784866030405586\n",
      "epoch: 1510, train_loss: 0.0007376209908650945, test_loss: 0.0006662891537416726\n",
      "epoch: 1511, train_loss: 0.0007349424679135984, test_loss: 0.0006762337385832021\n",
      "epoch: 1512, train_loss: 0.0007375690316700417, test_loss: 0.0006668601417914033\n",
      "epoch: 1513, train_loss: 0.0007345449437549257, test_loss: 0.0006752228218829259\n",
      "epoch: 1514, train_loss: 0.0007361366195887651, test_loss: 0.0006766646887020519\n",
      "epoch: 1515, train_loss: 0.0007354634808902831, test_loss: 0.0006636813923250884\n",
      "epoch: 1516, train_loss: 0.0007349453302120547, test_loss: 0.0006640698702540249\n",
      "epoch: 1517, train_loss: 0.0007346095061739502, test_loss: 0.0006654065170247728\n",
      "epoch: 1518, train_loss: 0.0007361475702213204, test_loss: 0.0006634180899709463\n",
      "epoch: 1519, train_loss: 0.0007342769607456158, test_loss: 0.0006805229252980401\n",
      "epoch: 1520, train_loss: 0.0007344287195060726, test_loss: 0.0006839211418991908\n",
      "epoch: 1521, train_loss: 0.0007347838431799217, test_loss: 0.0006690801722773662\n",
      "epoch: 1522, train_loss: 0.0007343916779222047, test_loss: 0.0006674254545941949\n",
      "epoch: 1523, train_loss: 0.0007335370527985303, test_loss: 0.0006673159126269942\n",
      "epoch: 1524, train_loss: 0.0007348493116138422, test_loss: 0.0006666490177546317\n",
      "epoch: 1525, train_loss: 0.0007359981220519251, test_loss: 0.0006675087546076005\n",
      "epoch: 1526, train_loss: 0.0007324355753118415, test_loss: 0.0006651545457619553\n",
      "epoch: 1527, train_loss: 0.0007350438922319723, test_loss: 0.0006671415030723438\n",
      "epoch: 1528, train_loss: 0.0007325917831622064, test_loss: 0.000668597873300314\n",
      "epoch: 1529, train_loss: 0.0007326528936138619, test_loss: 0.0006611414767879372\n",
      "epoch: 1530, train_loss: 0.0007352359256322455, test_loss: 0.0006752723469010865\n",
      "epoch: 1531, train_loss: 0.0007339356057143406, test_loss: 0.0006646546874738609\n",
      "epoch: 1532, train_loss: 0.000731767104614688, test_loss: 0.000668589913402684\n",
      "epoch: 1533, train_loss: 0.0007324212549618729, test_loss: 0.0006722981391552215\n",
      "epoch: 1534, train_loss: 0.0007329365521993326, test_loss: 0.0006771769306700056\n",
      "epoch: 1535, train_loss: 0.000731040991883239, test_loss: 0.0006649101851508021\n",
      "epoch: 1536, train_loss: 0.0007350881072773558, test_loss: 0.0006645517763293659\n",
      "epoch: 1537, train_loss: 0.0007329647500148934, test_loss: 0.0006645753358801206\n",
      "epoch: 1538, train_loss: 0.0007313383495151672, test_loss: 0.0006653009865355367\n",
      "epoch: 1539, train_loss: 0.000732747279866801, test_loss: 0.0006669813980503628\n",
      "epoch: 1540, train_loss: 0.000732687686610481, test_loss: 0.0006625533375578622\n",
      "epoch: 1541, train_loss: 0.0007306766321720636, test_loss: 0.0006706938147544861\n",
      "epoch: 1542, train_loss: 0.0007307866130165918, test_loss: 0.0006699170092664039\n",
      "epoch: 1543, train_loss: 0.0007309443317353725, test_loss: 0.0006770114559913054\n",
      "epoch: 1544, train_loss: 0.0007315028367726052, test_loss: 0.0006635535003927847\n",
      "epoch: 1545, train_loss: 0.0007313686478680567, test_loss: 0.0006607018149225041\n",
      "epoch: 1546, train_loss: 0.0007302608649732302, test_loss: 0.0006636123289354146\n",
      "epoch: 1547, train_loss: 0.0007317572143738685, test_loss: 0.000658826373789149\n",
      "epoch: 1548, train_loss: 0.0007307310538042499, test_loss: 0.0006619647659438973\n",
      "epoch: 1549, train_loss: 0.0007311284514245293, test_loss: 0.0006700042528488362\n",
      "epoch: 1550, train_loss: 0.000730171818640488, test_loss: 0.0006604952601871142\n",
      "epoch: 1551, train_loss: 0.0007300633992797331, test_loss: 0.0006637703481828794\n",
      "epoch: 1552, train_loss: 0.0007317802924460367, test_loss: 0.0006714198874154439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1553, train_loss: 0.0007302078605710489, test_loss: 0.0006607931960994998\n",
      "epoch: 1554, train_loss: 0.0007298111994042183, test_loss: 0.0006714651293198889\n",
      "epoch: 1555, train_loss: 0.0007298087875824422, test_loss: 0.000670989965631937\n",
      "epoch: 1556, train_loss: 0.0007324237692797476, test_loss: 0.0006680056249024346\n",
      "epoch: 1557, train_loss: 0.0007307326456572374, test_loss: 0.0006656189652858302\n",
      "epoch: 1558, train_loss: 0.0007281343303317124, test_loss: 0.0006613900283506761\n",
      "epoch: 1559, train_loss: 0.0007286214411420667, test_loss: 0.0006626797839999199\n",
      "epoch: 1560, train_loss: 0.0007291645224145411, test_loss: 0.0006626463485493635\n",
      "epoch: 1561, train_loss: 0.000729981837692954, test_loss: 0.0006583075009984896\n",
      "epoch: 1562, train_loss: 0.0007299221558597586, test_loss: 0.0006604316440643743\n",
      "epoch: 1563, train_loss: 0.0007277565664085357, test_loss: 0.0006655872809157396\n",
      "epoch: 1564, train_loss: 0.0007280945150262636, test_loss: 0.0006618145562242717\n",
      "epoch: 1565, train_loss: 0.0007294745212319591, test_loss: 0.0006625833048019558\n",
      "epoch: 1566, train_loss: 0.0007289285953764034, test_loss: 0.0006580479287852844\n",
      "epoch: 1567, train_loss: 0.0007313704801439915, test_loss: 0.0006689938648681467\n",
      "epoch: 1568, train_loss: 0.0007270034731608694, test_loss: 0.0006614976834195355\n",
      "epoch: 1569, train_loss: 0.0007270924334475042, test_loss: 0.0006657044771903505\n",
      "epoch: 1570, train_loss: 0.0007273385873185875, test_loss: 0.0006624240486416966\n",
      "epoch: 1571, train_loss: 0.0007267881617577666, test_loss: 0.0006586385085635508\n",
      "epoch: 1572, train_loss: 0.000727746812829181, test_loss: 0.0006674119310143093\n",
      "epoch: 1573, train_loss: 0.0007260455863813505, test_loss: 0.0006681867962470278\n",
      "epoch: 1574, train_loss: 0.0007273256576255611, test_loss: 0.0006624729139730334\n",
      "epoch: 1575, train_loss: 0.0007259356561521797, test_loss: 0.000659421663537311\n",
      "epoch: 1576, train_loss: 0.0007268291083164513, test_loss: 0.0006570919940713793\n",
      "epoch: 1577, train_loss: 0.000727563015812927, test_loss: 0.0006715806327216948\n",
      "epoch: 1578, train_loss: 0.0007262586694433476, test_loss: 0.000656835938571021\n",
      "epoch: 1579, train_loss: 0.0007263074310132018, test_loss: 0.0006578254833584651\n",
      "epoch: 1580, train_loss: 0.0007258006257673159, test_loss: 0.0006665849117174124\n",
      "epoch: 1581, train_loss: 0.0007268842512174793, test_loss: 0.0006652394561873128\n",
      "epoch: 1582, train_loss: 0.0007253148157716445, test_loss: 0.0006582759621475512\n",
      "epoch: 1583, train_loss: 0.0007273779458204365, test_loss: 0.0006741309916833416\n",
      "epoch: 1584, train_loss: 0.0007267494270901965, test_loss: 0.0006699883815599605\n",
      "epoch: 1585, train_loss: 0.000725082751950654, test_loss: 0.000657907459147585\n",
      "epoch: 1586, train_loss: 0.0007250857585028785, test_loss: 0.0006600430206162855\n",
      "epoch: 1587, train_loss: 0.0007245318708516171, test_loss: 0.0006603316772573938\n",
      "epoch: 1588, train_loss: 0.0007241275123543228, test_loss: 0.0006581062237576892\n",
      "epoch: 1589, train_loss: 0.0007248940907985619, test_loss: 0.0006559874503485238\n",
      "epoch: 1590, train_loss: 0.0007244059714772131, test_loss: 0.000657903384611321\n",
      "epoch: 1591, train_loss: 0.000724605158092859, test_loss: 0.0006592577847186476\n",
      "epoch: 1592, train_loss: 0.000725806149168183, test_loss: 0.0006662203862409418\n",
      "epoch: 1593, train_loss: 0.0007267254151646857, test_loss: 0.0006654691484679157\n",
      "epoch: 1594, train_loss: 0.0007250272382921337, test_loss: 0.0006605406476107115\n",
      "epoch: 1595, train_loss: 0.0007242478693471006, test_loss: 0.0006636085842425624\n",
      "epoch: 1596, train_loss: 0.0007253635241953737, test_loss: 0.0006560636150728291\n",
      "epoch: 1597, train_loss: 0.000724025424974768, test_loss: 0.0006627724748492861\n",
      "epoch: 1598, train_loss: 0.0007239774517391039, test_loss: 0.0006686117170223346\n",
      "epoch: 1599, train_loss: 0.0007237895725942824, test_loss: 0.0006551235467971613\n",
      "epoch: 1600, train_loss: 0.0007246564795344096, test_loss: 0.0006651492537154505\n",
      "epoch: 1601, train_loss: 0.0007234250977331692, test_loss: 0.0006589600961888209\n",
      "epoch: 1602, train_loss: 0.0007237451424336304, test_loss: 0.0006564468834161138\n",
      "epoch: 1603, train_loss: 0.0007232227021038694, test_loss: 0.0006546862859977409\n",
      "epoch: 1604, train_loss: 0.0007245836149314014, test_loss: 0.000653840489879561\n",
      "epoch: 1605, train_loss: 0.0007231929795006695, test_loss: 0.0006589248029437537\n",
      "epoch: 1606, train_loss: 0.0007228450001845056, test_loss: 0.0006644064706051722\n",
      "epoch: 1607, train_loss: 0.0007228771333440976, test_loss: 0.0006600893975701183\n",
      "epoch: 1608, train_loss: 0.0007219654773665673, test_loss: 0.0006626279015714923\n",
      "epoch: 1609, train_loss: 0.0007223228812622635, test_loss: 0.0006529136832493047\n",
      "epoch: 1610, train_loss: 0.0007241898110014913, test_loss: 0.000666038686176762\n",
      "epoch: 1611, train_loss: 0.000724466509975331, test_loss: 0.000663427029697535\n",
      "epoch: 1612, train_loss: 0.0007237189894785051, test_loss: 0.0006708329650185382\n",
      "epoch: 1613, train_loss: 0.0007229916543861771, test_loss: 0.0006554924524001157\n",
      "epoch: 1614, train_loss: 0.0007209383614320794, test_loss: 0.0006597327786342552\n",
      "epoch: 1615, train_loss: 0.0007214725128901393, test_loss: 0.0006521633816494917\n",
      "epoch: 1616, train_loss: 0.0007210203633724672, test_loss: 0.0006671158917015418\n",
      "epoch: 1617, train_loss: 0.0007210982477535372, test_loss: 0.0006524058262584731\n",
      "epoch: 1618, train_loss: 0.0007212024976742332, test_loss: 0.0006637412783068916\n",
      "epoch: 1619, train_loss: 0.000721332399458017, test_loss: 0.0006507195018154258\n",
      "epoch: 1620, train_loss: 0.0007214911494647031, test_loss: 0.0006636627076659352\n",
      "epoch: 1621, train_loss: 0.0007207975381195707, test_loss: 0.000660122575936839\n",
      "epoch: 1622, train_loss: 0.0007200962661401085, test_loss: 0.0006654633519550165\n",
      "epoch: 1623, train_loss: 0.0007204466522165129, test_loss: 0.00065807548041145\n",
      "epoch: 1624, train_loss: 0.0007218665724276038, test_loss: 0.0006564277233943964\n",
      "epoch: 1625, train_loss: 0.0007222800353622955, test_loss: 0.0006622868919900308\n",
      "epoch: 1626, train_loss: 0.000720405801827007, test_loss: 0.0006559184645690644\n",
      "epoch: 1627, train_loss: 0.0007215821027310322, test_loss: 0.0006558583554578945\n",
      "epoch: 1628, train_loss: 0.0007189414210860496, test_loss: 0.0006529283224760244\n",
      "epoch: 1629, train_loss: 0.0007194599855448241, test_loss: 0.0006584578125815218\n",
      "epoch: 1630, train_loss: 0.0007207571116336824, test_loss: 0.0006527110429791113\n",
      "epoch: 1631, train_loss: 0.0007213742052123922, test_loss: 0.0006624047479514653\n",
      "epoch: 1632, train_loss: 0.0007207446222942647, test_loss: 0.0006541805147814254\n",
      "epoch: 1633, train_loss: 0.0007199616254934961, test_loss: 0.0006602739692122365\n",
      "epoch: 1634, train_loss: 0.0007200275051771947, test_loss: 0.0006465135617569709\n",
      "epoch: 1635, train_loss: 0.0007209061245110048, test_loss: 0.0006599396632130569\n",
      "epoch: 1636, train_loss: 0.0007196116924245397, test_loss: 0.0006566552037838846\n",
      "epoch: 1637, train_loss: 0.0007197076528150912, test_loss: 0.0006600408717834702\n",
      "epoch: 1638, train_loss: 0.0007195940112148452, test_loss: 0.0006556646985700354\n",
      "epoch: 1639, train_loss: 0.0007204579432373462, test_loss: 0.0006554449209943414\n",
      "epoch: 1640, train_loss: 0.0007189645763466377, test_loss: 0.0006528283993247896\n",
      "epoch: 1641, train_loss: 0.0007203729600524125, test_loss: 0.0006666183568692455\n",
      "epoch: 1642, train_loss: 0.0007183719768047171, test_loss: 0.000652298602896432\n",
      "epoch: 1643, train_loss: 0.0007183863452392752, test_loss: 0.0006513661937788129\n",
      "epoch: 1644, train_loss: 0.0007179546417951908, test_loss: 0.0006587603323472043\n",
      "epoch: 1645, train_loss: 0.0007199250090786296, test_loss: 0.0006646302014511699\n",
      "epoch: 1646, train_loss: 0.0007168779468528279, test_loss: 0.0006485815150275206\n",
      "epoch: 1647, train_loss: 0.0007167438060328689, test_loss: 0.0006484373734565452\n",
      "epoch: 1648, train_loss: 0.0007200530494826481, test_loss: 0.0006592603070506206\n",
      "epoch: 1649, train_loss: 0.0007188257143792251, test_loss: 0.0006512050943759581\n",
      "epoch: 1650, train_loss: 0.0007197672081098932, test_loss: 0.000655483347751821\n",
      "epoch: 1651, train_loss: 0.000717947546787479, test_loss: 0.0006487706074646363\n",
      "epoch: 1652, train_loss: 0.000718805536567031, test_loss: 0.0006500424011998499\n",
      "epoch: 1653, train_loss: 0.0007178960481921778, test_loss: 0.0006559059353700528\n",
      "epoch: 1654, train_loss: 0.0007168457807932535, test_loss: 0.0006645179867822056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1655, train_loss: 0.0007183850823861102, test_loss: 0.0006525247978667418\n",
      "epoch: 1656, train_loss: 0.0007173518900779765, test_loss: 0.0006485220316487054\n",
      "epoch: 1657, train_loss: 0.000717296492834778, test_loss: 0.0006545293338907262\n",
      "epoch: 1658, train_loss: 0.0007164882225207175, test_loss: 0.000652179708898378\n",
      "epoch: 1659, train_loss: 0.0007171211182139814, test_loss: 0.0006653214271257942\n",
      "epoch: 1660, train_loss: 0.0007181194251524689, test_loss: 0.0006489003814446429\n",
      "epoch: 1661, train_loss: 0.0007178329447608279, test_loss: 0.0006544054631376639\n",
      "epoch: 1662, train_loss: 0.00071679507938983, test_loss: 0.0006519896608854955\n",
      "epoch: 1663, train_loss: 0.000717543822247535, test_loss: 0.0006478470798659449\n",
      "epoch: 1664, train_loss: 0.0007162915818575445, test_loss: 0.0006566780793946236\n",
      "epoch: 1665, train_loss: 0.0007185039348373919, test_loss: 0.00065421819454059\n",
      "epoch: 1666, train_loss: 0.0007157924600228991, test_loss: 0.0006526888319058344\n",
      "epoch: 1667, train_loss: 0.0007159310461366144, test_loss: 0.000658870761981234\n",
      "epoch: 1668, train_loss: 0.0007165833958424628, test_loss: 0.0006544045900227502\n",
      "epoch: 1669, train_loss: 0.0007160982780118028, test_loss: 0.0006491413757127399\n",
      "epoch: 1670, train_loss: 0.0007151512064687583, test_loss: 0.0006487779416299114\n",
      "epoch: 1671, train_loss: 0.0007139064811939454, test_loss: 0.0006524082903827851\n",
      "epoch: 1672, train_loss: 0.0007161970526162211, test_loss: 0.0006518620308876658\n",
      "epoch: 1673, train_loss: 0.0007139381879191521, test_loss: 0.0006499760105119398\n",
      "epoch: 1674, train_loss: 0.0007171412631261932, test_loss: 0.0006578421695545936\n",
      "epoch: 1675, train_loss: 0.0007148962493216538, test_loss: 0.0006485711589145163\n",
      "epoch: 1676, train_loss: 0.0007150325045981647, test_loss: 0.0006523660219196851\n",
      "epoch: 1677, train_loss: 0.0007152531609830002, test_loss: 0.0006482241296907887\n",
      "epoch: 1678, train_loss: 0.0007154023219102427, test_loss: 0.0006462354115986576\n",
      "epoch: 1679, train_loss: 0.0007142511743080357, test_loss: 0.0006540895313567793\n",
      "epoch: 1680, train_loss: 0.0007141315018879654, test_loss: 0.0006548488890985027\n",
      "epoch: 1681, train_loss: 0.0007139168105229897, test_loss: 0.0006518310983665287\n",
      "epoch: 1682, train_loss: 0.000715622036849432, test_loss: 0.0006638839258812368\n",
      "epoch: 1683, train_loss: 0.0007158476952969542, test_loss: 0.0006515693724698698\n",
      "epoch: 1684, train_loss: 0.0007159561500884593, test_loss: 0.0006549552781507373\n",
      "epoch: 1685, train_loss: 0.0007158206780846028, test_loss: 0.0006500413486113151\n",
      "epoch: 1686, train_loss: 0.0007141725560042845, test_loss: 0.0006486284643566856\n",
      "epoch: 1687, train_loss: 0.0007142812613418083, test_loss: 0.0006523558840854093\n",
      "epoch: 1688, train_loss: 0.000715371160565511, test_loss: 0.0006452997040469199\n",
      "epoch: 1689, train_loss: 0.0007136000102669325, test_loss: 0.0006488283446136242\n",
      "epoch: 1690, train_loss: 0.0007185501707009162, test_loss: 0.0006487131613539532\n",
      "epoch: 1691, train_loss: 0.0007138722715393195, test_loss: 0.0006552619161084294\n",
      "epoch: 1692, train_loss: 0.0007137170418307347, test_loss: 0.0006452159044177582\n",
      "epoch: 1693, train_loss: 0.0007149763506553743, test_loss: 0.0006481487750230978\n",
      "epoch: 1694, train_loss: 0.0007126966033272607, test_loss: 0.0006659820161682243\n",
      "epoch: 1695, train_loss: 0.0007116962211591232, test_loss: 0.0006429791101254523\n",
      "epoch: 1696, train_loss: 0.0007122755544158914, test_loss: 0.0006552127403362343\n",
      "epoch: 1697, train_loss: 0.0007130745026465181, test_loss: 0.0006480029890857016\n",
      "epoch: 1698, train_loss: 0.0007128507536633507, test_loss: 0.0006458268035203218\n",
      "epoch: 1699, train_loss: 0.0007126372682092631, test_loss: 0.00064525914300854\n",
      "epoch: 1700, train_loss: 0.0007122432807986827, test_loss: 0.0006480339216068387\n",
      "epoch: 1701, train_loss: 0.000712330326559427, test_loss: 0.0006557974556926638\n",
      "epoch: 1702, train_loss: 0.0007125461086853529, test_loss: 0.0006430337380152196\n",
      "epoch: 1703, train_loss: 0.0007110335509819181, test_loss: 0.0006499451458997404\n",
      "epoch: 1704, train_loss: 0.0007118118152467777, test_loss: 0.0006492545750613014\n",
      "epoch: 1705, train_loss: 0.0007108675528560643, test_loss: 0.0006424003368010744\n",
      "epoch: 1706, train_loss: 0.0007110235468565446, test_loss: 0.000650640286039561\n",
      "epoch: 1707, train_loss: 0.0007135848180674341, test_loss: 0.0006502720401234304\n",
      "epoch: 1708, train_loss: 0.0007118691093004916, test_loss: 0.0006485857108297447\n",
      "epoch: 1709, train_loss: 0.0007121765229385346, test_loss: 0.0006428312917705625\n",
      "epoch: 1710, train_loss: 0.000711055540823904, test_loss: 0.0006484351033577695\n",
      "epoch: 1711, train_loss: 0.0007107011827073344, test_loss: 0.0006415786095506822\n",
      "epoch: 1712, train_loss: 0.0007108042077363833, test_loss: 0.0006418693859207755\n",
      "epoch: 1713, train_loss: 0.0007112424696653919, test_loss: 0.0006407001977398371\n",
      "epoch: 1714, train_loss: 0.0007120313074783949, test_loss: 0.0006500744833222901\n",
      "epoch: 1715, train_loss: 0.0007126432028599083, test_loss: 0.0006602696133389448\n",
      "epoch: 1716, train_loss: 0.0007122916665495089, test_loss: 0.0006515832452957208\n",
      "epoch: 1717, train_loss: 0.0007122803160556308, test_loss: 0.0006467727701722955\n",
      "epoch: 1718, train_loss: 0.000708271600002342, test_loss: 0.0006701483556147044\n",
      "epoch: 1719, train_loss: 0.0007109915098661314, test_loss: 0.000651770281062151\n",
      "epoch: 1720, train_loss: 0.0007102032895604877, test_loss: 0.0006473889128149798\n",
      "epoch: 1721, train_loss: 0.0007111962540480106, test_loss: 0.000644489720192117\n",
      "epoch: 1722, train_loss: 0.0007100874411306628, test_loss: 0.000648663534472386\n",
      "epoch: 1723, train_loss: 0.0007085558862182434, test_loss: 0.0006445143032275761\n",
      "epoch: 1724, train_loss: 0.0007110415456776062, test_loss: 0.0006464533313798407\n",
      "epoch: 1725, train_loss: 0.0007094180391109346, test_loss: 0.000652867461515901\n",
      "epoch: 1726, train_loss: 0.0007117898886739883, test_loss: 0.0006468747839486847\n",
      "epoch: 1727, train_loss: 0.0007095507120855315, test_loss: 0.0006460604151167596\n",
      "epoch: 1728, train_loss: 0.0007116234198253115, test_loss: 0.0006443642002219955\n",
      "epoch: 1729, train_loss: 0.0007099897104679891, test_loss: 0.00063924232381396\n",
      "epoch: 1730, train_loss: 0.0007113183914359821, test_loss: 0.0006517680158140138\n",
      "epoch: 1731, train_loss: 0.0007092805551466248, test_loss: 0.000641550577711314\n",
      "epoch: 1732, train_loss: 0.0007095553180830473, test_loss: 0.0006444513419410214\n",
      "epoch: 1733, train_loss: 0.0007098176486223289, test_loss: 0.0006397598723803336\n",
      "epoch: 1734, train_loss: 0.0007104897168060036, test_loss: 0.0006599299134298539\n",
      "epoch: 1735, train_loss: 0.0007072956625210202, test_loss: 0.0006427009454152236\n",
      "epoch: 1736, train_loss: 0.000708174514685474, test_loss: 0.0006424025632441044\n",
      "epoch: 1737, train_loss: 0.0007082104705699274, test_loss: 0.0006489209578527758\n",
      "epoch: 1738, train_loss: 0.0007075741444208214, test_loss: 0.0006425250079094743\n",
      "epoch: 1739, train_loss: 0.0007102183425677536, test_loss: 0.0006414393816764156\n",
      "epoch: 1740, train_loss: 0.0007091361685129611, test_loss: 0.0006445959831277529\n",
      "epoch: 1741, train_loss: 0.0007082330235077635, test_loss: 0.0006520576716866344\n",
      "epoch: 1742, train_loss: 0.0007083785375985115, test_loss: 0.0006442112595929453\n",
      "epoch: 1743, train_loss: 0.0007081715093986334, test_loss: 0.0006571272970177233\n",
      "epoch: 1744, train_loss: 0.0007066049653550853, test_loss: 0.0006386446621036157\n",
      "epoch: 1745, train_loss: 0.0007084103569428881, test_loss: 0.0006441076548071578\n",
      "epoch: 1746, train_loss: 0.0007088401509226178, test_loss: 0.0006507607661963751\n",
      "epoch: 1747, train_loss: 0.0007077054722922976, test_loss: 0.0006452138671496263\n",
      "epoch: 1748, train_loss: 0.0007069142985805545, test_loss: 0.0006437668344005942\n",
      "epoch: 1749, train_loss: 0.0007092276481790063, test_loss: 0.0006442031153710559\n",
      "epoch: 1750, train_loss: 0.0007080757894310291, test_loss: 0.0006459903864500424\n",
      "epoch: 1751, train_loss: 0.0007090919370175866, test_loss: 0.0006451288015038396\n",
      "epoch: 1752, train_loss: 0.0007074235396905113, test_loss: 0.0006583866682679703\n",
      "epoch: 1753, train_loss: 0.0007090974869915162, test_loss: 0.000642875544144772\n",
      "epoch: 1754, train_loss: 0.000706642122088891, test_loss: 0.0006425961619243026\n",
      "epoch: 1755, train_loss: 0.0007068265619201828, test_loss: 0.0006372999050654471\n",
      "epoch: 1756, train_loss: 0.0007056967436295489, test_loss: 0.00064005063904915\n",
      "epoch: 1757, train_loss: 0.0007068221419341053, test_loss: 0.0006479700775040934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1758, train_loss: 0.000707306227211476, test_loss: 0.0006442171870730817\n",
      "epoch: 1759, train_loss: 0.0007070094706369159, test_loss: 0.000641387960058637\n",
      "epoch: 1760, train_loss: 0.0007071324710167297, test_loss: 0.0006386958460401123\n",
      "epoch: 1761, train_loss: 0.0007063230745109689, test_loss: 0.0006465474580181763\n",
      "epoch: 1762, train_loss: 0.0007056549771020756, test_loss: 0.0006373552120445917\n",
      "epoch: 1763, train_loss: 0.0007055944753000917, test_loss: 0.0006740627868566662\n",
      "epoch: 1764, train_loss: 0.0007065033562906573, test_loss: 0.0006404567344967896\n",
      "epoch: 1765, train_loss: 0.0007057285123585683, test_loss: 0.000642540854945158\n",
      "epoch: 1766, train_loss: 0.0007044766414606863, test_loss: 0.0006522083519181857\n",
      "epoch: 1767, train_loss: 0.0007066679245326668, test_loss: 0.0006484467206367602\n",
      "epoch: 1768, train_loss: 0.000706208223203683, test_loss: 0.0006383613023596505\n",
      "epoch: 1769, train_loss: 0.0007057981148017975, test_loss: 0.00065509787236806\n",
      "epoch: 1770, train_loss: 0.0007047039006188836, test_loss: 0.0006341693854968374\n",
      "epoch: 1771, train_loss: 0.000705598809240062, test_loss: 0.0006524640145168329\n",
      "epoch: 1772, train_loss: 0.0007053607766030599, test_loss: 0.0006396841830185925\n",
      "epoch: 1773, train_loss: 0.0007058952444071031, test_loss: 0.0006482173145438234\n",
      "epoch: 1774, train_loss: 0.0007049410183833021, test_loss: 0.0006419532389069597\n",
      "epoch: 1775, train_loss: 0.0007050452733655338, test_loss: 0.0006370648964851474\n",
      "epoch: 1776, train_loss: 0.0007075585764022949, test_loss: 0.0006340355224286517\n",
      "epoch: 1777, train_loss: 0.0007050593520271714, test_loss: 0.0006484038506944975\n",
      "epoch: 1778, train_loss: 0.0007041858726828966, test_loss: 0.0006368770070063571\n",
      "epoch: 1779, train_loss: 0.0007058677109181071, test_loss: 0.0006387390894815326\n",
      "epoch: 1780, train_loss: 0.0007054587476886809, test_loss: 0.0006332248255300025\n",
      "epoch: 1781, train_loss: 0.0007056602271800132, test_loss: 0.0006378760105386997\n",
      "epoch: 1782, train_loss: 0.0007027500617803763, test_loss: 0.0006379389330201471\n",
      "epoch: 1783, train_loss: 0.0007029073338186288, test_loss: 0.0006424139331405362\n",
      "epoch: 1784, train_loss: 0.0007041655961707559, test_loss: 0.0006428680547590678\n",
      "epoch: 1785, train_loss: 0.0007053571879742262, test_loss: 0.000643829819940341\n",
      "epoch: 1786, train_loss: 0.0007040858845752866, test_loss: 0.0006349894780820856\n",
      "epoch: 1787, train_loss: 0.0007058199338169526, test_loss: 0.0006380490182588497\n",
      "epoch: 1788, train_loss: 0.0007040797930170337, test_loss: 0.0006356031857042884\n",
      "epoch: 1789, train_loss: 0.0007045131503179183, test_loss: 0.0006459857686422765\n",
      "epoch: 1790, train_loss: 0.0007045025919543822, test_loss: 0.0006415181996999308\n",
      "epoch: 1791, train_loss: 0.0007029261981623005, test_loss: 0.000646109843122152\n",
      "epoch: 1792, train_loss: 0.0007036252342083532, test_loss: 0.0006409243651432917\n",
      "epoch: 1793, train_loss: 0.0007034190221811118, test_loss: 0.0006427639455068856\n",
      "epoch: 1794, train_loss: 0.0007042871590744218, test_loss: 0.0006388964247889817\n",
      "epoch: 1795, train_loss: 0.0007020015998379043, test_loss: 0.0006359010779609283\n",
      "epoch: 1796, train_loss: 0.0007018701682049457, test_loss: 0.0006340433610603213\n",
      "epoch: 1797, train_loss: 0.0007043529324658701, test_loss: 0.0006383286527125165\n",
      "epoch: 1798, train_loss: 0.0007026537647976985, test_loss: 0.0006433618740023425\n",
      "epoch: 1799, train_loss: 0.0007039119347767985, test_loss: 0.000642722889703388\n",
      "epoch: 1800, train_loss: 0.0007021955224563894, test_loss: 0.000634841683980388\n",
      "epoch: 1801, train_loss: 0.0007022537187288475, test_loss: 0.0006452151234649742\n",
      "epoch: 1802, train_loss: 0.0007029872263640003, test_loss: 0.0006502374114158253\n",
      "epoch: 1803, train_loss: 0.0007027288083918393, test_loss: 0.0006471321345694984\n",
      "epoch: 1804, train_loss: 0.0007017107285640162, test_loss: 0.000634737079963088\n",
      "epoch: 1805, train_loss: 0.0007031409945541426, test_loss: 0.0006323883426375687\n",
      "epoch: 1806, train_loss: 0.0007031930018337848, test_loss: 0.0006478296272689477\n",
      "epoch: 1807, train_loss: 0.0007036211647336249, test_loss: 0.0006393729903114339\n",
      "epoch: 1808, train_loss: 0.0007017433729387172, test_loss: 0.0006475694002195572\n",
      "epoch: 1809, train_loss: 0.0007015105195181525, test_loss: 0.0006368213944369927\n",
      "epoch: 1810, train_loss: 0.0007023338441048627, test_loss: 0.000633160753447252\n",
      "epoch: 1811, train_loss: 0.0007013169613302401, test_loss: 0.0006356794280388082\n",
      "epoch: 1812, train_loss: 0.0007015898261907632, test_loss: 0.0006332625683474665\n",
      "epoch: 1813, train_loss: 0.0007015566414971229, test_loss: 0.0006337887704527626\n",
      "epoch: 1814, train_loss: 0.0007010927390934819, test_loss: 0.000633427383339343\n",
      "epoch: 1815, train_loss: 0.0007014230321384157, test_loss: 0.0006335364208401492\n",
      "epoch: 1816, train_loss: 0.0007012774611053908, test_loss: 0.0006351730650446067\n",
      "epoch: 1817, train_loss: 0.0007026875189141086, test_loss: 0.0006330042670015246\n",
      "epoch: 1818, train_loss: 0.0007017313138298366, test_loss: 0.0006326986088727912\n",
      "epoch: 1819, train_loss: 0.000702795499186639, test_loss: 0.000631367006765989\n",
      "epoch: 1820, train_loss: 0.0007011450930883217, test_loss: 0.0006428613560274243\n",
      "epoch: 1821, train_loss: 0.0007027200316888807, test_loss: 0.0006347424205159768\n",
      "epoch: 1822, train_loss: 0.0007028137751267818, test_loss: 0.0006370459159370512\n",
      "epoch: 1823, train_loss: 0.0006995258711116469, test_loss: 0.0006388111311631898\n",
      "epoch: 1824, train_loss: 0.0007017616349596368, test_loss: 0.0006357869909455379\n",
      "epoch: 1825, train_loss: 0.000699747384161405, test_loss: 0.0006359572338017946\n",
      "epoch: 1826, train_loss: 0.0007004837300527193, test_loss: 0.0006352803660168623\n",
      "epoch: 1827, train_loss: 0.0006996263008382496, test_loss: 0.0006351291667670012\n",
      "epoch: 1828, train_loss: 0.0006989235711364967, test_loss: 0.0006381969287758693\n",
      "epoch: 1829, train_loss: 0.0007013170511724994, test_loss: 0.000638815836282447\n",
      "epoch: 1830, train_loss: 0.0007022859290768595, test_loss: 0.000648179188525925\n",
      "epoch: 1831, train_loss: 0.0007000880837238029, test_loss: 0.0006460260871487359\n",
      "epoch: 1832, train_loss: 0.0006995198858456444, test_loss: 0.0006496383139165118\n",
      "epoch: 1833, train_loss: 0.0007000134387256011, test_loss: 0.0006363684757767866\n",
      "epoch: 1834, train_loss: 0.0007010614448834373, test_loss: 0.0006344652889917294\n",
      "epoch: 1835, train_loss: 0.0007001906785223147, test_loss: 0.0006349343214727318\n",
      "epoch: 1836, train_loss: 0.0007023797522339484, test_loss: 0.000632929295534268\n",
      "epoch: 1837, train_loss: 0.0007009552614560918, test_loss: 0.0006329936683565999\n",
      "epoch: 1838, train_loss: 0.0006979558447315155, test_loss: 0.0006477568676928058\n",
      "epoch: 1839, train_loss: 0.000698839529397209, test_loss: 0.0006350390516066303\n",
      "epoch: 1840, train_loss: 0.0007006813994730296, test_loss: 0.0006339132426849877\n",
      "epoch: 1841, train_loss: 0.0006990518633519177, test_loss: 0.0006321300946486493\n",
      "epoch: 1842, train_loss: 0.0006982255904206439, test_loss: 0.000638698756423158\n",
      "epoch: 1843, train_loss: 0.000699876695010892, test_loss: 0.0006345194851746783\n",
      "epoch: 1844, train_loss: 0.000699361311727325, test_loss: 0.0006336119355789075\n",
      "epoch: 1845, train_loss: 0.0006992886141549958, test_loss: 0.0006347942932431275\n",
      "epoch: 1846, train_loss: 0.0006988075607375282, test_loss: 0.0006340969945692146\n",
      "epoch: 1847, train_loss: 0.0006989370196369355, test_loss: 0.000637659565351593\n",
      "epoch: 1848, train_loss: 0.0006994848182607118, test_loss: 0.0006476476313158249\n",
      "epoch: 1849, train_loss: 0.0006989316594705957, test_loss: 0.0006336850686542069\n",
      "epoch: 1850, train_loss: 0.0006994439476250631, test_loss: 0.0006356282101478428\n",
      "epoch: 1851, train_loss: 0.0006988375060483004, test_loss: 0.000638002534590972\n",
      "epoch: 1852, train_loss: 0.000698494078318143, test_loss: 0.000636325382705157\n",
      "epoch: 1853, train_loss: 0.0006971727011462107, test_loss: 0.0006320108950603753\n",
      "epoch: 1854, train_loss: 0.0006979717139114181, test_loss: 0.000636380078503862\n",
      "epoch: 1855, train_loss: 0.0006981353141347189, test_loss: 0.0006346913578454405\n",
      "epoch: 1856, train_loss: 0.0006979260740437261, test_loss: 0.0006402756019573038\n",
      "epoch: 1857, train_loss: 0.0006997079927595737, test_loss: 0.0006313479680102319\n",
      "epoch: 1858, train_loss: 0.0006968785727740792, test_loss: 0.0006316584525241827\n",
      "epoch: 1859, train_loss: 0.000697517600517882, test_loss: 0.0006315422021240616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1860, train_loss: 0.0006978146493961306, test_loss: 0.0006283961362593496\n",
      "epoch: 1861, train_loss: 0.0006982524532561555, test_loss: 0.0006317349470918998\n",
      "epoch: 1862, train_loss: 0.0006981103582327943, test_loss: 0.0006375452746093894\n",
      "epoch: 1863, train_loss: 0.000697712615162701, test_loss: 0.0006367986837479597\n",
      "epoch: 1864, train_loss: 0.0006987978185466288, test_loss: 0.0006300278667670985\n",
      "epoch: 1865, train_loss: 0.0006974996383929545, test_loss: 0.0006310705163438494\n",
      "epoch: 1866, train_loss: 0.0006969684529948332, test_loss: 0.0006326495592171947\n",
      "epoch: 1867, train_loss: 0.0006972709088586271, test_loss: 0.0006291262155476337\n",
      "epoch: 1868, train_loss: 0.0006965129894366407, test_loss: 0.0006361597042996436\n",
      "epoch: 1869, train_loss: 0.0006959139832558677, test_loss: 0.0006284563375326494\n",
      "epoch: 1870, train_loss: 0.0006969386557339812, test_loss: 0.0006324274145299569\n",
      "epoch: 1871, train_loss: 0.0006964010320624094, test_loss: 0.000637850874530462\n",
      "epoch: 1872, train_loss: 0.0006969154308772768, test_loss: 0.0006285426158380384\n",
      "epoch: 1873, train_loss: 0.0006966495724475902, test_loss: 0.000631852691488651\n",
      "epoch: 1874, train_loss: 0.0006964807423924947, test_loss: 0.0006334061181405559\n",
      "epoch: 1875, train_loss: 0.0006960520277852597, test_loss: 0.0006284531021568304\n",
      "epoch: 1876, train_loss: 0.0006958776211831719, test_loss: 0.0006288198492256925\n",
      "epoch: 1877, train_loss: 0.0006953945646629385, test_loss: 0.000641359991277568\n",
      "epoch: 1878, train_loss: 0.0006962069259632541, test_loss: 0.0006341865131010612\n",
      "epoch: 1879, train_loss: 0.0006954080460633596, test_loss: 0.0006267295539146289\n",
      "epoch: 1880, train_loss: 0.0006941578695145638, test_loss: 0.0006255403180451443\n",
      "epoch: 1881, train_loss: 0.0006958977078877227, test_loss: 0.000631920857510219\n",
      "epoch: 1882, train_loss: 0.0006965870953812871, test_loss: 0.000629016237022976\n",
      "epoch: 1883, train_loss: 0.0006964273014328564, test_loss: 0.0006303278690514466\n",
      "epoch: 1884, train_loss: 0.0006939235684947799, test_loss: 0.0006267381298433369\n",
      "epoch: 1885, train_loss: 0.0006969018748192036, test_loss: 0.0006315190015205493\n",
      "epoch: 1886, train_loss: 0.0006945924453801759, test_loss: 0.0006381405983120203\n",
      "epoch: 1887, train_loss: 0.0006957832577071437, test_loss: 0.0006314475661686932\n",
      "epoch: 1888, train_loss: 0.0006947956204859782, test_loss: 0.0006309482996584848\n",
      "epoch: 1889, train_loss: 0.0006955445164551392, test_loss: 0.0006327475324117889\n",
      "epoch: 1890, train_loss: 0.0006950623912574804, test_loss: 0.0006318495967813457\n",
      "epoch: 1891, train_loss: 0.0006936266436241567, test_loss: 0.000632232030814824\n",
      "epoch: 1892, train_loss: 0.0006947211260684644, test_loss: 0.0006507221163095286\n",
      "epoch: 1893, train_loss: 0.0006958312600466382, test_loss: 0.0006382611657803258\n",
      "epoch: 1894, train_loss: 0.0006940116265626705, test_loss: 0.0006316263607004657\n",
      "epoch: 1895, train_loss: 0.0006945360459529026, test_loss: 0.0006350053299684078\n",
      "epoch: 1896, train_loss: 0.0006962903830951647, test_loss: 0.0006399784033419564\n",
      "epoch: 1897, train_loss: 0.0006929699384668113, test_loss: 0.0006383494716525698\n",
      "epoch: 1898, train_loss: 0.0006949744900971975, test_loss: 0.0006322613238201787\n",
      "epoch: 1899, train_loss: 0.0006955505738480259, test_loss: 0.0006309470578950519\n",
      "epoch: 1900, train_loss: 0.0006972799335768366, test_loss: 0.0006341046973830089\n",
      "epoch: 1901, train_loss: 0.0006926883930725086, test_loss: 0.000629858230240643\n",
      "epoch: 1902, train_loss: 0.0006948764114539423, test_loss: 0.0006288680451689288\n",
      "epoch: 1903, train_loss: 0.0006925567475896653, test_loss: 0.0006365520869924998\n",
      "epoch: 1904, train_loss: 0.0006947988801149894, test_loss: 0.00063548476221816\n",
      "epoch: 1905, train_loss: 0.0006942008697913717, test_loss: 0.0006267925976620367\n",
      "epoch: 1906, train_loss: 0.0006958721521938139, test_loss: 0.0006280372763285413\n",
      "epoch: 1907, train_loss: 0.0006934122736666999, test_loss: 0.0006261193193495274\n",
      "epoch: 1908, train_loss: 0.0006927656789269785, test_loss: 0.0006309706222964451\n",
      "epoch: 1909, train_loss: 0.0006943991655767288, test_loss: 0.0006343380712981647\n",
      "epoch: 1910, train_loss: 0.0006932417226894556, test_loss: 0.000631697597176147\n",
      "epoch: 1911, train_loss: 0.0006924857418356544, test_loss: 0.0006350732389061401\n",
      "epoch: 1912, train_loss: 0.0006945142838800245, test_loss: 0.0006327869729526961\n",
      "epoch: 1913, train_loss: 0.0006945491844342779, test_loss: 0.0006274313491303474\n",
      "epoch: 1914, train_loss: 0.0006951322771467106, test_loss: 0.0006316062354017049\n",
      "epoch: 1915, train_loss: 0.0006937665090410281, test_loss: 0.0006284462579060346\n",
      "epoch: 1916, train_loss: 0.0006948030305742893, test_loss: 0.0006285752557838956\n",
      "epoch: 1917, train_loss: 0.0006943864358143638, test_loss: 0.0006272743921726942\n",
      "epoch: 1918, train_loss: 0.0006925366419043554, test_loss: 0.0006315789263074597\n",
      "epoch: 1919, train_loss: 0.0006946363871026298, test_loss: 0.0006428274209611118\n",
      "epoch: 1920, train_loss: 0.0006936968091632361, test_loss: 0.000629786498999844\n",
      "epoch: 1921, train_loss: 0.0006942217840570147, test_loss: 0.0006268002083137011\n",
      "epoch: 1922, train_loss: 0.0006934617944669141, test_loss: 0.0006273542191289986\n",
      "epoch: 1923, train_loss: 0.0006943145455569838, test_loss: 0.0006374581716954708\n",
      "epoch: 1924, train_loss: 0.0006930780680546457, test_loss: 0.0006260111549636349\n",
      "epoch: 1925, train_loss: 0.000692937978664818, test_loss: 0.0006240089957524712\n",
      "epoch: 1926, train_loss: 0.0006941727567565343, test_loss: 0.0006285736211187517\n",
      "epoch: 1927, train_loss: 0.0006925262442445787, test_loss: 0.0006508696921324978\n",
      "epoch: 1928, train_loss: 0.0006922202199980941, test_loss: 0.0006289050264361625\n",
      "epoch: 1929, train_loss: 0.0006907413253034263, test_loss: 0.0006264560361159965\n",
      "epoch: 1930, train_loss: 0.0006922859136703546, test_loss: 0.0006268859870033339\n",
      "epoch: 1931, train_loss: 0.0006920850010710242, test_loss: 0.0006282706987500811\n",
      "epoch: 1932, train_loss: 0.0006915464296268866, test_loss: 0.000628053099111033\n",
      "epoch: 1933, train_loss: 0.0006924456835764905, test_loss: 0.0006249771589258065\n",
      "epoch: 1934, train_loss: 0.0006923076491701701, test_loss: 0.0006356179122424995\n",
      "epoch: 1935, train_loss: 0.0006915826347919748, test_loss: 0.0006415873018947119\n",
      "epoch: 1936, train_loss: 0.0006909968303617737, test_loss: 0.0006341914122458547\n",
      "epoch: 1937, train_loss: 0.0006908769959725602, test_loss: 0.0006256294242727259\n",
      "epoch: 1938, train_loss: 0.0006907409305036392, test_loss: 0.0006261402741074562\n",
      "epoch: 1939, train_loss: 0.0006924751784105825, test_loss: 0.0006352315685944632\n",
      "epoch: 1940, train_loss: 0.0006910736632087957, test_loss: 0.0006358040166863551\n",
      "epoch: 1941, train_loss: 0.0006931323783330457, test_loss: 0.0006284370562449718\n",
      "epoch: 1942, train_loss: 0.0006911536240848997, test_loss: 0.0006279789910574133\n",
      "epoch: 1943, train_loss: 0.0006896317987095402, test_loss: 0.0006264447535310561\n",
      "epoch: 1944, train_loss: 0.0006909657335516227, test_loss: 0.0006419176740261415\n",
      "epoch: 1945, train_loss: 0.0006906277925261985, test_loss: 0.0006289447386128207\n",
      "epoch: 1946, train_loss: 0.000691580253339413, test_loss: 0.0006359151108578468\n",
      "epoch: 1947, train_loss: 0.0006914378748700509, test_loss: 0.0006295392522588372\n",
      "epoch: 1948, train_loss: 0.0006906872529172055, test_loss: 0.0006265391954608882\n",
      "epoch: 1949, train_loss: 0.000689677987753859, test_loss: 0.0006386128031105424\n",
      "epoch: 1950, train_loss: 0.0006914108450038601, test_loss: 0.0006246278013956422\n",
      "epoch: 1951, train_loss: 0.0006912900400651699, test_loss: 0.0006240758957574144\n",
      "epoch: 1952, train_loss: 0.0006894135073272754, test_loss: 0.0006231463194126263\n",
      "epoch: 1953, train_loss: 0.000691986599754866, test_loss: 0.0006224791723070666\n",
      "epoch: 1954, train_loss: 0.0006900601640708097, test_loss: 0.0006237667742728566\n",
      "epoch: 1955, train_loss: 0.0006913343221759019, test_loss: 0.0006267936405492947\n",
      "epoch: 1956, train_loss: 0.0006894227851222714, test_loss: 0.0006248267503300061\n",
      "epoch: 1957, train_loss: 0.0006897347490809372, test_loss: 0.0006237261356242622\n",
      "epoch: 1958, train_loss: 0.0006907821554467891, test_loss: 0.0006326726337041085\n",
      "epoch: 1959, train_loss: 0.0006895486313505503, test_loss: 0.0006293887175464382\n",
      "epoch: 1960, train_loss: 0.0006903785751606135, test_loss: 0.0006291630949514607\n",
      "epoch: 1961, train_loss: 0.0006882590899729859, test_loss: 0.0006255113063768173\n",
      "epoch: 1962, train_loss: 0.0006903954984053322, test_loss: 0.0006214728637132794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1963, train_loss: 0.000689080717679842, test_loss: 0.0006326506797146673\n",
      "epoch: 1964, train_loss: 0.0006897110572975615, test_loss: 0.0006357975556359937\n",
      "epoch: 1965, train_loss: 0.0006919550208334366, test_loss: 0.0006226347565340499\n",
      "epoch: 1966, train_loss: 0.0006898555261811808, test_loss: 0.0006296525680227205\n",
      "epoch: 1967, train_loss: 0.0006902124264540718, test_loss: 0.0006311367033049464\n",
      "epoch: 1968, train_loss: 0.00068868197116029, test_loss: 0.0006289918043573076\n",
      "epoch: 1969, train_loss: 0.0006891511426226276, test_loss: 0.0006287318489436681\n",
      "epoch: 1970, train_loss: 0.0006890648750730021, test_loss: 0.000624369223563311\n",
      "epoch: 1971, train_loss: 0.000690512764065162, test_loss: 0.0006337991944747046\n",
      "epoch: 1972, train_loss: 0.0006926652922234296, test_loss: 0.0006435996959529197\n",
      "epoch: 1973, train_loss: 0.0006900962654744153, test_loss: 0.0006320441510373106\n",
      "epoch: 1974, train_loss: 0.0006889306317569445, test_loss: 0.0006385485273009787\n",
      "epoch: 1975, train_loss: 0.0006878740728691058, test_loss: 0.0006271602906053886\n",
      "epoch: 1976, train_loss: 0.0006866984286993418, test_loss: 0.0006260889058467001\n",
      "epoch: 1977, train_loss: 0.0006901931736375327, test_loss: 0.0006288299870599682\n",
      "epoch: 1978, train_loss: 0.0006900207929151213, test_loss: 0.0006295445588572571\n",
      "epoch: 1979, train_loss: 0.000690056519765083, test_loss: 0.0006242109666345641\n",
      "epoch: 1980, train_loss: 0.0006879611565913681, test_loss: 0.0006289193503713856\n",
      "epoch: 1981, train_loss: 0.0006896457356481773, test_loss: 0.0006361723450633386\n",
      "epoch: 1982, train_loss: 0.0006902097526978215, test_loss: 0.0006250774846800292\n",
      "epoch: 1983, train_loss: 0.0006891156663186848, test_loss: 0.0006299697997746989\n",
      "epoch: 1984, train_loss: 0.0006900450161597489, test_loss: 0.0006406824541045353\n",
      "epoch: 1985, train_loss: 0.0006904638797530662, test_loss: 0.0006280826346483082\n",
      "epoch: 1986, train_loss: 0.0006878706829055496, test_loss: 0.0006321742063543448\n",
      "epoch: 1987, train_loss: 0.0006891578440959363, test_loss: 0.000627218178124167\n",
      "epoch: 1988, train_loss: 0.0006871559130756751, test_loss: 0.0006294493311240027\n",
      "epoch: 1989, train_loss: 0.0006896103200826632, test_loss: 0.0006222434506829208\n",
      "epoch: 1990, train_loss: 0.0006883298591002012, test_loss: 0.0006212866719579324\n",
      "epoch: 1991, train_loss: 0.0006864677365545346, test_loss: 0.0006344560097204521\n",
      "epoch: 1992, train_loss: 0.0006887476863440774, test_loss: 0.0006239465486335879\n",
      "epoch: 1993, train_loss: 0.000687699835828465, test_loss: 0.0006298500278110927\n",
      "epoch: 1994, train_loss: 0.0006891972734592855, test_loss: 0.0006323904332627232\n",
      "epoch: 1995, train_loss: 0.0006887236389878166, test_loss: 0.0006285443814704195\n",
      "epoch: 1996, train_loss: 0.0006871700486795896, test_loss: 0.0006324461525461326\n",
      "epoch: 1997, train_loss: 0.0006868555881184242, test_loss: 0.0006197866702374691\n",
      "epoch: 1998, train_loss: 0.0006866542452885567, test_loss: 0.000621784779165561\n",
      "epoch: 1999, train_loss: 0.0006868621086418304, test_loss: 0.0006224547881477823\n",
      "epoch: 2000, train_loss: 0.0006888159791149361, test_loss: 0.0006308169443703567\n",
      "epoch: 2001, train_loss: 0.0006889269305089408, test_loss: 0.0006261468709756931\n",
      "epoch: 2002, train_loss: 0.0006877952571654612, test_loss: 0.0006256342652098587\n",
      "epoch: 2003, train_loss: 0.000688034274271163, test_loss: 0.0006281893777971467\n",
      "epoch: 2004, train_loss: 0.0006881583173273374, test_loss: 0.0006259855096383641\n",
      "epoch: 2005, train_loss: 0.000688391115071009, test_loss: 0.0006211069315516701\n",
      "epoch: 2006, train_loss: 0.0006872056401334703, test_loss: 0.0006248827412491664\n",
      "epoch: 2007, train_loss: 0.0006881236027845222, test_loss: 0.0006284801056608558\n",
      "epoch: 2008, train_loss: 0.0006858897015816816, test_loss: 0.0006168768304632977\n",
      "epoch: 2009, train_loss: 0.0006853316520826648, test_loss: 0.0006191583476417387\n",
      "epoch: 2010, train_loss: 0.0006866755581501385, test_loss: 0.0006263765875094881\n",
      "epoch: 2011, train_loss: 0.0006856794276720156, test_loss: 0.0006242408853722736\n",
      "epoch: 2012, train_loss: 0.0006861488825803542, test_loss: 0.0006316809255319337\n",
      "epoch: 2013, train_loss: 0.0006887944245650231, test_loss: 0.0006275429477682337\n",
      "epoch: 2014, train_loss: 0.0006848542467670758, test_loss: 0.0006330045531891907\n",
      "epoch: 2015, train_loss: 0.0006866383748432702, test_loss: 0.000621217851100179\n",
      "epoch: 2016, train_loss: 0.0006856367905603964, test_loss: 0.0006466473908706879\n",
      "epoch: 2017, train_loss: 0.0006883755128871163, test_loss: 0.000625470444598856\n",
      "epoch: 2018, train_loss: 0.0006854732523409083, test_loss: 0.0006217697518877685\n",
      "epoch: 2019, train_loss: 0.0006849820315635399, test_loss: 0.0006216114464526375\n",
      "epoch: 2020, train_loss: 0.0006886665613147552, test_loss: 0.0006259690320196872\n",
      "epoch: 2021, train_loss: 0.0006863020421208247, test_loss: 0.000622948229041261\n",
      "epoch: 2022, train_loss: 0.0006842802610228081, test_loss: 0.0006220690508295471\n",
      "epoch: 2023, train_loss: 0.0006869385998351904, test_loss: 0.0006223247619345784\n",
      "epoch: 2024, train_loss: 0.0006866343395339082, test_loss: 0.0006269102110915507\n",
      "epoch: 2025, train_loss: 0.0006863899217695807, test_loss: 0.0006204839883139357\n",
      "epoch: 2026, train_loss: 0.0006842130830551943, test_loss: 0.0006254248243446151\n",
      "epoch: 2027, train_loss: 0.0006860554681422756, test_loss: 0.0006207713886396959\n",
      "epoch: 2028, train_loss: 0.0006869241175160784, test_loss: 0.0006261378633401667\n",
      "epoch: 2029, train_loss: 0.000685087787291116, test_loss: 0.0006302673330840965\n",
      "epoch: 2030, train_loss: 0.0006852521454793928, test_loss: 0.0006234590546227992\n",
      "epoch: 2031, train_loss: 0.0006845203283971742, test_loss: 0.0006191716674948111\n",
      "epoch: 2032, train_loss: 0.0006848869835148039, test_loss: 0.0006235441881775235\n",
      "epoch: 2033, train_loss: 0.0006846745888216664, test_loss: 0.0006236767124695083\n",
      "epoch: 2034, train_loss: 0.0006853596411098766, test_loss: 0.000621548630685235\n",
      "epoch: 2035, train_loss: 0.0006856070211379911, test_loss: 0.0006197599529211099\n",
      "epoch: 2036, train_loss: 0.0006837490313362492, test_loss: 0.0006191663220912839\n",
      "epoch: 2037, train_loss: 0.0006855090361331468, test_loss: 0.0006345646155144399\n",
      "epoch: 2038, train_loss: 0.0006843529016528603, test_loss: 0.0006306714446206266\n",
      "epoch: 2039, train_loss: 0.000685763058161525, test_loss: 0.000618812928829963\n",
      "epoch: 2040, train_loss: 0.0006855107140322418, test_loss: 0.000622483120726732\n",
      "epoch: 2041, train_loss: 0.0006859445559751729, test_loss: 0.0006263169586115206\n",
      "epoch: 2042, train_loss: 0.0006878824130146076, test_loss: 0.000625074685861667\n",
      "epoch: 2043, train_loss: 0.0006836856469896662, test_loss: 0.0006189339086025333\n",
      "epoch: 2044, train_loss: 0.000683929324504393, test_loss: 0.0006180418034394582\n",
      "epoch: 2045, train_loss: 0.0006838678977067542, test_loss: 0.000616457691648975\n",
      "epoch: 2046, train_loss: 0.0006851330285628691, test_loss: 0.0006186373066157103\n",
      "epoch: 2047, train_loss: 0.0006840297694156027, test_loss: 0.0006166206828008095\n",
      "epoch: 2048, train_loss: 0.0006851412054738435, test_loss: 0.0006178494562239697\n",
      "epoch: 2049, train_loss: 0.0006843225881153637, test_loss: 0.0006274534098338336\n",
      "epoch: 2050, train_loss: 0.0006847804267991982, test_loss: 0.0006225383597969388\n",
      "epoch: 2051, train_loss: 0.0006839140449934032, test_loss: 0.0006241133232833818\n",
      "epoch: 2052, train_loss: 0.0006833942695622048, test_loss: 0.0006184355491617074\n",
      "epoch: 2053, train_loss: 0.000685696844416468, test_loss: 0.0006190848992749428\n",
      "epoch: 2054, train_loss: 0.0006856168987249713, test_loss: 0.0006222424223475779\n",
      "epoch: 2055, train_loss: 0.0006850085261723269, test_loss: 0.0006229274925620606\n",
      "epoch: 2056, train_loss: 0.0006826110108269621, test_loss: 0.0006190878484630957\n",
      "epoch: 2057, train_loss: 0.00068407478038748, test_loss: 0.0006221216463018209\n",
      "epoch: 2058, train_loss: 0.0006828571077557685, test_loss: 0.0006219349500800794\n",
      "epoch: 2059, train_loss: 0.0006830086653201801, test_loss: 0.0006193094159243628\n",
      "epoch: 2060, train_loss: 0.00068599967609929, test_loss: 0.0006271604458258176\n",
      "epoch: 2061, train_loss: 0.0006841291159735588, test_loss: 0.0006310311728157103\n",
      "epoch: 2062, train_loss: 0.0006827537258884505, test_loss: 0.0006228151808803281\n",
      "epoch: 2063, train_loss: 0.0006837479835983528, test_loss: 0.00061892669085258\n",
      "epoch: 2064, train_loss: 0.0006867870005131092, test_loss: 0.0006160042830742896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2065, train_loss: 0.0006839680515796593, test_loss: 0.0006217299863540878\n",
      "epoch: 2066, train_loss: 0.000685521857003155, test_loss: 0.0006177131096289182\n",
      "epoch: 2067, train_loss: 0.0006830178671921401, test_loss: 0.0006277574769531687\n",
      "epoch: 2068, train_loss: 0.0006833671789575854, test_loss: 0.0006183440467187514\n",
      "epoch: 2069, train_loss: 0.0006844108752177461, test_loss: 0.00061744866737475\n",
      "epoch: 2070, train_loss: 0.0006830456727386817, test_loss: 0.000618900652625598\n",
      "epoch: 2071, train_loss: 0.0006847474597515943, test_loss: 0.0006345966927862415\n",
      "epoch: 2072, train_loss: 0.0006828883766581345, test_loss: 0.0006157511961646378\n",
      "epoch: 2073, train_loss: 0.0006831349531674515, test_loss: 0.0006385678425431252\n",
      "epoch: 2074, train_loss: 0.0006838518842730833, test_loss: 0.0006178144637184838\n",
      "epoch: 2075, train_loss: 0.0006813949857251314, test_loss: 0.0006176708072113494\n",
      "epoch: 2076, train_loss: 0.0006828440186243666, test_loss: 0.0006188649228230739\n",
      "epoch: 2077, train_loss: 0.0006821753315465605, test_loss: 0.0006192258394245679\n",
      "epoch: 2078, train_loss: 0.0006832869928431413, test_loss: 0.0006207280433348691\n",
      "epoch: 2079, train_loss: 0.0006817104256934608, test_loss: 0.0006169772095745429\n",
      "epoch: 2080, train_loss: 0.0006836150031354603, test_loss: 0.0006253595881086463\n",
      "epoch: 2081, train_loss: 0.0006822760548422356, test_loss: 0.0006290604408908015\n",
      "epoch: 2082, train_loss: 0.000684097429494495, test_loss: 0.0006300583093737563\n",
      "epoch: 2083, train_loss: 0.000685316297914023, test_loss: 0.0006234828373029208\n",
      "epoch: 2084, train_loss: 0.0006821110133466351, test_loss: 0.0006236704842497905\n",
      "epoch: 2085, train_loss: 0.0006826883232544945, test_loss: 0.0006145117852914458\n",
      "epoch: 2086, train_loss: 0.0006819585396442562, test_loss: 0.000624909308195735\n",
      "epoch: 2087, train_loss: 0.0006833589818004681, test_loss: 0.0006195745663717389\n",
      "epoch: 2088, train_loss: 0.0006804414619894131, test_loss: 0.0006138252889892707\n",
      "epoch: 2089, train_loss: 0.0006824443331899364, test_loss: 0.0006163728685351089\n",
      "epoch: 2090, train_loss: 0.0006835942596966481, test_loss: 0.0006192580864687139\n",
      "epoch: 2091, train_loss: 0.0006812579218682873, test_loss: 0.000623859086772427\n",
      "epoch: 2092, train_loss: 0.0006813115020201582, test_loss: 0.0006180058408062905\n",
      "epoch: 2093, train_loss: 0.0006819557469419163, test_loss: 0.0006193421674349034\n",
      "epoch: 2094, train_loss: 0.0006813248670052575, test_loss: 0.0006248750432860106\n",
      "epoch: 2095, train_loss: 0.0006819018491786783, test_loss: 0.0006134255042222018\n",
      "epoch: 2096, train_loss: 0.0006813548237044851, test_loss: 0.0006118896805370847\n",
      "epoch: 2097, train_loss: 0.0006807043379091698, test_loss: 0.0006161772907944396\n",
      "epoch: 2098, train_loss: 0.0006812888478516074, test_loss: 0.0006268998404266313\n",
      "epoch: 2099, train_loss: 0.0006819079609830742, test_loss: 0.0006236600553772101\n",
      "epoch: 2100, train_loss: 0.0006837995581166899, test_loss: 0.0006256924437669417\n",
      "epoch: 2101, train_loss: 0.0006818946440825644, test_loss: 0.000617621544127663\n",
      "epoch: 2102, train_loss: 0.0006810367390837358, test_loss: 0.0006306510767899454\n",
      "epoch: 2103, train_loss: 0.0006806052596126076, test_loss: 0.000623089867682817\n",
      "epoch: 2104, train_loss: 0.0006799141486661266, test_loss: 0.000614501540743125\n",
      "epoch: 2105, train_loss: 0.000680237790853109, test_loss: 0.0006299934369356682\n",
      "epoch: 2106, train_loss: 0.0006802373682148755, test_loss: 0.0006141423364169896\n",
      "epoch: 2107, train_loss: 0.0006804726777456539, test_loss: 0.0006158985488582402\n",
      "epoch: 2108, train_loss: 0.0006793590019578519, test_loss: 0.00062308601627592\n",
      "epoch: 2109, train_loss: 0.0006823015131015817, test_loss: 0.0006192733271745965\n",
      "epoch: 2110, train_loss: 0.0006812529602978865, test_loss: 0.0006179332946582387\n",
      "epoch: 2111, train_loss: 0.0006803647341692578, test_loss: 0.0006141177679334456\n",
      "epoch: 2112, train_loss: 0.0006816306989133844, test_loss: 0.0006167884857859462\n",
      "epoch: 2113, train_loss: 0.0006803718797923268, test_loss: 0.0006167931278469041\n",
      "epoch: 2114, train_loss: 0.0006800398873367711, test_loss: 0.0006346401884608591\n",
      "epoch: 2115, train_loss: 0.0006803666233874695, test_loss: 0.0006271165766520426\n",
      "epoch: 2116, train_loss: 0.0006815847338420217, test_loss: 0.0006290164553017045\n",
      "epoch: 2117, train_loss: 0.0006807716335574894, test_loss: 0.0006191250189052274\n",
      "epoch: 2118, train_loss: 0.0006798353203086425, test_loss: 0.000612720328111512\n",
      "epoch: 2119, train_loss: 0.0006801178058832074, test_loss: 0.0006158711039461195\n",
      "epoch: 2120, train_loss: 0.0006789612386416158, test_loss: 0.000625792303859877\n",
      "epoch: 2121, train_loss: 0.0006798214681507291, test_loss: 0.0006222048201986278\n",
      "epoch: 2122, train_loss: 0.0006788271395793265, test_loss: 0.0006150651315692812\n",
      "epoch: 2123, train_loss: 0.0006794867880196999, test_loss: 0.000616438701399602\n",
      "epoch: 2124, train_loss: 0.000681538619455355, test_loss: 0.0006161956892659267\n",
      "epoch: 2125, train_loss: 0.0006809500843266268, test_loss: 0.0006294041668297723\n",
      "epoch: 2126, train_loss: 0.0006803450359375743, test_loss: 0.0006214300325761238\n",
      "epoch: 2127, train_loss: 0.0006819456112166138, test_loss: 0.0006205571553437039\n",
      "epoch: 2128, train_loss: 0.0006797142357201031, test_loss: 0.000617772398982197\n",
      "epoch: 2129, train_loss: 0.000680460152975486, test_loss: 0.000623846470261924\n",
      "epoch: 2130, train_loss: 0.0006832449289504439, test_loss: 0.0006234024282700071\n",
      "epoch: 2131, train_loss: 0.0006798936810810119, test_loss: 0.0006177716665357972\n",
      "epoch: 2132, train_loss: 0.0006798852903201529, test_loss: 0.000615322554949671\n",
      "epoch: 2133, train_loss: 0.0006798778511280113, test_loss: 0.0006168484396766871\n",
      "epoch: 2134, train_loss: 0.0006794841496941998, test_loss: 0.0006346305841968084\n",
      "epoch: 2135, train_loss: 0.0006812955518556839, test_loss: 0.0006362422136589885\n",
      "epoch: 2136, train_loss: 0.0006817947420210618, test_loss: 0.0006153263432982689\n",
      "epoch: 2137, train_loss: 0.0006820465458314056, test_loss: 0.0006288298997484768\n",
      "epoch: 2138, train_loss: 0.000679611233467965, test_loss: 0.0006151604611659423\n",
      "epoch: 2139, train_loss: 0.00067940143281189, test_loss: 0.0006202171401431164\n",
      "epoch: 2140, train_loss: 0.0006777257127079951, test_loss: 0.000614169849238048\n",
      "epoch: 2141, train_loss: 0.0006803744662370857, test_loss: 0.0006176050228532404\n",
      "epoch: 2142, train_loss: 0.0006780990743604691, test_loss: 0.0006117182250212257\n",
      "epoch: 2143, train_loss: 0.0006785286532249302, test_loss: 0.0006111669936217368\n",
      "epoch: 2144, train_loss: 0.0006792565754554032, test_loss: 0.0006176457876184335\n",
      "epoch: 2145, train_loss: 0.0006798706561549689, test_loss: 0.000616191677787962\n",
      "epoch: 2146, train_loss: 0.000679708017623457, test_loss: 0.0006297561970617002\n",
      "epoch: 2147, train_loss: 0.0006802906168361559, test_loss: 0.0006230717456977194\n",
      "epoch: 2148, train_loss: 0.0006812867928681004, test_loss: 0.0006164927860178674\n",
      "epoch: 2149, train_loss: 0.0006798787191813892, test_loss: 0.0006106642831582576\n",
      "epoch: 2150, train_loss: 0.0006806550663895905, test_loss: 0.0006205149984452873\n",
      "epoch: 2151, train_loss: 0.0006796524495534275, test_loss: 0.0006168845963353912\n",
      "epoch: 2152, train_loss: 0.0006802424246890713, test_loss: 0.0006281266687437892\n",
      "epoch: 2153, train_loss: 0.0006780065520180632, test_loss: 0.0006184838421177119\n",
      "epoch: 2154, train_loss: 0.0006772405341389062, test_loss: 0.0006147038705724602\n",
      "epoch: 2155, train_loss: 0.0006779993089604313, test_loss: 0.0006107261676030854\n",
      "epoch: 2156, train_loss: 0.000677651600436429, test_loss: 0.0006180360117771974\n",
      "epoch: 2157, train_loss: 0.0006781229648091223, test_loss: 0.000615771597949788\n",
      "epoch: 2158, train_loss: 0.0006784159948280001, test_loss: 0.0006256612396100536\n",
      "epoch: 2159, train_loss: 0.0006796085394655719, test_loss: 0.0006161769803535814\n",
      "epoch: 2160, train_loss: 0.000679682480910307, test_loss: 0.0006130981200840324\n",
      "epoch: 2161, train_loss: 0.0006795372211617296, test_loss: 0.000631346891168505\n",
      "epoch: 2162, train_loss: 0.0006795424826281226, test_loss: 0.0006209760370741909\n",
      "epoch: 2163, train_loss: 0.0006783566179523325, test_loss: 0.0006192094103122751\n",
      "epoch: 2164, train_loss: 0.0006763209442041167, test_loss: 0.0006106742609214658\n",
      "epoch: 2165, train_loss: 0.0006783667663314744, test_loss: 0.0006127505815432718\n",
      "epoch: 2166, train_loss: 0.000677621615898755, test_loss: 0.0006156265541600684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2167, train_loss: 0.0006768280227729322, test_loss: 0.0006122907846778011\n",
      "epoch: 2168, train_loss: 0.000680481745486917, test_loss: 0.0006095953237187738\n",
      "epoch: 2169, train_loss: 0.0006777835951652378, test_loss: 0.0006198226522731906\n",
      "epoch: 2170, train_loss: 0.0006798216794698459, test_loss: 0.0006116419681347907\n",
      "epoch: 2171, train_loss: 0.0006768085852103388, test_loss: 0.0006182212237035856\n",
      "epoch: 2172, train_loss: 0.0006794900387910235, test_loss: 0.0006211050446533287\n",
      "epoch: 2173, train_loss: 0.0006774273402381526, test_loss: 0.0006388791468149672\n",
      "epoch: 2174, train_loss: 0.0006795295250966498, test_loss: 0.0006221240328159183\n",
      "epoch: 2175, train_loss: 0.0006783673648580747, test_loss: 0.0006183542185074961\n",
      "epoch: 2176, train_loss: 0.0006788708547980565, test_loss: 0.0006216578282571087\n",
      "epoch: 2177, train_loss: 0.000677167198813075, test_loss: 0.0006177238246891648\n",
      "epoch: 2178, train_loss: 0.0006758232560494672, test_loss: 0.0006123691564425826\n",
      "epoch: 2179, train_loss: 0.0006772129310537939, test_loss: 0.000610932576819323\n",
      "epoch: 2180, train_loss: 0.0006779906284266516, test_loss: 0.0006124164501670748\n",
      "epoch: 2181, train_loss: 0.0006794991913130102, test_loss: 0.0006094988639233634\n",
      "epoch: 2182, train_loss: 0.0006779320500082458, test_loss: 0.0006133835413493216\n",
      "epoch: 2183, train_loss: 0.0006755550553941208, test_loss: 0.0006165345063588271\n",
      "epoch: 2184, train_loss: 0.0006765634309925625, test_loss: 0.000613803708498987\n",
      "epoch: 2185, train_loss: 0.000675946463952246, test_loss: 0.0006256244378164411\n",
      "epoch: 2186, train_loss: 0.0006782971322536469, test_loss: 0.0006110727845225483\n",
      "epoch: 2187, train_loss: 0.0006784969958496969, test_loss: 0.0006141863899150243\n",
      "epoch: 2188, train_loss: 0.0006767834192546813, test_loss: 0.000614302009732152\n",
      "epoch: 2189, train_loss: 0.0006760387180332581, test_loss: 0.0006187158141983673\n",
      "epoch: 2190, train_loss: 0.0006771003207414533, test_loss: 0.0006099948417007303\n",
      "epoch: 2191, train_loss: 0.0006751671468879541, test_loss: 0.0006109848861039305\n",
      "epoch: 2192, train_loss: 0.0006762599982523724, test_loss: 0.0006184911083740493\n",
      "epoch: 2193, train_loss: 0.0006760126688396153, test_loss: 0.0006123581406427547\n",
      "epoch: 2194, train_loss: 0.0006744259469838732, test_loss: 0.0006164385267766193\n",
      "epoch: 2195, train_loss: 0.0006774384680244585, test_loss: 0.0006170945610695829\n",
      "epoch: 2196, train_loss: 0.0006746635658403292, test_loss: 0.000614632740810824\n",
      "epoch: 2197, train_loss: 0.0006754227924039183, test_loss: 0.0006178713132006427\n",
      "epoch: 2198, train_loss: 0.0006762916632199093, test_loss: 0.0006118907331256196\n",
      "epoch: 2199, train_loss: 0.0006765142999305997, test_loss: 0.0006187971205993866\n",
      "epoch: 2200, train_loss: 0.0006770288480607712, test_loss: 0.0006185106807000315\n",
      "epoch: 2201, train_loss: 0.0006778568064834436, test_loss: 0.0006113351555541158\n",
      "epoch: 2202, train_loss: 0.0006786533795884284, test_loss: 0.0006118649277292813\n",
      "epoch: 2203, train_loss: 0.0006767262251662981, test_loss: 0.0006147686702509721\n",
      "epoch: 2204, train_loss: 0.0006754784313354479, test_loss: 0.0006102879366759831\n",
      "epoch: 2205, train_loss: 0.0006772854147762384, test_loss: 0.0006110875498658667\n",
      "epoch: 2206, train_loss: 0.0006764097222754651, test_loss: 0.0006136100855655968\n",
      "epoch: 2207, train_loss: 0.0006740013258936613, test_loss: 0.0006080317980377004\n",
      "epoch: 2208, train_loss: 0.0006740784232059251, test_loss: 0.0006166634702822194\n",
      "epoch: 2209, train_loss: 0.0006746716896051784, test_loss: 0.0006241588853299618\n",
      "epoch: 2210, train_loss: 0.0006750442401465515, test_loss: 0.0006115260766819119\n",
      "epoch: 2211, train_loss: 0.000674789412068608, test_loss: 0.0006088538308783124\n",
      "epoch: 2212, train_loss: 0.0006745680622533773, test_loss: 0.0006155484491803994\n",
      "epoch: 2213, train_loss: 0.0006769747870630058, test_loss: 0.0006115155556472018\n",
      "epoch: 2214, train_loss: 0.0006756380595185834, test_loss: 0.0006162190111353993\n",
      "epoch: 2215, train_loss: 0.0006774707796031852, test_loss: 0.000609505019383505\n",
      "epoch: 2216, train_loss: 0.0006750246203686718, test_loss: 0.0006219227410232028\n",
      "epoch: 2217, train_loss: 0.0006755013600923121, test_loss: 0.0006128549672818432\n",
      "epoch: 2218, train_loss: 0.0006737312613277818, test_loss: 0.0006169262293648595\n",
      "epoch: 2219, train_loss: 0.0006777213521949623, test_loss: 0.0006133642503603672\n",
      "epoch: 2220, train_loss: 0.000674848005671621, test_loss: 0.0006094543738678718\n",
      "epoch: 2221, train_loss: 0.00067416470340939, test_loss: 0.0006150817692590257\n",
      "epoch: 2222, train_loss: 0.0006757228655497665, test_loss: 0.0006104994778676579\n",
      "epoch: 2223, train_loss: 0.0006755281444740198, test_loss: 0.0006118474217752615\n",
      "epoch: 2224, train_loss: 0.0006757799318343725, test_loss: 0.0006139997373490284\n",
      "epoch: 2225, train_loss: 0.0006745826204955254, test_loss: 0.0006129605074723562\n",
      "epoch: 2226, train_loss: 0.0006754394853487611, test_loss: 0.0006150949581448609\n",
      "epoch: 2227, train_loss: 0.0006736746126198736, test_loss: 0.000609483186660024\n",
      "epoch: 2228, train_loss: 0.0006743069502788231, test_loss: 0.0006098166729013125\n",
      "epoch: 2229, train_loss: 0.0006759890808177221, test_loss: 0.0006137339466173822\n",
      "epoch: 2230, train_loss: 0.0006763454306486022, test_loss: 0.0006348066817736253\n",
      "epoch: 2231, train_loss: 0.000677036074668412, test_loss: 0.0006276419250449786\n",
      "epoch: 2232, train_loss: 0.0006755401757444539, test_loss: 0.0006133286903301874\n",
      "epoch: 2233, train_loss: 0.0006744805584236493, test_loss: 0.0006289903297632312\n",
      "epoch: 2234, train_loss: 0.0006751084558503783, test_loss: 0.0006100168781510243\n",
      "epoch: 2235, train_loss: 0.0006765044641012893, test_loss: 0.0006068394044026112\n",
      "epoch: 2236, train_loss: 0.0006769729838909014, test_loss: 0.0006184808347218981\n",
      "epoch: 2237, train_loss: 0.0006784624635221679, test_loss: 0.0006083564415651684\n",
      "epoch: 2238, train_loss: 0.0006728177477666379, test_loss: 0.0006133593997219577\n",
      "epoch: 2239, train_loss: 0.0006746825529262424, test_loss: 0.0006307033666719993\n",
      "epoch: 2240, train_loss: 0.0006737159223437471, test_loss: 0.0006177028699312359\n",
      "epoch: 2241, train_loss: 0.0006742413097526878, test_loss: 0.000610215327469632\n",
      "epoch: 2242, train_loss: 0.000673901512409034, test_loss: 0.0006118174739337215\n",
      "epoch: 2243, train_loss: 0.0006757219797810135, test_loss: 0.0006083548408544933\n",
      "epoch: 2244, train_loss: 0.0006733035387815503, test_loss: 0.000618436800626417\n",
      "epoch: 2245, train_loss: 0.000674485373209514, test_loss: 0.0006122937872229764\n",
      "epoch: 2246, train_loss: 0.0006731251829162078, test_loss: 0.0006169897290722778\n",
      "epoch: 2247, train_loss: 0.0006733158256595387, test_loss: 0.000613226662001883\n",
      "epoch: 2248, train_loss: 0.0006749295217045785, test_loss: 0.0006055694539099932\n",
      "epoch: 2249, train_loss: 0.0006744337923642572, test_loss: 0.0006270878172169129\n",
      "epoch: 2250, train_loss: 0.0006730999739374965, test_loss: 0.0006105662517560025\n",
      "epoch: 2251, train_loss: 0.0006736052392111362, test_loss: 0.0006093759293435141\n",
      "epoch: 2252, train_loss: 0.0006721788706035231, test_loss: 0.0006157586079401275\n",
      "epoch: 2253, train_loss: 0.0006730438883254385, test_loss: 0.0006136130881107723\n",
      "epoch: 2254, train_loss: 0.0006738050382726057, test_loss: 0.0006131320163452377\n",
      "epoch: 2255, train_loss: 0.0006737175002775115, test_loss: 0.0006078818211487184\n",
      "epoch: 2256, train_loss: 0.0006737206093258346, test_loss: 0.000612953609864538\n",
      "epoch: 2257, train_loss: 0.0006725952793520106, test_loss: 0.0006107892113504931\n",
      "epoch: 2258, train_loss: 0.000672525230228253, test_loss: 0.0006115221379635235\n",
      "epoch: 2259, train_loss: 0.000673321007406744, test_loss: 0.000627588039302888\n",
      "epoch: 2260, train_loss: 0.0006710419231398112, test_loss: 0.0006118058032977084\n",
      "epoch: 2261, train_loss: 0.0006724634352038898, test_loss: 0.0006093127061224853\n",
      "epoch: 2262, train_loss: 0.0006734219812484378, test_loss: 0.0006078285320351521\n",
      "epoch: 2263, train_loss: 0.0006730771223690523, test_loss: 0.0006056035490473732\n",
      "epoch: 2264, train_loss: 0.000674738437342255, test_loss: 0.0006217773431368793\n",
      "epoch: 2265, train_loss: 0.0006730515261828575, test_loss: 0.0006090133586743226\n",
      "epoch: 2266, train_loss: 0.0006720942847491444, test_loss: 0.0006077252813459685\n",
      "epoch: 2267, train_loss: 0.0006717612154518619, test_loss: 0.0006056130077922717\n",
      "epoch: 2268, train_loss: 0.000671750991149684, test_loss: 0.0006183245326004302\n",
      "epoch: 2269, train_loss: 0.0006732807757899813, test_loss: 0.0006127895467216149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2270, train_loss: 0.000673012819353734, test_loss: 0.0006172238014793644\n",
      "epoch: 2271, train_loss: 0.0006720488081159799, test_loss: 0.0006091336399549618\n",
      "epoch: 2272, train_loss: 0.0006739072192905713, test_loss: 0.0006320271592509622\n",
      "epoch: 2273, train_loss: 0.000674270631229181, test_loss: 0.0006085769855417311\n",
      "epoch: 2274, train_loss: 0.000672367364724936, test_loss: 0.0006160759755099813\n",
      "epoch: 2275, train_loss: 0.0006754908839787316, test_loss: 0.0006102899593921999\n",
      "epoch: 2276, train_loss: 0.0006748186386413063, test_loss: 0.0006180226967747634\n",
      "epoch: 2277, train_loss: 0.0006717723280535606, test_loss: 0.0006074543634895235\n",
      "epoch: 2278, train_loss: 0.0006727780311611359, test_loss: 0.0006127093268635994\n",
      "epoch: 2279, train_loss: 0.0006723453508406553, test_loss: 0.0006170714041218162\n",
      "epoch: 2280, train_loss: 0.0006721416442736012, test_loss: 0.0006122007568289215\n",
      "epoch: 2281, train_loss: 0.0006726045622085423, test_loss: 0.0006101212444870422\n",
      "epoch: 2282, train_loss: 0.0006714068687237475, test_loss: 0.0006143176966967682\n",
      "epoch: 2283, train_loss: 0.000671907403992246, test_loss: 0.0006109777605161071\n",
      "epoch: 2284, train_loss: 0.0006704189391988937, test_loss: 0.0006048465826703856\n",
      "epoch: 2285, train_loss: 0.0006728140047609644, test_loss: 0.0006101595451279233\n",
      "epoch: 2286, train_loss: 0.0006706106157608978, test_loss: 0.0006082125764805824\n",
      "epoch: 2287, train_loss: 0.0006717584480572006, test_loss: 0.0006102919918096935\n",
      "epoch: 2288, train_loss: 0.000674326943344963, test_loss: 0.0006118287177135547\n",
      "epoch: 2289, train_loss: 0.0006727740426709795, test_loss: 0.000611017492095319\n",
      "epoch: 2290, train_loss: 0.0006743196041181521, test_loss: 0.0006058435246814042\n",
      "epoch: 2291, train_loss: 0.0006709001824239512, test_loss: 0.000613342912402004\n",
      "epoch: 2292, train_loss: 0.0006720054181009207, test_loss: 0.0006127193482825533\n",
      "epoch: 2293, train_loss: 0.0006717655051033944, test_loss: 0.0006072488564920301\n",
      "epoch: 2294, train_loss: 0.0006730009728293542, test_loss: 0.0006055659177945927\n",
      "epoch: 2295, train_loss: 0.0006721070069192058, test_loss: 0.0006060483234856898\n",
      "epoch: 2296, train_loss: 0.0006732461105971395, test_loss: 0.000627634619983534\n",
      "epoch: 2297, train_loss: 0.000672472615564323, test_loss: 0.000615586638256597\n",
      "epoch: 2298, train_loss: 0.0006719902613321724, test_loss: 0.0006095498635356004\n",
      "epoch: 2299, train_loss: 0.0006708971303179051, test_loss: 0.0006081725005060434\n",
      "epoch: 2300, train_loss: 0.0006708058113556193, test_loss: 0.0006087445217417553\n",
      "epoch: 2301, train_loss: 0.0006708303231077836, test_loss: 0.0006087263609515503\n",
      "epoch: 2302, train_loss: 0.0006725046652085755, test_loss: 0.0006129333002415175\n",
      "epoch: 2303, train_loss: 0.0006735622477920159, test_loss: 0.0006192812725203112\n",
      "epoch: 2304, train_loss: 0.0006734248764468762, test_loss: 0.0006146386973947907\n",
      "epoch: 2305, train_loss: 0.0006713837425669898, test_loss: 0.0006064158854618048\n",
      "epoch: 2306, train_loss: 0.0006700570533133072, test_loss: 0.0006059832861258959\n",
      "epoch: 2307, train_loss: 0.0006718822380365884, test_loss: 0.000608315211138688\n",
      "epoch: 2308, train_loss: 0.0006726611691587807, test_loss: 0.0006067382200853899\n",
      "epoch: 2309, train_loss: 0.0006713668180568873, test_loss: 0.0006076383967107782\n",
      "epoch: 2310, train_loss: 0.0006727057486347368, test_loss: 0.0006051719683455303\n",
      "epoch: 2311, train_loss: 0.0006717865218651359, test_loss: 0.0006134224871251112\n",
      "epoch: 2312, train_loss: 0.0006704793726701452, test_loss: 0.0006085784601358076\n",
      "epoch: 2313, train_loss: 0.0006696015125666947, test_loss: 0.0006079717471341913\n",
      "epoch: 2314, train_loss: 0.0006706873625618122, test_loss: 0.0006193591107148677\n",
      "epoch: 2315, train_loss: 0.0006718180185366098, test_loss: 0.0006152107186305026\n",
      "epoch: 2316, train_loss: 0.0006709100157224938, test_loss: 0.0006105963451166948\n",
      "epoch: 2317, train_loss: 0.000670249902886217, test_loss: 0.0006061923583426202\n",
      "epoch: 2318, train_loss: 0.0006702776173251154, test_loss: 0.000608362975375106\n",
      "epoch: 2319, train_loss: 0.0006700125333103958, test_loss: 0.0006056063624176508\n",
      "epoch: 2320, train_loss: 0.0006719212042694182, test_loss: 0.0006141312430069471\n",
      "epoch: 2321, train_loss: 0.0006689836926576074, test_loss: 0.0006181927650080373\n",
      "epoch: 2322, train_loss: 0.0006700214163056048, test_loss: 0.0006108183297328651\n",
      "epoch: 2323, train_loss: 0.0006716007166845804, test_loss: 0.0006179623402810345\n",
      "epoch: 2324, train_loss: 0.0006713438386846658, test_loss: 0.0006080566866633793\n",
      "epoch: 2325, train_loss: 0.0006714153417345623, test_loss: 0.0006171457498567179\n",
      "epoch: 2326, train_loss: 0.0006681373431448541, test_loss: 0.0006072209362173453\n",
      "epoch: 2327, train_loss: 0.0006710126902401934, test_loss: 0.000603834560024552\n",
      "epoch: 2328, train_loss: 0.0006698502946402068, test_loss: 0.0006044059749304628\n",
      "epoch: 2329, train_loss: 0.0006699995036520388, test_loss: 0.0006077378250968953\n",
      "epoch: 2330, train_loss: 0.0006696140474599341, test_loss: 0.0006080930907046422\n",
      "epoch: 2331, train_loss: 0.0006725360543224151, test_loss: 0.0006053399362523729\n",
      "epoch: 2332, train_loss: 0.0006711002661968055, test_loss: 0.0006064649351174012\n",
      "epoch: 2333, train_loss: 0.0006699832662454118, test_loss: 0.0006043455262746041\n",
      "epoch: 2334, train_loss: 0.0006706768446905619, test_loss: 0.0006057112332200631\n",
      "epoch: 2335, train_loss: 0.000672232412793876, test_loss: 0.0006057542874865854\n",
      "epoch: 2336, train_loss: 0.0006706629558365145, test_loss: 0.0006067469269813349\n",
      "epoch: 2337, train_loss: 0.0006691173235278415, test_loss: 0.0006119835209877541\n",
      "epoch: 2338, train_loss: 0.0006700407210028852, test_loss: 0.0006108035110325242\n",
      "epoch: 2339, train_loss: 0.0006705715103358355, test_loss: 0.000604767061304301\n",
      "epoch: 2340, train_loss: 0.0006703634116211501, test_loss: 0.0006075435521779582\n",
      "epoch: 2341, train_loss: 0.0006699172271233376, test_loss: 0.0006093143844433749\n",
      "epoch: 2342, train_loss: 0.0006702459207229803, test_loss: 0.0006100196042098105\n",
      "epoch: 2343, train_loss: 0.000669455974433652, test_loss: 0.0006165004306240007\n",
      "epoch: 2344, train_loss: 0.000670180296646836, test_loss: 0.0006204172629319752\n",
      "epoch: 2345, train_loss: 0.0006685324251125364, test_loss: 0.0006066435986819366\n",
      "epoch: 2346, train_loss: 0.0006708068439089086, test_loss: 0.0006177194494133195\n",
      "epoch: 2347, train_loss: 0.0006706056174943628, test_loss: 0.0006125439394963905\n",
      "epoch: 2348, train_loss: 0.0006695673661812654, test_loss: 0.0006021841254550964\n",
      "epoch: 2349, train_loss: 0.0006699551190452083, test_loss: 0.0006171410641400144\n",
      "epoch: 2350, train_loss: 0.0006710142567855022, test_loss: 0.000601517895120196\n",
      "epoch: 2351, train_loss: 0.0006695389861504183, test_loss: 0.000608289041944469\n",
      "epoch: 2352, train_loss: 0.0006708512082695961, test_loss: 0.000625099787915436\n",
      "epoch: 2353, train_loss: 0.0006682890525553375, test_loss: 0.0006204120145412162\n",
      "epoch: 2354, train_loss: 0.0006696815227927721, test_loss: 0.0006122397026047111\n",
      "epoch: 2355, train_loss: 0.0006688988524610581, test_loss: 0.0006060404314969977\n",
      "epoch: 2356, train_loss: 0.0006689697797612652, test_loss: 0.0006244485994102433\n",
      "epoch: 2357, train_loss: 0.0006701296294087787, test_loss: 0.0006126121540243427\n",
      "epoch: 2358, train_loss: 0.0006681691941238292, test_loss: 0.0006091545074013993\n",
      "epoch: 2359, train_loss: 0.0006692891310313312, test_loss: 0.0006216310042267045\n",
      "epoch: 2360, train_loss: 0.0006694367570478631, test_loss: 0.0006074131088098511\n",
      "epoch: 2361, train_loss: 0.0006697060434026239, test_loss: 0.0006128165793294708\n",
      "epoch: 2362, train_loss: 0.0006701409659834336, test_loss: 0.0006119630706962198\n",
      "epoch: 2363, train_loss: 0.0006693875108359624, test_loss: 0.0006066481582820415\n",
      "epoch: 2364, train_loss: 0.0006698058948487691, test_loss: 0.0006064237483466665\n",
      "epoch: 2365, train_loss: 0.0006689204335840338, test_loss: 0.0006098505449093258\n",
      "epoch: 2366, train_loss: 0.0006694296278747852, test_loss: 0.000605803657284317\n",
      "epoch: 2367, train_loss: 0.0006687204750841888, test_loss: 0.0006062397442292422\n",
      "epoch: 2368, train_loss: 0.0006684285535416363, test_loss: 0.0006049469763335461\n",
      "epoch: 2369, train_loss: 0.0006686028601783935, test_loss: 0.0006121229913939411\n",
      "epoch: 2370, train_loss: 0.0006715915122818526, test_loss: 0.0006078261794755235\n",
      "epoch: 2371, train_loss: 0.0006675992449543074, test_loss: 0.0006217526436860984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2372, train_loss: 0.0006704908332524254, test_loss: 0.0006148545605052883\n",
      "epoch: 2373, train_loss: 0.0006689682461159384, test_loss: 0.0006145510269561782\n",
      "epoch: 2374, train_loss: 0.0006686435992141132, test_loss: 0.0006061285481943438\n",
      "epoch: 2375, train_loss: 0.0006672225996305275, test_loss: 0.000607378775991189\n",
      "epoch: 2376, train_loss: 0.0006710930750199149, test_loss: 0.0006224156580477332\n",
      "epoch: 2377, train_loss: 0.0006674920821197978, test_loss: 0.0006022938808503872\n",
      "epoch: 2378, train_loss: 0.0006671321714985306, test_loss: 0.0006279992327714959\n",
      "epoch: 2379, train_loss: 0.0006686871587906195, test_loss: 0.0006206046430937325\n",
      "epoch: 2380, train_loss: 0.000668684377476735, test_loss: 0.0006000800446296731\n",
      "epoch: 2381, train_loss: 0.0006691805028822273, test_loss: 0.0006082724285079166\n",
      "epoch: 2382, train_loss: 0.000667368198501999, test_loss: 0.0006034405523678288\n",
      "epoch: 2383, train_loss: 0.0006684527185786029, test_loss: 0.0006053290465691438\n",
      "epoch: 2384, train_loss: 0.0006664135688207234, test_loss: 0.0006044020653159047\n",
      "epoch: 2385, train_loss: 0.0006682005882992045, test_loss: 0.000600521244147482\n",
      "epoch: 2386, train_loss: 0.0006678660562180954, test_loss: 0.0006147411622805521\n",
      "epoch: 2387, train_loss: 0.0006686669303630681, test_loss: 0.0006052638830927511\n",
      "epoch: 2388, train_loss: 0.0006672792749114982, test_loss: 0.0006053550556922952\n",
      "epoch: 2389, train_loss: 0.0006692870684555205, test_loss: 0.0006049634394003078\n",
      "epoch: 2390, train_loss: 0.0006681111775358896, test_loss: 0.0006131476305502778\n",
      "epoch: 2391, train_loss: 0.0006671390640448132, test_loss: 0.0006065018678782508\n",
      "epoch: 2392, train_loss: 0.0006691532528392323, test_loss: 0.000606499229130956\n",
      "epoch: 2393, train_loss: 0.00066876956059233, test_loss: 0.0006090721047560995\n",
      "epoch: 2394, train_loss: 0.000665860906061109, test_loss: 0.0006045466725481674\n",
      "epoch: 2395, train_loss: 0.0006676980587856277, test_loss: 0.0006094163206095496\n",
      "epoch: 2396, train_loss: 0.0006676954204601276, test_loss: 0.0006081223303529745\n",
      "epoch: 2397, train_loss: 0.0006675048333936897, test_loss: 0.0006221816584002227\n",
      "epoch: 2398, train_loss: 0.0006671213233620738, test_loss: 0.000604057606930534\n",
      "epoch: 2399, train_loss: 0.0006669700758167259, test_loss: 0.0006055304256733507\n",
      "epoch: 2400, train_loss: 0.0006681427438034798, test_loss: 0.0006029820942785591\n",
      "epoch: 2401, train_loss: 0.0006671685930442712, test_loss: 0.0006071758058775837\n",
      "epoch: 2402, train_loss: 0.0006697505039324903, test_loss: 0.0006133399923176815\n",
      "epoch: 2403, train_loss: 0.0006703799362699299, test_loss: 0.0006034268784181526\n",
      "epoch: 2404, train_loss: 0.0006664635843860552, test_loss: 0.0006018190518564855\n",
      "epoch: 2405, train_loss: 0.0006674140648734149, test_loss: 0.0006004314054735005\n",
      "epoch: 2406, train_loss: 0.0006682986720039954, test_loss: 0.0006200354352282981\n",
      "epoch: 2407, train_loss: 0.000668555593026964, test_loss: 0.0006039825530024245\n",
      "epoch: 2408, train_loss: 0.0006690734944245576, test_loss: 0.0006238470668904483\n",
      "epoch: 2409, train_loss: 0.0006685356784146279, test_loss: 0.0006133015607095634\n",
      "epoch: 2410, train_loss: 0.0006668725640654726, test_loss: 0.0006159525413143759\n",
      "epoch: 2411, train_loss: 0.0006653540147691156, test_loss: 0.0006054281499624873\n",
      "epoch: 2412, train_loss: 0.0006662766036638261, test_loss: 0.0006048857952312877\n",
      "epoch: 2413, train_loss: 0.000667555370297202, test_loss: 0.0006022752835027253\n",
      "epoch: 2414, train_loss: 0.0006692782057064545, test_loss: 0.0006099028784471253\n",
      "epoch: 2415, train_loss: 0.0006663438474314045, test_loss: 0.0006118219559236119\n",
      "epoch: 2416, train_loss: 0.0006669110203485774, test_loss: 0.0006122478031708548\n",
      "epoch: 2417, train_loss: 0.0006660373069082751, test_loss: 0.0006063781960013633\n",
      "epoch: 2418, train_loss: 0.0006650449384165847, test_loss: 0.0006079849505719418\n",
      "epoch: 2419, train_loss: 0.0006657937483396381, test_loss: 0.0006114987481851131\n",
      "epoch: 2420, train_loss: 0.0006689427081374047, test_loss: 0.0006068460449265937\n",
      "epoch: 2421, train_loss: 0.0006669696190131261, test_loss: 0.0006017927856494983\n",
      "epoch: 2422, train_loss: 0.000666848964655124, test_loss: 0.0006034485365186507\n",
      "epoch: 2423, train_loss: 0.0006682403694392871, test_loss: 0.0006236861081561074\n",
      "epoch: 2424, train_loss: 0.000666351247396644, test_loss: 0.000602805710514076\n",
      "epoch: 2425, train_loss: 0.0006661828564297732, test_loss: 0.0006186539540067315\n",
      "epoch: 2426, train_loss: 0.0006703183108070136, test_loss: 0.000609402937698178\n",
      "epoch: 2427, train_loss: 0.0006699075507324027, test_loss: 0.000602150369862405\n",
      "epoch: 2428, train_loss: 0.0006642534281127155, test_loss: 0.0006006518039309109\n",
      "epoch: 2429, train_loss: 0.0006648310493050224, test_loss: 0.0006134792832502475\n",
      "epoch: 2430, train_loss: 0.0006702830344337323, test_loss: 0.0006057865927383924\n",
      "epoch: 2431, train_loss: 0.0006662713713012636, test_loss: 0.0006084404400705049\n",
      "epoch: 2432, train_loss: 0.0006661214157129111, test_loss: 0.0006059239919219787\n",
      "epoch: 2433, train_loss: 0.0006651652296093981, test_loss: 0.0006031189647425587\n",
      "epoch: 2434, train_loss: 0.0006656884595387331, test_loss: 0.0006122608077324306\n",
      "epoch: 2435, train_loss: 0.0006681517672563052, test_loss: 0.0006180541628661255\n",
      "epoch: 2436, train_loss: 0.0006669100460029491, test_loss: 0.0006040934434471031\n",
      "epoch: 2437, train_loss: 0.0006677106987057334, test_loss: 0.0006082714389776811\n",
      "epoch: 2438, train_loss: 0.0006645052293922914, test_loss: 0.0006032416131347418\n",
      "epoch: 2439, train_loss: 0.0006677886033329465, test_loss: 0.0006020898387456933\n",
      "epoch: 2440, train_loss: 0.0006664956580726025, test_loss: 0.000613205823659276\n",
      "epoch: 2441, train_loss: 0.0006659650091972688, test_loss: 0.0006116618654535463\n",
      "epoch: 2442, train_loss: 0.0006641947661789701, test_loss: 0.0006035981205059215\n",
      "epoch: 2443, train_loss: 0.0006645679190430952, test_loss: 0.0006148192672602212\n",
      "epoch: 2444, train_loss: 0.0006652343461451971, test_loss: 0.0006057800395259013\n",
      "epoch: 2445, train_loss: 0.0006652628742259643, test_loss: 0.0006051709060557187\n",
      "epoch: 2446, train_loss: 0.0006679114341513132, test_loss: 0.0006011365476297215\n",
      "epoch: 2447, train_loss: 0.0006667946619690274, test_loss: 0.0006017397002627453\n",
      "epoch: 2448, train_loss: 0.0006655425936712519, test_loss: 0.0006019891540442283\n",
      "epoch: 2449, train_loss: 0.0006645394225969263, test_loss: 0.0006074626726331189\n",
      "epoch: 2450, train_loss: 0.000665601089839702, test_loss: 0.0006138246681075543\n",
      "epoch: 2451, train_loss: 0.0006686469309700086, test_loss: 0.0006166461195486287\n",
      "epoch: 2452, train_loss: 0.0006658178425151045, test_loss: 0.0006022706220392138\n",
      "epoch: 2453, train_loss: 0.0006648949727051607, test_loss: 0.0006107709389956047\n",
      "epoch: 2454, train_loss: 0.000665084270345371, test_loss: 0.0006053627342528974\n",
      "epoch: 2455, train_loss: 0.0006657904596067965, test_loss: 0.0006126249548591053\n",
      "epoch: 2456, train_loss: 0.0006691139841796426, test_loss: 0.0006166349921841174\n",
      "epoch: 2457, train_loss: 0.0006653230381704381, test_loss: 0.0006087666891592866\n",
      "epoch: 2458, train_loss: 0.0006643642200683446, test_loss: 0.0006090054569843536\n",
      "epoch: 2459, train_loss: 0.0006661828956566751, test_loss: 0.0006111383797057594\n",
      "epoch: 2460, train_loss: 0.0006649667288318439, test_loss: 0.0006047105562174693\n",
      "epoch: 2461, train_loss: 0.000666312870832727, test_loss: 0.0006032568732431779\n",
      "epoch: 2462, train_loss: 0.0006662016537080964, test_loss: 0.0006080112070776522\n",
      "epoch: 2463, train_loss: 0.0006656587002393992, test_loss: 0.0006154933507787064\n",
      "epoch: 2464, train_loss: 0.0006643002903412865, test_loss: 0.0005991622989919657\n",
      "epoch: 2465, train_loss: 0.0006652717243211911, test_loss: 0.0006033783186770355\n",
      "epoch: 2466, train_loss: 0.0006660865442624882, test_loss: 0.0006020204794670766\n",
      "epoch: 2467, train_loss: 0.0006660953285577505, test_loss: 0.0006003062590025365\n",
      "epoch: 2468, train_loss: 0.0006643887495358839, test_loss: 0.0006207595918870842\n",
      "epoch: 2469, train_loss: 0.0006682417145424077, test_loss: 0.0006111730520691102\n",
      "epoch: 2470, train_loss: 0.0006645829467426823, test_loss: 0.0006067535044470181\n",
      "epoch: 2471, train_loss: 0.0006667021674650681, test_loss: 0.000602734619557547\n",
      "epoch: 2472, train_loss: 0.0006644855134452329, test_loss: 0.0006008386020160591\n",
      "epoch: 2473, train_loss: 0.0006631893908058333, test_loss: 0.0006149624532554299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2474, train_loss: 0.0006652279066063626, test_loss: 0.0006165555096231401\n",
      "epoch: 2475, train_loss: 0.000665825546172488, test_loss: 0.0006000951301151266\n",
      "epoch: 2476, train_loss: 0.0006649483541917542, test_loss: 0.0006105120507224152\n",
      "epoch: 2477, train_loss: 0.0006661267822061706, test_loss: 0.0006064865592634305\n",
      "epoch: 2478, train_loss: 0.0006646240881704928, test_loss: 0.0005995852843625471\n",
      "epoch: 2479, train_loss: 0.0006655269218912429, test_loss: 0.0006045732880011201\n",
      "epoch: 2480, train_loss: 0.0006656845646869878, test_loss: 0.0006038680682346845\n",
      "epoch: 2481, train_loss: 0.0006646822793814151, test_loss: 0.0006048092764103785\n",
      "epoch: 2482, train_loss: 0.0006644571346797696, test_loss: 0.0005968894741575544\n",
      "epoch: 2483, train_loss: 0.0006656240919888344, test_loss: 0.0006094836183668425\n",
      "epoch: 2484, train_loss: 0.0006650888130736902, test_loss: 0.0006014190051549425\n",
      "epoch: 2485, train_loss: 0.0006648795502057866, test_loss: 0.0006079999729990959\n",
      "epoch: 2486, train_loss: 0.0006645620172924322, test_loss: 0.0005977031881532943\n",
      "epoch: 2487, train_loss: 0.0006644625441960828, test_loss: 0.0006016055219030628\n",
      "epoch: 2488, train_loss: 0.0006634597603292407, test_loss: 0.0006152824838257706\n",
      "epoch: 2489, train_loss: 0.0006654508470091969, test_loss: 0.0006008307342805589\n",
      "epoch: 2490, train_loss: 0.0006652444717474282, test_loss: 0.0006131611250263328\n",
      "epoch: 2491, train_loss: 0.0006690268030228174, test_loss: 0.0006234336712320024\n",
      "epoch: 2492, train_loss: 0.0006654632654871144, test_loss: 0.000601112493313849\n",
      "epoch: 2493, train_loss: 0.0006642562144881357, test_loss: 0.0006036410244026532\n",
      "epoch: 2494, train_loss: 0.0006637644065726224, test_loss: 0.0006011367271033426\n",
      "epoch: 2495, train_loss: 0.0006638421618845314, test_loss: 0.0006090454893031468\n",
      "epoch: 2496, train_loss: 0.0006655004880208846, test_loss: 0.000602941281006982\n",
      "epoch: 2497, train_loss: 0.000664151921544386, test_loss: 0.0006040040316293016\n",
      "epoch: 2498, train_loss: 0.0006673986512317282, test_loss: 0.0005980470693126941\n",
      "epoch: 2499, train_loss: 0.0006632702576968333, test_loss: 0.0006077620685876658\n",
      "epoch: 2500, train_loss: 0.0006666210221896029, test_loss: 0.0006048530097662782\n",
      "epoch: 2501, train_loss: 0.0006665971772947713, test_loss: 0.0005995078148165097\n",
      "epoch: 2502, train_loss: 0.0006631826134094888, test_loss: 0.0006036497507011518\n",
      "epoch: 2503, train_loss: 0.0006649256380193907, test_loss: 0.0006039907360294213\n",
      "epoch: 2504, train_loss: 0.0006650301701207038, test_loss: 0.0005989234244528537\n",
      "epoch: 2505, train_loss: 0.000663687830598539, test_loss: 0.0006032187957316637\n",
      "epoch: 2506, train_loss: 0.0006650680810233335, test_loss: 0.000602024262965036\n",
      "epoch: 2507, train_loss: 0.0006637035808323518, test_loss: 0.0005988024640828371\n",
      "epoch: 2508, train_loss: 0.0006644122882078037, test_loss: 0.0005992012156639248\n",
      "epoch: 2509, train_loss: 0.000662297668421398, test_loss: 0.0006174214989490187\n",
      "epoch: 2510, train_loss: 0.0006633610996093763, test_loss: 0.000602274012635462\n",
      "epoch: 2511, train_loss: 0.0006640447447906532, test_loss: 0.0006023787670225526\n",
      "epoch: 2512, train_loss: 0.0006638391920284408, test_loss: 0.000600408394044886\n",
      "epoch: 2513, train_loss: 0.0006622381776611766, test_loss: 0.0006059048367508998\n",
      "epoch: 2514, train_loss: 0.0006634064952579691, test_loss: 0.0006044925685273483\n",
      "epoch: 2515, train_loss: 0.0006666171589724557, test_loss: 0.000599482916489554\n",
      "epoch: 2516, train_loss: 0.0006618066387170035, test_loss: 0.0006064578046789393\n",
      "epoch: 2517, train_loss: 0.0006628056770474043, test_loss: 0.0006054974170789743\n",
      "epoch: 2518, train_loss: 0.0006636166097292596, test_loss: 0.000600372203431713\n",
      "epoch: 2519, train_loss: 0.0006628039953521575, test_loss: 0.0006015126321775218\n",
      "epoch: 2520, train_loss: 0.0006616438559317232, test_loss: 0.0006055063907600319\n",
      "epoch: 2521, train_loss: 0.0006622458041301402, test_loss: 0.0006043876249653598\n",
      "epoch: 2522, train_loss: 0.0006621337936097837, test_loss: 0.0006000218078649292\n",
      "epoch: 2523, train_loss: 0.000662585865939036, test_loss: 0.0006032309368796026\n",
      "epoch: 2524, train_loss: 0.0006633854304116381, test_loss: 0.0006055301006805772\n",
      "epoch: 2525, train_loss: 0.000663452209783313, test_loss: 0.0006077103413796673\n",
      "epoch: 2526, train_loss: 0.0006632600118831286, test_loss: 0.0006118319288361818\n",
      "epoch: 2527, train_loss: 0.0006639248395399393, test_loss: 0.0006055018893675879\n",
      "epoch: 2528, train_loss: 0.0006636901639665113, test_loss: 0.0006020151971218487\n",
      "epoch: 2529, train_loss: 0.0006617927169629737, test_loss: 0.0006023077148711309\n",
      "epoch: 2530, train_loss: 0.0006629571144003421, test_loss: 0.0006014607982554784\n",
      "epoch: 2531, train_loss: 0.0006637611798435936, test_loss: 0.0006220362314100688\n",
      "epoch: 2532, train_loss: 0.0006627518045918449, test_loss: 0.0006143698653128619\n",
      "epoch: 2533, train_loss: 0.0006621190645408047, test_loss: 0.0006016205637327706\n",
      "epoch: 2534, train_loss: 0.0006632279508504207, test_loss: 0.0005955586869580051\n",
      "epoch: 2535, train_loss: 0.0006633990155735418, test_loss: 0.0006032774108462036\n",
      "epoch: 2536, train_loss: 0.0006632477806820332, test_loss: 0.0005977991822874174\n",
      "epoch: 2537, train_loss: 0.0006645668434667523, test_loss: 0.0006069191828525314\n",
      "epoch: 2538, train_loss: 0.0006617678838032905, test_loss: 0.0006021331694986051\n",
      "epoch: 2539, train_loss: 0.0006632691454243562, test_loss: 0.0006021264610656848\n",
      "epoch: 2540, train_loss: 0.0006646323979467801, test_loss: 0.0005964359491675472\n",
      "epoch: 2541, train_loss: 0.0006638733067792718, test_loss: 0.0006044446442198629\n",
      "epoch: 2542, train_loss: 0.0006647596614050639, test_loss: 0.0006182680808706209\n",
      "epoch: 2543, train_loss: 0.0006625700005552853, test_loss: 0.0006013776971182475\n",
      "epoch: 2544, train_loss: 0.0006631658837685119, test_loss: 0.0006060055699587489\n",
      "epoch: 2545, train_loss: 0.0006615577971704466, test_loss: 0.0006040968340433513\n",
      "epoch: 2546, train_loss: 0.0006634073342075167, test_loss: 0.0005964478962899497\n",
      "epoch: 2547, train_loss: 0.0006648094884281897, test_loss: 0.0006007083575241268\n",
      "epoch: 2548, train_loss: 0.0006635441905413957, test_loss: 0.0005991402285872027\n",
      "epoch: 2549, train_loss: 0.0006657980810142244, test_loss: 0.0006043937270684788\n",
      "epoch: 2550, train_loss: 0.000663491341781438, test_loss: 0.0005953520885668695\n",
      "epoch: 2551, train_loss: 0.0006611021045777622, test_loss: 0.0006018420487331847\n",
      "epoch: 2552, train_loss: 0.0006625938758193313, test_loss: 0.0006052856382060176\n",
      "epoch: 2553, train_loss: 0.0006612732920431248, test_loss: 0.0006103708825927848\n",
      "epoch: 2554, train_loss: 0.0006623520432343787, test_loss: 0.0006033906878049796\n",
      "epoch: 2555, train_loss: 0.0006667461420875043, test_loss: 0.0006017564931729188\n",
      "epoch: 2556, train_loss: 0.0006621971539140719, test_loss: 0.0006144055757128323\n",
      "epoch: 2557, train_loss: 0.0006625133607050647, test_loss: 0.0006035583695241561\n",
      "epoch: 2558, train_loss: 0.0006618820606609402, test_loss: 0.0005985288492714366\n",
      "epoch: 2559, train_loss: 0.0006604409427382052, test_loss: 0.0005995088916582366\n",
      "epoch: 2560, train_loss: 0.0006617923613900886, test_loss: 0.000599899169174023\n",
      "epoch: 2561, train_loss: 0.0006608473903843728, test_loss: 0.0006091295411655059\n",
      "epoch: 2562, train_loss: 0.0006628988308163927, test_loss: 0.0006100783890966947\n",
      "epoch: 2563, train_loss: 0.0006639255987702991, test_loss: 0.0006001697232325872\n",
      "epoch: 2564, train_loss: 0.0006637976899662095, test_loss: 0.000602309800645647\n",
      "epoch: 2565, train_loss: 0.0006622562195052919, test_loss: 0.0006045461923349649\n",
      "epoch: 2566, train_loss: 0.0006613853126865528, test_loss: 0.0006109050251931573\n",
      "epoch: 2567, train_loss: 0.0006626261215980934, test_loss: 0.0005992742226226255\n",
      "epoch: 2568, train_loss: 0.0006610188587649685, test_loss: 0.0006024985826419046\n",
      "epoch: 2569, train_loss: 0.0006624976307173948, test_loss: 0.000595670435965682\n",
      "epoch: 2570, train_loss: 0.0006649779388681054, test_loss: 0.0005991669895593077\n",
      "epoch: 2571, train_loss: 0.0006623717402006783, test_loss: 0.0006005461133706073\n",
      "epoch: 2572, train_loss: 0.0006614978129104913, test_loss: 0.0006012060281742985\n",
      "epoch: 2573, train_loss: 0.0006615432047629324, test_loss: 0.0006032309320289642\n",
      "epoch: 2574, train_loss: 0.0006625655299538504, test_loss: 0.0006159011924561734\n",
      "epoch: 2575, train_loss: 0.000664225415043209, test_loss: 0.0006201190747863924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2576, train_loss: 0.0006627145213196459, test_loss: 0.0006001663617401695\n",
      "epoch: 2577, train_loss: 0.0006607044779229909, test_loss: 0.0005957207176834345\n",
      "epoch: 2578, train_loss: 0.0006620988791363071, test_loss: 0.0006153735012048855\n",
      "epoch: 2579, train_loss: 0.0006618761956064111, test_loss: 0.0006018111307639629\n",
      "epoch: 2580, train_loss: 0.000660049310207124, test_loss: 0.0006083696644054726\n",
      "epoch: 2581, train_loss: 0.0006622036668451746, test_loss: 0.0006011992954881862\n",
      "epoch: 2582, train_loss: 0.0006603510549251476, test_loss: 0.0005990312589953343\n",
      "epoch: 2583, train_loss: 0.0006621643867971295, test_loss: 0.0006069598360530412\n",
      "epoch: 2584, train_loss: 0.0006603877624476571, test_loss: 0.0006022238764368618\n",
      "epoch: 2585, train_loss: 0.0006613852797865706, test_loss: 0.0006054904758154104\n",
      "epoch: 2586, train_loss: 0.0006619112226990578, test_loss: 0.0005978248324633265\n",
      "epoch: 2587, train_loss: 0.0006600727261368023, test_loss: 0.0005993390902100751\n",
      "epoch: 2588, train_loss: 0.0006607684772461653, test_loss: 0.0006071054861725619\n",
      "epoch: 2589, train_loss: 0.0006623110941449261, test_loss: 0.0005993330851197243\n",
      "epoch: 2590, train_loss: 0.0006625200216860875, test_loss: 0.000614797419984825\n",
      "epoch: 2591, train_loss: 0.000663552820459818, test_loss: 0.0006081752508180216\n",
      "epoch: 2592, train_loss: 0.0006614295479780792, test_loss: 0.0005993510567350313\n",
      "epoch: 2593, train_loss: 0.0006617746004612063, test_loss: 0.0006039229871627564\n",
      "epoch: 2594, train_loss: 0.0006618984853443892, test_loss: 0.0006177664423982302\n",
      "epoch: 2595, train_loss: 0.0006602462621547444, test_loss: 0.0006172864183705921\n",
      "epoch: 2596, train_loss: 0.0006600287021643927, test_loss: 0.0006038093173022693\n",
      "epoch: 2597, train_loss: 0.0006625918676550298, test_loss: 0.0006013220991007984\n",
      "epoch: 2598, train_loss: 0.0006610553056183878, test_loss: 0.0006013459787936881\n",
      "epoch: 2599, train_loss: 0.0006617759746681575, test_loss: 0.0006054907523017997\n",
      "epoch: 2600, train_loss: 0.0006628669836335694, test_loss: 0.000598827541883414\n",
      "epoch: 2601, train_loss: 0.0006598376152713014, test_loss: 0.0005983983331437533\n",
      "epoch: 2602, train_loss: 0.0006606594593588101, test_loss: 0.0006009395874571055\n",
      "epoch: 2603, train_loss: 0.0006606734823435545, test_loss: 0.0005969447423315918\n",
      "epoch: 2604, train_loss: 0.00066052635741906, test_loss: 0.0006013252374638493\n",
      "epoch: 2605, train_loss: 0.0006610563811947308, test_loss: 0.0006171634401349971\n",
      "epoch: 2606, train_loss: 0.0006605256197002271, test_loss: 0.0005987941743417954\n",
      "epoch: 2607, train_loss: 0.0006598194924426143, test_loss: 0.0006008217460475862\n",
      "epoch: 2608, train_loss: 0.0006613265178874944, test_loss: 0.0006002478718680019\n",
      "epoch: 2609, train_loss: 0.000659980778278702, test_loss: 0.0005959535968334725\n",
      "epoch: 2610, train_loss: 0.0006605712772832941, test_loss: 0.0005994123639538884\n",
      "epoch: 2611, train_loss: 0.0006607273623914174, test_loss: 0.0005989454851563399\n",
      "epoch: 2612, train_loss: 0.000661408656489347, test_loss: 0.0005999975352703283\n",
      "epoch: 2613, train_loss: 0.000660055663699851, test_loss: 0.0005964072721932704\n",
      "epoch: 2614, train_loss: 0.0006613426274903443, test_loss: 0.0006080166252407556\n",
      "epoch: 2615, train_loss: 0.0006599578166218555, test_loss: 0.0005984220527655756\n",
      "epoch: 2616, train_loss: 0.0006597417674999198, test_loss: 0.0006062472433162233\n",
      "epoch: 2617, train_loss: 0.0006602241533666687, test_loss: 0.0006046804919606075\n",
      "epoch: 2618, train_loss: 0.0006623369763078897, test_loss: 0.000607059831963852\n",
      "epoch: 2619, train_loss: 0.0006619101686342417, test_loss: 0.0006044226029189304\n",
      "epoch: 2620, train_loss: 0.0006599246509089742, test_loss: 0.0006203065277077258\n",
      "epoch: 2621, train_loss: 0.0006613604833230214, test_loss: 0.0005975821113679558\n",
      "epoch: 2622, train_loss: 0.0006603597582358381, test_loss: 0.0005952969416587924\n",
      "epoch: 2623, train_loss: 0.000660293962067479, test_loss: 0.0005967586572902898\n",
      "epoch: 2624, train_loss: 0.0006606596516971679, test_loss: 0.000610547101435562\n",
      "epoch: 2625, train_loss: 0.000659226830896881, test_loss: 0.0005996477654358993\n",
      "epoch: 2626, train_loss: 0.0006601074432103854, test_loss: 0.0006005117757013068\n",
      "epoch: 2627, train_loss: 0.0006597570710532043, test_loss: 0.0006030247119876245\n",
      "epoch: 2628, train_loss: 0.0006586337112821639, test_loss: 0.0005969710958500704\n",
      "epoch: 2629, train_loss: 0.0006609685179959658, test_loss: 0.0005977997546627497\n",
      "epoch: 2630, train_loss: 0.0006646958797279259, test_loss: 0.0006026342792514091\n",
      "epoch: 2631, train_loss: 0.0006612566851443895, test_loss: 0.0006110198204017555\n",
      "epoch: 2632, train_loss: 0.0006588167794372724, test_loss: 0.0005979379978574192\n",
      "epoch: 2633, train_loss: 0.0006591070281422657, test_loss: 0.0006016277135737861\n",
      "epoch: 2634, train_loss: 0.0006597656857870195, test_loss: 0.0005964857458214586\n",
      "epoch: 2635, train_loss: 0.0006608809825316396, test_loss: 0.0006105776604575416\n",
      "epoch: 2636, train_loss: 0.0006597328503393446, test_loss: 0.0005939315645567452\n",
      "epoch: 2637, train_loss: 0.000660341826480125, test_loss: 0.0006043337489245459\n",
      "epoch: 2638, train_loss: 0.0006604976623076136, test_loss: 0.0006034076068317518\n",
      "epoch: 2639, train_loss: 0.0006595754973165205, test_loss: 0.0006003978924127296\n",
      "epoch: 2640, train_loss: 0.000660539408588944, test_loss: 0.0005966975877527148\n",
      "epoch: 2641, train_loss: 0.0006601312994936724, test_loss: 0.0006128137805111086\n",
      "epoch: 2642, train_loss: 0.0006602510491021625, test_loss: 0.0006026644502223159\n",
      "epoch: 2643, train_loss: 0.0006591620546279718, test_loss: 0.000602006281648452\n",
      "epoch: 2644, train_loss: 0.0006601092096863557, test_loss: 0.000600082009138229\n",
      "epoch: 2645, train_loss: 0.0006600708710839567, test_loss: 0.0005976519896648824\n",
      "epoch: 2646, train_loss: 0.0006589243307446494, test_loss: 0.0006074873526813462\n",
      "epoch: 2647, train_loss: 0.000659716191559868, test_loss: 0.0006056931160856038\n",
      "epoch: 2648, train_loss: 0.0006619199639712663, test_loss: 0.0005990898450060437\n",
      "epoch: 2649, train_loss: 0.0006635159648873884, test_loss: 0.0006149034840442861\n",
      "epoch: 2650, train_loss: 0.0006617812690345328, test_loss: 0.0005985172271418074\n",
      "epoch: 2651, train_loss: 0.0006591147140842741, test_loss: 0.0006045579696850231\n",
      "epoch: 2652, train_loss: 0.0006591343313313859, test_loss: 0.0006211527797859162\n",
      "epoch: 2653, train_loss: 0.000659854878904298, test_loss: 0.0005979880176406974\n",
      "epoch: 2654, train_loss: 0.0006589089537990968, test_loss: 0.0006063795541801179\n",
      "epoch: 2655, train_loss: 0.0006587758164285966, test_loss: 0.0006029291059045742\n",
      "epoch: 2656, train_loss: 0.0006595154928104223, test_loss: 0.0006104685793009897\n",
      "epoch: 2657, train_loss: 0.0006598350224996228, test_loss: 0.000606935466445672\n",
      "epoch: 2658, train_loss: 0.0006583071890813501, test_loss: 0.0006059709849068895\n",
      "epoch: 2659, train_loss: 0.0006604818614584434, test_loss: 0.0005988935251176978\n",
      "epoch: 2660, train_loss: 0.0006596855135917988, test_loss: 0.0006007728807162493\n",
      "epoch: 2661, train_loss: 0.0006576706490823594, test_loss: 0.0006073401843120033\n",
      "epoch: 2662, train_loss: 0.00065673295150349, test_loss: 0.0006057534725793327\n",
      "epoch: 2663, train_loss: 0.0006595048003161893, test_loss: 0.000605340018713226\n",
      "epoch: 2664, train_loss: 0.0006583673770681186, test_loss: 0.0005977307543313751\n",
      "epoch: 2665, train_loss: 0.000658533286617097, test_loss: 0.0006063205073587596\n",
      "epoch: 2666, train_loss: 0.0006594667894482288, test_loss: 0.0006073463591746986\n",
      "epoch: 2667, train_loss: 0.0006581506914580646, test_loss: 0.0006130451074568555\n",
      "epoch: 2668, train_loss: 0.0006602069441451813, test_loss: 0.0006025557183117295\n",
      "epoch: 2669, train_loss: 0.0006610992840369759, test_loss: 0.0006047375063644722\n",
      "epoch: 2670, train_loss: 0.0006602956273127347, test_loss: 0.0005946249875705689\n",
      "epoch: 2671, train_loss: 0.000658000570104417, test_loss: 0.000600300554651767\n",
      "epoch: 2672, train_loss: 0.000658066386518919, test_loss: 0.0005945710387701789\n",
      "epoch: 2673, train_loss: 0.0006587530293947328, test_loss: 0.0006034162555200359\n",
      "epoch: 2674, train_loss: 0.0006589444136530485, test_loss: 0.0006047976688326647\n",
      "epoch: 2675, train_loss: 0.0006615640380440037, test_loss: 0.0005968559999018908\n",
      "epoch: 2676, train_loss: 0.0006578985611786661, test_loss: 0.0005974502370615179\n",
      "epoch: 2677, train_loss: 0.0006596059702923926, test_loss: 0.0006239058081215868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2678, train_loss: 0.0006575582058006978, test_loss: 0.0006038992869434878\n",
      "epoch: 2679, train_loss: 0.0006601399294120948, test_loss: 0.0005970644851913676\n",
      "epoch: 2680, train_loss: 0.0006567369754243966, test_loss: 0.0006017074532185992\n",
      "epoch: 2681, train_loss: 0.0006579079857581984, test_loss: 0.0006051933936153849\n",
      "epoch: 2682, train_loss: 0.0006573098780000178, test_loss: 0.0005984520054577539\n",
      "epoch: 2683, train_loss: 0.0006569843314102162, test_loss: 0.0006006443970060597\n",
      "epoch: 2684, train_loss: 0.0006575680593453834, test_loss: 0.0006008255149936303\n",
      "epoch: 2685, train_loss: 0.0006599025699593451, test_loss: 0.0006067597666212047\n",
      "epoch: 2686, train_loss: 0.0006586396459328091, test_loss: 0.0005971868037401388\n",
      "epoch: 2687, train_loss: 0.0006599915593498102, test_loss: 0.0005946704040979967\n",
      "epoch: 2688, train_loss: 0.0006604098649088131, test_loss: 0.0006119967729318887\n",
      "epoch: 2689, train_loss: 0.0006621480405674843, test_loss: 0.0006117585386770467\n",
      "epoch: 2690, train_loss: 0.0006565131783566397, test_loss: 0.0005980310379527509\n",
      "epoch: 2691, train_loss: 0.000658199169581918, test_loss: 0.0006077393627492711\n",
      "epoch: 2692, train_loss: 0.0006597591931020598, test_loss: 0.0006025272838693733\n",
      "epoch: 2693, train_loss: 0.000657284074290858, test_loss: 0.0005960249012180915\n",
      "epoch: 2694, train_loss: 0.0006605842082417043, test_loss: 0.0005977103234423945\n",
      "epoch: 2695, train_loss: 0.0006574484944829475, test_loss: 0.000597464405776312\n",
      "epoch: 2696, train_loss: 0.0006588049139321337, test_loss: 0.0005953680035114909\n",
      "epoch: 2697, train_loss: 0.0006593515699142185, test_loss: 0.0006027882676183557\n",
      "epoch: 2698, train_loss: 0.0006573037611340861, test_loss: 0.0005998535780236125\n",
      "epoch: 2699, train_loss: 0.0006585957611526322, test_loss: 0.0006105393792192141\n",
      "epoch: 2700, train_loss: 0.0006581204614359076, test_loss: 0.0005976157844997942\n",
      "epoch: 2701, train_loss: 0.0006569761304569471, test_loss: 0.0005961196050823977\n",
      "epoch: 2702, train_loss: 0.0006577594284190918, test_loss: 0.0006011495327887436\n",
      "epoch: 2703, train_loss: 0.0006603115939272001, test_loss: 0.0006080474995542318\n",
      "epoch: 2704, train_loss: 0.0006591151228032844, test_loss: 0.0005949880530048782\n",
      "epoch: 2705, train_loss: 0.000658527516466363, test_loss: 0.000596711179241538\n",
      "epoch: 2706, train_loss: 0.0006582569191738477, test_loss: 0.0006037808101003369\n",
      "epoch: 2707, train_loss: 0.0006573970996491287, test_loss: 0.0005976934674739217\n",
      "epoch: 2708, train_loss: 0.0006558713110912915, test_loss: 0.0005991282572116082\n",
      "epoch: 2709, train_loss: 0.0006585013217535679, test_loss: 0.000598429687670432\n",
      "epoch: 2710, train_loss: 0.0006589830989706452, test_loss: 0.0005974418697102616\n",
      "epoch: 2711, train_loss: 0.0006584273790434489, test_loss: 0.0006001985699792082\n",
      "epoch: 2712, train_loss: 0.0006594792636030394, test_loss: 0.000593556464688542\n",
      "epoch: 2713, train_loss: 0.0006580840512786223, test_loss: 0.0005997415913346534\n",
      "epoch: 2714, train_loss: 0.0006567948857200859, test_loss: 0.000597464216601414\n",
      "epoch: 2715, train_loss: 0.0006580917827744523, test_loss: 0.0005959926590245838\n",
      "epoch: 2716, train_loss: 0.0006584466571676667, test_loss: 0.0005966853253388157\n",
      "epoch: 2717, train_loss: 0.0006578840004057502, test_loss: 0.0005993999657221138\n",
      "epoch: 2718, train_loss: 0.0006583269088245604, test_loss: 0.0005932796484557912\n",
      "epoch: 2719, train_loss: 0.0006587624704242562, test_loss: 0.000596899728407152\n",
      "epoch: 2720, train_loss: 0.0006582127117207679, test_loss: 0.0006071027892176062\n",
      "epoch: 2721, train_loss: 0.0006586782945542718, test_loss: 0.0005942399681468183\n",
      "epoch: 2722, train_loss: 0.000657894358838625, test_loss: 0.0005964719457551837\n",
      "epoch: 2723, train_loss: 0.0006558217649833988, test_loss: 0.0005934375270347422\n",
      "epoch: 2724, train_loss: 0.0006583186243559518, test_loss: 0.0006024636337921644\n",
      "epoch: 2725, train_loss: 0.0006579364822043673, test_loss: 0.0005976459166655937\n",
      "epoch: 2726, train_loss: 0.0006586642956118221, test_loss: 0.0006019151187501848\n",
      "epoch: 2727, train_loss: 0.000658533907920608, test_loss: 0.0006083009745149562\n",
      "epoch: 2728, train_loss: 0.0006587324264135374, test_loss: 0.0005986573087284341\n",
      "epoch: 2729, train_loss: 0.0006595838513812455, test_loss: 0.0006101180285137767\n",
      "epoch: 2730, train_loss: 0.0006576864208276991, test_loss: 0.0005985733005218208\n",
      "epoch: 2731, train_loss: 0.0006599079035526222, test_loss: 0.0006073294061934575\n",
      "epoch: 2732, train_loss: 0.0006584311460914171, test_loss: 0.0005941659134502212\n",
      "epoch: 2733, train_loss: 0.0006569729100548378, test_loss: 0.0005999000568408519\n",
      "epoch: 2734, train_loss: 0.0006574754977760756, test_loss: 0.0005998042809854572\n",
      "epoch: 2735, train_loss: 0.0006592639028499632, test_loss: 0.0006203317170729861\n",
      "epoch: 2736, train_loss: 0.0006562688200653571, test_loss: 0.0006037426840824386\n",
      "epoch: 2737, train_loss: 0.0006579526386264225, test_loss: 0.0006013468810124323\n",
      "epoch: 2738, train_loss: 0.0006599611622969742, test_loss: 0.0005989398487145081\n",
      "epoch: 2739, train_loss: 0.0006567041703459362, test_loss: 0.0005982610600767657\n",
      "epoch: 2740, train_loss: 0.0006577639015512946, test_loss: 0.0005937215125110621\n",
      "epoch: 2741, train_loss: 0.0006556282485531563, test_loss: 0.0006011160294292495\n",
      "epoch: 2742, train_loss: 0.0006565031388005161, test_loss: 0.000600702129304409\n",
      "epoch: 2743, train_loss: 0.0006590198824161906, test_loss: 0.0006084880733396858\n",
      "epoch: 2744, train_loss: 0.000657024468123184, test_loss: 0.0006030313137064999\n",
      "epoch: 2745, train_loss: 0.0006568690739146879, test_loss: 0.000599559115168328\n",
      "epoch: 2746, train_loss: 0.0006560553928189304, test_loss: 0.0006099638946276779\n",
      "epoch: 2747, train_loss: 0.0006553818517283577, test_loss: 0.0006262781146991377\n",
      "epoch: 2748, train_loss: 0.0006574340134292194, test_loss: 0.0005970069226653626\n",
      "epoch: 2749, train_loss: 0.0006600029212321439, test_loss: 0.0006143741872316847\n",
      "epoch: 2750, train_loss: 0.0006563447367744116, test_loss: 0.0006032728463954603\n",
      "epoch: 2751, train_loss: 0.0006559008603368926, test_loss: 0.0005990582430968061\n",
      "epoch: 2752, train_loss: 0.0006574707703017023, test_loss: 0.0006020422442816198\n",
      "epoch: 2753, train_loss: 0.0006576218622048264, test_loss: 0.0005948978990394002\n",
      "epoch: 2754, train_loss: 0.0006559266779652756, test_loss: 0.0005949410066629449\n",
      "epoch: 2755, train_loss: 0.0006560833299654009, test_loss: 0.0006019789288984612\n",
      "epoch: 2756, train_loss: 0.0006567559372026311, test_loss: 0.0005936718904801334\n",
      "epoch: 2757, train_loss: 0.0006562815738700168, test_loss: 0.0006244054772347832\n",
      "epoch: 2758, train_loss: 0.0006568694725106268, test_loss: 0.0005987779489563158\n",
      "epoch: 2759, train_loss: 0.000657804570990898, test_loss: 0.0005999068186307946\n",
      "epoch: 2760, train_loss: 0.0006585644378387572, test_loss: 0.0006205794100727265\n",
      "epoch: 2761, train_loss: 0.0006607758974575478, test_loss: 0.0006124514669257527\n",
      "epoch: 2762, train_loss: 0.0006568451999160259, test_loss: 0.0006119283886315922\n",
      "epoch: 2763, train_loss: 0.0006565848775410458, test_loss: 0.0005965083837509155\n",
      "epoch: 2764, train_loss: 0.0006575564595708704, test_loss: 0.000601686542116416\n",
      "epoch: 2765, train_loss: 0.0006548020325641593, test_loss: 0.0005912818354166424\n",
      "epoch: 2766, train_loss: 0.0006560172997010143, test_loss: 0.0006098631177640831\n",
      "epoch: 2767, train_loss: 0.0006609328291075224, test_loss: 0.0006009058949227134\n",
      "epoch: 2768, train_loss: 0.0006584327240251814, test_loss: 0.0005970246129436418\n",
      "epoch: 2769, train_loss: 0.0006564427888446042, test_loss: 0.0005949520952223489\n",
      "epoch: 2770, train_loss: 0.0006562133481645066, test_loss: 0.0005948085114747906\n",
      "epoch: 2771, train_loss: 0.0006575929545088793, test_loss: 0.0006002714314187566\n",
      "epoch: 2772, train_loss: 0.000657026919171862, test_loss: 0.0005986391576395059\n",
      "epoch: 2773, train_loss: 0.0006547594966832548, test_loss: 0.0005995612737024203\n",
      "epoch: 2774, train_loss: 0.00065771321912863, test_loss: 0.0005966139530452589\n",
      "epoch: 2775, train_loss: 0.0006570487799446868, test_loss: 0.0006043441292907422\n",
      "epoch: 2776, train_loss: 0.0006569693758375133, test_loss: 0.000605208226867641\n",
      "epoch: 2777, train_loss: 0.0006595400501164081, test_loss: 0.0006105825013946742\n",
      "epoch: 2778, train_loss: 0.0006564216784444516, test_loss: 0.0006013823634323975\n",
      "epoch: 2779, train_loss: 0.0006574134787787561, test_loss: 0.0005953293342220908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2780, train_loss: 0.0006564524462547801, test_loss: 0.0005967972392681986\n",
      "epoch: 2781, train_loss: 0.0006553990394383183, test_loss: 0.0006062761967768893\n",
      "epoch: 2782, train_loss: 0.0006577708143437201, test_loss: 0.000603491518025597\n",
      "epoch: 2783, train_loss: 0.0006552584021372478, test_loss: 0.0006056157192991426\n",
      "epoch: 2784, train_loss: 0.0006537828670369219, test_loss: 0.0006068915875706201\n",
      "epoch: 2785, train_loss: 0.0006556474140582039, test_loss: 0.0005998711470359316\n",
      "epoch: 2786, train_loss: 0.0006548527833175561, test_loss: 0.0005948262987658381\n",
      "epoch: 2787, train_loss: 0.0006587894825750719, test_loss: 0.0005935668790092071\n",
      "epoch: 2788, train_loss: 0.0006561081618597002, test_loss: 0.0006111116769413153\n",
      "epoch: 2789, train_loss: 0.0006552047675738678, test_loss: 0.0006017792184138671\n",
      "epoch: 2790, train_loss: 0.0006565921256602135, test_loss: 0.0006020362973989298\n",
      "epoch: 2791, train_loss: 0.0006547272192698944, test_loss: 0.0006048501042338709\n",
      "epoch: 2792, train_loss: 0.0006557057469921267, test_loss: 0.0005927880702074617\n",
      "epoch: 2793, train_loss: 0.0006566676463040968, test_loss: 0.0005975928312788407\n",
      "epoch: 2794, train_loss: 0.0006591143635729247, test_loss: 0.0006004850098785633\n",
      "epoch: 2795, train_loss: 0.0006561574764023333, test_loss: 0.0006014851969666779\n",
      "epoch: 2796, train_loss: 0.0006551256203396326, test_loss: 0.0006035310992350181\n",
      "epoch: 2797, train_loss: 0.0006577608696913913, test_loss: 0.0006147623450184861\n",
      "epoch: 2798, train_loss: 0.0006564341956223159, test_loss: 0.0006029461898530523\n",
      "epoch: 2799, train_loss: 0.0006553826236125568, test_loss: 0.0006114788751195496\n",
      "epoch: 2800, train_loss: 0.0006554622428349988, test_loss: 0.0005930848080121601\n",
      "epoch: 2801, train_loss: 0.0006552054837811738, test_loss: 0.0006040001996249581\n",
      "epoch: 2802, train_loss: 0.0006551869446411729, test_loss: 0.0005950192280579358\n",
      "epoch: 2803, train_loss: 0.0006548147180380867, test_loss: 0.0005929903078746671\n",
      "epoch: 2804, train_loss: 0.0006560072563487389, test_loss: 0.0005984162271488458\n",
      "epoch: 2805, train_loss: 0.0006565070134061186, test_loss: 0.0005951704855154579\n",
      "epoch: 2806, train_loss: 0.0006564100963853138, test_loss: 0.0005989950150251389\n",
      "epoch: 2807, train_loss: 0.0006551344134925824, test_loss: 0.0006330842831327269\n",
      "epoch: 2808, train_loss: 0.0006554426217917353, test_loss: 0.0005936240534841394\n",
      "epoch: 2809, train_loss: 0.00065611604773237, test_loss: 0.0005967880327564975\n",
      "epoch: 2810, train_loss: 0.0006562517285245755, test_loss: 0.0005970325534387181\n",
      "epoch: 2811, train_loss: 0.0006577351899898571, test_loss: 0.0005924846821775039\n",
      "epoch: 2812, train_loss: 0.000656910786030652, test_loss: 0.0005939176383738717\n",
      "epoch: 2813, train_loss: 0.0006570094707928112, test_loss: 0.0005961002267819518\n",
      "epoch: 2814, train_loss: 0.000657411161860775, test_loss: 0.0005981197852330903\n",
      "epoch: 2815, train_loss: 0.0006541425827890635, test_loss: 0.0005975352784541125\n",
      "epoch: 2816, train_loss: 0.0006562120030613859, test_loss: 0.0005921393215733891\n",
      "epoch: 2817, train_loss: 0.0006558756817273958, test_loss: 0.000598179748825108\n",
      "epoch: 2818, train_loss: 0.0006596942852332216, test_loss: 0.0006146179463636751\n",
      "epoch: 2819, train_loss: 0.0006550276682347707, test_loss: 0.000594971192185767\n",
      "epoch: 2820, train_loss: 0.0006537969014101217, test_loss: 0.0005945304389266918\n",
      "epoch: 2821, train_loss: 0.0006560483775304064, test_loss: 0.0005988406240552043\n",
      "epoch: 2822, train_loss: 0.0006560803082285692, test_loss: 0.0005960225438078245\n",
      "epoch: 2823, train_loss: 0.0006570938630434482, test_loss: 0.0005940514286824813\n",
      "epoch: 2824, train_loss: 0.0006546910773740028, test_loss: 0.0006039507182625433\n",
      "epoch: 2825, train_loss: 0.0006557170709129423, test_loss: 0.0005979555765710151\n",
      "epoch: 2826, train_loss: 0.0006547803097181832, test_loss: 0.0006144267244962975\n",
      "epoch: 2827, train_loss: 0.0006526898790110389, test_loss: 0.0006098859594203532\n",
      "epoch: 2828, train_loss: 0.000653756083920598, test_loss: 0.0005940627064167833\n",
      "epoch: 2829, train_loss: 0.0006586874825070086, test_loss: 0.0005963866618306687\n",
      "epoch: 2830, train_loss: 0.0006559373185787674, test_loss: 0.0006134584740114709\n",
      "epoch: 2831, train_loss: 0.0006540668951145009, test_loss: 0.000600331496874181\n",
      "epoch: 2832, train_loss: 0.0006568558164872229, test_loss: 0.0005944442430821558\n",
      "epoch: 2833, train_loss: 0.0006539569788045534, test_loss: 0.0005940678480934972\n",
      "epoch: 2834, train_loss: 0.0006544868709802952, test_loss: 0.0006053471248984957\n",
      "epoch: 2835, train_loss: 0.0006551564982383634, test_loss: 0.0005947481404291466\n",
      "epoch: 2836, train_loss: 0.0006561442354248594, test_loss: 0.000595989084104076\n",
      "epoch: 2837, train_loss: 0.0006561661227707467, test_loss: 0.0006060829230894645\n",
      "epoch: 2838, train_loss: 0.0006575085318890278, test_loss: 0.0005977318117705485\n",
      "epoch: 2839, train_loss: 0.0006573256257030627, test_loss: 0.0006134265422588214\n",
      "epoch: 2840, train_loss: 0.0006561128551687074, test_loss: 0.0005944175645709038\n",
      "epoch: 2841, train_loss: 0.0006554390572051963, test_loss: 0.0005989124329062179\n",
      "epoch: 2842, train_loss: 0.0006563373051745737, test_loss: 0.0006095032343485703\n",
      "epoch: 2843, train_loss: 0.0006535303065269861, test_loss: 0.0005985903117107227\n",
      "epoch: 2844, train_loss: 0.0006546876342643214, test_loss: 0.0005971041585629185\n",
      "epoch: 2845, train_loss: 0.0006555022049249839, test_loss: 0.0006079077187071865\n",
      "epoch: 2846, train_loss: 0.0006543409203320904, test_loss: 0.0005955809513883045\n",
      "epoch: 2847, train_loss: 0.0006527282745557148, test_loss: 0.0005938054140036305\n",
      "epoch: 2848, train_loss: 0.0006547148033227447, test_loss: 0.0006101397205687439\n",
      "epoch: 2849, train_loss: 0.0006551407859660685, test_loss: 0.0006013123541682338\n",
      "epoch: 2850, train_loss: 0.0006548580220070384, test_loss: 0.0005929062220578393\n",
      "epoch: 2851, train_loss: 0.0006552523776443433, test_loss: 0.0005990399076836184\n",
      "epoch: 2852, train_loss: 0.0006539828305983025, test_loss: 0.0005945291486568749\n",
      "epoch: 2853, train_loss: 0.0006554907633234625, test_loss: 0.0005943947326159105\n",
      "epoch: 2854, train_loss: 0.0006550936757222465, test_loss: 0.0005931983372041335\n",
      "epoch: 2855, train_loss: 0.0006570498061710564, test_loss: 0.0006139631586847827\n",
      "epoch: 2856, train_loss: 0.0006602407539384844, test_loss: 0.0005959043240485092\n",
      "epoch: 2857, train_loss: 0.0006545837373857427, test_loss: 0.0005904477681421364\n",
      "epoch: 2858, train_loss: 0.000653897203332947, test_loss: 0.0005929377851619696\n",
      "epoch: 2859, train_loss: 0.0006536522781496625, test_loss: 0.0005924483654477323\n",
      "epoch: 2860, train_loss: 0.0006568287246172195, test_loss: 0.0006035713740857318\n",
      "epoch: 2861, train_loss: 0.0006544513744302094, test_loss: 0.0006032820965629071\n",
      "epoch: 2862, train_loss: 0.0006546927843769283, test_loss: 0.0005959107026380176\n",
      "epoch: 2863, train_loss: 0.0006556507660602422, test_loss: 0.0006024343262348945\n",
      "epoch: 2864, train_loss: 0.0006540700737589403, test_loss: 0.0005928909716506799\n",
      "epoch: 2865, train_loss: 0.0006544700806008894, test_loss: 0.0005940837630381187\n",
      "epoch: 2866, train_loss: 0.0006567178314308758, test_loss: 0.0005952160766658684\n",
      "epoch: 2867, train_loss: 0.0006527619046644996, test_loss: 0.0006077985696416969\n",
      "epoch: 2868, train_loss: 0.0006544992869274448, test_loss: 0.0005947140404411281\n",
      "epoch: 2869, train_loss: 0.0006545597254602319, test_loss: 0.0006045215122867376\n",
      "epoch: 2870, train_loss: 0.0006532106718109192, test_loss: 0.000598858343437314\n",
      "epoch: 2871, train_loss: 0.0006547306902180223, test_loss: 0.0006079585630989944\n",
      "epoch: 2872, train_loss: 0.0006544963170713543, test_loss: 0.0005989841689976553\n",
      "epoch: 2873, train_loss: 0.0006547581401916788, test_loss: 0.0005984650003180528\n",
      "epoch: 2874, train_loss: 0.0006536232350576345, test_loss: 0.0005942287631720925\n",
      "epoch: 2875, train_loss: 0.0006556248358126892, test_loss: 0.0006064823489092911\n",
      "epoch: 2876, train_loss: 0.0006549426597421584, test_loss: 0.0005926695836630339\n",
      "epoch: 2877, train_loss: 0.000653734697666748, test_loss: 0.0005921547077984238\n",
      "epoch: 2878, train_loss: 0.000652706869321106, test_loss: 0.0005997366630860294\n",
      "epoch: 2879, train_loss: 0.0006542126344435889, test_loss: 0.0005942329444224015\n",
      "epoch: 2880, train_loss: 0.0006537615212753577, test_loss: 0.0005950945875762651\n",
      "epoch: 2881, train_loss: 0.0006548953748753537, test_loss: 0.0005958052012526119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2882, train_loss: 0.0006541228035728083, test_loss: 0.0006020650568340594\n",
      "epoch: 2883, train_loss: 0.0006549620479547783, test_loss: 0.0005962497768147538\n",
      "epoch: 2884, train_loss: 0.0006530160051466817, test_loss: 0.0005908137876152372\n",
      "epoch: 2885, train_loss: 0.0006537724301502433, test_loss: 0.0005983965384075418\n",
      "epoch: 2886, train_loss: 0.0006542533431100943, test_loss: 0.0005941499354473004\n",
      "epoch: 2887, train_loss: 0.0006572091901350929, test_loss: 0.0005921904084971175\n",
      "epoch: 2888, train_loss: 0.0006562179326504955, test_loss: 0.0006008469596660385\n",
      "epoch: 2889, train_loss: 0.0006556162299365615, test_loss: 0.0005931623697203273\n",
      "epoch: 2890, train_loss: 0.0006545841068778511, test_loss: 0.0006099655680979291\n",
      "epoch: 2891, train_loss: 0.000652845555400152, test_loss: 0.0006008558169317743\n",
      "epoch: 2892, train_loss: 0.0006553925480187426, test_loss: 0.0006200503994477913\n",
      "epoch: 2893, train_loss: 0.0006532550247831513, test_loss: 0.0005921341895979518\n",
      "epoch: 2894, train_loss: 0.000653527292382458, test_loss: 0.000609101596637629\n",
      "epoch: 2895, train_loss: 0.0006535991358206323, test_loss: 0.0006006664140538002\n",
      "epoch: 2896, train_loss: 0.000652586814755088, test_loss: 0.0006144395107791448\n",
      "epoch: 2897, train_loss: 0.0006551585203218881, test_loss: 0.0005981447320664302\n",
      "epoch: 2898, train_loss: 0.0006532265258062145, test_loss: 0.0005953702833115434\n",
      "epoch: 2899, train_loss: 0.0006559681534544443, test_loss: 0.0005916822265135124\n",
      "epoch: 2900, train_loss: 0.0006541365772769179, test_loss: 0.0005926766267900044\n",
      "epoch: 2901, train_loss: 0.0006533928402780515, test_loss: 0.0005919656105106696\n",
      "epoch: 2902, train_loss: 0.0006544104152176853, test_loss: 0.0005927062690413246\n",
      "epoch: 2903, train_loss: 0.0006530122659371599, test_loss: 0.0005900003452552482\n",
      "epoch: 2904, train_loss: 0.000652372071509371, test_loss: 0.0005935722341140112\n",
      "epoch: 2905, train_loss: 0.0006532642177974238, test_loss: 0.0005995516160813471\n",
      "epoch: 2906, train_loss: 0.0006564668475893204, test_loss: 0.000595298949823094\n",
      "epoch: 2907, train_loss: 0.000656370846706483, test_loss: 0.000601502896946234\n",
      "epoch: 2908, train_loss: 0.0006565920699833204, test_loss: 0.0005990124967259666\n",
      "epoch: 2909, train_loss: 0.0006525348251908208, test_loss: 0.0006098728626966476\n",
      "epoch: 2910, train_loss: 0.0006551996883227612, test_loss: 0.0006040945930484062\n",
      "epoch: 2911, train_loss: 0.0006543009190152035, test_loss: 0.0005939423281233758\n",
      "epoch: 2912, train_loss: 0.0006528363472012722, test_loss: 0.0005984516804649805\n",
      "epoch: 2913, train_loss: 0.0006542878754376231, test_loss: 0.0005928075746245062\n",
      "epoch: 2914, train_loss: 0.0006536381843034178, test_loss: 0.0005946831176212678\n",
      "epoch: 2915, train_loss: 0.0006525855848019052, test_loss: 0.000593289810543259\n",
      "epoch: 2916, train_loss: 0.0006525559178755983, test_loss: 0.0006019623154619088\n",
      "epoch: 2917, train_loss: 0.0006518322840312738, test_loss: 0.0005979266279609874\n",
      "epoch: 2918, train_loss: 0.0006546102560368245, test_loss: 0.0005987612045525262\n",
      "epoch: 2919, train_loss: 0.0006538707340318385, test_loss: 0.0006004810226537908\n",
      "epoch: 2920, train_loss: 0.0006543233947646197, test_loss: 0.0005923452942321698\n",
      "epoch: 2921, train_loss: 0.0006519523145549971, test_loss: 0.0005915100239993384\n",
      "epoch: 2922, train_loss: 0.0006523468903691063, test_loss: 0.0006093204865464941\n",
      "epoch: 2923, train_loss: 0.0006524209431676275, test_loss: 0.0005949461143851901\n",
      "epoch: 2924, train_loss: 0.0006521933626019112, test_loss: 0.0006081450070875386\n",
      "epoch: 2925, train_loss: 0.000652416840792917, test_loss: 0.0005997593349699551\n",
      "epoch: 2926, train_loss: 0.0006515112953543987, test_loss: 0.0005927600723225623\n",
      "epoch: 2927, train_loss: 0.0006525656651280334, test_loss: 0.0005962261008486772\n",
      "epoch: 2928, train_loss: 0.0006532583527428949, test_loss: 0.0006083324697101489\n",
      "epoch: 2929, train_loss: 0.0006538943789960088, test_loss: 0.0005960946776516115\n",
      "epoch: 2930, train_loss: 0.0006512038083240876, test_loss: 0.0006066117978965243\n",
      "epoch: 2931, train_loss: 0.0006519282671987363, test_loss: 0.0005930922003850961\n",
      "epoch: 2932, train_loss: 0.000654129012811767, test_loss: 0.0005945678567513824\n",
      "epoch: 2933, train_loss: 0.0006546447795066658, test_loss: 0.0005940149809854726\n",
      "epoch: 2934, train_loss: 0.0006539499331468149, test_loss: 0.0005878817707222576\n",
      "epoch: 2935, train_loss: 0.0006518073686216351, test_loss: 0.0006167365014941121\n",
      "epoch: 2936, train_loss: 0.0006528404014913932, test_loss: 0.0005929040829263007\n",
      "epoch: 2937, train_loss: 0.0006531056462098722, test_loss: 0.0005984808861588439\n",
      "epoch: 2938, train_loss: 0.0006513235794441046, test_loss: 0.0005994664582734307\n",
      "epoch: 2939, train_loss: 0.0006525241162465966, test_loss: 0.0006082331431874385\n",
      "epoch: 2940, train_loss: 0.0006527792328320768, test_loss: 0.0006184222341592734\n",
      "epoch: 2941, train_loss: 0.0006538974627366532, test_loss: 0.0006042738047350819\n",
      "epoch: 2942, train_loss: 0.0006565205517488167, test_loss: 0.0006058849297308674\n",
      "epoch: 2943, train_loss: 0.0006524798442589362, test_loss: 0.0006031752806544924\n",
      "epoch: 2944, train_loss: 0.0006531013084737503, test_loss: 0.0005992632310759897\n",
      "epoch: 2945, train_loss: 0.0006520209780833482, test_loss: 0.000600383752801766\n",
      "epoch: 2946, train_loss: 0.0006524216758249247, test_loss: 0.0006053787704634791\n",
      "epoch: 2947, train_loss: 0.0006538518355228007, test_loss: 0.0006017016761082535\n",
      "epoch: 2948, train_loss: 0.0006533070750858473, test_loss: 0.0005901321031463643\n",
      "epoch: 2949, train_loss: 0.0006536681966795384, test_loss: 0.0005959952638174096\n",
      "epoch: 2950, train_loss: 0.0006524285253481535, test_loss: 0.0005992218696822723\n",
      "epoch: 2951, train_loss: 0.0006516516365556289, test_loss: 0.0005998568133994316\n",
      "epoch: 2952, train_loss: 0.0006570266622989236, test_loss: 0.0005912825096553812\n",
      "epoch: 2953, train_loss: 0.0006505429818112727, test_loss: 0.0005899889317030708\n",
      "epoch: 2954, train_loss: 0.0006521737871124693, test_loss: 0.0005919715476920828\n",
      "epoch: 2955, train_loss: 0.000651282082443409, test_loss: 0.0005918582707333068\n",
      "epoch: 2956, train_loss: 0.0006535191787406802, test_loss: 0.0005914059535522634\n",
      "epoch: 2957, train_loss: 0.0006557639824914868, test_loss: 0.0005951572772270689\n",
      "epoch: 2958, train_loss: 0.0006533432030625155, test_loss: 0.0006044257509832581\n",
      "epoch: 2959, train_loss: 0.0006517830643924358, test_loss: 0.0006010859506204724\n",
      "epoch: 2960, train_loss: 0.0006513657091367666, test_loss: 0.0006037864562434455\n",
      "epoch: 2961, train_loss: 0.0006527913969678237, test_loss: 0.0006045321836912384\n",
      "epoch: 2962, train_loss: 0.0006536529348839236, test_loss: 0.0005873611565524092\n",
      "epoch: 2963, train_loss: 0.0006523877648009068, test_loss: 0.0005941207006496066\n",
      "epoch: 2964, train_loss: 0.0006501590415491196, test_loss: 0.0005943260863811398\n",
      "epoch: 2965, train_loss: 0.000651133778181089, test_loss: 0.0006035244684123123\n",
      "epoch: 2966, train_loss: 0.0006507848232539128, test_loss: 0.0005935573911604782\n",
      "epoch: 2967, train_loss: 0.0006546206524312172, test_loss: 0.0005934319876056785\n",
      "epoch: 2968, train_loss: 0.0006525933074400476, test_loss: 0.0006013569897428775\n",
      "epoch: 2969, train_loss: 0.00065524297204557, test_loss: 0.0006257233471842483\n",
      "epoch: 2970, train_loss: 0.0006538561352974047, test_loss: 0.0005916641093790531\n",
      "epoch: 2971, train_loss: 0.0006534976140676957, test_loss: 0.0005941161313482249\n",
      "epoch: 2972, train_loss: 0.0006510343860693114, test_loss: 0.0006021693310079476\n",
      "epoch: 2973, train_loss: 0.0006515466185468856, test_loss: 0.0005972237753060957\n",
      "epoch: 2974, train_loss: 0.0006521504850673449, test_loss: 0.0005932045023655519\n",
      "epoch: 2975, train_loss: 0.0006550627117774085, test_loss: 0.0006013267120579258\n",
      "epoch: 2976, train_loss: 0.0006529301564391378, test_loss: 0.0005957106623100117\n",
      "epoch: 2977, train_loss: 0.0006515478712769792, test_loss: 0.0005972751387162134\n",
      "epoch: 2978, train_loss: 0.000653147177375934, test_loss: 0.0005913286101228247\n",
      "epoch: 2979, train_loss: 0.0006521596995931443, test_loss: 0.0005942716913220162\n",
      "epoch: 2980, train_loss: 0.0006496948848033081, test_loss: 0.0005907472805120051\n",
      "epoch: 2981, train_loss: 0.0006529466760263819, test_loss: 0.0005954849863580117\n",
      "epoch: 2982, train_loss: 0.0006510390363552648, test_loss: 0.0005954516576215004\n",
      "epoch: 2983, train_loss: 0.000652751997973689, test_loss: 0.0005968588423759987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2984, train_loss: 0.0006533694496560518, test_loss: 0.0005987533271157494\n",
      "epoch: 2985, train_loss: 0.0006506987012234395, test_loss: 0.0005990157272511473\n",
      "epoch: 2986, train_loss: 0.0006512470072661729, test_loss: 0.0005996020287663365\n",
      "epoch: 2987, train_loss: 0.000649718743617363, test_loss: 0.0006033780324893693\n",
      "epoch: 2988, train_loss: 0.0006517187601117337, test_loss: 0.0006055133853806183\n",
      "epoch: 2989, train_loss: 0.0006510465793088888, test_loss: 0.0005959003271224598\n",
      "epoch: 2990, train_loss: 0.0006495207578510693, test_loss: 0.0005894488179668164\n",
      "epoch: 2991, train_loss: 0.0006520184030270447, test_loss: 0.0005898873593347768\n",
      "epoch: 2992, train_loss: 0.0006513535057741177, test_loss: 0.000596818271636342\n",
      "epoch: 2993, train_loss: 0.0006509675839207257, test_loss: 0.0005906782559274385\n",
      "epoch: 2994, train_loss: 0.0006518883987471623, test_loss: 0.0006034971981231744\n",
      "epoch: 2995, train_loss: 0.0006506328050897497, test_loss: 0.0005920748953940347\n",
      "epoch: 2996, train_loss: 0.0006537730881498884, test_loss: 0.0005878634692635387\n",
      "epoch: 2997, train_loss: 0.0006503942738914782, test_loss: 0.0005949251256727924\n",
      "epoch: 2998, train_loss: 0.0006521710108601204, test_loss: 0.000603385871121039\n",
      "epoch: 2999, train_loss: 0.0006532477387024657, test_loss: 0.0005939340432329724\n",
      "epoch: 3000, train_loss: 0.0006524233157625017, test_loss: 0.0005900499624355385\n",
      "epoch: 3001, train_loss: 0.0006524058964872813, test_loss: 0.0005943463523484146\n",
      "epoch: 3002, train_loss: 0.0006527815687308168, test_loss: 0.0006193465136069184\n",
      "epoch: 3003, train_loss: 0.0006517070097565327, test_loss: 0.0005893481041615208\n",
      "epoch: 3004, train_loss: 0.0006507422633307135, test_loss: 0.0005889079329790547\n",
      "epoch: 3005, train_loss: 0.0006511845618344681, test_loss: 0.0005931083821148301\n",
      "epoch: 3006, train_loss: 0.0006546940307801023, test_loss: 0.0005988504611498987\n",
      "epoch: 3007, train_loss: 0.0006532747419955938, test_loss: 0.0005971511758010214\n",
      "epoch: 3008, train_loss: 0.000651839862415648, test_loss: 0.0006016127784581234\n",
      "epoch: 3009, train_loss: 0.000651857816948272, test_loss: 0.0005921611785500621\n",
      "epoch: 3010, train_loss: 0.0006525634886676688, test_loss: 0.0005926276838484531\n",
      "epoch: 3011, train_loss: 0.0006499263500976983, test_loss: 0.0005938527756370604\n",
      "epoch: 3012, train_loss: 0.0006516631781224809, test_loss: 0.0005909592340079447\n",
      "epoch: 3013, train_loss: 0.0006512550538426022, test_loss: 0.0005890281172469258\n",
      "epoch: 3014, train_loss: 0.0006534036744952849, test_loss: 0.0005975893000140786\n",
      "epoch: 3015, train_loss: 0.0006495267038901701, test_loss: 0.0005983351729810238\n",
      "epoch: 3016, train_loss: 0.0006499018484686056, test_loss: 0.0005991885021406537\n",
      "epoch: 3017, train_loss: 0.0006526350031060207, test_loss: 0.0005878252074277649\n",
      "epoch: 3018, train_loss: 0.0006508580257144312, test_loss: 0.0006108426362819349\n",
      "epoch: 3019, train_loss: 0.0006504409273317003, test_loss: 0.0006025024680032706\n",
      "epoch: 3020, train_loss: 0.000650796191463166, test_loss: 0.0006054269518548002\n",
      "epoch: 3021, train_loss: 0.0006528589912467514, test_loss: 0.0006044914237766837\n",
      "epoch: 3022, train_loss: 0.000650505466973814, test_loss: 0.0005916103691561148\n",
      "epoch: 3023, train_loss: 0.0006489244595919129, test_loss: 0.0005897505082733309\n",
      "epoch: 3024, train_loss: 0.0006510389844745235, test_loss: 0.0006004725631404048\n",
      "epoch: 3025, train_loss: 0.0006506715397573198, test_loss: 0.0005879604189734285\n",
      "epoch: 3026, train_loss: 0.0006515666483091595, test_loss: 0.0005934414463505769\n",
      "epoch: 3027, train_loss: 0.0006539246128143176, test_loss: 0.0005992075360457724\n",
      "epoch: 3028, train_loss: 0.0006504314344214356, test_loss: 0.0005929040538224702\n",
      "epoch: 3029, train_loss: 0.0006500080280997993, test_loss: 0.0005908861300364757\n",
      "epoch: 3030, train_loss: 0.0006512519372019755, test_loss: 0.0005933699333885064\n",
      "epoch: 3031, train_loss: 0.0006507888864017214, test_loss: 0.0005911348610728359\n",
      "epoch: 3032, train_loss: 0.0006528119442721262, test_loss: 0.0006023293923741827\n",
      "epoch: 3033, train_loss: 0.0006509362087261094, test_loss: 0.0005937871804538494\n",
      "epoch: 3034, train_loss: 0.0006510669861555747, test_loss: 0.0005957125589096298\n",
      "epoch: 3035, train_loss: 0.0006510820923089657, test_loss: 0.0005913439963478595\n",
      "epoch: 3036, train_loss: 0.0006512347115304968, test_loss: 0.0005919967613105351\n",
      "epoch: 3037, train_loss: 0.000649884548139475, test_loss: 0.0005936124701596176\n",
      "epoch: 3038, train_loss: 0.0006498348995354836, test_loss: 0.0005914521122273678\n",
      "epoch: 3039, train_loss: 0.0006532607405223762, test_loss: 0.0005963679189638545\n",
      "epoch: 3040, train_loss: 0.0006508151317298737, test_loss: 0.000589698669500649\n",
      "epoch: 3041, train_loss: 0.0006511657126754036, test_loss: 0.000590904198664551\n",
      "epoch: 3042, train_loss: 0.0006508944510563236, test_loss: 0.0005979729273046056\n",
      "epoch: 3043, train_loss: 0.000649900896899888, test_loss: 0.000588427297770977\n",
      "epoch: 3044, train_loss: 0.0006532089698695294, test_loss: 0.000590281687133635\n",
      "epoch: 3045, train_loss: 0.0006497998977505157, test_loss: 0.0005907511173669869\n",
      "epoch: 3046, train_loss: 0.000651487676963291, test_loss: 0.0006059301764859507\n",
      "epoch: 3047, train_loss: 0.0006539087411036472, test_loss: 0.0005999445226431513\n",
      "epoch: 3048, train_loss: 0.0006516050767538178, test_loss: 0.0005922504011929656\n",
      "epoch: 3049, train_loss: 0.0006521356079484458, test_loss: 0.0005883992998860776\n",
      "epoch: 3050, train_loss: 0.000649243411000656, test_loss: 0.0005907611775910482\n",
      "epoch: 3051, train_loss: 0.0006494474630175239, test_loss: 0.0005950593137337515\n",
      "epoch: 3052, train_loss: 0.0006499850234198991, test_loss: 0.0005955103212424243\n",
      "epoch: 3053, train_loss: 0.0006510126353848888, test_loss: 0.0005931446163837487\n",
      "epoch: 3054, train_loss: 0.0006483542193840865, test_loss: 0.0005965285090496764\n",
      "epoch: 3055, train_loss: 0.0006508891693437877, test_loss: 0.0005953102372586727\n",
      "epoch: 3056, train_loss: 0.000649176143190783, test_loss: 0.0005928263320432355\n",
      "epoch: 3057, train_loss: 0.0006502059404738247, test_loss: 0.0005898033220243329\n",
      "epoch: 3058, train_loss: 0.0006516525564897481, test_loss: 0.0005938385826690743\n",
      "epoch: 3059, train_loss: 0.0006493843418707991, test_loss: 0.0005906649360743662\n",
      "epoch: 3060, train_loss: 0.0006503060905505781, test_loss: 0.0005955023564941561\n",
      "epoch: 3061, train_loss: 0.0006495631482128216, test_loss: 0.0005897683925771465\n",
      "epoch: 3062, train_loss: 0.000649066386319211, test_loss: 0.000592229732622703\n",
      "epoch: 3063, train_loss: 0.0006507503301532859, test_loss: 0.0005994766591660058\n",
      "epoch: 3064, train_loss: 0.0006498451162453579, test_loss: 0.0005901672945280249\n",
      "epoch: 3065, train_loss: 0.0006495344037514018, test_loss: 0.0005974737529565269\n",
      "epoch: 3066, train_loss: 0.0006490792274353621, test_loss: 0.0005933837916624422\n",
      "epoch: 3067, train_loss: 0.0006493367267387879, test_loss: 0.0005916049267398193\n",
      "epoch: 3068, train_loss: 0.0006492463264452374, test_loss: 0.0005910110824819034\n",
      "epoch: 3069, train_loss: 0.0006491389497608433, test_loss: 0.0005903624987695366\n",
      "epoch: 3070, train_loss: 0.0006498639451582794, test_loss: 0.0005911948877231529\n",
      "epoch: 3071, train_loss: 0.0006528059248407574, test_loss: 0.0005926812494484087\n",
      "epoch: 3072, train_loss: 0.000649376344644343, test_loss: 0.0005913177398421491\n",
      "epoch: 3073, train_loss: 0.0006500088834993379, test_loss: 0.0005929340453197559\n",
      "epoch: 3074, train_loss: 0.0006521920453372371, test_loss: 0.0005914660529621566\n",
      "epoch: 3075, train_loss: 0.0006490517610117146, test_loss: 0.0005919802691399431\n",
      "epoch: 3076, train_loss: 0.0006490625433482068, test_loss: 0.0005941813287790865\n",
      "epoch: 3077, train_loss: 0.0006499652378767242, test_loss: 0.0005922206667795157\n",
      "epoch: 3078, train_loss: 0.0006540982943514119, test_loss: 0.0005925764126004651\n",
      "epoch: 3079, train_loss: 0.0006506558160965695, test_loss: 0.0005944696216223141\n",
      "epoch: 3080, train_loss: 0.0006485700571869055, test_loss: 0.0005968105542706326\n",
      "epoch: 3081, train_loss: 0.0006491901206217059, test_loss: 0.0005975876119919121\n",
      "epoch: 3082, train_loss: 0.0006491375629000528, test_loss: 0.0005963097016016642\n",
      "epoch: 3083, train_loss: 0.0006488015882812603, test_loss: 0.0006028739832496891\n",
      "epoch: 3084, train_loss: 0.0006493743174992826, test_loss: 0.0006109355987670521\n",
      "epoch: 3085, train_loss: 0.0006485605655420247, test_loss: 0.0005902282622021934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3086, train_loss: 0.0006498511647805572, test_loss: 0.00061717328208033\n",
      "epoch: 3087, train_loss: 0.0006528715349976783, test_loss: 0.0005903567313604677\n",
      "epoch: 3088, train_loss: 0.0006479500949828197, test_loss: 0.000588922198706617\n",
      "epoch: 3089, train_loss: 0.0006526275322792809, test_loss: 0.0006087881047278643\n",
      "epoch: 3090, train_loss: 0.0006488857704781643, test_loss: 0.0005915981552485997\n",
      "epoch: 3091, train_loss: 0.0006493594100111691, test_loss: 0.0005886133052020645\n",
      "epoch: 3092, train_loss: 0.0006516219658331703, test_loss: 0.0005972438375465572\n",
      "epoch: 3093, train_loss: 0.0006511113517816462, test_loss: 0.0005960336760229742\n",
      "epoch: 3094, train_loss: 0.0006516425523643746, test_loss: 0.0005917392360667387\n",
      "epoch: 3095, train_loss: 0.0006509715597570429, test_loss: 0.0005934752261964604\n",
      "epoch: 3096, train_loss: 0.0006474986616724535, test_loss: 0.0005995856142059589\n",
      "epoch: 3097, train_loss: 0.0006495710163701164, test_loss: 0.0005864609847776592\n",
      "epoch: 3098, train_loss: 0.0006493183090756445, test_loss: 0.0005999731850655129\n",
      "epoch: 3099, train_loss: 0.000652584305498749, test_loss: 0.0006103970469363654\n",
      "epoch: 3100, train_loss: 0.0006497571834504766, test_loss: 0.0005986155980887512\n",
      "epoch: 3101, train_loss: 0.0006521760091266555, test_loss: 0.0005889762542210519\n",
      "epoch: 3102, train_loss: 0.0006501934384805677, test_loss: 0.0005974097633346295\n",
      "epoch: 3103, train_loss: 0.0006507828366011381, test_loss: 0.0006043159228283912\n",
      "epoch: 3104, train_loss: 0.0006505416556889111, test_loss: 0.0005923663654054204\n",
      "epoch: 3105, train_loss: 0.0006499748092407928, test_loss: 0.0006048182985978201\n",
      "epoch: 3106, train_loss: 0.0006482244125040977, test_loss: 0.0005912286918222284\n",
      "epoch: 3107, train_loss: 0.00064894710616816, test_loss: 0.0005942999850958586\n",
      "epoch: 3108, train_loss: 0.0006494283709047443, test_loss: 0.0005906254712802669\n",
      "epoch: 3109, train_loss: 0.0006491248419953753, test_loss: 0.0006136003648862243\n",
      "epoch: 3110, train_loss: 0.0006493145761930424, test_loss: 0.0005974822852294892\n",
      "epoch: 3111, train_loss: 0.0006476554389455882, test_loss: 0.0005993025455002984\n",
      "epoch: 3112, train_loss: 0.0006488472268835682, test_loss: 0.0005946754002555584\n",
      "epoch: 3113, train_loss: 0.0006501072354655227, test_loss: 0.0005952431044230858\n",
      "epoch: 3114, train_loss: 0.000648659965246106, test_loss: 0.0005984826372393096\n",
      "epoch: 3115, train_loss: 0.0006485458643114923, test_loss: 0.0005912203002177799\n",
      "epoch: 3116, train_loss: 0.0006487489052865978, test_loss: 0.0006019270610219488\n",
      "epoch: 3117, train_loss: 0.0006508378516983888, test_loss: 0.000595851486044315\n",
      "epoch: 3118, train_loss: 0.000648628167413256, test_loss: 0.0005944366227292145\n",
      "epoch: 3119, train_loss: 0.0006498564692700039, test_loss: 0.0005890082684345543\n",
      "epoch: 3120, train_loss: 0.0006517423696451536, test_loss: 0.0005961295197873066\n",
      "epoch: 3121, train_loss: 0.0006484863988589495, test_loss: 0.0005944094349009296\n",
      "epoch: 3122, train_loss: 0.0006481577444862088, test_loss: 0.0005973859418494006\n",
      "epoch: 3123, train_loss: 0.0006485952864117596, test_loss: 0.0005884198277878264\n",
      "epoch: 3124, train_loss: 0.0006499009829459955, test_loss: 0.0006041138258296996\n",
      "epoch: 3125, train_loss: 0.0006505803207603648, test_loss: 0.0005870719275359685\n",
      "epoch: 3126, train_loss: 0.0006497543932789046, test_loss: 0.0005873219730953375\n",
      "epoch: 3127, train_loss: 0.000648876782456089, test_loss: 0.000591043324675411\n",
      "epoch: 3128, train_loss: 0.0006509007159721754, test_loss: 0.000602346335654147\n",
      "epoch: 3129, train_loss: 0.0006514696945922207, test_loss: 0.0005917944363318384\n",
      "epoch: 3130, train_loss: 0.0006505204086272937, test_loss: 0.0005880488800661018\n",
      "epoch: 3131, train_loss: 0.0006509376031792034, test_loss: 0.0005922745428203294\n",
      "epoch: 3132, train_loss: 0.0006498454832066984, test_loss: 0.0006055783984872202\n",
      "epoch: 3133, train_loss: 0.000648382838572497, test_loss: 0.0005940687309096878\n",
      "epoch: 3134, train_loss: 0.0006503764977779887, test_loss: 0.0005953605238270635\n",
      "epoch: 3135, train_loss: 0.0006472284528528056, test_loss: 0.00059706984514681\n",
      "epoch: 3136, train_loss: 0.0006490902565217212, test_loss: 0.0005986849234128991\n",
      "epoch: 3137, train_loss: 0.0006481856462019293, test_loss: 0.000596909264762265\n",
      "epoch: 3138, train_loss: 0.0006501859916961225, test_loss: 0.0005896876779540131\n",
      "epoch: 3139, train_loss: 0.0006485259243914777, test_loss: 0.0005868211010238156\n",
      "epoch: 3140, train_loss: 0.0006478395738193523, test_loss: 0.0005905051997009044\n",
      "epoch: 3141, train_loss: 0.0006505234430179647, test_loss: 0.0005874512135051191\n",
      "epoch: 3142, train_loss: 0.0006500478484667838, test_loss: 0.0005988640914438292\n",
      "epoch: 3143, train_loss: 0.0006486822106956464, test_loss: 0.0005952095283040156\n",
      "epoch: 3144, train_loss: 0.000648119440049176, test_loss: 0.0005972762543630475\n",
      "epoch: 3145, train_loss: 0.0006499289719732074, test_loss: 0.0005965111243616169\n",
      "epoch: 3146, train_loss: 0.0006505553193046185, test_loss: 0.0005934420332778245\n",
      "epoch: 3147, train_loss: 0.0006494323897641152, test_loss: 0.0005907737116406983\n",
      "epoch: 3148, train_loss: 0.0006498008113577152, test_loss: 0.0005934662331128493\n",
      "epoch: 3149, train_loss: 0.0006488430954050273, test_loss: 0.0005946806631982327\n",
      "epoch: 3150, train_loss: 0.0006490751086106604, test_loss: 0.0005957332565837229\n",
      "epoch: 3151, train_loss: 0.0006501104381522568, test_loss: 0.0006001650763209909\n",
      "epoch: 3152, train_loss: 0.0006507585880488319, test_loss: 0.000588289297108228\n",
      "epoch: 3153, train_loss: 0.0006468962047896955, test_loss: 0.0005893760632413129\n",
      "epoch: 3154, train_loss: 0.0006521966943578066, test_loss: 0.0005949186549211541\n",
      "epoch: 3155, train_loss: 0.0006471569599259807, test_loss: 0.0005946386081632227\n",
      "epoch: 3156, train_loss: 0.0006473364951291486, test_loss: 0.0006016886618454009\n",
      "epoch: 3157, train_loss: 0.0006481236892084227, test_loss: 0.0005887096437315146\n",
      "epoch: 3158, train_loss: 0.0006479566813061905, test_loss: 0.000593803832695509\n",
      "epoch: 3159, train_loss: 0.0006481119945301148, test_loss: 0.0005939231098939975\n",
      "epoch: 3160, train_loss: 0.0006487590030503824, test_loss: 0.0005983520046963046\n",
      "epoch: 3161, train_loss: 0.0006504698362930313, test_loss: 0.000593545802985318\n",
      "epoch: 3162, train_loss: 0.0006500229419148325, test_loss: 0.0005927670184367647\n",
      "epoch: 3163, train_loss: 0.0006486193983026011, test_loss: 0.0005884864416051035\n",
      "epoch: 3164, train_loss: 0.0006501472760093114, test_loss: 0.0005965816914491976\n",
      "epoch: 3165, train_loss: 0.0006512400615737652, test_loss: 0.0005927544601339226\n",
      "epoch: 3166, train_loss: 0.0006494761024520773, test_loss: 0.0005938795366091654\n",
      "epoch: 3167, train_loss: 0.0006483130868139636, test_loss: 0.0005990293769476315\n",
      "epoch: 3168, train_loss: 0.0006480882888275158, test_loss: 0.0005986462007664765\n",
      "epoch: 3169, train_loss: 0.0006484390393344928, test_loss: 0.000603200780460611\n",
      "epoch: 3170, train_loss: 0.0006480378797277808, test_loss: 0.0006017873238306493\n",
      "epoch: 3171, train_loss: 0.0006482939377589071, test_loss: 0.0005895986008302619\n",
      "epoch: 3172, train_loss: 0.0006487730348928143, test_loss: 0.0005850773304700851\n",
      "epoch: 3173, train_loss: 0.0006455985181357549, test_loss: 0.0005884569400222972\n",
      "epoch: 3174, train_loss: 0.0006463973853818101, test_loss: 0.0005909410635164628\n",
      "epoch: 3175, train_loss: 0.0006486381968463082, test_loss: 0.0005963753598431746\n",
      "epoch: 3176, train_loss: 0.0006459330603399354, test_loss: 0.0005964114194891105\n",
      "epoch: 3177, train_loss: 0.0006459732919566981, test_loss: 0.000588549364086551\n",
      "epoch: 3178, train_loss: 0.0006459366477033852, test_loss: 0.0005963823932688683\n",
      "epoch: 3179, train_loss: 0.0006483371683355907, test_loss: 0.0005929783898560951\n",
      "epoch: 3180, train_loss: 0.0006476377374897509, test_loss: 0.0005976671139554431\n",
      "epoch: 3181, train_loss: 0.0006502060910545127, test_loss: 0.0005922810863315439\n",
      "epoch: 3182, train_loss: 0.0006479656389590515, test_loss: 0.0005990612941483656\n",
      "epoch: 3183, train_loss: 0.000649587467626628, test_loss: 0.0005945504526607692\n",
      "epoch: 3184, train_loss: 0.0006509778056921357, test_loss: 0.0005885985301574692\n",
      "epoch: 3185, train_loss: 0.0006481748372923745, test_loss: 0.0005935712882395213\n",
      "epoch: 3186, train_loss: 0.0006481515238587947, test_loss: 0.0005956006498308852\n",
      "epoch: 3187, train_loss: 0.0006470282982184511, test_loss: 0.0005920438998145983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3188, train_loss: 0.0006475436169674377, test_loss: 0.000599529865818719\n",
      "epoch: 3189, train_loss: 0.000648267724065353, test_loss: 0.0006047593342373148\n",
      "epoch: 3190, train_loss: 0.0006487028124114579, test_loss: 0.0005873665844167893\n",
      "epoch: 3191, train_loss: 0.000650282974516892, test_loss: 0.0005914235856228819\n",
      "epoch: 3192, train_loss: 0.0006472754163120914, test_loss: 0.0006052895575218523\n",
      "epoch: 3193, train_loss: 0.000649360222387654, test_loss: 0.0005971617162382851\n",
      "epoch: 3194, train_loss: 0.0006471327923582462, test_loss: 0.000601765985872286\n",
      "epoch: 3195, train_loss: 0.0006469019901250367, test_loss: 0.0005952643308167657\n",
      "epoch: 3196, train_loss: 0.0006469123270463846, test_loss: 0.0005884665927927321\n",
      "epoch: 3197, train_loss: 0.0006482771473795013, test_loss: 0.0005942831048741937\n",
      "epoch: 3198, train_loss: 0.0006488526857498547, test_loss: 0.0005950424529146403\n",
      "epoch: 3199, train_loss: 0.0006456266640705745, test_loss: 0.0005966841854387894\n"
     ]
    }
   ],
   "source": [
    "seq_dim = 10 # = window_size\n",
    "\n",
    "num_epochs = 3200 # 400 # 200 will overfit, 100 is good, see the plot: plt.plot(train_loss[20:]) and plt.plot(test_loss[20:])\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    train_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(train_loader):\n",
    "        # print(\"train: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            # print(seqs.shape)\n",
    "            # print(seqs[0])\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "            # print(labels.shape)\n",
    "            # print(seqs[0])\n",
    "            # seqs = Variable(seqs)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # outputs = model(seqs.float())\n",
    "        outputs = model(seqs)\n",
    "        # print(outputs.is_cuda)\n",
    "        # print(labels.is_cuda)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs.dtype)\n",
    "        \n",
    "        # loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_train_loss += loss.data.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_batch = i + 1;\n",
    "        \n",
    "        # print(\"loss: \", loss.data)\n",
    "    \n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    # test_seq = []\n",
    "    test_pred = []\n",
    "    # test_gt = []\n",
    "    for i, (seqs, labels) in enumerate(test_loader):\n",
    "        # print(\"test: \", i)\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        outputs = model(seqs)\n",
    "        # print(i, outputs.shape)\n",
    "        # print(i, outputs.is_cuda)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        # test_gt.append(labels)\n",
    "        test_pred.append(outputs)\n",
    "        # test_seq.append(seqs)\n",
    "        \n",
    "        test_batch = i + 1\n",
    "    \n",
    "    # print(\"train batch: \", train_batch)\n",
    "    # print(\"test batch: \", test_batch)\n",
    "    train_loss.append(total_train_loss/train_batch)\n",
    "    test_loss.append(total_test_loss/test_batch)\n",
    "    # train_loss.append(total_train_loss)\n",
    "    # test_loss.append(total_test_loss)\n",
    "    print(\"epoch: {}, train_loss: {}, test_loss: {}\".format(epoch, total_train_loss/train_batch, total_test_loss/test_batch))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 3200\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "test_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), test_loss[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, test_loss_curve], ['Train Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x mean: 0.5067009838472856 ; std: 0.2332880326514156\n",
      "y mean: 0.49486543289715496 ; std: 0.23134116200489124\n",
      "z mean: 0.4910597234688982 ; std: 0.23926200186407484\n"
     ]
    }
   ],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_1600.pt\"\n",
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005966841854387894\n"
     ]
    }
   ],
   "source": [
    "# get test results\n",
    "seq_dim = 10 # = window_size\n",
    "input_dim = 3\n",
    "# test_seq = []\n",
    "test_predd = []\n",
    "# test_gt = []\n",
    "total_test_loss = 0.0\n",
    "test_batch = 0\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "        \n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_test_loss += loss.data.item()\n",
    "    test_predd.append(outputs)\n",
    "    test_batch = i + 1\n",
    "print(total_test_loss/test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predd[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of ground true around predication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore testing data\n",
    "test_seq = []\n",
    "test_gt = []\n",
    "for i, (seqs, labels) in enumerate(test_loader):\n",
    "    test_gt.append(labels)\n",
    "    test_seq.append(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = test_seq[i].numpy()\n",
    "        gt = test_gt[i].numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        test = np.append(test, test_seq[i].numpy(), axis = 0)\n",
    "        gt = np.append(gt, test_gt[i].numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test[0][-1,:].reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x mean:\", x_mean, \"; std:\", x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct analysis matrix l(0-2 col): last sequence point, g(3-5 col): next ground true point, p(6-8 col): predicted point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    last_point = test[i][-1, :].reshape(1, -1)\n",
    "    gt_point = gt[i].reshape(1, -1)\n",
    "    pred_point = pred[i].reshape(1, -1)\n",
    "    row = np.append(np.append(last_point, gt_point, axis = 1), pred_point, axis = 1)\n",
    "    if (i == 0):\n",
    "        analysis = row\n",
    "    else:\n",
    "        analysis = np.append(analysis, row, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rotation matrix\n",
    "# note: input vec_1 and vec_2 have to be normalized before passing to the function\n",
    "def get_rotatin_mat(vec_1, vec_2):\n",
    "    a,b = vec_1.reshape(3), vec_2.reshape(3)\n",
    "    v = np.cross(a,b)\n",
    "    c = np.dot(a,b)\n",
    "    s = np.linalg.norm(v)\n",
    "    # print(\"s\", s)\n",
    "    if (s == 0):\n",
    "        return np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])\n",
    "    I = np.identity(3)\n",
    "    vXStr = '{} {} {}; {} {} {}; {} {} {}'.format(0, -v[2], v[1], v[2], 0, -v[0], -v[1], v[0], 0)\n",
    "    k = np.matrix(vXStr)\n",
    "    r = I + k + np.matmul(k,k) * ((1 -c)/(s**2))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([2, 1, 0])\n",
    "a = a/np.linalg.norm(a)\n",
    "b = np.array([4, 5, 6])\n",
    "b = b/np.linalg.norm(b)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "m = get_rotatin_mat(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(m, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.square(b[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b/np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3]])\n",
    "a_n = a/np.linalg.norm(a)\n",
    "\n",
    "b = np.array([[4,5,6]])\n",
    "b_n = b/np.linalg.norm(b)\n",
    "\n",
    "c = np.array([[7,8,9]])\n",
    "c_n = c/np.linalg.norm(c)\n",
    "\n",
    "r = get_rotatin_mat(b_n, a_n) # b to a\n",
    "\n",
    "b_p = np.dot(r, b.reshape(3, -1))\n",
    "c_p = np.dot(r, c.reshape(3, -1))\n",
    "c_p = np.asarray(c_p)\n",
    "c_p_n = c_p/np.linalg.norm(c_p)\n",
    "print(b.shape)\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"c\", c)\n",
    "print(\"b_p\", b_p)\n",
    "print(\"c_p\", c_p)\n",
    "\n",
    "print(get_rotatin_mat(c_n, b_n))\n",
    "\n",
    "print(get_rotatin_mat(c_p_n, a_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c_p.reshape(1, -1)\n",
    "d = np.asarray(d)\n",
    "d_n = d/np.linalg.norm(d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rotatin_mat(d_n, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(b_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all point to the same starting point\n",
    "# first sequence last direction\n",
    "# direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = (analysis[0, 3:6] - analysis[0, 0:3]).reshape(1, -1)\n",
    "# normalize\n",
    "direction = direction/np.linalg.norm(direction)\n",
    "# first diff between prediction and ground true\n",
    "diff_correct = (analysis[0, 6:9] - analysis[0, 3:6]).reshape(1, -1)\n",
    "# print(type(diff_correct))\n",
    "# print(diff_correct.shape)\n",
    "for i in range(1, analysis.shape[0]):\n",
    "    # cur_direction = (analysis[i, 0:3] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    cur_direction = (analysis[i, 3:6] - analysis[i, 0:3]).reshape(1, -1)\n",
    "    # normalize\n",
    "    if (np.linalg.norm(cur_direction) == 0):\n",
    "        print(i, np.linalg.norm(cur_direction))\n",
    "    cur_direction = cur_direction/np.linalg.norm(cur_direction)\n",
    "    cur_diff = (analysis[i, 6:9] - analysis[i, 3:6]).reshape(1, -1)\n",
    "    # get rotation matrix from cur_direction to direction\n",
    "    r = get_rotatin_mat(cur_direction, direction)\n",
    "    # apply rotation matrix to the cur_diff\n",
    "    cur_diff = np.dot(r, cur_diff.reshape(3, -1))\n",
    "    cur_diff = cur_diff.reshape(1, -1)\n",
    "    # matrix type to np array type\n",
    "    cur_diff = np.asarray(cur_diff)\n",
    "    # print(cur_diff.shape)\n",
    "    diff_correct = np.append(diff_correct, cur_diff, axis = 0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_correct[:, 0], diff_correct[:, 1], diff_correct[:, 2])\n",
    "x = [-direction[0, 0], 0]\n",
    "y = [-direction[0, 1], 0]\n",
    "z = [-direction[0, 2], 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "# ax.arrow(0, 0, 0.5, 0.5, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "# ax.quiver(0, 0, 0, -direction[0, 0], -direction[0, 1], -direction[0, 2], length=5, normalize=True, color='r')\n",
    "# x = np.zeros(10)\n",
    "# y = np.zeros(10)\n",
    "# z = np.arange(10)*10 # remove *100 and the arrow heads will reappear.\n",
    "# dx = np.zeros(10)\n",
    "# dy = np.arange(10)\n",
    "# dz = np.zeros(10)\n",
    "x = np.array([0, -direction[0, 0]])\n",
    "y = np.array([0, -direction[0, 1]])\n",
    "z = np.array([0, -direction[0, 2]])\n",
    "dx = np.array([0, 0])\n",
    "dy = np.array([0, 0])\n",
    "dz = np.array([0, 0])\n",
    "# ax.quiver(x, y, z, dx, dy, dz, length=1)\n",
    "# ax.quiver(-direction[0, 0], -direction[0, 1], -direction[0, 2], 0, 0, 0, length=100, normalize=True)\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.array([[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note view_angle must be normalized\n",
    "def getProjection(view_angle, point):\n",
    "    v = point - np.array([[0, 0, 0]])\n",
    "    dist = v[0, 0]*view_angle[0, 0] + v[0, 1]*view_angle[0, 1] + v[0, 2]*view_angle[0, 2]\n",
    "    projected_point = point - dist*view_angle\n",
    "    \n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angle = np.cross(direction[0, :].reshape(1, -1), np.array([[0, 0, 1]]))\n",
    "# normalization\n",
    "view_angle = view_angle/np.linalg.norm(view_angle)\n",
    "print(view_angle)\n",
    "for i in range(diff_correct.shape[0]):\n",
    "    if (i == 0):\n",
    "        diff_corrrect_projection = getProjection(view_angle, diff_correct[i, :])\n",
    "    else:\n",
    "        diff_corrrect_projection = np.append(diff_corrrect_projection, getProjection(view_angle, diff_correct[i, :]), axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff_corrrect_projection[:, 0], diff_corrrect_projection[:, 1], diff_corrrect_projection[:, 2])\n",
    "x = [-direction[0, 0]*10, 0]\n",
    "y = [-direction[0, 1]*10, 0]\n",
    "z = [-direction[0, 2]*10, 0]\n",
    "ax.plot(x, y, z, label='parametric curve', color='r')\n",
    "ax.set_xlabel(\"x direction\")\n",
    "ax.set_ylabel(\"y direction\")\n",
    "ax.set_zlabel(\"z direction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform from 3D to 2D\n",
    "diff_corrrect_projection_2D = np.delete(diff_corrrect_projection, 1, 1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from matplotlib.colors import LogNorm\n",
    "clf = mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
    "clf.fit(diff_corrrect_projection_2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(-5., 5.)\n",
    "y = np.linspace(-5., 5.)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -clf.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n",
    "                 levels=np.logspace(0, 3, 300))\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(diff_corrrect_projection_2D[:, 0], diff_corrrect_projection_2D[:, 1], .8)\n",
    "\n",
    "x = [-direction[0, 0]*5, 0]\n",
    "z = [-direction[0, 2]*5, 0]\n",
    "plt.plot(x, z, label='parametric curve', color='r')\n",
    "\n",
    "plt.title('Negative log-likelihood predicted by a GMM')\n",
    "plt.axis('tight')\n",
    "plt.xlabel(\"Direction 1\")\n",
    "plt.ylabel(\"Direction 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "C = np.array([[0., -0.7], [3.5, .7]])\n",
    "stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretched_gaussian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_corrrect_projection_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(direction[0, 0], 2) + pow(direction[0, 1], 2) + pow(direction[0, 2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[2, 0:3] - analysis[2, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[3, 0:3] - analysis[3, 3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = (analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)\n",
    "direction = direction/np.linalg.norm(direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 0:3] - analysis[0, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (analysis[1, 0:3] - analysis[1, 3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d/np.linalg.norm(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow(0.84804916, 2) + pow(0.5299187, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(analysis[0, 3:6] - analysis[0, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([13.259178  ,  0.37397736, 20.36138]) - np.asarray([12.658086  ,  1.293065  ,20.224268])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_pred)):\n",
    "    if (i == 0):\n",
    "        test = \n",
    "        gt = test_gt[i].cpu().detach().numpy()\n",
    "        pred = test_pred[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        gt = np.append(gt, test_gt[i].cpu().detach().numpy(), axis = 0)\n",
    "        pred = np.append(pred, test_pred[i].cpu().detach().numpy(), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pred - gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(diff[:, 0], diff[:, 1], diff[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(torch.from_numpy(gt), torch.from_numpy(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i in range(len(test_pred)):\n",
    "    l = criterion(test_gt[i], test_pred[i])\n",
    "    loss += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
