{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, min_max_scaling, normalize_one, construct_train_valid_tensor, construct_test_tensor, show_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14970\n",
      "1970\n"
     ]
    }
   ],
   "source": [
    "window_size = 6\n",
    "# load each of the 5 dataset and do min_max_scaling on each of them\n",
    "train_set_x_1, valid_set_x_1, train_set_y_1, valid_set_y_1, train_set_z_1, valid_set_z_1 = load_data_2('data_preprocessing/test_1_training_xyz.txt', 400)\n",
    "train_set_x_2, valid_set_x_2, train_set_y_2, valid_set_y_2, train_set_z_2, valid_set_z_2 = load_data_2('data_preprocessing/test_2_training_xyz.txt', 400)\n",
    "train_set_x_3, valid_set_x_3, train_set_y_3, valid_set_y_3, train_set_z_3, valid_set_z_3 = load_data_2('data_preprocessing/test_3_training_xyz.txt', 400)\n",
    "train_set_x_4, valid_set_x_4, train_set_y_4, valid_set_y_4, train_set_z_4, valid_set_z_4 = load_data_2('data_preprocessing/test_4_training_xyz.txt', 400)\n",
    "train_set_x_5, valid_set_x_5, train_set_y_5, valid_set_y_5, train_set_z_5, valid_set_z_5 = load_data_2('data_preprocessing/test_5_training_xyz.txt', 400)\n",
    "\n",
    "# do min-max-scaling for each data set\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_1)\n",
    "# show_statistic(train_set_x_1)\n",
    "min_max_scaling(train_set_x_2)\n",
    "min_max_scaling(train_set_x_3)\n",
    "min_max_scaling(train_set_x_4)\n",
    "min_max_scaling(train_set_x_5)\n",
    "\n",
    "min_max_scaling(valid_set_x_1)\n",
    "min_max_scaling(valid_set_x_2)\n",
    "min_max_scaling(valid_set_x_3)\n",
    "min_max_scaling(valid_set_x_4)\n",
    "min_max_scaling(valid_set_x_5)\n",
    "\n",
    "min_max_scaling(train_set_y_1)\n",
    "min_max_scaling(train_set_y_2)\n",
    "min_max_scaling(train_set_y_3)\n",
    "min_max_scaling(train_set_y_4)\n",
    "min_max_scaling(train_set_y_5)\n",
    "\n",
    "min_max_scaling(valid_set_y_1)\n",
    "min_max_scaling(valid_set_y_2)\n",
    "min_max_scaling(valid_set_y_3)\n",
    "min_max_scaling(valid_set_y_4)\n",
    "min_max_scaling(valid_set_y_5)\n",
    "\n",
    "min_max_scaling(train_set_z_1)\n",
    "min_max_scaling(train_set_z_2)\n",
    "min_max_scaling(train_set_z_3)\n",
    "min_max_scaling(train_set_z_4)\n",
    "min_max_scaling(train_set_z_5)\n",
    "\n",
    "min_max_scaling(valid_set_z_1)\n",
    "min_max_scaling(valid_set_z_2)\n",
    "min_max_scaling(valid_set_z_3)\n",
    "min_max_scaling(valid_set_z_4)\n",
    "min_max_scaling(valid_set_z_5)\n",
    "\n",
    "'''\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "x_mean, x_std = normalize_all(train_set_x_1, train_set_x_2, train_set_x_3, train_set_x_4, train_set_x_5, valid_set_x_1, valid_set_x_2, valid_set_x_3, valid_set_x_4, valid_set_x_5)\n",
    "y_mean, y_std = normalize_all(train_set_y_1, train_set_y_2, train_set_y_3, train_set_y_4, train_set_y_5, valid_set_y_1, valid_set_y_2, valid_set_y_3, valid_set_y_4, valid_set_y_5)\n",
    "z_mean, z_std = normalize_all(train_set_z_1, train_set_z_2, train_set_z_3, train_set_z_4, train_set_z_5, valid_set_z_1, valid_set_z_2, valid_set_z_3, valid_set_z_4, valid_set_z_5)\n",
    "print(\"train x 1:\")\n",
    "show_statistic(train_set_x_1)\n",
    "print(\"valid x 1:\")\n",
    "show_statistic(valid_set_x_1)\n",
    "\n",
    "print(\"x mean:\", x_mean, \"; std:\", x_std)\n",
    "print(\"y mean:\", y_mean, \"; std:\", y_std)\n",
    "print(\"z mean:\", z_mean, \"; std:\", z_std)\n",
    "'''\n",
    "train_dataset_1, train_label_1, valid_dataset_1, valid_label_1 = construct_train_valid_tensor(train_set_x_1,\n",
    "                                                                                           train_set_y_1,\n",
    "                                                                                           train_set_z_1,\n",
    "                                                                                           valid_set_x_1,\n",
    "                                                                                           valid_set_y_1,\n",
    "                                                                                           valid_set_z_1,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_2, train_label_2, valid_dataset_2, valid_label_2 = construct_train_valid_tensor(train_set_x_2,\n",
    "                                                                                           train_set_y_2,\n",
    "                                                                                           train_set_z_2,\n",
    "                                                                                           valid_set_x_2,\n",
    "                                                                                           valid_set_y_2,\n",
    "                                                                                           valid_set_z_2,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_3, train_label_3, valid_dataset_3, valid_label_3 = construct_train_valid_tensor(train_set_x_3,\n",
    "                                                                                           train_set_y_3,\n",
    "                                                                                           train_set_z_3,\n",
    "                                                                                           valid_set_x_3,\n",
    "                                                                                           valid_set_y_3,\n",
    "                                                                                           valid_set_z_3,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_4, train_label_4, valid_dataset_4, valid_label_4 = construct_train_valid_tensor(train_set_x_4,\n",
    "                                                                                           train_set_y_4,\n",
    "                                                                                           train_set_z_4,\n",
    "                                                                                           valid_set_x_4,\n",
    "                                                                                           valid_set_y_4,\n",
    "                                                                                           valid_set_z_4,\n",
    "                                                                                           window_size)\n",
    "\n",
    "train_dataset_5, train_label_5, valid_dataset_5, valid_label_5 = construct_train_valid_tensor(train_set_x_5,\n",
    "                                                                                           train_set_y_5,\n",
    "                                                                                           train_set_z_5,\n",
    "                                                                                           valid_set_x_5,\n",
    "                                                                                           valid_set_y_5,\n",
    "                                                                                           valid_set_z_5,\n",
    "                                                                                           window_size)\n",
    "\n",
    "# Concatenate tensors\n",
    "train_dataset = np.concatenate((train_dataset_1,\n",
    "                                train_dataset_2,\n",
    "                                train_dataset_3,\n",
    "                                train_dataset_4,\n",
    "                                train_dataset_5), axis=0)\n",
    "train_label = np.concatenate((train_label_1,\n",
    "                              train_label_2,\n",
    "                              train_label_3,\n",
    "                              train_label_4,\n",
    "                              train_label_5), axis=0)\n",
    "valid_dataset = np.concatenate((valid_dataset_1,\n",
    "                               valid_dataset_2,\n",
    "                               valid_dataset_3,\n",
    "                               valid_dataset_4,\n",
    "                               valid_dataset_5), axis=0)\n",
    "valid_label = np.concatenate((valid_label_1,\n",
    "                             valid_label_2,\n",
    "                             valid_label_3,\n",
    "                             valid_label_4,\n",
    "                             valid_label_5), axis=0)\n",
    "\n",
    "train_set = MyTrainDataSet(train_dataset, train_label)\n",
    "print(len(train_set))\n",
    "valid_set = MyTestDataSet(valid_dataset, valid_label)\n",
    "print(len(valid_set))\n",
    "\n",
    "# batch_size = 30\n",
    "# batch_size = 50\n",
    "batch_size = 650\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=0)\n",
    "# keep the valid data trajectory order\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, num_workers=0) # dont shuffle valid data for using continous trajectory later on\n",
    "# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_set_x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "390*5/650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2994.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14970/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14970, 6, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([650, 3])\n",
      "1 torch.Size([650, 3])\n",
      "2 torch.Size([650, 3])\n",
      "3 torch.Size([650, 3])\n",
      "4 torch.Size([650, 3])\n",
      "5 torch.Size([650, 3])\n",
      "6 torch.Size([650, 3])\n",
      "7 torch.Size([650, 3])\n",
      "8 torch.Size([650, 3])\n",
      "9 torch.Size([650, 3])\n",
      "10 torch.Size([650, 3])\n",
      "11 torch.Size([650, 3])\n",
      "12 torch.Size([650, 3])\n",
      "13 torch.Size([650, 3])\n",
      "14 torch.Size([650, 3])\n",
      "15 torch.Size([650, 3])\n",
      "16 torch.Size([650, 3])\n",
      "17 torch.Size([650, 3])\n",
      "18 torch.Size([650, 3])\n",
      "19 torch.Size([650, 3])\n",
      "20 torch.Size([650, 3])\n",
      "21 torch.Size([650, 3])\n",
      "22 torch.Size([650, 3])\n",
      "23 torch.Size([20, 3])\n",
      "test_batch:  24\n",
      "6.44664484070745e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x7f36416b1278>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"model/with_early_stop_after_400_without_normalization/model_w_6.pt\"\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 3\n",
    "\n",
    "load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "load_model.load_state_dict(torch.load(PATH))\n",
    "load_model.eval()\n",
    "# mmodel = torch.load(PATH)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    load_model.cuda()\n",
    "\n",
    "# get test results\n",
    "seq_dim = window_size\n",
    "# test_seq = []\n",
    "train_predd = []\n",
    "train_gt = []\n",
    "total_test_loss = 0.0\n",
    "test_batch = 0\n",
    "for i, (seqs, labels) in enumerate(train_loader):\n",
    "    if torch.cuda.is_available():\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "    outputs = load_model(seqs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    total_test_loss += loss.data.item()\n",
    "    train_predd.append(outputs)\n",
    "    train_gt.append(labels)\n",
    "    test_batch = i + 1\n",
    "    print(i, labels.shape)\n",
    "\n",
    "print(\"test_batch: \", test_batch)\n",
    "print(total_test_loss/test_batch)\n",
    "# test_1_loss.append(total_test_loss/test_batch)\n",
    "\n",
    "for i in range(len(train_predd)):\n",
    "    if (i == 0):\n",
    "        pred = train_predd[i].cpu().detach().numpy()\n",
    "        gt = train_gt[i].cpu().detach().numpy()\n",
    "    else:\n",
    "        pred = np.append(pred, train_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "        gt = np.append(gt, train_gt[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(train_set_x_1, train_set_y_1, train_set_z_1, 'gray')\n",
    "# ax.plot3D(pred[:2994,0], pred[:2994,1], pred[:2994,2], 'red')\n",
    "# ax.plot3D(train_set_x_2, train_set_y_2, train_set_z_2, 'gray')\n",
    "# ax.plot3D(pred[2994:2994*2,0], pred[2994:2994*2,1], pred[2994:2994*2,2], 'red')\n",
    "# ax.plot3D(train_set_x_3, train_set_y_3, train_set_z_3, 'gray')\n",
    "# ax.plot3D(pred[2994*2:2994*3,0], pred[2994*2:2994*3,1], pred[2994*2:2994*3,2], 'red')\n",
    "# ax.plot3D(train_set_x_4, train_set_y_4, train_set_z_4, 'gray')\n",
    "# ax.plot3D(pred[2994*3:2994*4,0], pred[2994*3:2994*4,1], pred[2994*3:2994*4,2], 'red')\n",
    "ax.plot3D(train_set_x_5, train_set_y_5, train_set_z_5, 'gray')\n",
    "ax.plot3D(pred[2994*4:2994*5,0], pred[2994*4:2994*5,1], pred[2994*4:2994*5,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21497917"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.679335848502888e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0010135570773854852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23*650+20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot result without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "min: -1.5190000000000001\n",
      "max: 1.41144\n",
      "mean: -0.058533702725000004\n",
      "std: 0.7861266362964188\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "394\n",
      "394\n",
      "394\n",
      "394\n",
      "394\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4983778194656777\n",
      "std: 0.26826232111779097\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4999879488895008\n",
      "std: 0.3247696467306799\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.49462499118347864\n",
      "std: 0.2888055402177855\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.5101133475610831\n",
      "std: 0.25434808389556507\n",
      "-----------------------------\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "mean: 0.4851962413697185\n",
      "std: 0.2642834718504606\n",
      "0.0010135570773854852\n"
     ]
    }
   ],
   "source": [
    "test_1_loss = []\n",
    "for i in range(6, 7):\n",
    "    window_size = i\n",
    "    PATH = \"model/with_early_stop_after_400_without_normalization/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "\n",
    "    '''\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "    '''\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_1_loss.append(total_test_loss/test_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_1, test_set_y_1, test_set_z_1, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0010135570773854852]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_1_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input window size\", fontsize=12)\n",
    "plt.ylabel(\"MSE loss\", fontsize=12)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "# plt.ylim([0.017,0.0255])\n",
    "plt.ylim([0.001,0.0015])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0010135570773854852"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
