{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to analyze the gt points' distribution with respect to predicted points location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, min_max_scaling, min_max_scaling_radius, normalize_one, construct_train_valid_tensor, construct_test_tensor, show_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = np.load(\"input_tensor.npy\")\n",
    "gt_label = np.load(\"gt_label_tensor.npy\")\n",
    "pd_label = np.load(\"pd_label_tensor.npy\")\n",
    "\n",
    "input_valid_train = np.load(\"input_valid_tensor.npy\")\n",
    "gt_valid_label = np.load(\"gt_valid_label_tensor.npy\")\n",
    "pd_valid_label = np.load(\"pd_valid_label_tensor.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to sperical coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistanceThetaPhi(xyz): # N x 3 -> N x 2, xyz_tensor must be two dimension\n",
    "    size = xyz.shape[0]\n",
    "    dtp = np.zeros([size, 3])\n",
    "    for i in range(size):\n",
    "        x = xyz[i, 0]\n",
    "        y = xyz[i, 1]\n",
    "        z = xyz[i, 2]\n",
    "        # print(z)\n",
    "        d = math.sqrt(x*x + y*y + z*z)\n",
    "        theta = math.atan(math.sqrt(x*x + y*y)/z)/math.pi*180\n",
    "        phi = math.atan(y/x)/math.pi*180\n",
    "        \n",
    "        if (d >= 2.0):\n",
    "            d= 2.0;\n",
    "        elif (d <= 1.0):\n",
    "            d= 1.0;\n",
    "        \n",
    "        if (theta < 0):\n",
    "            theta = 180 + theta;\n",
    "        \n",
    "        if (phi > 0):\n",
    "            if (x > 0):\n",
    "                phi = phi;\n",
    "            else:\n",
    "                phi = 180 + phi;\n",
    "        else:\n",
    "            if (x < 0):\n",
    "                phi = 180 + phi;\n",
    "            else:\n",
    "                phi = 360 + phi;\n",
    "        \n",
    "        \n",
    "        dtp[i, 0] = d\n",
    "        dtp[i, 1] = theta\n",
    "        dtp[i, 2] = phi\n",
    "    \n",
    "    return dtp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function's validity\n",
    "pd_dthetaphi_label_tensor = getDistanceThetaPhi(pd_label_tensor)\n",
    "gt_dthetaphi_label_tensor = getDistanceThetaPhi(gt_label_tensor)\n",
    "\n",
    "test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('test/test_1.csv')\n",
    "test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('test/test_2.csv')\n",
    "test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('test/test_3.csv')\n",
    "test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('test/test_4.csv')\n",
    "test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('test/test_5.csv')\n",
    "\n",
    "test_set_x_1_direct, test_set_y_1_direct, test_set_z_1_direct = load_test_data('test/test_1_direct.csv')\n",
    "test_set_x_2_direct, test_set_y_2_direct, test_set_z_2_direct = load_test_data('test/test_2_direct.csv')\n",
    "test_set_x_3_direct, test_set_y_3_direct, test_set_z_3_direct = load_test_data('test/test_3_direct.csv')\n",
    "test_set_x_4_direct, test_set_y_4_direct, test_set_z_4_direct = load_test_data('test/test_4_direct.csv')\n",
    "test_set_x_5_direct, test_set_y_5_direct, test_set_z_5_direct = load_test_data('test/test_5_direct.csv')\n",
    "\n",
    "input_xyz = np.zeros([400, 3])\n",
    "for i in range(400):\n",
    "    input_xyz[i, 0] = test_set_x_1[i]\n",
    "    input_xyz[i, 1] = test_set_y_1[i]\n",
    "    input_xyz[i, 2] = test_set_z_1[i]\n",
    "    \n",
    "input_dthetaphi_tensor = getDistanceThetaPhi(input_xyz)\n",
    "\n",
    "x_diff = [i - j for i, j in zip(test_set_x_1_direct, input_dthetaphi_tensor[:,0].tolist())]\n",
    "y_diff = [i - j for i, j in zip(test_set_y_1_direct, input_dthetaphi_tensor[:,1].tolist())]\n",
    "z_diff = [i - j for i, j in zip(test_set_z_1_direct, input_dthetaphi_tensor[:,2].tolist())]\n",
    "\n",
    "# plt.plot(test_set_x_1_direct, label='d')\n",
    "# plt.plot(test_set_y_1_direct, label='theta')\n",
    "plt.plot(test_set_z_1_direct, label='pi')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_thetaphi_label_tensor shape: torch.Size([14985, 2])\n",
      "pd_thetaphi_label_tensor shape: torch.Size([14985, 2])\n",
      "gt_thetaphi_valid_label_tensor shape: torch.Size([397, 2])\n",
      "pd_thetaphi_valid_label_tensor shape: torch.Size([397, 2])\n"
     ]
    }
   ],
   "source": [
    "# construct 2d theta phi training tensor\n",
    "gt_dthetaphi_label = getDistanceThetaPhi(gt_label)\n",
    "pd_dthetaphi_label = getDistanceThetaPhi(pd_label)\n",
    "gt_thetaphi_label_tensor = torch.Tensor(gt_dthetaphi_label[:,1:])\n",
    "pd_thetaphi_label_tensor = torch.Tensor(pd_dthetaphi_label[:,1:])\n",
    "print(\"gt_thetaphi_label_tensor shape:\", gt_thetaphi_label_tensor.shape)\n",
    "print(\"pd_thetaphi_label_tensor shape:\", pd_thetaphi_label_tensor.shape)\n",
    "\n",
    "# construct 2d theta phi valid tensor\n",
    "gt_dthetaphi_valid_label = getDistanceThetaPhi(gt_valid_label)\n",
    "pd_dthetaphi_valid_label = getDistanceThetaPhi(pd_valid_label)\n",
    "gt_thetaphi_valid_label_tensor = torch.Tensor(gt_dthetaphi_valid_label[:,1:])\n",
    "pd_thetaphi_valid_label_tensor = torch.Tensor(pd_dthetaphi_valid_label[:,1:])\n",
    "print(\"gt_thetaphi_valid_label_tensor shape:\", gt_thetaphi_valid_label_tensor.shape)\n",
    "print(\"pd_thetaphi_valid_label_tensor shape:\", pd_thetaphi_valid_label_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_thetaphi_label_tensor = torch.matmul(gt_thetaphi_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "pd_thetaphi_label_tensor = torch.matmul(pd_thetaphi_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "\n",
    "gt_thetaphi_valid_label_tensor = torch.matmul(gt_thetaphi_valid_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "pd_thetaphi_valid_label_tensor = torch.matmul(pd_thetaphi_valid_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_theta = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_theta = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_phi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_phi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        \n",
    "        self.l_correlation_theta_phi = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        \n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        \n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_theta = self.l_mu_theta(h)\n",
    "        # print(\"mu_theta\", pi.shape)\n",
    "        mu_phi = self.l_mu_phi(h)\n",
    "        # print(\"mu_phi\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_theta = torch.exp(self.l_sigma_theta(h))\n",
    "        # print(\"sigma_theta\", sigma_theta.shape)\n",
    "        sigma_phi = torch.exp(self.l_sigma_phi(h))\n",
    "        # print(\"sigma_phi\", sigma_phi.shape)\n",
    "\n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_theta_phi = self.l_correlation_theta_phi(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi):\n",
    "    size = y.shape[0]\n",
    "    n_gaussians = pi.shape[1]\n",
    "    '''\n",
    "    print(\"sample size: \", size)\n",
    "    print(\"num of gaus: \", n_gaussians)\n",
    "    print(\"mu_theta size: \", mu_theta.shape)\n",
    "    print(\"mu_phi size: \", mu_phi.shape)\n",
    "    print(\"sigma_theta size: \", sigma_theta.shape)\n",
    "    print(\"sigma_phi size: \", sigma_phi.shape)\n",
    "    print(\"correlation_theta_phi size: \", correlation_theta_phi.shape)\n",
    "    '''\n",
    "    # build mean matrix\n",
    "    mean = torch.stack((mu_theta, mu_phi), dim=2)\n",
    "    # build covariance matrix with standard bivariate normal distribution\n",
    "    cov = torch.zeros([size, n_gaussians, 2, 2])\n",
    "    cov[:, :, 0, 0] = sigma_theta**2\n",
    "    cov[:, :, 1, 1] = sigma_phi**2\n",
    "    cov[:, :, 1, 0] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "    cov[:, :, 0, 1] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "\n",
    "    \n",
    "    '''\n",
    "    print(\"sigma_theta**2 size: \", (sigma_theta**2).shape)\n",
    "    print(\"sigma_phi**2 size: \", (sigma_phi**2).shape)\n",
    "    print(\"correlation_theta_phi*sigma_theta*sigma_phi size: \", (correlation_theta_phi*sigma_theta*sigma_phi).shape)\n",
    "    print(\"mean size:\", mean.shape)\n",
    "    print(\"cov size:\", cov.shape)\n",
    "    print(\"correlation_theta_phi:\", correlation_theta_phi.min())\n",
    "    '''\n",
    "    # mean: N x n_gaussian x 2\n",
    "    # cov: N x 5 x 2 x 2\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "    # expand y to N x n_gaussian x 2\n",
    "    y = y.unsqueeze(1).repeat(1, n_gaussians, 1)\n",
    "    \n",
    "    # print(\"y size\", y.shape) # 14985 x 5 x 2\n",
    "    \n",
    "    # y: [x, y, z]; row: x, y, z; column: N, number of samples\n",
    "    # y: N x 3\n",
    "    # loss: N x 1\n",
    "    # print(y_normal.is_cuda)\n",
    "    likelihood = torch.exp(m.log_prob(y))\n",
    "    \n",
    "    # print(\"likelihood shape: \", likelihood.shape) # 14985 x 5\n",
    "    # print(\"pi shape: \", pi.shape) # 14985 x 5\n",
    "    loss = torch.sum(likelihood * pi, dim=1) # 14985 X 1\n",
    "    # print(likelihood.max())\n",
    "    '''\n",
    "    for i in range(loss.shape[0]):\n",
    "        if loss[i] > 1.5:\n",
    "            print(\"loss: \", loss[i])\n",
    "            print(\"likelihood: \", likelihood[i])\n",
    "            print(\"mu array  : \", pi[i])\n",
    "            # print(\"likelihood shape: \", likelihood.shape) # 14985 x 5\n",
    "    '''\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gaussians = 7\n",
    "PATH = \"model/with_early_stop_after_400_without_normalization/model_ng_7.pt\"\n",
    "PATH_LOG = \"model/with_early_stop_after_400_without_normalization/model_ng_7.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mdn = MDN(2, n_hidden=20, n_gaussians=num_gaussians)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mdn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  0 -------\n",
      "epoch: 0, train_loss: 2.2311391830444336, valid_loss: 2.123826026916504\n",
      "-------epoch  1 -------\n",
      "epoch: 1, train_loss: 2.202756404876709, valid_loss: 2.0981380939483643\n",
      "-------epoch  2 -------\n",
      "epoch: 2, train_loss: 2.1746091842651367, valid_loss: 2.0725746154785156\n",
      "-------epoch  3 -------\n",
      "epoch: 3, train_loss: 2.146663188934326, valid_loss: 2.0471138954162598\n",
      "-------epoch  4 -------\n",
      "epoch: 4, train_loss: 2.1188881397247314, valid_loss: 2.021713972091675\n",
      "-------epoch  5 -------\n",
      "epoch: 5, train_loss: 2.091248035430908, valid_loss: 1.9963469505310059\n",
      "-------epoch  6 -------\n",
      "epoch: 6, train_loss: 2.0637121200561523, valid_loss: 1.9710006713867188\n",
      "-------epoch  7 -------\n",
      "epoch: 7, train_loss: 2.0362534523010254, valid_loss: 1.9456671476364136\n",
      "-------epoch  8 -------\n",
      "epoch: 8, train_loss: 2.0088517665863037, valid_loss: 1.9203394651412964\n",
      "-------epoch  9 -------\n",
      "epoch: 9, train_loss: 1.9814907312393188, valid_loss: 1.8950083255767822\n",
      "-------epoch  10 -------\n",
      "epoch: 10, train_loss: 1.954154372215271, valid_loss: 1.8696630001068115\n",
      "-------epoch  11 -------\n",
      "epoch: 11, train_loss: 1.92682683467865, valid_loss: 1.844289779663086\n",
      "-------epoch  12 -------\n",
      "epoch: 12, train_loss: 1.8994892835617065, valid_loss: 1.8188751935958862\n",
      "-------epoch  13 -------\n",
      "epoch: 13, train_loss: 1.8721262216567993, valid_loss: 1.7934062480926514\n",
      "-------epoch  14 -------\n",
      "epoch: 14, train_loss: 1.8447190523147583, valid_loss: 1.767871618270874\n",
      "-------epoch  15 -------\n",
      "epoch: 15, train_loss: 1.8172521591186523, valid_loss: 1.7422618865966797\n",
      "-------epoch  16 -------\n",
      "epoch: 16, train_loss: 1.789710521697998, valid_loss: 1.7165688276290894\n",
      "-------epoch  17 -------\n",
      "epoch: 17, train_loss: 1.7620823383331299, valid_loss: 1.6907864809036255\n",
      "-------epoch  18 -------\n",
      "epoch: 18, train_loss: 1.7343568801879883, valid_loss: 1.6649099588394165\n",
      "-------epoch  19 -------\n",
      "epoch: 19, train_loss: 1.7065235376358032, valid_loss: 1.6389349699020386\n",
      "-------epoch  20 -------\n",
      "epoch: 20, train_loss: 1.678576111793518, valid_loss: 1.6128584146499634\n",
      "-------epoch  21 -------\n",
      "epoch: 21, train_loss: 1.6505070924758911, valid_loss: 1.5866775512695312\n",
      "-------epoch  22 -------\n",
      "epoch: 22, train_loss: 1.6223108768463135, valid_loss: 1.5603904724121094\n",
      "-------epoch  23 -------\n",
      "epoch: 23, train_loss: 1.5939828157424927, valid_loss: 1.5339956283569336\n",
      "-------epoch  24 -------\n",
      "epoch: 24, train_loss: 1.5655187368392944, valid_loss: 1.5074926614761353\n",
      "-------epoch  25 -------\n",
      "epoch: 25, train_loss: 1.5369153022766113, valid_loss: 1.4808800220489502\n",
      "-------epoch  26 -------\n",
      "epoch: 26, train_loss: 1.508169412612915, valid_loss: 1.4541581869125366\n",
      "-------epoch  27 -------\n",
      "epoch: 27, train_loss: 1.479278326034546, valid_loss: 1.4273267984390259\n",
      "-------epoch  28 -------\n",
      "epoch: 28, train_loss: 1.4502424001693726, valid_loss: 1.4003880023956299\n",
      "-------epoch  29 -------\n",
      "epoch: 29, train_loss: 1.4210585355758667, valid_loss: 1.3733415603637695\n",
      "-------epoch  30 -------\n",
      "epoch: 30, train_loss: 1.3917278051376343, valid_loss: 1.34619140625\n",
      "-------epoch  31 -------\n",
      "epoch: 31, train_loss: 1.362250566482544, valid_loss: 1.318939447402954\n",
      "-------epoch  32 -------\n",
      "epoch: 32, train_loss: 1.3326282501220703, valid_loss: 1.2915892601013184\n",
      "-------epoch  33 -------\n",
      "epoch: 33, train_loss: 1.3028631210327148, valid_loss: 1.2641454935073853\n",
      "-------epoch  34 -------\n",
      "epoch: 34, train_loss: 1.2729594707489014, valid_loss: 1.2366135120391846\n",
      "-------epoch  35 -------\n",
      "epoch: 35, train_loss: 1.2429200410842896, valid_loss: 1.208999752998352\n",
      "-------epoch  36 -------\n",
      "epoch: 36, train_loss: 1.212750792503357, valid_loss: 1.181311011314392\n",
      "-------epoch  37 -------\n",
      "epoch: 37, train_loss: 1.1824564933776855, valid_loss: 1.1535552740097046\n",
      "-------epoch  38 -------\n",
      "epoch: 38, train_loss: 1.152044415473938, valid_loss: 1.1257412433624268\n",
      "-------epoch  39 -------\n",
      "epoch: 39, train_loss: 1.121519923210144, valid_loss: 1.0978775024414062\n",
      "-------epoch  40 -------\n",
      "epoch: 40, train_loss: 1.090890884399414, valid_loss: 1.0699734687805176\n",
      "-------epoch  41 -------\n",
      "epoch: 41, train_loss: 1.0601634979248047, valid_loss: 1.0420396327972412\n",
      "-------epoch  42 -------\n",
      "epoch: 42, train_loss: 1.029344916343689, valid_loss: 1.014085054397583\n",
      "-------epoch  43 -------\n",
      "epoch: 43, train_loss: 0.9984415173530579, valid_loss: 0.9861196279525757\n",
      "-------epoch  44 -------\n",
      "epoch: 44, train_loss: 0.9674592018127441, valid_loss: 0.9581530690193176\n",
      "-------epoch  45 -------\n",
      "epoch: 45, train_loss: 0.936403214931488, valid_loss: 0.9301946759223938\n",
      "-------epoch  46 -------\n",
      "epoch: 46, train_loss: 0.9052784442901611, valid_loss: 0.9022536873817444\n",
      "-------epoch  47 -------\n",
      "epoch: 47, train_loss: 0.8740888237953186, valid_loss: 0.8743376731872559\n",
      "-------epoch  48 -------\n",
      "epoch: 48, train_loss: 0.8428385853767395, valid_loss: 0.8464552164077759\n",
      "-------epoch  49 -------\n",
      "epoch: 49, train_loss: 0.8115301132202148, valid_loss: 0.8186124563217163\n",
      "-------epoch  50 -------\n",
      "epoch: 50, train_loss: 0.7801666259765625, valid_loss: 0.7908160090446472\n",
      "-------epoch  51 -------\n",
      "epoch: 51, train_loss: 0.7487505674362183, valid_loss: 0.7630711197853088\n",
      "-------epoch  52 -------\n",
      "epoch: 52, train_loss: 0.7172856330871582, valid_loss: 0.7353821396827698\n",
      "-------epoch  53 -------\n",
      "epoch: 53, train_loss: 0.6857742667198181, valid_loss: 0.7077533602714539\n",
      "-------epoch  54 -------\n",
      "epoch: 54, train_loss: 0.6542215347290039, valid_loss: 0.6801881790161133\n",
      "-------epoch  55 -------\n",
      "epoch: 55, train_loss: 0.6226319670677185, valid_loss: 0.6526902318000793\n",
      "-------epoch  56 -------\n",
      "epoch: 56, train_loss: 0.5910118818283081, valid_loss: 0.6252643465995789\n",
      "-------epoch  57 -------\n",
      "epoch: 57, train_loss: 0.5593692064285278, valid_loss: 0.5979164838790894\n",
      "-------epoch  58 -------\n",
      "epoch: 58, train_loss: 0.5277141332626343, valid_loss: 0.5706551671028137\n",
      "-------epoch  59 -------\n",
      "epoch: 59, train_loss: 0.49605870246887207, valid_loss: 0.5434916019439697\n",
      "-------epoch  60 -------\n",
      "epoch: 60, train_loss: 0.4644181430339813, valid_loss: 0.5164412260055542\n",
      "-------epoch  61 -------\n",
      "epoch: 61, train_loss: 0.4328106641769409, valid_loss: 0.48952361941337585\n",
      "-------epoch  62 -------\n",
      "epoch: 62, train_loss: 0.40125757455825806, valid_loss: 0.46276208758354187\n",
      "-------epoch  63 -------\n",
      "epoch: 63, train_loss: 0.36978259682655334, valid_loss: 0.43618348240852356\n",
      "-------epoch  64 -------\n",
      "epoch: 64, train_loss: 0.338410884141922, valid_loss: 0.40981605648994446\n",
      "-------epoch  65 -------\n",
      "epoch: 65, train_loss: 0.30716705322265625, valid_loss: 0.38368767499923706\n",
      "-------epoch  66 -------\n",
      "epoch: 66, train_loss: 0.27607324719429016, valid_loss: 0.35782167315483093\n",
      "-------epoch  67 -------\n",
      "epoch: 67, train_loss: 0.24514545500278473, valid_loss: 0.3322347402572632\n",
      "-------epoch  68 -------\n",
      "epoch: 68, train_loss: 0.2143905609846115, valid_loss: 0.3069323003292084\n",
      "-------epoch  69 -------\n",
      "epoch: 69, train_loss: 0.18380358815193176, valid_loss: 0.28190651535987854\n",
      "-------epoch  70 -------\n",
      "epoch: 70, train_loss: 0.1533663421869278, valid_loss: 0.25713393092155457\n",
      "-------epoch  71 -------\n",
      "epoch: 71, train_loss: 0.12304724752902985, valid_loss: 0.23257586359977722\n",
      "-------epoch  72 -------\n",
      "epoch: 72, train_loss: 0.09280349314212799, valid_loss: 0.2081783562898636\n",
      "-------epoch  73 -------\n",
      "epoch: 73, train_loss: 0.06258382648229599, valid_loss: 0.1838734745979309\n",
      "-------epoch  74 -------\n",
      "epoch: 74, train_loss: 0.03233161196112633, valid_loss: 0.15957951545715332\n",
      "-------epoch  75 -------\n",
      "epoch: 75, train_loss: 0.0019875101279467344, valid_loss: 0.13519948720932007\n",
      "-------epoch  76 -------\n",
      "epoch: 76, train_loss: -0.02851041592657566, valid_loss: 0.1106172725558281\n",
      "-------epoch  77 -------\n",
      "epoch: 77, train_loss: -0.05922919884324074, valid_loss: 0.08569347113370895\n",
      "-------epoch  78 -------\n",
      "epoch: 78, train_loss: -0.09024618566036224, valid_loss: 0.06026465818285942\n",
      "-------epoch  79 -------\n",
      "epoch: 79, train_loss: -0.1216534972190857, valid_loss: 0.0341513566672802\n",
      "-------epoch  80 -------\n",
      "epoch: 80, train_loss: -0.15355874598026276, valid_loss: 0.00717624556273222\n",
      "-------epoch  81 -------\n",
      "epoch: 81, train_loss: -0.1860778033733368, valid_loss: -0.02081255055963993\n",
      "-------epoch  82 -------\n",
      "epoch: 82, train_loss: -0.21932266652584076, valid_loss: -0.049923744052648544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  83 -------\n",
      "epoch: 83, train_loss: -0.2533907890319824, valid_loss: -0.08021481335163116\n",
      "-------epoch  84 -------\n",
      "epoch: 84, train_loss: -0.28836166858673096, valid_loss: -0.11169707775115967\n",
      "-------epoch  85 -------\n",
      "epoch: 85, train_loss: -0.32430291175842285, valid_loss: -0.14434991776943207\n",
      "-------epoch  86 -------\n",
      "epoch: 86, train_loss: -0.36128196120262146, valid_loss: -0.17814111709594727\n",
      "-------epoch  87 -------\n",
      "epoch: 87, train_loss: -0.3993763029575348, valid_loss: -0.21304626762866974\n",
      "-------epoch  88 -------\n",
      "epoch: 88, train_loss: -0.4386758506298065, valid_loss: -0.2490670382976532\n",
      "-------epoch  89 -------\n",
      "epoch: 89, train_loss: -0.4792761504650116, valid_loss: -0.28624671697616577\n",
      "-------epoch  90 -------\n",
      "epoch: 90, train_loss: -0.5212711691856384, valid_loss: -0.3246854543685913\n",
      "-------epoch  91 -------\n",
      "epoch: 91, train_loss: -0.5647561550140381, valid_loss: -0.3645469546318054\n",
      "-------epoch  92 -------\n",
      "epoch: 92, train_loss: -0.6098455786705017, valid_loss: -0.40604084730148315\n",
      "-------epoch  93 -------\n",
      "epoch: 93, train_loss: -0.6566863059997559, valid_loss: -0.4493691325187683\n",
      "-------epoch  94 -------\n",
      "epoch: 94, train_loss: -0.705449640750885, valid_loss: -0.49465423822402954\n",
      "-------epoch  95 -------\n",
      "epoch: 95, train_loss: -0.7563055157661438, valid_loss: -0.5418955087661743\n",
      "-------epoch  96 -------\n",
      "epoch: 96, train_loss: -0.8094066977500916, valid_loss: -0.5910007953643799\n",
      "-------epoch  97 -------\n",
      "epoch: 97, train_loss: -0.8649133443832397, valid_loss: -0.6418894529342651\n",
      "-------epoch  98 -------\n",
      "epoch: 98, train_loss: -0.9230080246925354, valid_loss: -0.694607675075531\n",
      "-------epoch  99 -------\n",
      "epoch: 99, train_loss: -0.9838578701019287, valid_loss: -0.7494097948074341\n",
      "-------epoch  100 -------\n",
      "epoch: 100, train_loss: -1.0475820302963257, valid_loss: -0.8067517876625061\n",
      "-------epoch  101 -------\n",
      "epoch: 101, train_loss: -1.1142966747283936, valid_loss: -0.8670825958251953\n",
      "-------epoch  102 -------\n",
      "epoch: 102, train_loss: -1.1841360330581665, valid_loss: -0.9305095672607422\n",
      "-------epoch  103 -------\n",
      "epoch: 103, train_loss: -1.2571866512298584, valid_loss: -0.9967889189720154\n",
      "-------epoch  104 -------\n",
      "epoch: 104, train_loss: -1.333508849143982, valid_loss: -1.0657700300216675\n",
      "-------epoch  105 -------\n",
      "epoch: 105, train_loss: -1.4131611585617065, valid_loss: -1.137791395187378\n",
      "-------epoch  106 -------\n",
      "epoch: 106, train_loss: -1.4960730075836182, valid_loss: -1.2134292125701904\n",
      "-------epoch  107 -------\n",
      "epoch: 107, train_loss: -1.582123041152954, valid_loss: -1.2923706769943237\n",
      "-------epoch  108 -------\n",
      "epoch: 108, train_loss: -1.6710374355316162, valid_loss: -1.3733924627304077\n",
      "-------epoch  109 -------\n",
      "epoch: 109, train_loss: -1.7623988389968872, valid_loss: -1.4559379816055298\n",
      "-------epoch  110 -------\n",
      "epoch: 110, train_loss: -1.8555971384048462, valid_loss: -1.5400638580322266\n",
      "-------epoch  111 -------\n",
      "epoch: 111, train_loss: -1.9498612880706787, valid_loss: -1.6238757371902466\n",
      "-------epoch  112 -------\n",
      "epoch: 112, train_loss: -2.044201374053955, valid_loss: -1.705795168876648\n",
      "-------epoch  113 -------\n",
      "epoch: 113, train_loss: -2.137530565261841, valid_loss: -1.7858637571334839\n",
      "-------epoch  114 -------\n",
      "epoch: 114, train_loss: -2.2287330627441406, valid_loss: -1.8607187271118164\n",
      "-------epoch  115 -------\n",
      "epoch: 115, train_loss: -2.3167295455932617, valid_loss: -1.9318816661834717\n",
      "-------epoch  116 -------\n",
      "epoch: 116, train_loss: -2.4000442028045654, valid_loss: -1.9887198209762573\n",
      "-------epoch  117 -------\n",
      "epoch: 117, train_loss: -2.4714488983154297, valid_loss: -2.0439462661743164\n",
      "-------epoch  118 -------\n",
      "epoch: 118, train_loss: -2.542792558670044, valid_loss: -2.1110291481018066\n",
      "-------epoch  119 -------\n",
      "epoch: 119, train_loss: -2.6315457820892334, valid_loss: -2.1578848361968994\n",
      "-------epoch  120 -------\n",
      "epoch: 120, train_loss: -2.6944947242736816, valid_loss: -2.2100415229797363\n",
      "-------epoch  121 -------\n",
      "epoch: 121, train_loss: -2.768890142440796, valid_loss: -2.273128032684326\n",
      "-------epoch  122 -------\n",
      "epoch: 122, train_loss: -2.855255365371704, valid_loss: -2.3166847229003906\n",
      "-------epoch  123 -------\n",
      "epoch: 123, train_loss: -2.917102098464966, valid_loss: -2.3722007274627686\n",
      "-------epoch  124 -------\n",
      "epoch: 124, train_loss: -2.9964537620544434, valid_loss: -2.4326746463775635\n",
      "-------epoch  125 -------\n",
      "epoch: 125, train_loss: -3.0847737789154053, valid_loss: -2.4696779251098633\n",
      "-------epoch  126 -------\n",
      "epoch: 126, train_loss: -3.1459057331085205, valid_loss: -2.5161359310150146\n",
      "-------epoch  127 -------\n",
      "epoch: 127, train_loss: -3.2042975425720215, valid_loss: -2.5561487674713135\n",
      "-------epoch  128 -------\n",
      "epoch: 128, train_loss: -3.2864646911621094, valid_loss: -2.6041858196258545\n",
      "-------epoch  129 -------\n",
      "epoch: 129, train_loss: -3.3579282760620117, valid_loss: -2.636460065841675\n",
      "-------epoch  130 -------\n",
      "epoch: 130, train_loss: -3.3975942134857178, valid_loss: -2.607783079147339\n",
      "-------epoch  131 -------\n",
      "epoch: 131, train_loss: -3.4198639392852783, valid_loss: -2.679819107055664\n",
      "-------epoch  132 -------\n",
      "epoch: 132, train_loss: -3.473130226135254, valid_loss: -2.7023839950561523\n",
      "-------epoch  133 -------\n",
      "epoch: 133, train_loss: -3.561220169067383, valid_loss: -2.710470676422119\n",
      "-------epoch  134 -------\n",
      "epoch: 134, train_loss: -3.5974137783050537, valid_loss: -2.7463293075561523\n",
      "-------epoch  135 -------\n",
      "epoch: 135, train_loss: -3.5982513427734375, valid_loss: -2.7153830528259277\n",
      "-------epoch  136 -------\n",
      "epoch: 136, train_loss: -3.6510519981384277, valid_loss: -2.808750867843628\n",
      "-------epoch  137 -------\n",
      "epoch: 137, train_loss: -3.7426917552948, valid_loss: -2.8298001289367676\n",
      "-------epoch  138 -------\n",
      "epoch: 138, train_loss: -3.7680728435516357, valid_loss: -2.760881185531616\n",
      "-------epoch  139 -------\n",
      "epoch: 139, train_loss: -3.75675892829895, valid_loss: -2.8553383350372314\n",
      "-------epoch  140 -------\n",
      "epoch: 140, train_loss: -3.805980920791626, valid_loss: -2.8694844245910645\n",
      "-------epoch  141 -------\n",
      "epoch: 141, train_loss: -3.8950846195220947, valid_loss: -2.864487886428833\n",
      "-------epoch  142 -------\n",
      "epoch: 142, train_loss: -3.912013530731201, valid_loss: -2.8969316482543945\n",
      "-------epoch  143 -------\n",
      "epoch: 143, train_loss: -3.8887252807617188, valid_loss: -2.849581003189087\n",
      "-------epoch  144 -------\n",
      "epoch: 144, train_loss: -3.9282989501953125, valid_loss: -2.950148344039917\n",
      "-------epoch  145 -------\n",
      "epoch: 145, train_loss: -4.018887042999268, valid_loss: -2.9654314517974854\n",
      "-------epoch  146 -------\n",
      "epoch: 146, train_loss: -4.044957637786865, valid_loss: -2.8912885189056396\n",
      "-------epoch  147 -------\n",
      "epoch: 147, train_loss: -4.019301891326904, valid_loss: -2.975735664367676\n",
      "-------epoch  148 -------\n",
      "epoch: 148, train_loss: -4.039079666137695, valid_loss: -2.964871644973755\n",
      "-------epoch  149 -------\n",
      "epoch: 149, train_loss: -4.126950263977051, valid_loss: -3.0035455226898193\n",
      "-------epoch  150 -------\n",
      "epoch: 150, train_loss: -4.174067974090576, valid_loss: -3.0313243865966797\n",
      "-------epoch  151 -------\n",
      "epoch: 151, train_loss: -4.153572082519531, valid_loss: -2.9356813430786133\n",
      "-------epoch  152 -------\n",
      "epoch: 152, train_loss: -4.145196437835693, valid_loss: -3.058500289916992\n",
      "-------epoch  153 -------\n",
      "epoch: 153, train_loss: -4.20723295211792, valid_loss: -3.0551159381866455\n",
      "-------epoch  154 -------\n",
      "epoch: 154, train_loss: -4.281152725219727, valid_loss: -3.0416972637176514\n",
      "-------epoch  155 -------\n",
      "epoch: 155, train_loss: -4.289918422698975, valid_loss: -3.092721700668335\n",
      "-------epoch  156 -------\n",
      "epoch: 156, train_loss: -4.263462066650391, valid_loss: -2.9998648166656494\n",
      "-------epoch  157 -------\n",
      "epoch: 157, train_loss: -4.27326774597168, valid_loss: -3.127962589263916\n",
      "-------epoch  158 -------\n",
      "epoch: 158, train_loss: -4.341803073883057, valid_loss: -3.125048875808716\n",
      "-------epoch  159 -------\n",
      "epoch: 159, train_loss: -4.404748916625977, valid_loss: -3.107741117477417\n",
      "-------epoch  160 -------\n",
      "epoch: 160, train_loss: -4.409810543060303, valid_loss: -3.1576011180877686\n",
      "-------epoch  161 -------\n",
      "epoch: 161, train_loss: -4.383199691772461, valid_loss: -3.055767059326172\n",
      "-------epoch  162 -------\n",
      "epoch: 162, train_loss: -4.377955436706543, valid_loss: -3.1787798404693604\n",
      "-------epoch  163 -------\n",
      "epoch: 163, train_loss: -4.426283359527588, valid_loss: -3.1577951908111572\n",
      "-------epoch  164 -------\n",
      "epoch: 164, train_loss: -4.499754428863525, valid_loss: -3.186493396759033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  165 -------\n",
      "epoch: 165, train_loss: -4.532167911529541, valid_loss: -3.213017225265503\n",
      "-------epoch  166 -------\n",
      "epoch: 166, train_loss: -4.51732063293457, valid_loss: -3.1043920516967773\n",
      "-------epoch  167 -------\n",
      "epoch: 167, train_loss: -4.487064361572266, valid_loss: -3.2019803524017334\n",
      "-------epoch  168 -------\n",
      "epoch: 168, train_loss: -4.477367877960205, valid_loss: -3.118603229522705\n",
      "-------epoch  169 -------\n",
      "epoch: 169, train_loss: -4.528889179229736, valid_loss: -3.2393438816070557\n",
      "-------epoch  170 -------\n",
      "epoch: 170, train_loss: -4.606052875518799, valid_loss: -3.2347397804260254\n",
      "-------epoch  171 -------\n",
      "epoch: 171, train_loss: -4.644625186920166, valid_loss: -3.1811492443084717\n",
      "-------epoch  172 -------\n",
      "epoch: 172, train_loss: -4.629603385925293, valid_loss: -3.237459897994995\n",
      "-------epoch  173 -------\n",
      "epoch: 173, train_loss: -4.588708877563477, valid_loss: -3.0992093086242676\n",
      "-------epoch  174 -------\n",
      "epoch: 174, train_loss: -4.5652384757995605, valid_loss: -3.2382590770721436\n",
      "-------epoch  175 -------\n",
      "epoch: 175, train_loss: -4.5963969230651855, valid_loss: -3.1867165565490723\n",
      "-------epoch  176 -------\n",
      "epoch: 176, train_loss: -4.681194305419922, valid_loss: -3.2595412731170654\n",
      "-------epoch  177 -------\n",
      "epoch: 177, train_loss: -4.737279415130615, valid_loss: -3.272965431213379\n",
      "-------epoch  178 -------\n",
      "epoch: 178, train_loss: -4.729419231414795, valid_loss: -3.1642489433288574\n",
      "-------epoch  179 -------\n",
      "epoch: 179, train_loss: -4.685396194458008, valid_loss: -3.2489073276519775\n",
      "-------epoch  180 -------\n",
      "epoch: 180, train_loss: -4.647522449493408, valid_loss: -3.135780096054077\n",
      "-------epoch  181 -------\n",
      "epoch: 181, train_loss: -4.669516086578369, valid_loss: -3.2778499126434326\n",
      "-------epoch  182 -------\n",
      "epoch: 182, train_loss: -4.745277404785156, valid_loss: -3.2571489810943604\n",
      "-------epoch  183 -------\n",
      "epoch: 183, train_loss: -4.810396194458008, valid_loss: -3.245300531387329\n",
      "-------epoch  184 -------\n",
      "epoch: 184, train_loss: -4.814108371734619, valid_loss: -3.2789294719696045\n",
      "-------epoch  185 -------\n",
      "epoch: 185, train_loss: -4.773271083831787, valid_loss: -3.1444168090820312\n",
      "-------epoch  186 -------\n",
      "epoch: 186, train_loss: -4.730586528778076, valid_loss: -3.2567877769470215\n",
      "-------epoch  187 -------\n",
      "epoch: 187, train_loss: -4.726332664489746, valid_loss: -3.1730833053588867\n",
      "-------epoch  188 -------\n",
      "epoch: 188, train_loss: -4.78585958480835, valid_loss: -3.2861647605895996\n",
      "-------epoch  189 -------\n",
      "epoch: 189, train_loss: -4.859222412109375, valid_loss: -3.2774624824523926\n",
      "-------epoch  190 -------\n",
      "epoch: 190, train_loss: -4.888079643249512, valid_loss: -3.2208850383758545\n",
      "-------epoch  191 -------\n",
      "epoch: 191, train_loss: -4.865370273590088, valid_loss: -3.2734744548797607\n",
      "-------epoch  192 -------\n",
      "epoch: 192, train_loss: -4.8202409744262695, valid_loss: -3.139896869659424\n",
      "-------epoch  193 -------\n",
      "epoch: 193, train_loss: -4.790235996246338, valid_loss: -3.2633790969848633\n",
      "-------epoch  194 -------\n",
      "epoch: 194, train_loss: -4.804837703704834, valid_loss: -3.1915156841278076\n",
      "-------epoch  195 -------\n",
      "epoch: 195, train_loss: -4.868865013122559, valid_loss: -3.2866854667663574\n",
      "-------epoch  196 -------\n",
      "epoch: 196, train_loss: -4.930603981018066, valid_loss: -3.281562328338623\n",
      "-------epoch  197 -------\n",
      "epoch: 197, train_loss: -4.948390007019043, valid_loss: -3.221627712249756\n",
      "-------epoch  198 -------\n",
      "epoch: 198, train_loss: -4.9244065284729, valid_loss: -3.275944948196411\n",
      "-------epoch  199 -------\n",
      "epoch: 199, train_loss: -4.8835954666137695, valid_loss: -3.146388053894043\n",
      "-------epoch  200 -------\n",
      "epoch: 200, train_loss: -4.855445861816406, valid_loss: -3.264216423034668\n",
      "-------epoch  201 -------\n",
      "epoch: 201, train_loss: -4.8632073402404785, valid_loss: -3.1801366806030273\n",
      "-------epoch  202 -------\n",
      "epoch: 202, train_loss: -4.915476322174072, valid_loss: -3.2843353748321533\n",
      "-------epoch  203 -------\n",
      "epoch: 203, train_loss: -4.975208759307861, valid_loss: -3.2683913707733154\n",
      "-------epoch  204 -------\n",
      "epoch: 204, train_loss: -5.004078388214111, valid_loss: -3.2345573902130127\n",
      "-------epoch  205 -------\n",
      "epoch: 205, train_loss: -4.99422550201416, valid_loss: -3.279367208480835\n",
      "-------epoch  206 -------\n",
      "epoch: 206, train_loss: -4.9622955322265625, valid_loss: -3.158578872680664\n",
      "-------epoch  207 -------\n",
      "epoch: 207, train_loss: -4.930017471313477, valid_loss: -3.2633891105651855\n",
      "-------epoch  208 -------\n",
      "epoch: 208, train_loss: -4.915767192840576, valid_loss: -3.1521687507629395\n",
      "-------epoch  209 -------\n",
      "epoch: 209, train_loss: -4.938620567321777, valid_loss: -3.278724431991577\n",
      "-------epoch  210 -------\n",
      "epoch: 210, train_loss: -4.988589286804199, valid_loss: -3.233962059020996\n",
      "-------epoch  211 -------\n",
      "epoch: 211, train_loss: -5.037600994110107, valid_loss: -3.263115882873535\n",
      "-------epoch  212 -------\n",
      "epoch: 212, train_loss: -5.058324337005615, valid_loss: -3.2800023555755615\n",
      "-------epoch  213 -------\n",
      "epoch: 213, train_loss: -5.049355983734131, valid_loss: -3.196047306060791\n",
      "-------epoch  214 -------\n",
      "epoch: 214, train_loss: -5.023910045623779, valid_loss: -3.272181749343872\n",
      "-------epoch  215 -------\n",
      "epoch: 215, train_loss: -4.995234489440918, valid_loss: -3.1424732208251953\n",
      "-------epoch  216 -------\n",
      "epoch: 216, train_loss: -4.978790760040283, valid_loss: -3.2660133838653564\n",
      "-------epoch  217 -------\n",
      "epoch: 217, train_loss: -4.98477029800415, valid_loss: -3.164100408554077\n",
      "-------epoch  218 -------\n",
      "epoch: 218, train_loss: -5.020480155944824, valid_loss: -3.2780823707580566\n",
      "-------epoch  219 -------\n",
      "epoch: 219, train_loss: -5.06724739074707, valid_loss: -3.2398791313171387\n",
      "-------epoch  220 -------\n",
      "epoch: 220, train_loss: -5.102458477020264, valid_loss: -3.249737501144409\n",
      "-------epoch  221 -------\n",
      "epoch: 221, train_loss: -5.1128644943237305, valid_loss: -3.2749221324920654\n",
      "-------epoch  222 -------\n",
      "epoch: 222, train_loss: -5.10227108001709, valid_loss: -3.1875364780426025\n",
      "-------epoch  223 -------\n",
      "epoch: 223, train_loss: -5.08077335357666, valid_loss: -3.2708451747894287\n",
      "-------epoch  224 -------\n",
      "epoch: 224, train_loss: -5.056321620941162, valid_loss: -3.137298345565796\n",
      "-------epoch  225 -------\n",
      "epoch: 225, train_loss: -5.039100646972656, valid_loss: -3.2645885944366455\n",
      "-------epoch  226 -------\n",
      "epoch: 226, train_loss: -5.0364251136779785, valid_loss: -3.1440353393554688\n",
      "-------epoch  227 -------\n",
      "epoch: 227, train_loss: -5.05940055847168, valid_loss: -3.2742433547973633\n",
      "-------epoch  228 -------\n",
      "epoch: 228, train_loss: -5.09947395324707, valid_loss: -3.212204694747925\n",
      "-------epoch  229 -------\n",
      "epoch: 229, train_loss: -5.1407270431518555, valid_loss: -3.259223222732544\n",
      "-------epoch  230 -------\n",
      "epoch: 230, train_loss: -5.164684772491455, valid_loss: -3.263467788696289\n",
      "-------epoch  231 -------\n",
      "epoch: 231, train_loss: -5.167229175567627, valid_loss: -3.204904317855835\n",
      "-------epoch  232 -------\n",
      "epoch: 232, train_loss: -5.154508113861084, valid_loss: -3.272540807723999\n",
      "-------epoch  233 -------\n",
      "epoch: 233, train_loss: -5.1343770027160645, valid_loss: -3.1499969959259033\n",
      "-------epoch  234 -------\n",
      "epoch: 234, train_loss: -5.114199161529541, valid_loss: -3.265537738800049\n",
      "-------epoch  235 -------\n",
      "epoch: 235, train_loss: -5.098629951477051, valid_loss: -3.126040458679199\n",
      "-------epoch  236 -------\n",
      "epoch: 236, train_loss: -5.0982136726379395, valid_loss: -3.2666258811950684\n",
      "-------epoch  237 -------\n",
      "epoch: 237, train_loss: -5.115570068359375, valid_loss: -3.162132740020752\n",
      "-------epoch  238 -------\n",
      "epoch: 238, train_loss: -5.151010036468506, valid_loss: -3.269864559173584\n",
      "-------epoch  239 -------\n",
      "epoch: 239, train_loss: -5.188828945159912, valid_loss: -3.2275846004486084\n",
      "-------epoch  240 -------\n",
      "epoch: 240, train_loss: -5.215388774871826, valid_loss: -3.237917900085449\n",
      "-------epoch  241 -------\n",
      "epoch: 241, train_loss: -5.224274158477783, valid_loss: -3.2631609439849854\n",
      "-------epoch  242 -------\n",
      "epoch: 242, train_loss: -5.218500137329102, valid_loss: -3.1846890449523926\n",
      "-------epoch  243 -------\n",
      "epoch: 243, train_loss: -5.204062461853027, valid_loss: -3.2680017948150635\n",
      "-------epoch  244 -------\n",
      "epoch: 244, train_loss: -5.185077667236328, valid_loss: -3.1346278190612793\n",
      "-------epoch  245 -------\n",
      "epoch: 245, train_loss: -5.165877819061279, valid_loss: -3.261744976043701\n",
      "-------epoch  246 -------\n",
      "epoch: 246, train_loss: -5.148975372314453, valid_loss: -3.107179641723633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  247 -------\n",
      "epoch: 247, train_loss: -5.145107269287109, valid_loss: -3.262385606765747\n",
      "-------epoch  248 -------\n",
      "epoch: 248, train_loss: -5.158353805541992, valid_loss: -3.139995813369751\n",
      "-------epoch  249 -------\n",
      "epoch: 249, train_loss: -5.1934123039245605, valid_loss: -3.2677385807037354\n",
      "-------epoch  250 -------\n",
      "epoch: 250, train_loss: -5.2350382804870605, valid_loss: -3.214097261428833\n",
      "-------epoch  251 -------\n",
      "epoch: 251, train_loss: -5.267380237579346, valid_loss: -3.2365241050720215\n",
      "-------epoch  252 -------\n",
      "epoch: 252, train_loss: -5.279935836791992, valid_loss: -3.2590138912200928\n",
      "-------epoch  253 -------\n",
      "epoch: 253, train_loss: -5.274507522583008, valid_loss: -3.1794135570526123\n",
      "-------epoch  254 -------\n",
      "epoch: 254, train_loss: -5.25889778137207, valid_loss: -3.2671120166778564\n",
      "-------epoch  255 -------\n",
      "epoch: 255, train_loss: -5.240471839904785, valid_loss: -3.1344664096832275\n",
      "-------epoch  256 -------\n",
      "epoch: 256, train_loss: -5.226588726043701, valid_loss: -3.2643747329711914\n",
      "-------epoch  257 -------\n",
      "epoch: 257, train_loss: -5.220497131347656, valid_loss: -3.1265950202941895\n",
      "-------epoch  258 -------\n",
      "epoch: 258, train_loss: -5.228769779205322, valid_loss: -3.265559196472168\n",
      "-------epoch  259 -------\n",
      "epoch: 259, train_loss: -5.248464107513428, valid_loss: -3.163529396057129\n",
      "-------epoch  260 -------\n",
      "epoch: 260, train_loss: -5.2767791748046875, valid_loss: -3.2601468563079834\n",
      "-------epoch  261 -------\n",
      "epoch: 261, train_loss: -5.303548812866211, valid_loss: -3.2135069370269775\n",
      "-------epoch  262 -------\n",
      "epoch: 262, train_loss: -5.322830677032471, valid_loss: -3.232909917831421\n",
      "-------epoch  263 -------\n",
      "epoch: 263, train_loss: -5.332363128662109, valid_loss: -3.2439608573913574\n",
      "-------epoch  264 -------\n",
      "epoch: 264, train_loss: -5.333670139312744, valid_loss: -3.192559003829956\n",
      "-------epoch  265 -------\n",
      "epoch: 265, train_loss: -5.3288116455078125, valid_loss: -3.255324363708496\n",
      "-------epoch  266 -------\n",
      "epoch: 266, train_loss: -5.318001747131348, valid_loss: -3.1403684616088867\n",
      "-------epoch  267 -------\n",
      "epoch: 267, train_loss: -5.299299716949463, valid_loss: -3.2537777423858643\n",
      "-------epoch  268 -------\n",
      "epoch: 268, train_loss: -5.266607761383057, valid_loss: -3.058957815170288\n",
      "-------epoch  269 -------\n",
      "epoch: 269, train_loss: -5.217652320861816, valid_loss: -3.2330355644226074\n",
      "-------epoch  270 -------\n",
      "epoch: 270, train_loss: -5.1530985832214355, valid_loss: -2.985430955886841\n",
      "-------epoch  271 -------\n",
      "epoch: 271, train_loss: -5.118566513061523, valid_loss: -3.2401442527770996\n",
      "-------epoch  272 -------\n",
      "epoch: 272, train_loss: -5.156921863555908, valid_loss: -3.111367702484131\n",
      "-------epoch  273 -------\n",
      "epoch: 273, train_loss: -5.278557777404785, valid_loss: -3.262077808380127\n",
      "-------epoch  274 -------\n",
      "epoch: 274, train_loss: -5.372312068939209, valid_loss: -3.273594379425049\n",
      "-------epoch  275 -------\n",
      "epoch: 275, train_loss: -5.364515781402588, valid_loss: -3.130927562713623\n",
      "-------epoch  276 -------\n",
      "epoch: 276, train_loss: -5.298020839691162, valid_loss: -3.282928466796875\n",
      "-------epoch  277 -------\n",
      "epoch: 277, train_loss: -5.276352405548096, valid_loss: -3.1682467460632324\n",
      "-------epoch  278 -------\n",
      "epoch: 278, train_loss: -5.338587760925293, valid_loss: -3.271486759185791\n",
      "-------epoch  279 -------\n",
      "epoch: 279, train_loss: -5.396041393280029, valid_loss: -3.28716778755188\n",
      "-------epoch  280 -------\n",
      "epoch: 280, train_loss: -5.382709503173828, valid_loss: -3.1654558181762695\n",
      "-------epoch  281 -------\n",
      "epoch: 281, train_loss: -5.342498779296875, valid_loss: -3.2954907417297363\n",
      "-------epoch  282 -------\n",
      "epoch: 282, train_loss: -5.348460674285889, valid_loss: -3.219461441040039\n",
      "-------epoch  283 -------\n",
      "epoch: 283, train_loss: -5.3955512046813965, valid_loss: -3.258788824081421\n",
      "-------epoch  284 -------\n",
      "epoch: 284, train_loss: -5.417299270629883, valid_loss: -3.2938525676727295\n",
      "-------epoch  285 -------\n",
      "epoch: 285, train_loss: -5.3972039222717285, valid_loss: -3.1847612857818604\n",
      "-------epoch  286 -------\n",
      "epoch: 286, train_loss: -5.379525184631348, valid_loss: -3.2943336963653564\n",
      "-------epoch  287 -------\n",
      "epoch: 287, train_loss: -5.39537239074707, valid_loss: -3.235285520553589\n",
      "-------epoch  288 -------\n",
      "epoch: 288, train_loss: -5.426301002502441, valid_loss: -3.2538256645202637\n",
      "-------epoch  289 -------\n",
      "epoch: 289, train_loss: -5.43786096572876, valid_loss: -3.284165620803833\n",
      "-------epoch  290 -------\n",
      "epoch: 290, train_loss: -5.426817417144775, valid_loss: -3.1915972232818604\n",
      "-------epoch  291 -------\n",
      "epoch: 291, train_loss: -5.4150309562683105, valid_loss: -3.2819812297821045\n",
      "-------epoch  292 -------\n",
      "epoch: 292, train_loss: -5.4189958572387695, valid_loss: -3.2018864154815674\n",
      "-------epoch  293 -------\n",
      "epoch: 293, train_loss: -5.437024116516113, valid_loss: -3.2606353759765625\n",
      "-------epoch  294 -------\n",
      "epoch: 294, train_loss: -5.4553399085998535, valid_loss: -3.240755319595337\n",
      "-------epoch  295 -------\n",
      "epoch: 295, train_loss: -5.464617729187012, valid_loss: -3.214240074157715\n",
      "-------epoch  296 -------\n",
      "epoch: 296, train_loss: -5.464160919189453, valid_loss: -3.2553930282592773\n",
      "-------epoch  297 -------\n",
      "epoch: 297, train_loss: -5.458043575286865, valid_loss: -3.1689698696136475\n",
      "-------epoch  298 -------\n",
      "epoch: 298, train_loss: -5.450070858001709, valid_loss: -3.256760597229004\n",
      "-------epoch  299 -------\n",
      "epoch: 299, train_loss: -5.44126033782959, valid_loss: -3.130896806716919\n",
      "-------epoch  300 -------\n",
      "epoch: 300, train_loss: -5.431299209594727, valid_loss: -3.2523605823516846\n",
      "-------epoch  301 -------\n",
      "epoch: 301, train_loss: -5.417293548583984, valid_loss: -3.088061809539795\n",
      "-------epoch  302 -------\n",
      "epoch: 302, train_loss: -5.3983635902404785, valid_loss: -3.247302770614624\n",
      "-------epoch  303 -------\n",
      "epoch: 303, train_loss: -5.372299671173096, valid_loss: -3.0397536754608154\n",
      "-------epoch  304 -------\n",
      "epoch: 304, train_loss: -5.347070693969727, valid_loss: -3.2439005374908447\n",
      "-------epoch  305 -------\n",
      "epoch: 305, train_loss: -5.332594394683838, valid_loss: -3.0460383892059326\n",
      "-------epoch  306 -------\n",
      "epoch: 306, train_loss: -5.35344123840332, valid_loss: -3.2615108489990234\n",
      "-------epoch  307 -------\n",
      "epoch: 307, train_loss: -5.4087910652160645, valid_loss: -3.161515712738037\n",
      "-------epoch  308 -------\n",
      "epoch: 308, train_loss: -5.476925849914551, valid_loss: -3.2482709884643555\n",
      "-------epoch  309 -------\n",
      "epoch: 309, train_loss: -5.517148017883301, valid_loss: -3.260582685470581\n",
      "-------epoch  310 -------\n",
      "epoch: 310, train_loss: -5.5133280754089355, valid_loss: -3.1647050380706787\n",
      "-------epoch  311 -------\n",
      "epoch: 311, train_loss: -5.483229160308838, valid_loss: -3.278244733810425\n",
      "-------epoch  312 -------\n",
      "epoch: 312, train_loss: -5.461098670959473, valid_loss: -3.148672342300415\n",
      "-------epoch  313 -------\n",
      "epoch: 313, train_loss: -5.471395492553711, valid_loss: -3.2790632247924805\n",
      "-------epoch  314 -------\n",
      "epoch: 314, train_loss: -5.505655288696289, valid_loss: -3.2296946048736572\n",
      "-------epoch  315 -------\n",
      "epoch: 315, train_loss: -5.535843849182129, valid_loss: -3.237105369567871\n",
      "-------epoch  316 -------\n",
      "epoch: 316, train_loss: -5.541504859924316, valid_loss: -3.2801120281219482\n",
      "-------epoch  317 -------\n",
      "epoch: 317, train_loss: -5.527547836303711, valid_loss: -3.184112548828125\n",
      "-------epoch  318 -------\n",
      "epoch: 318, train_loss: -5.513942241668701, valid_loss: -3.288430690765381\n",
      "-------epoch  319 -------\n",
      "epoch: 319, train_loss: -5.515738010406494, valid_loss: -3.200716733932495\n",
      "-------epoch  320 -------\n",
      "epoch: 320, train_loss: -5.53320837020874, valid_loss: -3.275855541229248\n",
      "-------epoch  321 -------\n",
      "epoch: 321, train_loss: -5.553745746612549, valid_loss: -3.250943660736084\n",
      "-------epoch  322 -------\n",
      "epoch: 322, train_loss: -5.565845012664795, valid_loss: -3.2367358207702637\n",
      "-------epoch  323 -------\n",
      "epoch: 323, train_loss: -5.566515922546387, valid_loss: -3.275951862335205\n",
      "-------epoch  324 -------\n",
      "epoch: 324, train_loss: -5.560269832611084, valid_loss: -3.1962335109710693\n",
      "-------epoch  325 -------\n",
      "epoch: 325, train_loss: -5.553577899932861, valid_loss: -3.278146266937256\n",
      "-------epoch  326 -------\n",
      "epoch: 326, train_loss: -5.550514221191406, valid_loss: -3.1803483963012695\n",
      "-------epoch  327 -------\n",
      "epoch: 327, train_loss: -5.552488803863525, valid_loss: -3.2743709087371826\n",
      "-------epoch  328 -------\n",
      "epoch: 328, train_loss: -5.5582170486450195, valid_loss: -3.185173988342285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  329 -------\n",
      "epoch: 329, train_loss: -5.566189289093018, valid_loss: -3.269015073776245\n",
      "-------epoch  330 -------\n",
      "epoch: 330, train_loss: -5.574483394622803, valid_loss: -3.194591760635376\n",
      "-------epoch  331 -------\n",
      "epoch: 331, train_loss: -5.5821661949157715, valid_loss: -3.263514995574951\n",
      "-------epoch  332 -------\n",
      "epoch: 332, train_loss: -5.588509559631348, valid_loss: -3.1978373527526855\n",
      "-------epoch  333 -------\n",
      "epoch: 333, train_loss: -5.593327522277832, valid_loss: -3.2613449096679688\n",
      "-------epoch  334 -------\n",
      "epoch: 334, train_loss: -5.596205711364746, valid_loss: -3.1885857582092285\n",
      "-------epoch  335 -------\n",
      "epoch: 335, train_loss: -5.596465110778809, valid_loss: -3.2634832859039307\n",
      "-------epoch  336 -------\n",
      "epoch: 336, train_loss: -5.592214107513428, valid_loss: -3.1563560962677\n",
      "-------epoch  337 -------\n",
      "epoch: 337, train_loss: -5.579751014709473, valid_loss: -3.268387794494629\n",
      "-------epoch  338 -------\n",
      "epoch: 338, train_loss: -5.55112886428833, valid_loss: -3.0717320442199707\n",
      "-------epoch  339 -------\n",
      "epoch: 339, train_loss: -5.493885517120361, valid_loss: -3.250676393508911\n",
      "-------epoch  340 -------\n",
      "epoch: 340, train_loss: -5.393787860870361, valid_loss: -2.923867702484131\n",
      "-------epoch  341 -------\n",
      "epoch: 341, train_loss: -5.273888111114502, valid_loss: -3.2241930961608887\n",
      "-------epoch  342 -------\n",
      "epoch: 342, train_loss: -5.234563827514648, valid_loss: -3.023921489715576\n",
      "-------epoch  343 -------\n",
      "epoch: 343, train_loss: -5.390383720397949, valid_loss: -3.3217649459838867\n",
      "-------epoch  344 -------\n",
      "epoch: 344, train_loss: -5.600010871887207, valid_loss: -3.3271262645721436\n",
      "-------epoch  345 -------\n",
      "epoch: 345, train_loss: -5.6303019523620605, valid_loss: -3.1488921642303467\n",
      "-------epoch  346 -------\n",
      "epoch: 346, train_loss: -5.508970260620117, valid_loss: -3.3470618724823\n",
      "-------epoch  347 -------\n",
      "epoch: 347, train_loss: -5.497275352478027, valid_loss: -3.2754650115966797\n",
      "-------epoch  348 -------\n",
      "epoch: 348, train_loss: -5.625158786773682, valid_loss: -3.2998907566070557\n",
      "-------epoch  349 -------\n",
      "epoch: 349, train_loss: -5.638683319091797, valid_loss: -3.3842825889587402\n",
      "-------epoch  350 -------\n",
      "epoch: 350, train_loss: -5.558722972869873, valid_loss: -3.2629544734954834\n",
      "-------epoch  351 -------\n",
      "epoch: 351, train_loss: -5.600271224975586, valid_loss: -3.3646326065063477\n",
      "-------epoch  352 -------\n",
      "epoch: 352, train_loss: -5.664689540863037, valid_loss: -3.4026262760162354\n",
      "-------epoch  353 -------\n",
      "epoch: 353, train_loss: -5.618246555328369, valid_loss: -3.283355712890625\n",
      "-------epoch  354 -------\n",
      "epoch: 354, train_loss: -5.610492706298828, valid_loss: -3.3989434242248535\n",
      "-------epoch  355 -------\n",
      "epoch: 355, train_loss: -5.6688666343688965, valid_loss: -3.4140830039978027\n",
      "-------epoch  356 -------\n",
      "epoch: 356, train_loss: -5.655300140380859, valid_loss: -3.308272123336792\n",
      "-------epoch  357 -------\n",
      "epoch: 357, train_loss: -5.631848335266113, valid_loss: -3.412595510482788\n",
      "-------epoch  358 -------\n",
      "epoch: 358, train_loss: -5.672204494476318, valid_loss: -3.4107606410980225\n",
      "-------epoch  359 -------\n",
      "epoch: 359, train_loss: -5.680330276489258, valid_loss: -3.3276729583740234\n",
      "-------epoch  360 -------\n",
      "epoch: 360, train_loss: -5.655800819396973, valid_loss: -3.4189529418945312\n",
      "-------epoch  361 -------\n",
      "epoch: 361, train_loss: -5.676387310028076, valid_loss: -3.39528489112854\n",
      "-------epoch  362 -------\n",
      "epoch: 362, train_loss: -5.698102951049805, valid_loss: -3.3406763076782227\n",
      "-------epoch  363 -------\n",
      "epoch: 363, train_loss: -5.682168483734131, valid_loss: -3.410414218902588\n",
      "-------epoch  364 -------\n",
      "epoch: 364, train_loss: -5.683025360107422, valid_loss: -3.364241600036621\n",
      "-------epoch  365 -------\n",
      "epoch: 365, train_loss: -5.707272052764893, valid_loss: -3.3549256324768066\n",
      "-------epoch  366 -------\n",
      "epoch: 366, train_loss: -5.709234714508057, valid_loss: -3.397817850112915\n",
      "-------epoch  367 -------\n",
      "epoch: 367, train_loss: -5.698765754699707, valid_loss: -3.329258441925049\n",
      "-------epoch  368 -------\n",
      "epoch: 368, train_loss: -5.708703517913818, valid_loss: -3.3682916164398193\n",
      "-------epoch  369 -------\n",
      "epoch: 369, train_loss: -5.72551155090332, valid_loss: -3.3719441890716553\n",
      "-------epoch  370 -------\n",
      "epoch: 370, train_loss: -5.725802898406982, valid_loss: -3.314675807952881\n",
      "-------epoch  371 -------\n",
      "epoch: 371, train_loss: -5.719841003417969, valid_loss: -3.3718175888061523\n",
      "-------epoch  372 -------\n",
      "epoch: 372, train_loss: -5.725680351257324, valid_loss: -3.3284952640533447\n",
      "-------epoch  373 -------\n",
      "epoch: 373, train_loss: -5.739062309265137, valid_loss: -3.3350956439971924\n",
      "-------epoch  374 -------\n",
      "epoch: 374, train_loss: -5.745772361755371, valid_loss: -3.3521270751953125\n",
      "-------epoch  375 -------\n",
      "epoch: 375, train_loss: -5.743852615356445, valid_loss: -3.2972164154052734\n",
      "-------epoch  376 -------\n",
      "epoch: 376, train_loss: -5.742122173309326, valid_loss: -3.3481953144073486\n",
      "-------epoch  377 -------\n",
      "epoch: 377, train_loss: -5.74666166305542, valid_loss: -3.2997212409973145\n",
      "-------epoch  378 -------\n",
      "epoch: 378, train_loss: -5.7556939125061035, valid_loss: -3.3252639770507812\n",
      "-------epoch  379 -------\n",
      "epoch: 379, train_loss: -5.764035701751709, valid_loss: -3.316023349761963\n",
      "-------epoch  380 -------\n",
      "epoch: 380, train_loss: -5.7686591148376465, valid_loss: -3.294733762741089\n",
      "-------epoch  381 -------\n",
      "epoch: 381, train_loss: -5.769914150238037, valid_loss: -3.3237507343292236\n",
      "-------epoch  382 -------\n",
      "epoch: 382, train_loss: -5.769685745239258, valid_loss: -3.271806240081787\n",
      "-------epoch  383 -------\n",
      "epoch: 383, train_loss: -5.769545078277588, valid_loss: -3.3245432376861572\n",
      "-------epoch  384 -------\n",
      "epoch: 384, train_loss: -5.7700958251953125, valid_loss: -3.2586376667022705\n",
      "-------epoch  385 -------\n",
      "epoch: 385, train_loss: -5.771115303039551, valid_loss: -3.324528932571411\n",
      "-------epoch  386 -------\n",
      "epoch: 386, train_loss: -5.771989822387695, valid_loss: -3.2481601238250732\n",
      "-------epoch  387 -------\n",
      "epoch: 387, train_loss: -5.7718305587768555, valid_loss: -3.328481674194336\n",
      "-------epoch  388 -------\n",
      "epoch: 388, train_loss: -5.769547462463379, valid_loss: -3.2304954528808594\n",
      "-------epoch  389 -------\n",
      "epoch: 389, train_loss: -5.76352596282959, valid_loss: -3.335280179977417\n",
      "-------epoch  390 -------\n",
      "epoch: 390, train_loss: -5.751587867736816, valid_loss: -3.195319652557373\n",
      "-------epoch  391 -------\n",
      "epoch: 391, train_loss: -5.730783939361572, valid_loss: -3.342184066772461\n",
      "-------epoch  392 -------\n",
      "epoch: 392, train_loss: -5.699197769165039, valid_loss: -3.141301393508911\n",
      "-------epoch  393 -------\n",
      "epoch: 393, train_loss: -5.659623146057129, valid_loss: -3.3458621501922607\n",
      "-------epoch  394 -------\n",
      "epoch: 394, train_loss: -5.627418041229248, valid_loss: -3.134564161300659\n",
      "-------epoch  395 -------\n",
      "epoch: 395, train_loss: -5.632107257843018, valid_loss: -3.3731861114501953\n",
      "-------epoch  396 -------\n",
      "epoch: 396, train_loss: -5.691812515258789, valid_loss: -3.2733612060546875\n",
      "-------epoch  397 -------\n",
      "epoch: 397, train_loss: -5.779076099395752, valid_loss: -3.3735098838806152\n",
      "-------epoch  398 -------\n",
      "epoch: 398, train_loss: -5.83099889755249, valid_loss: -3.400552988052368\n",
      "-------epoch  399 -------\n",
      "epoch: 399, train_loss: -5.8166022300720215, valid_loss: -3.2897372245788574\n",
      "-------epoch  400 -------\n",
      "epoch: 400, train_loss: -5.776330471038818, valid_loss: -3.4246251583099365\n",
      "-------epoch  401 -------\n",
      "epoch: 401, train_loss: -5.775707244873047, valid_loss: -3.345123529434204\n",
      "-------epoch  402 -------\n",
      "epoch: 402, train_loss: -5.8197760581970215, valid_loss: -3.4062039852142334\n",
      "-------epoch  403 -------\n",
      "epoch: 403, train_loss: -5.84859037399292, valid_loss: -3.4378392696380615\n",
      "-------epoch  404 -------\n",
      "epoch: 404, train_loss: -5.83287239074707, valid_loss: -3.3478989601135254\n",
      "-------epoch  405 -------\n",
      "epoch: 405, train_loss: -5.815009117126465, valid_loss: -3.451446771621704\n",
      "-------epoch  406 -------\n",
      "epoch: 406, train_loss: -5.833026885986328, valid_loss: -3.416579246520996\n",
      "-------epoch  407 -------\n",
      "epoch: 407, train_loss: -5.860097885131836, valid_loss: -3.4094927310943604\n",
      "-------epoch  408 -------\n",
      "epoch: 408, train_loss: -5.858881950378418, valid_loss: -3.4647982120513916\n",
      "-------epoch  409 -------\n",
      "epoch: 409, train_loss: -5.845322608947754, valid_loss: -3.3945722579956055\n",
      "-------epoch  410 -------\n",
      "epoch: 410, train_loss: -5.852640628814697, valid_loss: -3.455500602722168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  411 -------\n",
      "epoch: 411, train_loss: -5.872862339019775, valid_loss: -3.4537384510040283\n",
      "-------epoch  412 -------\n",
      "epoch: 412, train_loss: -5.87814998626709, valid_loss: -3.408226251602173\n",
      "-------epoch  413 -------\n",
      "epoch: 413, train_loss: -5.870309829711914, valid_loss: -3.4683399200439453\n",
      "-------epoch  414 -------\n",
      "epoch: 414, train_loss: -5.871685028076172, valid_loss: -3.4211745262145996\n",
      "-------epoch  415 -------\n",
      "epoch: 415, train_loss: -5.885590076446533, valid_loss: -3.4416251182556152\n",
      "-------epoch  416 -------\n",
      "epoch: 416, train_loss: -5.895327568054199, valid_loss: -3.456815004348755\n",
      "-------epoch  417 -------\n",
      "epoch: 417, train_loss: -5.89385986328125, valid_loss: -3.406968832015991\n",
      "-------epoch  418 -------\n",
      "epoch: 418, train_loss: -5.891646862030029, valid_loss: -3.4581456184387207\n",
      "-------epoch  419 -------\n",
      "epoch: 419, train_loss: -5.897824764251709, valid_loss: -3.422830581665039\n",
      "-------epoch  420 -------\n",
      "epoch: 420, train_loss: -5.908413410186768, valid_loss: -3.4329111576080322\n",
      "-------epoch  421 -------\n",
      "epoch: 421, train_loss: -5.914741516113281, valid_loss: -3.447279453277588\n",
      "-------epoch  422 -------\n",
      "epoch: 422, train_loss: -5.91519832611084, valid_loss: -3.4067211151123047\n",
      "-------epoch  423 -------\n",
      "epoch: 423, train_loss: -5.91506290435791, valid_loss: -3.4497427940368652\n",
      "-------epoch  424 -------\n",
      "epoch: 424, train_loss: -5.918893337249756, valid_loss: -3.410776138305664\n",
      "-------epoch  425 -------\n",
      "epoch: 425, train_loss: -5.926218509674072, valid_loss: -3.434947967529297\n",
      "-------epoch  426 -------\n",
      "epoch: 426, train_loss: -5.933534622192383, valid_loss: -3.42806339263916\n",
      "-------epoch  427 -------\n",
      "epoch: 427, train_loss: -5.938281536102295, valid_loss: -3.41263484954834\n",
      "-------epoch  428 -------\n",
      "epoch: 428, train_loss: -5.940521240234375, valid_loss: -3.4372315406799316\n",
      "-------epoch  429 -------\n",
      "epoch: 429, train_loss: -5.9419426918029785, valid_loss: -3.3983898162841797\n",
      "-------epoch  430 -------\n",
      "epoch: 430, train_loss: -5.944157600402832, valid_loss: -3.437077760696411\n",
      "-------epoch  431 -------\n",
      "epoch: 431, train_loss: -5.947811126708984, valid_loss: -3.3974950313568115\n",
      "-------epoch  432 -------\n",
      "epoch: 432, train_loss: -5.952663421630859, valid_loss: -3.432101249694824\n",
      "-------epoch  433 -------\n",
      "epoch: 433, train_loss: -5.9580888748168945, valid_loss: -3.404228925704956\n",
      "-------epoch  434 -------\n",
      "epoch: 434, train_loss: -5.963512420654297, valid_loss: -3.4257454872131348\n",
      "-------epoch  435 -------\n",
      "epoch: 435, train_loss: -5.968608856201172, valid_loss: -3.4121780395507812\n",
      "-------epoch  436 -------\n",
      "epoch: 436, train_loss: -5.973282814025879, valid_loss: -3.4198408126831055\n",
      "-------epoch  437 -------\n",
      "epoch: 437, train_loss: -5.977581024169922, valid_loss: -3.4189040660858154\n",
      "-------epoch  438 -------\n",
      "epoch: 438, train_loss: -5.981599807739258, valid_loss: -3.4151694774627686\n",
      "-------epoch  439 -------\n",
      "epoch: 439, train_loss: -5.985415458679199, valid_loss: -3.424833059310913\n",
      "-------epoch  440 -------\n",
      "epoch: 440, train_loss: -5.9890570640563965, valid_loss: -3.4108269214630127\n",
      "-------epoch  441 -------\n",
      "epoch: 441, train_loss: -5.992493629455566, valid_loss: -3.4314348697662354\n",
      "-------epoch  442 -------\n",
      "epoch: 442, train_loss: -5.995600700378418, valid_loss: -3.4048004150390625\n",
      "-------epoch  443 -------\n",
      "epoch: 443, train_loss: -5.998110294342041, valid_loss: -3.4410641193389893\n",
      "-------epoch  444 -------\n",
      "epoch: 444, train_loss: -5.9994916915893555, valid_loss: -3.392695665359497\n",
      "-------epoch  445 -------\n",
      "epoch: 445, train_loss: -5.998722553253174, valid_loss: -3.4557433128356934\n",
      "-------epoch  446 -------\n",
      "epoch: 446, train_loss: -5.993829727172852, valid_loss: -3.3641483783721924\n",
      "-------epoch  447 -------\n",
      "epoch: 447, train_loss: -5.981121063232422, valid_loss: -3.4742252826690674\n",
      "-------epoch  448 -------\n",
      "epoch: 448, train_loss: -5.954150676727295, valid_loss: -3.2975268363952637\n",
      "-------epoch  449 -------\n",
      "epoch: 449, train_loss: -5.9039692878723145, valid_loss: -3.481393575668335\n",
      "-------epoch  450 -------\n",
      "epoch: 450, train_loss: -5.824862957000732, valid_loss: -3.200411558151245\n",
      "-------epoch  451 -------\n",
      "epoch: 451, train_loss: -5.739774227142334, valid_loss: -3.484440803527832\n",
      "-------epoch  452 -------\n",
      "epoch: 452, train_loss: -5.718625068664551, valid_loss: -3.2866387367248535\n",
      "-------epoch  453 -------\n",
      "epoch: 453, train_loss: -5.83016300201416, valid_loss: -3.547112464904785\n",
      "-------epoch  454 -------\n",
      "epoch: 454, train_loss: -5.992276191711426, valid_loss: -3.541454792022705\n",
      "-------epoch  455 -------\n",
      "epoch: 455, train_loss: -6.045263290405273, valid_loss: -3.4337961673736572\n",
      "-------epoch  456 -------\n",
      "epoch: 456, train_loss: -5.969398498535156, valid_loss: -3.588061809539795\n",
      "-------epoch  457 -------\n",
      "epoch: 457, train_loss: -5.933144569396973, valid_loss: -3.495185375213623\n",
      "-------epoch  458 -------\n",
      "epoch: 458, train_loss: -6.015649795532227, valid_loss: -3.5675315856933594\n",
      "-------epoch  459 -------\n",
      "epoch: 459, train_loss: -6.060179233551025, valid_loss: -3.6236648559570312\n",
      "-------epoch  460 -------\n",
      "epoch: 460, train_loss: -6.008121013641357, valid_loss: -3.507319927215576\n",
      "-------epoch  461 -------\n",
      "epoch: 461, train_loss: -6.00775671005249, valid_loss: -3.623983144760132\n",
      "-------epoch  462 -------\n",
      "epoch: 462, train_loss: -6.066872596740723, valid_loss: -3.6422269344329834\n",
      "-------epoch  463 -------\n",
      "epoch: 463, train_loss: -6.057827472686768, valid_loss: -3.546820878982544\n",
      "-------epoch  464 -------\n",
      "epoch: 464, train_loss: -6.032036781311035, valid_loss: -3.6553187370300293\n",
      "-------epoch  465 -------\n",
      "epoch: 465, train_loss: -6.069321632385254, valid_loss: -3.651580333709717\n",
      "-------epoch  466 -------\n",
      "epoch: 466, train_loss: -6.083644390106201, valid_loss: -3.581712007522583\n",
      "-------epoch  467 -------\n",
      "epoch: 467, train_loss: -6.059936046600342, valid_loss: -3.6699979305267334\n",
      "-------epoch  468 -------\n",
      "epoch: 468, train_loss: -6.077645301818848, valid_loss: -3.650686740875244\n",
      "-------epoch  469 -------\n",
      "epoch: 469, train_loss: -6.10038423538208, valid_loss: -3.6039509773254395\n",
      "-------epoch  470 -------\n",
      "epoch: 470, train_loss: -6.086064338684082, valid_loss: -3.6732401847839355\n",
      "-------epoch  471 -------\n",
      "epoch: 471, train_loss: -6.089809417724609, valid_loss: -3.636873722076416\n",
      "-------epoch  472 -------\n",
      "epoch: 472, train_loss: -6.113093376159668, valid_loss: -3.6165592670440674\n",
      "-------epoch  473 -------\n",
      "epoch: 473, train_loss: -6.110602378845215, valid_loss: -3.666341543197632\n",
      "-------epoch  474 -------\n",
      "epoch: 474, train_loss: -6.105570316314697, valid_loss: -3.6166059970855713\n",
      "-------epoch  475 -------\n",
      "epoch: 475, train_loss: -6.122602462768555, valid_loss: -3.626781463623047\n",
      "-------epoch  476 -------\n",
      "epoch: 476, train_loss: -6.131935119628906, valid_loss: -3.6538949012756348\n",
      "-------epoch  477 -------\n",
      "epoch: 477, train_loss: -6.126321792602539, valid_loss: -3.597404956817627\n",
      "-------epoch  478 -------\n",
      "epoch: 478, train_loss: -6.131640911102295, valid_loss: -3.6345646381378174\n",
      "-------epoch  479 -------\n",
      "epoch: 479, train_loss: -6.145956516265869, valid_loss: -3.6345536708831787\n",
      "-------epoch  480 -------\n",
      "epoch: 480, train_loss: -6.149681568145752, valid_loss: -3.5911712646484375\n",
      "-------epoch  481 -------\n",
      "epoch: 481, train_loss: -6.147474765777588, valid_loss: -3.6360819339752197\n",
      "-------epoch  482 -------\n",
      "epoch: 482, train_loss: -6.153899192810059, valid_loss: -3.6045141220092773\n",
      "-------epoch  483 -------\n",
      "epoch: 483, train_loss: -6.1650919914245605, valid_loss: -3.60109543800354\n",
      "-------epoch  484 -------\n",
      "epoch: 484, train_loss: -6.1698102951049805, valid_loss: -3.624013662338257\n",
      "-------epoch  485 -------\n",
      "epoch: 485, train_loss: -6.169675827026367, valid_loss: -3.5784623622894287\n",
      "-------epoch  486 -------\n",
      "epoch: 486, train_loss: -6.173356056213379, valid_loss: -3.61287260055542\n",
      "-------epoch  487 -------\n",
      "epoch: 487, train_loss: -6.181933403015137, valid_loss: -3.5945138931274414\n",
      "-------epoch  488 -------\n",
      "epoch: 488, train_loss: -6.189502716064453, valid_loss: -3.583073377609253\n",
      "-------epoch  489 -------\n",
      "epoch: 489, train_loss: -6.192789077758789, valid_loss: -3.6069884300231934\n",
      "-------epoch  490 -------\n",
      "epoch: 490, train_loss: -6.194483757019043, valid_loss: -3.5682268142700195\n",
      "-------epoch  491 -------\n",
      "epoch: 491, train_loss: -6.198423862457275, valid_loss: -3.601667642593384\n",
      "-------epoch  492 -------\n",
      "epoch: 492, train_loss: -6.205079555511475, valid_loss: -3.5795507431030273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  493 -------\n",
      "epoch: 493, train_loss: -6.212125301361084, valid_loss: -3.584423780441284\n",
      "-------epoch  494 -------\n",
      "epoch: 494, train_loss: -6.217439651489258, valid_loss: -3.5948171615600586\n",
      "-------epoch  495 -------\n",
      "epoch: 495, train_loss: -6.220948696136475, valid_loss: -3.5702457427978516\n",
      "-------epoch  496 -------\n",
      "epoch: 496, train_loss: -6.224039077758789, valid_loss: -3.600780487060547\n",
      "-------epoch  497 -------\n",
      "epoch: 497, train_loss: -6.228037357330322, valid_loss: -3.570103645324707\n",
      "-------epoch  498 -------\n",
      "epoch: 498, train_loss: -6.233323574066162, valid_loss: -3.5971755981445312\n",
      "-------epoch  499 -------\n",
      "epoch: 499, train_loss: -6.23933744430542, valid_loss: -3.5808117389678955\n",
      "-------epoch  500 -------\n",
      "epoch: 500, train_loss: -6.245259761810303, valid_loss: -3.5889692306518555\n",
      "-------epoch  501 -------\n",
      "epoch: 501, train_loss: -6.250539779663086, valid_loss: -3.5935440063476562\n",
      "-------epoch  502 -------\n",
      "epoch: 502, train_loss: -6.255107402801514, valid_loss: -3.581990957260132\n",
      "-------epoch  503 -------\n",
      "epoch: 503, train_loss: -6.259265899658203, valid_loss: -3.602965831756592\n",
      "-------epoch  504 -------\n",
      "epoch: 504, train_loss: -6.263420581817627, valid_loss: -3.5811500549316406\n",
      "-------epoch  505 -------\n",
      "epoch: 505, train_loss: -6.267887115478516, valid_loss: -3.6086623668670654\n",
      "-------epoch  506 -------\n",
      "epoch: 506, train_loss: -6.272778511047363, valid_loss: -3.5874440670013428\n",
      "-------epoch  507 -------\n",
      "epoch: 507, train_loss: -6.278043270111084, valid_loss: -3.6111879348754883\n",
      "-------epoch  508 -------\n",
      "epoch: 508, train_loss: -6.283504486083984, valid_loss: -3.5981314182281494\n",
      "-------epoch  509 -------\n",
      "epoch: 509, train_loss: -6.288982391357422, valid_loss: -3.6117379665374756\n",
      "-------epoch  510 -------\n",
      "epoch: 510, train_loss: -6.294334888458252, valid_loss: -3.610124349594116\n",
      "-------epoch  511 -------\n",
      "epoch: 511, train_loss: -6.299499988555908, valid_loss: -3.612118721008301\n",
      "-------epoch  512 -------\n",
      "epoch: 512, train_loss: -6.304486274719238, valid_loss: -3.621090888977051\n",
      "-------epoch  513 -------\n",
      "epoch: 513, train_loss: -6.309359073638916, valid_loss: -3.613830804824829\n",
      "-------epoch  514 -------\n",
      "epoch: 514, train_loss: -6.314203262329102, valid_loss: -3.630180835723877\n",
      "-------epoch  515 -------\n",
      "epoch: 515, train_loss: -6.319097518920898, valid_loss: -3.6182758808135986\n",
      "-------epoch  516 -------\n",
      "epoch: 516, train_loss: -6.3240966796875, valid_loss: -3.637547016143799\n",
      "-------epoch  517 -------\n",
      "epoch: 517, train_loss: -6.329218864440918, valid_loss: -3.625548839569092\n",
      "-------epoch  518 -------\n",
      "epoch: 518, train_loss: -6.3344573974609375, valid_loss: -3.643303394317627\n",
      "-------epoch  519 -------\n",
      "epoch: 519, train_loss: -6.339779853820801, valid_loss: -3.6349239349365234\n",
      "-------epoch  520 -------\n",
      "epoch: 520, train_loss: -6.345148086547852, valid_loss: -3.6480472087860107\n",
      "-------epoch  521 -------\n",
      "epoch: 521, train_loss: -6.350523471832275, valid_loss: -3.6452600955963135\n",
      "-------epoch  522 -------\n",
      "epoch: 522, train_loss: -6.3558807373046875, valid_loss: -3.6521971225738525\n",
      "-------epoch  523 -------\n",
      "epoch: 523, train_loss: -6.361204147338867, valid_loss: -3.655327796936035\n",
      "-------epoch  524 -------\n",
      "epoch: 524, train_loss: -6.366497039794922, valid_loss: -3.656468391418457\n",
      "-------epoch  525 -------\n",
      "epoch: 525, train_loss: -6.3717732429504395, valid_loss: -3.6646180152893066\n",
      "-------epoch  526 -------\n",
      "epoch: 526, train_loss: -6.377051830291748, valid_loss: -3.6615138053894043\n",
      "-------epoch  527 -------\n",
      "epoch: 527, train_loss: -6.382348537445068, valid_loss: -3.6728179454803467\n",
      "-------epoch  528 -------\n",
      "epoch: 528, train_loss: -6.3876824378967285, valid_loss: -3.6676597595214844\n",
      "-------epoch  529 -------\n",
      "epoch: 529, train_loss: -6.393060684204102, valid_loss: -3.6800777912139893\n",
      "-------epoch  530 -------\n",
      "epoch: 530, train_loss: -6.398484230041504, valid_loss: -3.675002336502075\n",
      "-------epoch  531 -------\n",
      "epoch: 531, train_loss: -6.403951644897461, valid_loss: -3.6865789890289307\n",
      "-------epoch  532 -------\n",
      "epoch: 532, train_loss: -6.4094557762146, valid_loss: -3.683119535446167\n",
      "-------epoch  533 -------\n",
      "epoch: 533, train_loss: -6.414989948272705, valid_loss: -3.692495107650757\n",
      "-------epoch  534 -------\n",
      "epoch: 534, train_loss: -6.420542240142822, valid_loss: -3.691601514816284\n",
      "-------epoch  535 -------\n",
      "epoch: 535, train_loss: -6.426113128662109, valid_loss: -3.6981570720672607\n",
      "-------epoch  536 -------\n",
      "epoch: 536, train_loss: -6.431694507598877, valid_loss: -3.7000315189361572\n",
      "-------epoch  537 -------\n",
      "epoch: 537, train_loss: -6.4372878074646, valid_loss: -3.7037620544433594\n",
      "-------epoch  538 -------\n",
      "epoch: 538, train_loss: -6.442892074584961, valid_loss: -3.7081873416900635\n",
      "-------epoch  539 -------\n",
      "epoch: 539, train_loss: -6.448510646820068, valid_loss: -3.7096433639526367\n",
      "-------epoch  540 -------\n",
      "epoch: 540, train_loss: -6.454145908355713, valid_loss: -3.7160611152648926\n",
      "-------epoch  541 -------\n",
      "epoch: 541, train_loss: -6.459804058074951, valid_loss: -3.7159101963043213\n",
      "-------epoch  542 -------\n",
      "epoch: 542, train_loss: -6.465482234954834, valid_loss: -3.723651647567749\n",
      "-------epoch  543 -------\n",
      "epoch: 543, train_loss: -6.471189022064209, valid_loss: -3.72261905670166\n",
      "-------epoch  544 -------\n",
      "epoch: 544, train_loss: -6.476916790008545, valid_loss: -3.7310547828674316\n",
      "-------epoch  545 -------\n",
      "epoch: 545, train_loss: -6.482668399810791, valid_loss: -3.729687452316284\n",
      "-------epoch  546 -------\n",
      "epoch: 546, train_loss: -6.488446235656738, valid_loss: -3.7382619380950928\n",
      "-------epoch  547 -------\n",
      "epoch: 547, train_loss: -6.494248390197754, valid_loss: -3.7369883060455322\n",
      "-------epoch  548 -------\n",
      "epoch: 548, train_loss: -6.500073432922363, valid_loss: -3.745359182357788\n",
      "-------epoch  549 -------\n",
      "epoch: 549, train_loss: -6.505920886993408, valid_loss: -3.7444443702697754\n",
      "-------epoch  550 -------\n",
      "epoch: 550, train_loss: -6.511790752410889, valid_loss: -3.7524077892303467\n",
      "-------epoch  551 -------\n",
      "epoch: 551, train_loss: -6.517681121826172, valid_loss: -3.7519640922546387\n",
      "-------epoch  552 -------\n",
      "epoch: 552, train_loss: -6.523592472076416, valid_loss: -3.7595109939575195\n",
      "-------epoch  553 -------\n",
      "epoch: 553, train_loss: -6.5295233726501465, valid_loss: -3.7595813274383545\n",
      "-------epoch  554 -------\n",
      "epoch: 554, train_loss: -6.535475254058838, valid_loss: -3.7667407989501953\n",
      "-------epoch  555 -------\n",
      "epoch: 555, train_loss: -6.541445732116699, valid_loss: -3.7672595977783203\n",
      "-------epoch  556 -------\n",
      "epoch: 556, train_loss: -6.547436714172363, valid_loss: -3.7741124629974365\n",
      "-------epoch  557 -------\n",
      "epoch: 557, train_loss: -6.553445816040039, valid_loss: -3.7749905586242676\n",
      "-------epoch  558 -------\n",
      "epoch: 558, train_loss: -6.559475421905518, valid_loss: -3.7816436290740967\n",
      "-------epoch  559 -------\n",
      "epoch: 559, train_loss: -6.565523624420166, valid_loss: -3.782731533050537\n",
      "-------epoch  560 -------\n",
      "epoch: 560, train_loss: -6.571589946746826, valid_loss: -3.789299964904785\n",
      "-------epoch  561 -------\n",
      "epoch: 561, train_loss: -6.577673435211182, valid_loss: -3.790436267852783\n",
      "-------epoch  562 -------\n",
      "epoch: 562, train_loss: -6.583775043487549, valid_loss: -3.7971019744873047\n",
      "-------epoch  563 -------\n",
      "epoch: 563, train_loss: -6.589894771575928, valid_loss: -3.7981183528900146\n",
      "-------epoch  564 -------\n",
      "epoch: 564, train_loss: -6.596032619476318, valid_loss: -3.805061101913452\n",
      "-------epoch  565 -------\n",
      "epoch: 565, train_loss: -6.6021857261657715, valid_loss: -3.805767059326172\n",
      "-------epoch  566 -------\n",
      "epoch: 566, train_loss: -6.608355522155762, valid_loss: -3.813206672668457\n",
      "-------epoch  567 -------\n",
      "epoch: 567, train_loss: -6.614541053771973, valid_loss: -3.813389301300049\n",
      "-------epoch  568 -------\n",
      "epoch: 568, train_loss: -6.6207404136657715, valid_loss: -3.821587324142456\n",
      "-------epoch  569 -------\n",
      "epoch: 569, train_loss: -6.626953125, valid_loss: -3.8209147453308105\n",
      "-------epoch  570 -------\n",
      "epoch: 570, train_loss: -6.633179187774658, valid_loss: -3.8302199840545654\n",
      "-------epoch  571 -------\n",
      "epoch: 571, train_loss: -6.639414310455322, valid_loss: -3.828296661376953\n",
      "-------epoch  572 -------\n",
      "epoch: 572, train_loss: -6.645661354064941, valid_loss: -3.839160919189453\n",
      "-------epoch  573 -------\n",
      "epoch: 573, train_loss: -6.651913166046143, valid_loss: -3.835418224334717\n",
      "-------epoch  574 -------\n",
      "epoch: 574, train_loss: -6.658169269561768, valid_loss: -3.8484880924224854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  575 -------\n",
      "epoch: 575, train_loss: -6.664422988891602, valid_loss: -3.8421478271484375\n",
      "-------epoch  576 -------\n",
      "epoch: 576, train_loss: -6.670668125152588, valid_loss: -3.8583669662475586\n",
      "-------epoch  577 -------\n",
      "epoch: 577, train_loss: -6.676896095275879, valid_loss: -3.848278284072876\n",
      "-------epoch  578 -------\n",
      "epoch: 578, train_loss: -6.683094501495361, valid_loss: -3.869016170501709\n",
      "-------epoch  579 -------\n",
      "epoch: 579, train_loss: -6.689243793487549, valid_loss: -3.8534984588623047\n",
      "-------epoch  580 -------\n",
      "epoch: 580, train_loss: -6.6953206062316895, valid_loss: -3.8807573318481445\n",
      "-------epoch  581 -------\n",
      "epoch: 581, train_loss: -6.701282024383545, valid_loss: -3.857330799102783\n",
      "-------epoch  582 -------\n",
      "epoch: 582, train_loss: -6.707083702087402, valid_loss: -3.8940248489379883\n",
      "-------epoch  583 -------\n",
      "epoch: 583, train_loss: -6.712627410888672, valid_loss: -3.8590385913848877\n",
      "-------epoch  584 -------\n",
      "epoch: 584, train_loss: -6.717833995819092, valid_loss: -3.909372568130493\n",
      "-------epoch  585 -------\n",
      "epoch: 585, train_loss: -6.72249698638916, valid_loss: -3.857621431350708\n",
      "-------epoch  586 -------\n",
      "epoch: 586, train_loss: -6.726484775543213, valid_loss: -3.9273183345794678\n",
      "-------epoch  587 -------\n",
      "epoch: 587, train_loss: -6.729402542114258, valid_loss: -3.8520700931549072\n",
      "-------epoch  588 -------\n",
      "epoch: 588, train_loss: -6.731146812438965, valid_loss: -3.9479212760925293\n",
      "-------epoch  589 -------\n",
      "epoch: 589, train_loss: -6.731100559234619, valid_loss: -3.842611789703369\n",
      "-------epoch  590 -------\n",
      "epoch: 590, train_loss: -6.729726314544678, valid_loss: -3.9699697494506836\n",
      "-------epoch  591 -------\n",
      "epoch: 591, train_loss: -6.726588726043701, valid_loss: -3.834484577178955\n",
      "-------epoch  592 -------\n",
      "epoch: 592, train_loss: -6.7242655754089355, valid_loss: -3.990311861038208\n",
      "-------epoch  593 -------\n",
      "epoch: 593, train_loss: -6.723575592041016, valid_loss: -3.8431427478790283\n",
      "-------epoch  594 -------\n",
      "epoch: 594, train_loss: -6.729950904846191, valid_loss: -4.003907680511475\n",
      "-------epoch  595 -------\n",
      "epoch: 595, train_loss: -6.743516445159912, valid_loss: -3.886807680130005\n",
      "-------epoch  596 -------\n",
      "epoch: 596, train_loss: -6.765523433685303, valid_loss: -4.001561164855957\n",
      "-------epoch  597 -------\n",
      "epoch: 597, train_loss: -6.78923225402832, valid_loss: -3.9555931091308594\n",
      "-------epoch  598 -------\n",
      "epoch: 598, train_loss: -6.808104991912842, valid_loss: -3.976280927658081\n",
      "-------epoch  599 -------\n",
      "epoch: 599, train_loss: -6.817131042480469, valid_loss: -4.011561393737793\n",
      "-------epoch  600 -------\n",
      "epoch: 600, train_loss: -6.817616939544678, valid_loss: -3.951186418533325\n",
      "-------epoch  601 -------\n",
      "epoch: 601, train_loss: -6.816067695617676, valid_loss: -4.036944389343262\n",
      "-------epoch  602 -------\n",
      "epoch: 602, train_loss: -6.819177627563477, valid_loss: -3.965097427368164\n",
      "-------epoch  603 -------\n",
      "epoch: 603, train_loss: -6.829594612121582, valid_loss: -4.034797668457031\n",
      "-------epoch  604 -------\n",
      "epoch: 604, train_loss: -6.843336582183838, valid_loss: -4.011266708374023\n",
      "-------epoch  605 -------\n",
      "epoch: 605, train_loss: -6.854650020599365, valid_loss: -4.013296604156494\n",
      "-------epoch  606 -------\n",
      "epoch: 606, train_loss: -6.8604512214660645, valid_loss: -4.049037456512451\n",
      "-------epoch  607 -------\n",
      "epoch: 607, train_loss: -6.86266565322876, valid_loss: -4.000001907348633\n",
      "-------epoch  608 -------\n",
      "epoch: 608, train_loss: -6.865769386291504, valid_loss: -4.061198711395264\n",
      "-------epoch  609 -------\n",
      "epoch: 609, train_loss: -6.8724870681762695, valid_loss: -4.016443729400635\n",
      "-------epoch  610 -------\n",
      "epoch: 610, train_loss: -6.882175445556641, valid_loss: -4.051481246948242\n",
      "-------epoch  611 -------\n",
      "epoch: 611, train_loss: -6.891726493835449, valid_loss: -4.046804428100586\n",
      "-------epoch  612 -------\n",
      "epoch: 612, train_loss: -6.898832321166992, valid_loss: -4.03327751159668\n",
      "-------epoch  613 -------\n",
      "epoch: 613, train_loss: -6.903500080108643, valid_loss: -4.067388534545898\n",
      "-------epoch  614 -------\n",
      "epoch: 614, train_loss: -6.907510757446289, valid_loss: -4.027021408081055\n",
      "-------epoch  615 -------\n",
      "epoch: 615, train_loss: -6.912725925445557, valid_loss: -4.072080135345459\n",
      "-------epoch  616 -------\n",
      "epoch: 616, train_loss: -6.919649124145508, valid_loss: -4.039569854736328\n",
      "-------epoch  617 -------\n",
      "epoch: 617, train_loss: -6.927581310272217, valid_loss: -4.064764499664307\n",
      "-------epoch  618 -------\n",
      "epoch: 618, train_loss: -6.935245037078857, valid_loss: -4.059656620025635\n",
      "-------epoch  619 -------\n",
      "epoch: 619, train_loss: -6.941854000091553, valid_loss: -4.053664207458496\n",
      "-------epoch  620 -------\n",
      "epoch: 620, train_loss: -6.947380542755127, valid_loss: -4.075146675109863\n",
      "-------epoch  621 -------\n",
      "epoch: 621, train_loss: -6.952373027801514, valid_loss: -4.047882080078125\n",
      "-------epoch  622 -------\n",
      "epoch: 622, train_loss: -6.957497596740723, valid_loss: -4.082309246063232\n",
      "-------epoch  623 -------\n",
      "epoch: 623, train_loss: -6.963141441345215, valid_loss: -4.051313877105713\n",
      "-------epoch  624 -------\n",
      "epoch: 624, train_loss: -6.96937370300293, valid_loss: -4.0827484130859375\n",
      "-------epoch  625 -------\n",
      "epoch: 625, train_loss: -6.975947380065918, valid_loss: -4.061526298522949\n",
      "-------epoch  626 -------\n",
      "epoch: 626, train_loss: -6.9825758934021, valid_loss: -4.079578399658203\n",
      "-------epoch  627 -------\n",
      "epoch: 627, train_loss: -6.988992214202881, valid_loss: -4.073878288269043\n",
      "-------epoch  628 -------\n",
      "epoch: 628, train_loss: -6.995081901550293, valid_loss: -4.075930118560791\n",
      "-------epoch  629 -------\n",
      "epoch: 629, train_loss: -7.0008416175842285, valid_loss: -4.085126876831055\n",
      "-------epoch  630 -------\n",
      "epoch: 630, train_loss: -7.006348133087158, valid_loss: -4.074027061462402\n",
      "-------epoch  631 -------\n",
      "epoch: 631, train_loss: -7.011711120605469, valid_loss: -4.094166278839111\n",
      "-------epoch  632 -------\n",
      "epoch: 632, train_loss: -7.017022132873535, valid_loss: -4.074806213378906\n",
      "-------epoch  633 -------\n",
      "epoch: 633, train_loss: -7.0223469734191895, valid_loss: -4.101266860961914\n",
      "-------epoch  634 -------\n",
      "epoch: 634, train_loss: -7.027717113494873, valid_loss: -4.078141689300537\n",
      "-------epoch  635 -------\n",
      "epoch: 635, train_loss: -7.033146381378174, valid_loss: -4.107149600982666\n",
      "-------epoch  636 -------\n",
      "epoch: 636, train_loss: -7.038610458374023, valid_loss: -4.0833353996276855\n",
      "-------epoch  637 -------\n",
      "epoch: 637, train_loss: -7.044108867645264, valid_loss: -4.11248254776001\n",
      "-------epoch  638 -------\n",
      "epoch: 638, train_loss: -7.049602031707764, valid_loss: -4.089594841003418\n",
      "-------epoch  639 -------\n",
      "epoch: 639, train_loss: -7.055091857910156, valid_loss: -4.117686748504639\n",
      "-------epoch  640 -------\n",
      "epoch: 640, train_loss: -7.060543537139893, valid_loss: -4.096249103546143\n",
      "-------epoch  641 -------\n",
      "epoch: 641, train_loss: -7.065962314605713, valid_loss: -4.12302827835083\n",
      "-------epoch  642 -------\n",
      "epoch: 642, train_loss: -7.071329116821289, valid_loss: -4.102889060974121\n",
      "-------epoch  643 -------\n",
      "epoch: 643, train_loss: -7.076649188995361, valid_loss: -4.1287150382995605\n",
      "-------epoch  644 -------\n",
      "epoch: 644, train_loss: -7.081905364990234, valid_loss: -4.109307289123535\n",
      "-------epoch  645 -------\n",
      "epoch: 645, train_loss: -7.087105751037598, valid_loss: -4.134920120239258\n",
      "-------epoch  646 -------\n",
      "epoch: 646, train_loss: -7.092241287231445, valid_loss: -4.115415096282959\n",
      "-------epoch  647 -------\n",
      "epoch: 647, train_loss: -7.097316265106201, valid_loss: -4.141723155975342\n",
      "-------epoch  648 -------\n",
      "epoch: 648, train_loss: -7.102316379547119, valid_loss: -4.1210527420043945\n",
      "-------epoch  649 -------\n",
      "epoch: 649, train_loss: -7.107248306274414, valid_loss: -4.149129390716553\n",
      "-------epoch  650 -------\n",
      "epoch: 650, train_loss: -7.112094402313232, valid_loss: -4.126026153564453\n",
      "-------epoch  651 -------\n",
      "epoch: 651, train_loss: -7.116856575012207, valid_loss: -4.157116413116455\n",
      "-------epoch  652 -------\n",
      "epoch: 652, train_loss: -7.121509075164795, valid_loss: -4.130098819732666\n",
      "-------epoch  653 -------\n",
      "epoch: 653, train_loss: -7.1260552406311035, valid_loss: -4.1657586097717285\n",
      "-------epoch  654 -------\n",
      "epoch: 654, train_loss: -7.130446434020996, valid_loss: -4.133068561553955\n",
      "-------epoch  655 -------\n",
      "epoch: 655, train_loss: -7.134685516357422, valid_loss: -4.175257205963135\n",
      "-------epoch  656 -------\n",
      "epoch: 656, train_loss: -7.138700008392334, valid_loss: -4.134718418121338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  657 -------\n",
      "epoch: 657, train_loss: -7.142485618591309, valid_loss: -4.1858649253845215\n",
      "-------epoch  658 -------\n",
      "epoch: 658, train_loss: -7.145923137664795, valid_loss: -4.1348161697387695\n",
      "-------epoch  659 -------\n",
      "epoch: 659, train_loss: -7.149027347564697, valid_loss: -4.197756290435791\n",
      "-------epoch  660 -------\n",
      "epoch: 660, train_loss: -7.151607990264893, valid_loss: -4.133220672607422\n",
      "-------epoch  661 -------\n",
      "epoch: 661, train_loss: -7.153753280639648, valid_loss: -4.210873603820801\n",
      "-------epoch  662 -------\n",
      "epoch: 662, train_loss: -7.1551995277404785, valid_loss: -4.130382061004639\n",
      "-------epoch  663 -------\n",
      "epoch: 663, train_loss: -7.156264781951904, valid_loss: -4.22470760345459\n",
      "-------epoch  664 -------\n",
      "epoch: 664, train_loss: -7.1566925048828125, valid_loss: -4.128321170806885\n",
      "-------epoch  665 -------\n",
      "epoch: 665, train_loss: -7.157364845275879, valid_loss: -4.23801851272583\n",
      "-------epoch  666 -------\n",
      "epoch: 666, train_loss: -7.158172130584717, valid_loss: -4.1316399574279785\n",
      "-------epoch  667 -------\n",
      "epoch: 667, train_loss: -7.1607666015625, valid_loss: -4.248673439025879\n",
      "-------epoch  668 -------\n",
      "epoch: 668, train_loss: -7.1650710105896, valid_loss: -4.146688938140869\n",
      "-------epoch  669 -------\n",
      "epoch: 669, train_loss: -7.172652721405029, valid_loss: -4.253509998321533\n",
      "-------epoch  670 -------\n",
      "epoch: 670, train_loss: -7.1824727058410645, valid_loss: -4.1760573387146\n",
      "-------epoch  671 -------\n",
      "epoch: 671, train_loss: -7.1942009925842285, valid_loss: -4.248993396759033\n",
      "-------epoch  672 -------\n",
      "epoch: 672, train_loss: -7.205571174621582, valid_loss: -4.212709903717041\n",
      "-------epoch  673 -------\n",
      "epoch: 673, train_loss: -7.215004920959473, valid_loss: -4.235153675079346\n",
      "-------epoch  674 -------\n",
      "epoch: 674, train_loss: -7.221320152282715, valid_loss: -4.244463920593262\n",
      "-------epoch  675 -------\n",
      "epoch: 675, train_loss: -7.224605560302734, valid_loss: -4.219721794128418\n",
      "-------epoch  676 -------\n",
      "epoch: 676, train_loss: -7.22596549987793, valid_loss: -4.264422416687012\n",
      "-------epoch  677 -------\n",
      "epoch: 677, train_loss: -7.226946830749512, valid_loss: -4.213510990142822\n",
      "-------epoch  678 -------\n",
      "epoch: 678, train_loss: -7.228982448577881, valid_loss: -4.2725348472595215\n",
      "-------epoch  679 -------\n",
      "epoch: 679, train_loss: -7.2326483726501465, valid_loss: -4.220905303955078\n",
      "-------epoch  680 -------\n",
      "epoch: 680, train_loss: -7.237973213195801, valid_loss: -4.271013259887695\n",
      "-------epoch  681 -------\n",
      "epoch: 681, train_loss: -7.2441301345825195, valid_loss: -4.237149238586426\n",
      "-------epoch  682 -------\n",
      "epoch: 682, train_loss: -7.250330924987793, valid_loss: -4.262816905975342\n",
      "-------epoch  683 -------\n",
      "epoch: 683, train_loss: -7.255836009979248, valid_loss: -4.254144191741943\n",
      "-------epoch  684 -------\n",
      "epoch: 684, train_loss: -7.260355472564697, valid_loss: -4.252191543579102\n",
      "-------epoch  685 -------\n",
      "epoch: 685, train_loss: -7.263947486877441, valid_loss: -4.266881465911865\n",
      "-------epoch  686 -------\n",
      "epoch: 686, train_loss: -7.266898155212402, valid_loss: -4.243099689483643\n",
      "-------epoch  687 -------\n",
      "epoch: 687, train_loss: -7.269537448883057, valid_loss: -4.274684906005859\n",
      "-------epoch  688 -------\n",
      "epoch: 688, train_loss: -7.272115707397461, valid_loss: -4.237166404724121\n",
      "-------epoch  689 -------\n",
      "epoch: 689, train_loss: -7.2748003005981445, valid_loss: -4.2792229652404785\n",
      "-------epoch  690 -------\n",
      "epoch: 690, train_loss: -7.277568340301514, valid_loss: -4.233625411987305\n",
      "-------epoch  691 -------\n",
      "epoch: 691, train_loss: -7.2804274559021, valid_loss: -4.282745361328125\n",
      "-------epoch  692 -------\n",
      "epoch: 692, train_loss: -7.283183574676514, valid_loss: -4.230588436126709\n",
      "-------epoch  693 -------\n",
      "epoch: 693, train_loss: -7.285792350769043, valid_loss: -4.287346363067627\n",
      "-------epoch  694 -------\n",
      "epoch: 694, train_loss: -7.287935733795166, valid_loss: -4.225770473480225\n",
      "-------epoch  695 -------\n",
      "epoch: 695, train_loss: -7.289505481719971, valid_loss: -4.2947516441345215\n",
      "-------epoch  696 -------\n",
      "epoch: 696, train_loss: -7.289933204650879, valid_loss: -4.216538429260254\n",
      "-------epoch  697 -------\n",
      "epoch: 697, train_loss: -7.288971424102783, valid_loss: -4.306364059448242\n",
      "-------epoch  698 -------\n",
      "epoch: 698, train_loss: -7.2854228019714355, valid_loss: -4.200101375579834\n",
      "-------epoch  699 -------\n",
      "epoch: 699, train_loss: -7.2789225578308105, valid_loss: -4.322655200958252\n",
      "-------epoch  700 -------\n",
      "epoch: 700, train_loss: -7.267317295074463, valid_loss: -4.175881385803223\n",
      "-------epoch  701 -------\n",
      "epoch: 701, train_loss: -7.251737117767334, valid_loss: -4.341195583343506\n",
      "-------epoch  702 -------\n",
      "epoch: 702, train_loss: -7.230936050415039, valid_loss: -4.155986309051514\n",
      "-------epoch  703 -------\n",
      "epoch: 703, train_loss: -7.214824199676514, valid_loss: -4.357835292816162\n",
      "-------epoch  704 -------\n",
      "epoch: 704, train_loss: -7.208410263061523, valid_loss: -4.180138111114502\n",
      "-------epoch  705 -------\n",
      "epoch: 705, train_loss: -7.229448318481445, valid_loss: -4.367629051208496\n",
      "-------epoch  706 -------\n",
      "epoch: 706, train_loss: -7.270409107208252, valid_loss: -4.271329402923584\n",
      "-------epoch  707 -------\n",
      "epoch: 707, train_loss: -7.317347526550293, valid_loss: -4.339412212371826\n",
      "-------epoch  708 -------\n",
      "epoch: 708, train_loss: -7.343266010284424, valid_loss: -4.361507892608643\n",
      "-------epoch  709 -------\n",
      "epoch: 709, train_loss: -7.339312553405762, valid_loss: -4.286563396453857\n",
      "-------epoch  710 -------\n",
      "epoch: 710, train_loss: -7.320338726043701, valid_loss: -4.394998550415039\n",
      "-------epoch  711 -------\n",
      "epoch: 711, train_loss: -7.3115105628967285, valid_loss: -4.297478199005127\n",
      "-------epoch  712 -------\n",
      "epoch: 712, train_loss: -7.326199054718018, valid_loss: -4.384578227996826\n",
      "-------epoch  713 -------\n",
      "epoch: 713, train_loss: -7.3501386642456055, valid_loss: -4.365067005157471\n",
      "-------epoch  714 -------\n",
      "epoch: 714, train_loss: -7.362380504608154, valid_loss: -4.339655876159668\n",
      "-------epoch  715 -------\n",
      "epoch: 715, train_loss: -7.357184410095215, valid_loss: -4.404277801513672\n",
      "-------epoch  716 -------\n",
      "epoch: 716, train_loss: -7.348219394683838, valid_loss: -4.326642990112305\n",
      "-------epoch  717 -------\n",
      "epoch: 717, train_loss: -7.350855350494385, valid_loss: -4.3997650146484375\n",
      "-------epoch  718 -------\n",
      "epoch: 718, train_loss: -7.364388942718506, valid_loss: -4.3660502433776855\n",
      "-------epoch  719 -------\n",
      "epoch: 719, train_loss: -7.377013683319092, valid_loss: -4.365472793579102\n",
      "-------epoch  720 -------\n",
      "epoch: 720, train_loss: -7.379917621612549, valid_loss: -4.399354457855225\n",
      "-------epoch  721 -------\n",
      "epoch: 721, train_loss: -7.3760151863098145, valid_loss: -4.3395280838012695\n",
      "-------epoch  722 -------\n",
      "epoch: 722, train_loss: -7.374166011810303, valid_loss: -4.400666236877441\n",
      "-------epoch  723 -------\n",
      "epoch: 723, train_loss: -7.379005432128906, valid_loss: -4.349593162536621\n",
      "-------epoch  724 -------\n",
      "epoch: 724, train_loss: -7.388418674468994, valid_loss: -4.378897666931152\n",
      "-------epoch  725 -------\n",
      "epoch: 725, train_loss: -7.3967156410217285, valid_loss: -4.372786045074463\n",
      "-------epoch  726 -------\n",
      "epoch: 726, train_loss: -7.400698661804199, valid_loss: -4.34942102432251\n",
      "-------epoch  727 -------\n",
      "epoch: 727, train_loss: -7.400991916656494, valid_loss: -4.38515043258667\n",
      "-------epoch  728 -------\n",
      "epoch: 728, train_loss: -7.400269031524658, valid_loss: -4.331601619720459\n",
      "-------epoch  729 -------\n",
      "epoch: 729, train_loss: -7.400949954986572, valid_loss: -4.383897304534912\n",
      "-------epoch  730 -------\n",
      "epoch: 730, train_loss: -7.403736114501953, valid_loss: -4.328263282775879\n",
      "-------epoch  731 -------\n",
      "epoch: 731, train_loss: -7.408382892608643, valid_loss: -4.374773979187012\n",
      "-------epoch  732 -------\n",
      "epoch: 732, train_loss: -7.413771629333496, valid_loss: -4.331687927246094\n",
      "-------epoch  733 -------\n",
      "epoch: 733, train_loss: -7.419219017028809, valid_loss: -4.363051414489746\n",
      "-------epoch  734 -------\n",
      "epoch: 734, train_loss: -7.424198627471924, valid_loss: -4.334781169891357\n",
      "-------epoch  735 -------\n",
      "epoch: 735, train_loss: -7.428654670715332, valid_loss: -4.35298490524292\n",
      "-------epoch  736 -------\n",
      "epoch: 736, train_loss: -7.432641506195068, valid_loss: -4.335265159606934\n",
      "-------epoch  737 -------\n",
      "epoch: 737, train_loss: -7.436309337615967, valid_loss: -4.345649242401123\n",
      "-------epoch  738 -------\n",
      "epoch: 738, train_loss: -7.439761161804199, valid_loss: -4.332735538482666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  739 -------\n",
      "epoch: 739, train_loss: -7.443084239959717, valid_loss: -4.341483116149902\n",
      "-------epoch  740 -------\n",
      "epoch: 740, train_loss: -7.446296215057373, valid_loss: -4.3276190757751465\n",
      "-------epoch  741 -------\n",
      "epoch: 741, train_loss: -7.449368000030518, valid_loss: -4.341022968292236\n",
      "-------epoch  742 -------\n",
      "epoch: 742, train_loss: -7.452160835266113, valid_loss: -4.318623065948486\n",
      "-------epoch  743 -------\n",
      "epoch: 743, train_loss: -7.454329490661621, valid_loss: -4.347667217254639\n",
      "-------epoch  744 -------\n",
      "epoch: 744, train_loss: -7.4549336433410645, valid_loss: -4.299567699432373\n",
      "-------epoch  745 -------\n",
      "epoch: 745, train_loss: -7.4515862464904785, valid_loss: -4.368175029754639\n",
      "-------epoch  746 -------\n",
      "epoch: 746, train_loss: -7.4376325607299805, valid_loss: -4.248164653778076\n",
      "-------epoch  747 -------\n",
      "epoch: 747, train_loss: -7.396434783935547, valid_loss: -4.402737140655518\n",
      "-------epoch  748 -------\n",
      "epoch: 748, train_loss: -7.28423547744751, valid_loss: -4.09266996383667\n",
      "-------epoch  749 -------\n",
      "epoch: 749, train_loss: -7.0425567626953125, valid_loss: -4.333179950714111\n",
      "-------epoch  750 -------\n",
      "epoch: 750, train_loss: -6.67924690246582, valid_loss: -3.9738547801971436\n",
      "-------epoch  751 -------\n",
      "epoch: 751, train_loss: -6.715297222137451, valid_loss: -4.49137544631958\n",
      "-------epoch  752 -------\n",
      "epoch: 752, train_loss: -7.198773384094238, valid_loss: -4.449670314788818\n",
      "-------epoch  753 -------\n",
      "epoch: 753, train_loss: -7.476110935211182, valid_loss: -4.328180313110352\n",
      "-------epoch  754 -------\n",
      "epoch: 754, train_loss: -7.220237731933594, valid_loss: -4.57041597366333\n",
      "-------epoch  755 -------\n",
      "epoch: 755, train_loss: -7.174916744232178, valid_loss: -4.511194705963135\n",
      "-------epoch  756 -------\n",
      "epoch: 756, train_loss: -7.465765953063965, valid_loss: -4.442156791687012\n",
      "-------epoch  757 -------\n",
      "epoch: 757, train_loss: -7.335514068603516, valid_loss: -4.621955394744873\n",
      "-------epoch  758 -------\n",
      "epoch: 758, train_loss: -7.298126220703125, valid_loss: -4.60811185836792\n",
      "-------epoch  759 -------\n",
      "epoch: 759, train_loss: -7.479277610778809, valid_loss: -4.516315460205078\n",
      "-------epoch  760 -------\n",
      "epoch: 760, train_loss: -7.335871696472168, valid_loss: -4.697253227233887\n",
      "-------epoch  761 -------\n",
      "epoch: 761, train_loss: -7.422358989715576, valid_loss: -4.70607328414917\n",
      "-------epoch  762 -------\n",
      "epoch: 762, train_loss: -7.444839954376221, valid_loss: -4.5762104988098145\n",
      "-------epoch  763 -------\n",
      "epoch: 763, train_loss: -7.377613067626953, valid_loss: -4.682281017303467\n",
      "-------epoch  764 -------\n",
      "epoch: 764, train_loss: -7.483851432800293, valid_loss: -4.714491844177246\n",
      "-------epoch  765 -------\n",
      "epoch: 765, train_loss: -7.402602195739746, valid_loss: -4.663112640380859\n",
      "-------epoch  766 -------\n",
      "epoch: 766, train_loss: -7.466444969177246, valid_loss: -4.653235912322998\n",
      "-------epoch  767 -------\n",
      "epoch: 767, train_loss: -7.4561848640441895, valid_loss: -4.743137836456299\n",
      "-------epoch  768 -------\n",
      "epoch: 768, train_loss: -7.441728115081787, valid_loss: -4.717507839202881\n",
      "-------epoch  769 -------\n",
      "epoch: 769, train_loss: -7.493911266326904, valid_loss: -4.64200496673584\n",
      "-------epoch  770 -------\n",
      "epoch: 770, train_loss: -7.4447503089904785, valid_loss: -4.712261199951172\n",
      "-------epoch  771 -------\n",
      "epoch: 771, train_loss: -7.4983015060424805, valid_loss: -4.728141784667969\n",
      "-------epoch  772 -------\n",
      "epoch: 772, train_loss: -7.47719144821167, valid_loss: -4.653048992156982\n",
      "-------epoch  773 -------\n",
      "epoch: 773, train_loss: -7.4849162101745605, valid_loss: -4.674343585968018\n",
      "-------epoch  774 -------\n",
      "epoch: 774, train_loss: -7.511196613311768, valid_loss: -4.704204559326172\n",
      "-------epoch  775 -------\n",
      "epoch: 775, train_loss: -7.485215187072754, valid_loss: -4.6525702476501465\n",
      "-------epoch  776 -------\n",
      "epoch: 776, train_loss: -7.519567012786865, valid_loss: -4.627922534942627\n",
      "-------epoch  777 -------\n",
      "epoch: 777, train_loss: -7.512325763702393, valid_loss: -4.673686981201172\n",
      "-------epoch  778 -------\n",
      "epoch: 778, train_loss: -7.510129451751709, valid_loss: -4.635748386383057\n",
      "-------epoch  779 -------\n",
      "epoch: 779, train_loss: -7.5366716384887695, valid_loss: -4.590757846832275\n",
      "-------epoch  780 -------\n",
      "epoch: 780, train_loss: -7.522545337677002, valid_loss: -4.63046407699585\n",
      "-------epoch  781 -------\n",
      "epoch: 781, train_loss: -7.531064987182617, valid_loss: -4.600377559661865\n",
      "-------epoch  782 -------\n",
      "epoch: 782, train_loss: -7.549440383911133, valid_loss: -4.558259963989258\n",
      "-------epoch  783 -------\n",
      "epoch: 783, train_loss: -7.538470268249512, valid_loss: -4.596439838409424\n",
      "-------epoch  784 -------\n",
      "epoch: 784, train_loss: -7.544852256774902, valid_loss: -4.560174465179443\n",
      "-------epoch  785 -------\n",
      "epoch: 785, train_loss: -7.562026023864746, valid_loss: -4.528712749481201\n",
      "-------epoch  786 -------\n",
      "epoch: 786, train_loss: -7.5569963455200195, valid_loss: -4.5577168464660645\n",
      "-------epoch  787 -------\n",
      "epoch: 787, train_loss: -7.555017471313477, valid_loss: -4.513311862945557\n",
      "-------epoch  788 -------\n",
      "epoch: 788, train_loss: -7.569716930389404, valid_loss: -4.5096354484558105\n",
      "-------epoch  789 -------\n",
      "epoch: 789, train_loss: -7.576336860656738, valid_loss: -4.524208068847656\n",
      "-------epoch  790 -------\n",
      "epoch: 790, train_loss: -7.57120418548584, valid_loss: -4.472157001495361\n",
      "-------epoch  791 -------\n",
      "epoch: 791, train_loss: -7.5724310874938965, valid_loss: -4.497990131378174\n",
      "-------epoch  792 -------\n",
      "epoch: 792, train_loss: -7.5827860832214355, valid_loss: -4.475072860717773\n",
      "-------epoch  793 -------\n",
      "epoch: 793, train_loss: -7.591007232666016, valid_loss: -4.455282688140869\n",
      "-------epoch  794 -------\n",
      "epoch: 794, train_loss: -7.591135025024414, valid_loss: -4.477139472961426\n",
      "-------epoch  795 -------\n",
      "epoch: 795, train_loss: -7.588540077209473, valid_loss: -4.430237770080566\n",
      "-------epoch  796 -------\n",
      "epoch: 796, train_loss: -7.58981466293335, valid_loss: -4.462713718414307\n",
      "-------epoch  797 -------\n",
      "epoch: 797, train_loss: -7.595455169677734, valid_loss: -4.427971839904785\n",
      "-------epoch  798 -------\n",
      "epoch: 798, train_loss: -7.60295295715332, valid_loss: -4.441294193267822\n",
      "-------epoch  799 -------\n",
      "epoch: 799, train_loss: -7.609074115753174, valid_loss: -4.431699752807617\n",
      "-------epoch  800 -------\n",
      "epoch: 800, train_loss: -7.612950325012207, valid_loss: -4.420167446136475\n",
      "-------epoch  801 -------\n",
      "epoch: 801, train_loss: -7.614834785461426, valid_loss: -4.434843063354492\n",
      "-------epoch  802 -------\n",
      "epoch: 802, train_loss: -7.615232944488525, valid_loss: -4.403440952301025\n",
      "-------epoch  803 -------\n",
      "epoch: 803, train_loss: -7.614452838897705, valid_loss: -4.440399646759033\n",
      "-------epoch  804 -------\n",
      "epoch: 804, train_loss: -7.611666679382324, valid_loss: -4.387789249420166\n",
      "-------epoch  805 -------\n",
      "epoch: 805, train_loss: -7.605531215667725, valid_loss: -4.450624942779541\n",
      "-------epoch  806 -------\n",
      "epoch: 806, train_loss: -7.591963291168213, valid_loss: -4.362185478210449\n",
      "-------epoch  807 -------\n",
      "epoch: 807, train_loss: -7.565299034118652, valid_loss: -4.470651149749756\n",
      "-------epoch  808 -------\n",
      "epoch: 808, train_loss: -7.512751579284668, valid_loss: -4.316101551055908\n",
      "-------epoch  809 -------\n",
      "epoch: 809, train_loss: -7.4283294677734375, valid_loss: -4.486852169036865\n",
      "-------epoch  810 -------\n",
      "epoch: 810, train_loss: -7.314651012420654, valid_loss: -4.28415060043335\n",
      "-------epoch  811 -------\n",
      "epoch: 811, train_loss: -7.2590107917785645, valid_loss: -4.51209831237793\n",
      "-------epoch  812 -------\n",
      "epoch: 812, train_loss: -7.33445930480957, valid_loss: -4.40994930267334\n",
      "-------epoch  813 -------\n",
      "epoch: 813, train_loss: -7.521556854248047, valid_loss: -4.520430088043213\n",
      "-------epoch  814 -------\n",
      "epoch: 814, train_loss: -7.644396781921387, valid_loss: -4.549188137054443\n",
      "-------epoch  815 -------\n",
      "epoch: 815, train_loss: -7.6198625564575195, valid_loss: -4.46575927734375\n",
      "-------epoch  816 -------\n",
      "epoch: 816, train_loss: -7.530706405639648, valid_loss: -4.587437629699707\n",
      "-------epoch  817 -------\n",
      "epoch: 817, train_loss: -7.536721706390381, valid_loss: -4.534623146057129\n",
      "-------epoch  818 -------\n",
      "epoch: 818, train_loss: -7.631484508514404, valid_loss: -4.569146633148193\n",
      "-------epoch  819 -------\n",
      "epoch: 819, train_loss: -7.653781414031982, valid_loss: -4.622455596923828\n",
      "-------epoch  820 -------\n",
      "epoch: 820, train_loss: -7.595770835876465, valid_loss: -4.550257682800293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  821 -------\n",
      "epoch: 821, train_loss: -7.598447799682617, valid_loss: -4.619334697723389\n",
      "-------epoch  822 -------\n",
      "epoch: 822, train_loss: -7.657285213470459, valid_loss: -4.627260208129883\n",
      "-------epoch  823 -------\n",
      "epoch: 823, train_loss: -7.656253337860107, valid_loss: -4.580384254455566\n",
      "-------epoch  824 -------\n",
      "epoch: 824, train_loss: -7.620924949645996, valid_loss: -4.6475982666015625\n",
      "-------epoch  825 -------\n",
      "epoch: 825, train_loss: -7.642463684082031, valid_loss: -4.628722667694092\n",
      "-------epoch  826 -------\n",
      "epoch: 826, train_loss: -7.675145149230957, valid_loss: -4.611006259918213\n",
      "-------epoch  827 -------\n",
      "epoch: 827, train_loss: -7.6580810546875, valid_loss: -4.6537370681762695\n",
      "-------epoch  828 -------\n",
      "epoch: 828, train_loss: -7.645660877227783, valid_loss: -4.616134166717529\n",
      "-------epoch  829 -------\n",
      "epoch: 829, train_loss: -7.672514915466309, valid_loss: -4.627708435058594\n",
      "-------epoch  830 -------\n",
      "epoch: 830, train_loss: -7.684775352478027, valid_loss: -4.649190425872803\n",
      "-------epoch  831 -------\n",
      "epoch: 831, train_loss: -7.668766498565674, valid_loss: -4.606979846954346\n",
      "-------epoch  832 -------\n",
      "epoch: 832, train_loss: -7.669433116912842, valid_loss: -4.637967586517334\n",
      "-------epoch  833 -------\n",
      "epoch: 833, train_loss: -7.690221309661865, valid_loss: -4.633779048919678\n",
      "-------epoch  834 -------\n",
      "epoch: 834, train_loss: -7.696345329284668, valid_loss: -4.6057538986206055\n",
      "-------epoch  835 -------\n",
      "epoch: 835, train_loss: -7.686281204223633, valid_loss: -4.633401870727539\n",
      "-------epoch  836 -------\n",
      "epoch: 836, train_loss: -7.686568737030029, valid_loss: -4.599926471710205\n",
      "-------epoch  837 -------\n",
      "epoch: 837, train_loss: -7.700948715209961, valid_loss: -4.611011981964111\n",
      "-------epoch  838 -------\n",
      "epoch: 838, train_loss: -7.710583686828613, valid_loss: -4.616494178771973\n",
      "-------epoch  839 -------\n",
      "epoch: 839, train_loss: -7.707949161529541, valid_loss: -4.582274913787842\n",
      "-------epoch  840 -------\n",
      "epoch: 840, train_loss: -7.703165054321289, valid_loss: -4.612335681915283\n",
      "-------epoch  841 -------\n",
      "epoch: 841, train_loss: -7.706252574920654, valid_loss: -4.580981731414795\n",
      "-------epoch  842 -------\n",
      "epoch: 842, train_loss: -7.715998649597168, valid_loss: -4.59380578994751\n",
      "-------epoch  843 -------\n",
      "epoch: 843, train_loss: -7.724905490875244, valid_loss: -4.589017391204834\n",
      "-------epoch  844 -------\n",
      "epoch: 844, train_loss: -7.728677272796631, valid_loss: -4.571643352508545\n",
      "-------epoch  845 -------\n",
      "epoch: 845, train_loss: -7.72799825668335, valid_loss: -4.588372707366943\n",
      "-------epoch  846 -------\n",
      "epoch: 846, train_loss: -7.7255754470825195, valid_loss: -4.555547714233398\n",
      "-------epoch  847 -------\n",
      "epoch: 847, train_loss: -7.723531246185303, valid_loss: -4.586029052734375\n",
      "-------epoch  848 -------\n",
      "epoch: 848, train_loss: -7.722156524658203, valid_loss: -4.537786960601807\n",
      "-------epoch  849 -------\n",
      "epoch: 849, train_loss: -7.72059965133667, valid_loss: -4.579984664916992\n",
      "-------epoch  850 -------\n",
      "epoch: 850, train_loss: -7.7169718742370605, valid_loss: -4.519288539886475\n",
      "-------epoch  851 -------\n",
      "epoch: 851, train_loss: -7.70884895324707, valid_loss: -4.577914237976074\n",
      "-------epoch  852 -------\n",
      "epoch: 852, train_loss: -7.691286087036133, valid_loss: -4.492443561553955\n",
      "-------epoch  853 -------\n",
      "epoch: 853, train_loss: -7.656602382659912, valid_loss: -4.571250915527344\n",
      "-------epoch  854 -------\n",
      "epoch: 854, train_loss: -7.592514991760254, valid_loss: -4.426954746246338\n",
      "-------epoch  855 -------\n",
      "epoch: 855, train_loss: -7.491281986236572, valid_loss: -4.5426788330078125\n",
      "-------epoch  856 -------\n",
      "epoch: 856, train_loss: -7.377514362335205, valid_loss: -4.410318374633789\n",
      "-------epoch  857 -------\n",
      "epoch: 857, train_loss: -7.344514846801758, valid_loss: -4.586802959442139\n",
      "-------epoch  858 -------\n",
      "epoch: 858, train_loss: -7.486310958862305, valid_loss: -4.577718734741211\n",
      "-------epoch  859 -------\n",
      "epoch: 859, train_loss: -7.6850385665893555, valid_loss: -4.618334770202637\n",
      "-------epoch  860 -------\n",
      "epoch: 860, train_loss: -7.7668633460998535, valid_loss: -4.649198055267334\n",
      "-------epoch  861 -------\n",
      "epoch: 861, train_loss: -7.697351455688477, valid_loss: -4.59714412689209\n",
      "-------epoch  862 -------\n",
      "epoch: 862, train_loss: -7.612566947937012, valid_loss: -4.658241271972656\n",
      "-------epoch  863 -------\n",
      "epoch: 863, train_loss: -7.664839744567871, valid_loss: -4.6476311683654785\n",
      "-------epoch  864 -------\n",
      "epoch: 864, train_loss: -7.760197162628174, valid_loss: -4.671509742736816\n",
      "-------epoch  865 -------\n",
      "epoch: 865, train_loss: -7.755953788757324, valid_loss: -4.698296546936035\n",
      "-------epoch  866 -------\n",
      "epoch: 866, train_loss: -7.693317413330078, valid_loss: -4.641415119171143\n",
      "-------epoch  867 -------\n",
      "epoch: 867, train_loss: -7.707431793212891, valid_loss: -4.70634651184082\n",
      "-------epoch  868 -------\n",
      "epoch: 868, train_loss: -7.772406101226807, valid_loss: -4.72158145904541\n",
      "-------epoch  869 -------\n",
      "epoch: 869, train_loss: -7.766716480255127, valid_loss: -4.67243766784668\n",
      "-------epoch  870 -------\n",
      "epoch: 870, train_loss: -7.727868556976318, valid_loss: -4.709598541259766\n",
      "-------epoch  871 -------\n",
      "epoch: 871, train_loss: -7.7450408935546875, valid_loss: -4.714148998260498\n",
      "-------epoch  872 -------\n",
      "epoch: 872, train_loss: -7.7862548828125, valid_loss: -4.705929279327393\n",
      "-------epoch  873 -------\n",
      "epoch: 873, train_loss: -7.776356220245361, valid_loss: -4.718427658081055\n",
      "-------epoch  874 -------\n",
      "epoch: 874, train_loss: -7.752719879150391, valid_loss: -4.692777156829834\n",
      "-------epoch  875 -------\n",
      "epoch: 875, train_loss: -7.771206855773926, valid_loss: -4.715377330780029\n",
      "-------epoch  876 -------\n",
      "epoch: 876, train_loss: -7.797558307647705, valid_loss: -4.713125228881836\n",
      "-------epoch  877 -------\n",
      "epoch: 877, train_loss: -7.790798187255859, valid_loss: -4.682416915893555\n",
      "-------epoch  878 -------\n",
      "epoch: 878, train_loss: -7.776185512542725, valid_loss: -4.703888416290283\n",
      "-------epoch  879 -------\n",
      "epoch: 879, train_loss: -7.784482002258301, valid_loss: -4.683994293212891\n",
      "-------epoch  880 -------\n",
      "epoch: 880, train_loss: -7.806034088134766, valid_loss: -4.688967704772949\n",
      "-------epoch  881 -------\n",
      "epoch: 881, train_loss: -7.811394691467285, valid_loss: -4.696176052093506\n",
      "-------epoch  882 -------\n",
      "epoch: 882, train_loss: -7.801342964172363, valid_loss: -4.656282424926758\n",
      "-------epoch  883 -------\n",
      "epoch: 883, train_loss: -7.796167850494385, valid_loss: -4.686008930206299\n",
      "-------epoch  884 -------\n",
      "epoch: 884, train_loss: -7.80550479888916, valid_loss: -4.673033237457275\n",
      "-------epoch  885 -------\n",
      "epoch: 885, train_loss: -7.819301128387451, valid_loss: -4.66863489151001\n",
      "-------epoch  886 -------\n",
      "epoch: 886, train_loss: -7.82792329788208, valid_loss: -4.668881893157959\n",
      "-------epoch  887 -------\n",
      "epoch: 887, train_loss: -7.827629566192627, valid_loss: -4.654438495635986\n",
      "-------epoch  888 -------\n",
      "epoch: 888, train_loss: -7.822523593902588, valid_loss: -4.665972709655762\n",
      "-------epoch  889 -------\n",
      "epoch: 889, train_loss: -7.817622184753418, valid_loss: -4.634904384613037\n",
      "-------epoch  890 -------\n",
      "epoch: 890, train_loss: -7.81516170501709, valid_loss: -4.656789779663086\n",
      "-------epoch  891 -------\n",
      "epoch: 891, train_loss: -7.814483642578125, valid_loss: -4.615708827972412\n",
      "-------epoch  892 -------\n",
      "epoch: 892, train_loss: -7.813316822052002, valid_loss: -4.648814678192139\n",
      "-------epoch  893 -------\n",
      "epoch: 893, train_loss: -7.810365676879883, valid_loss: -4.607123851776123\n",
      "-------epoch  894 -------\n",
      "epoch: 894, train_loss: -7.802209377288818, valid_loss: -4.638908386230469\n",
      "-------epoch  895 -------\n",
      "epoch: 895, train_loss: -7.785114288330078, valid_loss: -4.588940143585205\n",
      "-------epoch  896 -------\n",
      "epoch: 896, train_loss: -7.750138282775879, valid_loss: -4.610062122344971\n",
      "-------epoch  897 -------\n",
      "epoch: 897, train_loss: -7.689277172088623, valid_loss: -4.518779277801514\n",
      "-------epoch  898 -------\n",
      "epoch: 898, train_loss: -7.588571548461914, valid_loss: -4.552842140197754\n",
      "-------epoch  899 -------\n",
      "epoch: 899, train_loss: -7.480403900146484, valid_loss: -4.516119480133057\n",
      "-------epoch  900 -------\n",
      "epoch: 900, train_loss: -7.435946464538574, valid_loss: -4.599606037139893\n",
      "-------epoch  901 -------\n",
      "epoch: 901, train_loss: -7.5851616859436035, valid_loss: -4.65932559967041\n",
      "-------epoch  902 -------\n",
      "epoch: 902, train_loss: -7.780914306640625, valid_loss: -4.684170246124268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  903 -------\n",
      "epoch: 903, train_loss: -7.865355491638184, valid_loss: -4.694345474243164\n",
      "-------epoch  904 -------\n",
      "epoch: 904, train_loss: -7.804769992828369, valid_loss: -4.683493614196777\n",
      "-------epoch  905 -------\n",
      "epoch: 905, train_loss: -7.71047830581665, valid_loss: -4.697815895080566\n",
      "-------epoch  906 -------\n",
      "epoch: 906, train_loss: -7.753082275390625, valid_loss: -4.707843780517578\n",
      "-------epoch  907 -------\n",
      "epoch: 907, train_loss: -7.848487854003906, valid_loss: -4.74277400970459\n",
      "-------epoch  908 -------\n",
      "epoch: 908, train_loss: -7.86614465713501, valid_loss: -4.7339186668396\n",
      "-------epoch  909 -------\n",
      "epoch: 909, train_loss: -7.806953430175781, valid_loss: -4.708601474761963\n",
      "-------epoch  910 -------\n",
      "epoch: 910, train_loss: -7.791387557983398, valid_loss: -4.76598596572876\n",
      "-------epoch  911 -------\n",
      "epoch: 911, train_loss: -7.851684093475342, valid_loss: -4.768479347229004\n",
      "-------epoch  912 -------\n",
      "epoch: 912, train_loss: -7.879920482635498, valid_loss: -4.736577987670898\n",
      "-------epoch  913 -------\n",
      "epoch: 913, train_loss: -7.845116138458252, valid_loss: -4.756730079650879\n",
      "-------epoch  914 -------\n",
      "epoch: 914, train_loss: -7.830324649810791, valid_loss: -4.759659767150879\n",
      "-------epoch  915 -------\n",
      "epoch: 915, train_loss: -7.864517688751221, valid_loss: -4.761229515075684\n",
      "-------epoch  916 -------\n",
      "epoch: 916, train_loss: -7.890069007873535, valid_loss: -4.754558086395264\n",
      "-------epoch  917 -------\n",
      "epoch: 917, train_loss: -7.872530460357666, valid_loss: -4.748333930969238\n",
      "-------epoch  918 -------\n",
      "epoch: 918, train_loss: -7.856457710266113, valid_loss: -4.754509925842285\n",
      "-------epoch  919 -------\n",
      "epoch: 919, train_loss: -7.875300407409668, valid_loss: -4.74485445022583\n",
      "-------epoch  920 -------\n",
      "epoch: 920, train_loss: -7.898654460906982, valid_loss: -4.7470526695251465\n",
      "-------epoch  921 -------\n",
      "epoch: 921, train_loss: -7.897002220153809, valid_loss: -4.740955829620361\n",
      "-------epoch  922 -------\n",
      "epoch: 922, train_loss: -7.883606910705566, valid_loss: -4.718751907348633\n",
      "-------epoch  923 -------\n",
      "epoch: 923, train_loss: -7.881943225860596, valid_loss: -4.734036922454834\n",
      "-------epoch  924 -------\n",
      "epoch: 924, train_loss: -7.896936416625977, valid_loss: -4.7254486083984375\n",
      "-------epoch  925 -------\n",
      "epoch: 925, train_loss: -7.912629127502441, valid_loss: -4.719803810119629\n",
      "-------epoch  926 -------\n",
      "epoch: 926, train_loss: -7.915726184844971, valid_loss: -4.724722862243652\n",
      "-------epoch  927 -------\n",
      "epoch: 927, train_loss: -7.9093804359436035, valid_loss: -4.704870700836182\n",
      "-------epoch  928 -------\n",
      "epoch: 928, train_loss: -7.902651786804199, valid_loss: -4.708794593811035\n",
      "-------epoch  929 -------\n",
      "epoch: 929, train_loss: -7.901370525360107, valid_loss: -4.701175689697266\n",
      "-------epoch  930 -------\n",
      "epoch: 930, train_loss: -7.905871868133545, valid_loss: -4.702110290527344\n",
      "-------epoch  931 -------\n",
      "epoch: 931, train_loss: -7.914057731628418, valid_loss: -4.688570499420166\n",
      "-------epoch  932 -------\n",
      "epoch: 932, train_loss: -7.921829700469971, valid_loss: -4.698027610778809\n",
      "-------epoch  933 -------\n",
      "epoch: 933, train_loss: -7.928436756134033, valid_loss: -4.6843085289001465\n",
      "-------epoch  934 -------\n",
      "epoch: 934, train_loss: -7.933778285980225, valid_loss: -4.684445858001709\n",
      "-------epoch  935 -------\n",
      "epoch: 935, train_loss: -7.937554359436035, valid_loss: -4.68087911605835\n",
      "-------epoch  936 -------\n",
      "epoch: 936, train_loss: -7.939968585968018, valid_loss: -4.676751613616943\n",
      "-------epoch  937 -------\n",
      "epoch: 937, train_loss: -7.940815448760986, valid_loss: -4.667806625366211\n",
      "-------epoch  938 -------\n",
      "epoch: 938, train_loss: -7.938570022583008, valid_loss: -4.665750503540039\n",
      "-------epoch  939 -------\n",
      "epoch: 939, train_loss: -7.928144454956055, valid_loss: -4.636073589324951\n",
      "-------epoch  940 -------\n",
      "epoch: 940, train_loss: -7.8952155113220215, valid_loss: -4.616839408874512\n",
      "-------epoch  941 -------\n",
      "epoch: 941, train_loss: -7.801732540130615, valid_loss: -4.540432929992676\n",
      "-------epoch  942 -------\n",
      "epoch: 942, train_loss: -7.5572028160095215, valid_loss: -4.351574420928955\n",
      "-------epoch  943 -------\n",
      "epoch: 943, train_loss: -7.123682498931885, valid_loss: -4.427273273468018\n",
      "-------epoch  944 -------\n",
      "epoch: 944, train_loss: -6.898828029632568, valid_loss: -4.504153251647949\n",
      "-------epoch  945 -------\n",
      "epoch: 945, train_loss: -7.589641094207764, valid_loss: -4.6547346115112305\n",
      "-------epoch  946 -------\n",
      "epoch: 946, train_loss: -7.9062275886535645, valid_loss: -4.665683269500732\n",
      "-------epoch  947 -------\n",
      "epoch: 947, train_loss: -7.805459976196289, valid_loss: -4.6365814208984375\n",
      "-------epoch  948 -------\n",
      "epoch: 948, train_loss: -7.576539039611816, valid_loss: -4.781252861022949\n",
      "-------epoch  949 -------\n",
      "epoch: 949, train_loss: -7.798075199127197, valid_loss: -4.755591869354248\n",
      "-------epoch  950 -------\n",
      "epoch: 950, train_loss: -7.922928333282471, valid_loss: -4.7315826416015625\n",
      "-------epoch  951 -------\n",
      "epoch: 951, train_loss: -7.780938625335693, valid_loss: -4.738555431365967\n",
      "-------epoch  952 -------\n",
      "epoch: 952, train_loss: -7.75250244140625, valid_loss: -4.824597358703613\n",
      "-------epoch  953 -------\n",
      "epoch: 953, train_loss: -7.935155868530273, valid_loss: -4.840310096740723\n",
      "-------epoch  954 -------\n",
      "epoch: 954, train_loss: -7.8806257247924805, valid_loss: -4.845921993255615\n",
      "-------epoch  955 -------\n",
      "epoch: 955, train_loss: -7.804927825927734, valid_loss: -4.856758117675781\n",
      "-------epoch  956 -------\n",
      "epoch: 956, train_loss: -7.943808078765869, valid_loss: -4.869824409484863\n",
      "-------epoch  957 -------\n",
      "epoch: 957, train_loss: -7.893040657043457, valid_loss: -4.849071502685547\n",
      "-------epoch  958 -------\n",
      "epoch: 958, train_loss: -7.850498199462891, valid_loss: -4.873003005981445\n",
      "-------epoch  959 -------\n",
      "epoch: 959, train_loss: -7.955996990203857, valid_loss: -4.837377548217773\n",
      "-------epoch  960 -------\n",
      "epoch: 960, train_loss: -7.907584190368652, valid_loss: -4.885026931762695\n",
      "-------epoch  961 -------\n",
      "epoch: 961, train_loss: -7.897517204284668, valid_loss: -4.880184650421143\n",
      "-------epoch  962 -------\n",
      "epoch: 962, train_loss: -7.963885307312012, valid_loss: -4.850961685180664\n",
      "-------epoch  963 -------\n",
      "epoch: 963, train_loss: -7.920264720916748, valid_loss: -4.852871417999268\n",
      "-------epoch  964 -------\n",
      "epoch: 964, train_loss: -7.926884651184082, valid_loss: -4.8462419509887695\n",
      "-------epoch  965 -------\n",
      "epoch: 965, train_loss: -7.970241546630859, valid_loss: -4.826069355010986\n",
      "-------epoch  966 -------\n",
      "epoch: 966, train_loss: -7.937283515930176, valid_loss: -4.8366007804870605\n",
      "-------epoch  967 -------\n",
      "epoch: 967, train_loss: -7.946066379547119, valid_loss: -4.820511341094971\n",
      "-------epoch  968 -------\n",
      "epoch: 968, train_loss: -7.980261325836182, valid_loss: -4.814204216003418\n",
      "-------epoch  969 -------\n",
      "epoch: 969, train_loss: -7.960109233856201, valid_loss: -4.821022987365723\n",
      "-------epoch  970 -------\n",
      "epoch: 970, train_loss: -7.957083702087402, valid_loss: -4.800981044769287\n",
      "-------epoch  971 -------\n",
      "epoch: 971, train_loss: -7.992743968963623, valid_loss: -4.788312911987305\n",
      "-------epoch  972 -------\n",
      "epoch: 972, train_loss: -7.980771064758301, valid_loss: -4.787978172302246\n",
      "-------epoch  973 -------\n",
      "epoch: 973, train_loss: -7.968937397003174, valid_loss: -4.787396430969238\n",
      "-------epoch  974 -------\n",
      "epoch: 974, train_loss: -7.995993614196777, valid_loss: -4.7736992835998535\n",
      "-------epoch  975 -------\n",
      "epoch: 975, train_loss: -8.003600120544434, valid_loss: -4.766855716705322\n",
      "-------epoch  976 -------\n",
      "epoch: 976, train_loss: -7.990021228790283, valid_loss: -4.7555766105651855\n",
      "-------epoch  977 -------\n",
      "epoch: 977, train_loss: -7.993857383728027, valid_loss: -4.759372234344482\n",
      "-------epoch  978 -------\n",
      "epoch: 978, train_loss: -8.01259994506836, valid_loss: -4.750550270080566\n",
      "-------epoch  979 -------\n",
      "epoch: 979, train_loss: -8.018056869506836, valid_loss: -4.727264404296875\n",
      "-------epoch  980 -------\n",
      "epoch: 980, train_loss: -8.00809097290039, valid_loss: -4.734263896942139\n",
      "-------epoch  981 -------\n",
      "epoch: 981, train_loss: -8.008881568908691, valid_loss: -4.717741012573242\n",
      "-------epoch  982 -------\n",
      "epoch: 982, train_loss: -8.019551277160645, valid_loss: -4.7141265869140625\n",
      "-------epoch  983 -------\n",
      "epoch: 983, train_loss: -8.031126022338867, valid_loss: -4.706080436706543\n",
      "-------epoch  984 -------\n",
      "epoch: 984, train_loss: -8.031618118286133, valid_loss: -4.694112777709961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  985 -------\n",
      "epoch: 985, train_loss: -8.027851104736328, valid_loss: -4.695624828338623\n",
      "-------epoch  986 -------\n",
      "epoch: 986, train_loss: -8.02482795715332, valid_loss: -4.678317546844482\n",
      "-------epoch  987 -------\n",
      "epoch: 987, train_loss: -8.027238845825195, valid_loss: -4.6830925941467285\n",
      "-------epoch  988 -------\n",
      "epoch: 988, train_loss: -8.033065795898438, valid_loss: -4.671977519989014\n",
      "-------epoch  989 -------\n",
      "epoch: 989, train_loss: -8.040079116821289, valid_loss: -4.6758551597595215\n",
      "-------epoch  990 -------\n",
      "epoch: 990, train_loss: -8.046568870544434, valid_loss: -4.668423652648926\n",
      "-------epoch  991 -------\n",
      "epoch: 991, train_loss: -8.051122665405273, valid_loss: -4.668727397918701\n",
      "-------epoch  992 -------\n",
      "epoch: 992, train_loss: -8.055057525634766, valid_loss: -4.664401531219482\n",
      "-------epoch  993 -------\n",
      "epoch: 993, train_loss: -8.05763053894043, valid_loss: -4.660470962524414\n",
      "-------epoch  994 -------\n",
      "epoch: 994, train_loss: -8.060236930847168, valid_loss: -4.66046667098999\n",
      "-------epoch  995 -------\n",
      "epoch: 995, train_loss: -8.061878204345703, valid_loss: -4.655467510223389\n",
      "-------epoch  996 -------\n",
      "epoch: 996, train_loss: -8.063072204589844, valid_loss: -4.657461166381836\n",
      "-------epoch  997 -------\n",
      "epoch: 997, train_loss: -8.061864852905273, valid_loss: -4.642742156982422\n",
      "-------epoch  998 -------\n",
      "epoch: 998, train_loss: -8.055807113647461, valid_loss: -4.655230522155762\n",
      "-------epoch  999 -------\n",
      "epoch: 999, train_loss: -8.034671783447266, valid_loss: -4.613533020019531\n",
      "-------epoch  1000 -------\n",
      "epoch: 1000, train_loss: -7.974804878234863, valid_loss: -4.6540632247924805\n",
      "-------epoch  1001 -------\n",
      "epoch: 1001, train_loss: -7.816789627075195, valid_loss: -4.493160247802734\n",
      "-------epoch  1002 -------\n",
      "epoch: 1002, train_loss: -7.547100067138672, valid_loss: -4.550624370574951\n",
      "-------epoch  1003 -------\n",
      "epoch: 1003, train_loss: -7.32388973236084, valid_loss: -4.3606438636779785\n",
      "-------epoch  1004 -------\n",
      "epoch: 1004, train_loss: -7.489743709564209, valid_loss: -4.741447448730469\n",
      "-------epoch  1005 -------\n",
      "epoch: 1005, train_loss: -7.794548511505127, valid_loss: -4.554152011871338\n",
      "-------epoch  1006 -------\n",
      "epoch: 1006, train_loss: -7.98877477645874, valid_loss: -4.67780065536499\n",
      "-------epoch  1007 -------\n",
      "epoch: 1007, train_loss: -8.017051696777344, valid_loss: -4.777127742767334\n",
      "-------epoch  1008 -------\n",
      "epoch: 1008, train_loss: -7.779397010803223, valid_loss: -4.650201320648193\n",
      "-------epoch  1009 -------\n",
      "epoch: 1009, train_loss: -7.8850836753845215, valid_loss: -4.68178653717041\n",
      "-------epoch  1010 -------\n",
      "epoch: 1010, train_loss: -8.025581359863281, valid_loss: -4.766719818115234\n",
      "-------epoch  1011 -------\n",
      "epoch: 1011, train_loss: -8.021586418151855, valid_loss: -4.708076000213623\n",
      "-------epoch  1012 -------\n",
      "epoch: 1012, train_loss: -7.913238048553467, valid_loss: -4.803177833557129\n",
      "-------epoch  1013 -------\n",
      "epoch: 1013, train_loss: -7.9622321128845215, valid_loss: -4.802813529968262\n",
      "-------epoch  1014 -------\n",
      "epoch: 1014, train_loss: -8.071410179138184, valid_loss: -4.800853729248047\n",
      "-------epoch  1015 -------\n",
      "epoch: 1015, train_loss: -7.990817546844482, valid_loss: -4.868429183959961\n",
      "-------epoch  1016 -------\n",
      "epoch: 1016, train_loss: -7.970932483673096, valid_loss: -4.795556545257568\n",
      "-------epoch  1017 -------\n",
      "epoch: 1017, train_loss: -8.044629096984863, valid_loss: -4.823383808135986\n",
      "-------epoch  1018 -------\n",
      "epoch: 1018, train_loss: -8.058150291442871, valid_loss: -4.818488121032715\n",
      "-------epoch  1019 -------\n",
      "epoch: 1019, train_loss: -7.985857009887695, valid_loss: -4.834932804107666\n",
      "-------epoch  1020 -------\n",
      "epoch: 1020, train_loss: -8.05909252166748, valid_loss: -4.815310478210449\n",
      "-------epoch  1021 -------\n",
      "epoch: 1021, train_loss: -8.070291519165039, valid_loss: -4.849991798400879\n",
      "-------epoch  1022 -------\n",
      "epoch: 1022, train_loss: -8.042073249816895, valid_loss: -4.801072597503662\n",
      "-------epoch  1023 -------\n",
      "epoch: 1023, train_loss: -8.055493354797363, valid_loss: -4.81525993347168\n",
      "-------epoch  1024 -------\n",
      "epoch: 1024, train_loss: -8.093179702758789, valid_loss: -4.818109035491943\n",
      "-------epoch  1025 -------\n",
      "epoch: 1025, train_loss: -8.068434715270996, valid_loss: -4.786118984222412\n",
      "-------epoch  1026 -------\n",
      "epoch: 1026, train_loss: -8.067621231079102, valid_loss: -4.814248085021973\n",
      "-------epoch  1027 -------\n",
      "epoch: 1027, train_loss: -8.101391792297363, valid_loss: -4.7894792556762695\n",
      "-------epoch  1028 -------\n",
      "epoch: 1028, train_loss: -8.093676567077637, valid_loss: -4.765734672546387\n",
      "-------epoch  1029 -------\n",
      "epoch: 1029, train_loss: -8.082911491394043, valid_loss: -4.797433853149414\n",
      "-------epoch  1030 -------\n",
      "epoch: 1030, train_loss: -8.096829414367676, valid_loss: -4.789348602294922\n",
      "-------epoch  1031 -------\n",
      "epoch: 1031, train_loss: -8.115424156188965, valid_loss: -4.76057767868042\n",
      "-------epoch  1032 -------\n",
      "epoch: 1032, train_loss: -8.104595184326172, valid_loss: -4.759434700012207\n",
      "-------epoch  1033 -------\n",
      "epoch: 1033, train_loss: -8.097865104675293, valid_loss: -4.750329971313477\n",
      "-------epoch  1034 -------\n",
      "epoch: 1034, train_loss: -8.112919807434082, valid_loss: -4.75861930847168\n",
      "-------epoch  1035 -------\n",
      "epoch: 1035, train_loss: -8.125486373901367, valid_loss: -4.742556095123291\n",
      "-------epoch  1036 -------\n",
      "epoch: 1036, train_loss: -8.124913215637207, valid_loss: -4.717952251434326\n",
      "-------epoch  1037 -------\n",
      "epoch: 1037, train_loss: -8.1156587600708, valid_loss: -4.744167327880859\n",
      "-------epoch  1038 -------\n",
      "epoch: 1038, train_loss: -8.119994163513184, valid_loss: -4.710475444793701\n",
      "-------epoch  1039 -------\n",
      "epoch: 1039, train_loss: -8.130208015441895, valid_loss: -4.7043609619140625\n",
      "-------epoch  1040 -------\n",
      "epoch: 1040, train_loss: -8.140813827514648, valid_loss: -4.697458267211914\n",
      "-------epoch  1041 -------\n",
      "epoch: 1041, train_loss: -8.140766143798828, valid_loss: -4.686036586761475\n",
      "-------epoch  1042 -------\n",
      "epoch: 1042, train_loss: -8.137619972229004, valid_loss: -4.698295593261719\n",
      "-------epoch  1043 -------\n",
      "epoch: 1043, train_loss: -8.13306713104248, valid_loss: -4.662205219268799\n",
      "-------epoch  1044 -------\n",
      "epoch: 1044, train_loss: -8.134765625, valid_loss: -4.677768707275391\n",
      "-------epoch  1045 -------\n",
      "epoch: 1045, train_loss: -8.137829780578613, valid_loss: -4.656634330749512\n",
      "-------epoch  1046 -------\n",
      "epoch: 1046, train_loss: -8.144132614135742, valid_loss: -4.671154975891113\n",
      "-------epoch  1047 -------\n",
      "epoch: 1047, train_loss: -8.147851943969727, valid_loss: -4.652075290679932\n",
      "-------epoch  1048 -------\n",
      "epoch: 1048, train_loss: -8.153555870056152, valid_loss: -4.66250467300415\n",
      "-------epoch  1049 -------\n",
      "epoch: 1049, train_loss: -8.155488014221191, valid_loss: -4.643800258636475\n",
      "-------epoch  1050 -------\n",
      "epoch: 1050, train_loss: -8.15831184387207, valid_loss: -4.659315586090088\n",
      "-------epoch  1051 -------\n",
      "epoch: 1051, train_loss: -8.156929969787598, valid_loss: -4.634684085845947\n",
      "-------epoch  1052 -------\n",
      "epoch: 1052, train_loss: -8.153670310974121, valid_loss: -4.655774116516113\n",
      "-------epoch  1053 -------\n",
      "epoch: 1053, train_loss: -8.140656471252441, valid_loss: -4.607415199279785\n",
      "-------epoch  1054 -------\n",
      "epoch: 1054, train_loss: -8.110060691833496, valid_loss: -4.671159267425537\n",
      "-------epoch  1055 -------\n",
      "epoch: 1055, train_loss: -8.039193153381348, valid_loss: -4.5367913246154785\n",
      "-------epoch  1056 -------\n",
      "epoch: 1056, train_loss: -7.915333271026611, valid_loss: -4.6315999031066895\n",
      "-------epoch  1057 -------\n",
      "epoch: 1057, train_loss: -7.7286906242370605, valid_loss: -4.40584135055542\n",
      "-------epoch  1058 -------\n",
      "epoch: 1058, train_loss: -7.637953758239746, valid_loss: -4.639223575592041\n",
      "-------epoch  1059 -------\n",
      "epoch: 1059, train_loss: -7.739224433898926, valid_loss: -4.581557273864746\n",
      "-------epoch  1060 -------\n",
      "epoch: 1060, train_loss: -8.017173767089844, valid_loss: -4.695433139801025\n",
      "-------epoch  1061 -------\n",
      "epoch: 1061, train_loss: -8.17039680480957, valid_loss: -4.689564228057861\n",
      "-------epoch  1062 -------\n",
      "epoch: 1062, train_loss: -8.144728660583496, valid_loss: -4.62945556640625\n",
      "-------epoch  1063 -------\n",
      "epoch: 1063, train_loss: -8.030058860778809, valid_loss: -4.755204677581787\n",
      "-------epoch  1064 -------\n",
      "epoch: 1064, train_loss: -8.0023193359375, valid_loss: -4.691081523895264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1065 -------\n",
      "epoch: 1065, train_loss: -8.12684154510498, valid_loss: -4.734801292419434\n",
      "-------epoch  1066 -------\n",
      "epoch: 1066, train_loss: -8.187994956970215, valid_loss: -4.764249324798584\n",
      "-------epoch  1067 -------\n",
      "epoch: 1067, train_loss: -8.122838973999023, valid_loss: -4.713118553161621\n",
      "-------epoch  1068 -------\n",
      "epoch: 1068, train_loss: -8.077213287353516, valid_loss: -4.781642436981201\n",
      "-------epoch  1069 -------\n",
      "epoch: 1069, train_loss: -8.136611938476562, valid_loss: -4.779980659484863\n",
      "-------epoch  1070 -------\n",
      "epoch: 1070, train_loss: -8.189409255981445, valid_loss: -4.763124942779541\n",
      "-------epoch  1071 -------\n",
      "epoch: 1071, train_loss: -8.159089088439941, valid_loss: -4.789762496948242\n",
      "-------epoch  1072 -------\n",
      "epoch: 1072, train_loss: -8.120399475097656, valid_loss: -4.763801097869873\n",
      "-------epoch  1073 -------\n",
      "epoch: 1073, train_loss: -8.162323951721191, valid_loss: -4.7799811363220215\n",
      "-------epoch  1074 -------\n",
      "epoch: 1074, train_loss: -8.199427604675293, valid_loss: -4.773129940032959\n",
      "-------epoch  1075 -------\n",
      "epoch: 1075, train_loss: -8.173083305358887, valid_loss: -4.737802982330322\n",
      "-------epoch  1076 -------\n",
      "epoch: 1076, train_loss: -8.153996467590332, valid_loss: -4.771503448486328\n",
      "-------epoch  1077 -------\n",
      "epoch: 1077, train_loss: -8.1832857131958, valid_loss: -4.743442535400391\n",
      "-------epoch  1078 -------\n",
      "epoch: 1078, train_loss: -8.204452514648438, valid_loss: -4.7385969161987305\n",
      "-------epoch  1079 -------\n",
      "epoch: 1079, train_loss: -8.192832946777344, valid_loss: -4.759653568267822\n",
      "-------epoch  1080 -------\n",
      "epoch: 1080, train_loss: -8.177411079406738, valid_loss: -4.713481426239014\n",
      "-------epoch  1081 -------\n",
      "epoch: 1081, train_loss: -8.192214012145996, valid_loss: -4.738098621368408\n",
      "-------epoch  1082 -------\n",
      "epoch: 1082, train_loss: -8.217219352722168, valid_loss: -4.743495941162109\n",
      "-------epoch  1083 -------\n",
      "epoch: 1083, train_loss: -8.21381664276123, valid_loss: -4.704524517059326\n",
      "-------epoch  1084 -------\n",
      "epoch: 1084, train_loss: -8.199484825134277, valid_loss: -4.724488735198975\n",
      "-------epoch  1085 -------\n",
      "epoch: 1085, train_loss: -8.201713562011719, valid_loss: -4.71026086807251\n",
      "-------epoch  1086 -------\n",
      "epoch: 1086, train_loss: -8.21414852142334, valid_loss: -4.708590507507324\n",
      "-------epoch  1087 -------\n",
      "epoch: 1087, train_loss: -8.227178573608398, valid_loss: -4.709305286407471\n",
      "-------epoch  1088 -------\n",
      "epoch: 1088, train_loss: -8.23277473449707, valid_loss: -4.6965651512146\n",
      "-------epoch  1089 -------\n",
      "epoch: 1089, train_loss: -8.227940559387207, valid_loss: -4.691481113433838\n",
      "-------epoch  1090 -------\n",
      "epoch: 1090, train_loss: -8.220317840576172, valid_loss: -4.6677632331848145\n",
      "-------epoch  1091 -------\n",
      "epoch: 1091, train_loss: -8.218387603759766, valid_loss: -4.681525230407715\n",
      "-------epoch  1092 -------\n",
      "epoch: 1092, train_loss: -8.221941947937012, valid_loss: -4.651660442352295\n",
      "-------epoch  1093 -------\n",
      "epoch: 1093, train_loss: -8.22632122039795, valid_loss: -4.666154384613037\n",
      "-------epoch  1094 -------\n",
      "epoch: 1094, train_loss: -8.233142852783203, valid_loss: -4.644497871398926\n",
      "-------epoch  1095 -------\n",
      "epoch: 1095, train_loss: -8.24036693572998, valid_loss: -4.651612281799316\n",
      "-------epoch  1096 -------\n",
      "epoch: 1096, train_loss: -8.244447708129883, valid_loss: -4.633636474609375\n",
      "-------epoch  1097 -------\n",
      "epoch: 1097, train_loss: -8.246267318725586, valid_loss: -4.635669231414795\n",
      "-------epoch  1098 -------\n",
      "epoch: 1098, train_loss: -8.247808456420898, valid_loss: -4.624904632568359\n",
      "-------epoch  1099 -------\n",
      "epoch: 1099, train_loss: -8.245196342468262, valid_loss: -4.631845951080322\n",
      "-------epoch  1100 -------\n",
      "epoch: 1100, train_loss: -8.2338228225708, valid_loss: -4.590942859649658\n",
      "-------epoch  1101 -------\n",
      "epoch: 1101, train_loss: -8.202634811401367, valid_loss: -4.607350826263428\n",
      "-------epoch  1102 -------\n",
      "epoch: 1102, train_loss: -8.125001907348633, valid_loss: -4.438810348510742\n",
      "-------epoch  1103 -------\n",
      "epoch: 1103, train_loss: -7.934867858886719, valid_loss: -4.427328109741211\n",
      "-------epoch  1104 -------\n",
      "epoch: 1104, train_loss: -7.587060928344727, valid_loss: -4.040588855743408\n",
      "-------epoch  1105 -------\n",
      "epoch: 1105, train_loss: -7.231597900390625, valid_loss: -4.676224708557129\n",
      "-------epoch  1106 -------\n",
      "epoch: 1106, train_loss: -7.536625862121582, valid_loss: -4.561820030212402\n",
      "-------epoch  1107 -------\n",
      "epoch: 1107, train_loss: -8.204469680786133, valid_loss: -4.5221357345581055\n",
      "-------epoch  1108 -------\n",
      "epoch: 1108, train_loss: -8.01895523071289, valid_loss: -4.760058879852295\n",
      "-------epoch  1109 -------\n",
      "epoch: 1109, train_loss: -7.591053485870361, valid_loss: -4.587404727935791\n",
      "-------epoch  1110 -------\n",
      "epoch: 1110, train_loss: -8.145639419555664, valid_loss: -4.5682692527771\n",
      "-------epoch  1111 -------\n",
      "epoch: 1111, train_loss: -8.068222999572754, valid_loss: -4.75579833984375\n",
      "-------epoch  1112 -------\n",
      "epoch: 1112, train_loss: -7.741233825683594, valid_loss: -4.674863815307617\n",
      "-------epoch  1113 -------\n",
      "epoch: 1113, train_loss: -8.18773365020752, valid_loss: -4.654294490814209\n",
      "-------epoch  1114 -------\n",
      "epoch: 1114, train_loss: -8.030426979064941, valid_loss: -4.778143405914307\n",
      "-------epoch  1115 -------\n",
      "epoch: 1115, train_loss: -7.915374755859375, valid_loss: -4.78599739074707\n",
      "-------epoch  1116 -------\n",
      "epoch: 1116, train_loss: -8.25222110748291, valid_loss: -4.685843467712402\n",
      "-------epoch  1117 -------\n",
      "epoch: 1117, train_loss: -7.992370128631592, valid_loss: -4.8983154296875\n",
      "-------epoch  1118 -------\n",
      "epoch: 1118, train_loss: -8.138087272644043, valid_loss: -4.8738250732421875\n",
      "-------epoch  1119 -------\n",
      "epoch: 1119, train_loss: -8.200006484985352, valid_loss: -4.770318508148193\n",
      "-------epoch  1120 -------\n",
      "epoch: 1120, train_loss: -8.048469543457031, valid_loss: -4.870521068572998\n",
      "-------epoch  1121 -------\n",
      "epoch: 1121, train_loss: -8.24468994140625, valid_loss: -4.928342819213867\n",
      "-------epoch  1122 -------\n",
      "epoch: 1122, train_loss: -8.107498168945312, valid_loss: -4.838104724884033\n",
      "-------epoch  1123 -------\n",
      "epoch: 1123, train_loss: -8.20871639251709, valid_loss: -4.80238676071167\n",
      "-------epoch  1124 -------\n",
      "epoch: 1124, train_loss: -8.197415351867676, valid_loss: -4.883418560028076\n",
      "-------epoch  1125 -------\n",
      "epoch: 1125, train_loss: -8.18575668334961, valid_loss: -4.870668888092041\n",
      "-------epoch  1126 -------\n",
      "epoch: 1126, train_loss: -8.239727973937988, valid_loss: -4.792495250701904\n",
      "-------epoch  1127 -------\n",
      "epoch: 1127, train_loss: -8.19434642791748, valid_loss: -4.80905818939209\n",
      "-------epoch  1128 -------\n",
      "epoch: 1128, train_loss: -8.247267723083496, valid_loss: -4.815206050872803\n",
      "-------epoch  1129 -------\n",
      "epoch: 1129, train_loss: -8.221967697143555, valid_loss: -4.806910514831543\n",
      "-------epoch  1130 -------\n",
      "epoch: 1130, train_loss: -8.241525650024414, valid_loss: -4.779544830322266\n",
      "-------epoch  1131 -------\n",
      "epoch: 1131, train_loss: -8.252141952514648, valid_loss: -4.757075309753418\n",
      "-------epoch  1132 -------\n",
      "epoch: 1132, train_loss: -8.234315872192383, valid_loss: -4.783825397491455\n",
      "-------epoch  1133 -------\n",
      "epoch: 1133, train_loss: -8.2758207321167, valid_loss: -4.746017932891846\n",
      "-------epoch  1134 -------\n",
      "epoch: 1134, train_loss: -8.242536544799805, valid_loss: -4.763562202453613\n",
      "-------epoch  1135 -------\n",
      "epoch: 1135, train_loss: -8.274968147277832, valid_loss: -4.746152400970459\n",
      "-------epoch  1136 -------\n",
      "epoch: 1136, train_loss: -8.278400421142578, valid_loss: -4.743322372436523\n",
      "-------epoch  1137 -------\n",
      "epoch: 1137, train_loss: -8.268129348754883, valid_loss: -4.770183563232422\n",
      "-------epoch  1138 -------\n",
      "epoch: 1138, train_loss: -8.30109977722168, valid_loss: -4.72402811050415\n",
      "-------epoch  1139 -------\n",
      "epoch: 1139, train_loss: -8.282427787780762, valid_loss: -4.738009452819824\n",
      "-------epoch  1140 -------\n",
      "epoch: 1140, train_loss: -8.301457405090332, valid_loss: -4.745061874389648\n",
      "-------epoch  1141 -------\n",
      "epoch: 1141, train_loss: -8.303832054138184, valid_loss: -4.719202518463135\n",
      "-------epoch  1142 -------\n",
      "epoch: 1142, train_loss: -8.2968111038208, valid_loss: -4.707598686218262\n",
      "-------epoch  1143 -------\n",
      "epoch: 1143, train_loss: -8.317123413085938, valid_loss: -4.700974941253662\n",
      "-------epoch  1144 -------\n",
      "epoch: 1144, train_loss: -8.307186126708984, valid_loss: -4.7091803550720215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1145 -------\n",
      "epoch: 1145, train_loss: -8.313639640808105, valid_loss: -4.676570415496826\n",
      "-------epoch  1146 -------\n",
      "epoch: 1146, train_loss: -8.324421882629395, valid_loss: -4.66572380065918\n",
      "-------epoch  1147 -------\n",
      "epoch: 1147, train_loss: -8.31855583190918, valid_loss: -4.690776348114014\n",
      "-------epoch  1148 -------\n",
      "epoch: 1148, train_loss: -8.328389167785645, valid_loss: -4.652674198150635\n",
      "-------epoch  1149 -------\n",
      "epoch: 1149, train_loss: -8.331552505493164, valid_loss: -4.642024517059326\n",
      "-------epoch  1150 -------\n",
      "epoch: 1150, train_loss: -8.329506874084473, valid_loss: -4.675426483154297\n",
      "-------epoch  1151 -------\n",
      "epoch: 1151, train_loss: -8.339851379394531, valid_loss: -4.6457366943359375\n",
      "-------epoch  1152 -------\n",
      "epoch: 1152, train_loss: -8.343055725097656, valid_loss: -4.626835346221924\n",
      "-------epoch  1153 -------\n",
      "epoch: 1153, train_loss: -8.341363906860352, valid_loss: -4.664364337921143\n",
      "-------epoch  1154 -------\n",
      "epoch: 1154, train_loss: -8.34783935546875, valid_loss: -4.643638610839844\n",
      "-------epoch  1155 -------\n",
      "epoch: 1155, train_loss: -8.351293563842773, valid_loss: -4.615828037261963\n",
      "-------epoch  1156 -------\n",
      "epoch: 1156, train_loss: -8.350308418273926, valid_loss: -4.659398555755615\n",
      "-------epoch  1157 -------\n",
      "epoch: 1157, train_loss: -8.355113983154297, valid_loss: -4.6311163902282715\n",
      "-------epoch  1158 -------\n",
      "epoch: 1158, train_loss: -8.360311508178711, valid_loss: -4.626330852508545\n",
      "-------epoch  1159 -------\n",
      "epoch: 1159, train_loss: -8.359158515930176, valid_loss: -4.651525497436523\n",
      "-------epoch  1160 -------\n",
      "epoch: 1160, train_loss: -8.359469413757324, valid_loss: -4.624338150024414\n",
      "-------epoch  1161 -------\n",
      "epoch: 1161, train_loss: -8.360941886901855, valid_loss: -4.591545104980469\n",
      "-------epoch  1162 -------\n",
      "epoch: 1162, train_loss: -8.351001739501953, valid_loss: -4.604616641998291\n",
      "-------epoch  1163 -------\n",
      "epoch: 1163, train_loss: -8.318690299987793, valid_loss: -4.487157821655273\n",
      "-------epoch  1164 -------\n",
      "epoch: 1164, train_loss: -8.257181167602539, valid_loss: -4.447251319885254\n",
      "-------epoch  1165 -------\n",
      "epoch: 1165, train_loss: -8.099493026733398, valid_loss: -4.394575595855713\n",
      "-------epoch  1166 -------\n",
      "epoch: 1166, train_loss: -7.86140775680542, valid_loss: -4.195577621459961\n",
      "-------epoch  1167 -------\n",
      "epoch: 1167, train_loss: -7.596513748168945, valid_loss: -4.495140075683594\n",
      "-------epoch  1168 -------\n",
      "epoch: 1168, train_loss: -7.823497772216797, valid_loss: -4.573851585388184\n",
      "-------epoch  1169 -------\n",
      "epoch: 1169, train_loss: -8.26404094696045, valid_loss: -4.677586078643799\n",
      "-------epoch  1170 -------\n",
      "epoch: 1170, train_loss: -8.35044002532959, valid_loss: -4.686186790466309\n",
      "-------epoch  1171 -------\n",
      "epoch: 1171, train_loss: -8.148221015930176, valid_loss: -4.510117053985596\n",
      "-------epoch  1172 -------\n",
      "epoch: 1172, train_loss: -8.087639808654785, valid_loss: -4.795670509338379\n",
      "-------epoch  1173 -------\n",
      "epoch: 1173, train_loss: -8.312993049621582, valid_loss: -4.779709339141846\n",
      "-------epoch  1174 -------\n",
      "epoch: 1174, train_loss: -8.354148864746094, valid_loss: -4.595280170440674\n",
      "-------epoch  1175 -------\n",
      "epoch: 1175, train_loss: -8.188241004943848, valid_loss: -4.798667907714844\n",
      "-------epoch  1176 -------\n",
      "epoch: 1176, train_loss: -8.241620063781738, valid_loss: -4.771559238433838\n",
      "-------epoch  1177 -------\n",
      "epoch: 1177, train_loss: -8.374530792236328, valid_loss: -4.706996917724609\n",
      "-------epoch  1178 -------\n",
      "epoch: 1178, train_loss: -8.309476852416992, valid_loss: -4.804172992706299\n",
      "-------epoch  1179 -------\n",
      "epoch: 1179, train_loss: -8.242770195007324, valid_loss: -4.738787651062012\n",
      "-------epoch  1180 -------\n",
      "epoch: 1180, train_loss: -8.353555679321289, valid_loss: -4.76489782333374\n",
      "-------epoch  1181 -------\n",
      "epoch: 1181, train_loss: -8.367690086364746, valid_loss: -4.841461181640625\n",
      "-------epoch  1182 -------\n",
      "epoch: 1182, train_loss: -8.296669960021973, valid_loss: -4.716766357421875\n",
      "-------epoch  1183 -------\n",
      "epoch: 1183, train_loss: -8.343436241149902, valid_loss: -4.768024444580078\n",
      "-------epoch  1184 -------\n",
      "epoch: 1184, train_loss: -8.391755104064941, valid_loss: -4.830936431884766\n",
      "-------epoch  1185 -------\n",
      "epoch: 1185, train_loss: -8.340413093566895, valid_loss: -4.710600852966309\n",
      "-------epoch  1186 -------\n",
      "epoch: 1186, train_loss: -8.352357864379883, valid_loss: -4.745988368988037\n",
      "-------epoch  1187 -------\n",
      "epoch: 1187, train_loss: -8.397021293640137, valid_loss: -4.787179470062256\n",
      "-------epoch  1188 -------\n",
      "epoch: 1188, train_loss: -8.378643989562988, valid_loss: -4.706573486328125\n",
      "-------epoch  1189 -------\n",
      "epoch: 1189, train_loss: -8.362577438354492, valid_loss: -4.742600917816162\n",
      "-------epoch  1190 -------\n",
      "epoch: 1190, train_loss: -8.39931869506836, valid_loss: -4.745058536529541\n",
      "-------epoch  1191 -------\n",
      "epoch: 1191, train_loss: -8.405245780944824, valid_loss: -4.697586536407471\n",
      "-------epoch  1192 -------\n",
      "epoch: 1192, train_loss: -8.382274627685547, valid_loss: -4.745162487030029\n",
      "-------epoch  1193 -------\n",
      "epoch: 1193, train_loss: -8.39653205871582, valid_loss: -4.722146034240723\n",
      "-------epoch  1194 -------\n",
      "epoch: 1194, train_loss: -8.4193696975708, valid_loss: -4.690759658813477\n",
      "-------epoch  1195 -------\n",
      "epoch: 1195, train_loss: -8.410736083984375, valid_loss: -4.739668369293213\n",
      "-------epoch  1196 -------\n",
      "epoch: 1196, train_loss: -8.400938034057617, valid_loss: -4.69139289855957\n",
      "-------epoch  1197 -------\n",
      "epoch: 1197, train_loss: -8.416833877563477, valid_loss: -4.6896467208862305\n",
      "-------epoch  1198 -------\n",
      "epoch: 1198, train_loss: -8.431053161621094, valid_loss: -4.724738121032715\n",
      "-------epoch  1199 -------\n",
      "epoch: 1199, train_loss: -8.42668628692627, valid_loss: -4.65253210067749\n",
      "-------epoch  1200 -------\n",
      "epoch: 1200, train_loss: -8.420029640197754, valid_loss: -4.68162202835083\n",
      "-------epoch  1201 -------\n",
      "epoch: 1201, train_loss: -8.424684524536133, valid_loss: -4.676449298858643\n",
      "-------epoch  1202 -------\n",
      "epoch: 1202, train_loss: -8.43599796295166, valid_loss: -4.65787935256958\n",
      "-------epoch  1203 -------\n",
      "epoch: 1203, train_loss: -8.445566177368164, valid_loss: -4.674038887023926\n",
      "-------epoch  1204 -------\n",
      "epoch: 1204, train_loss: -8.44526481628418, valid_loss: -4.6346330642700195\n",
      "-------epoch  1205 -------\n",
      "epoch: 1205, train_loss: -8.439199447631836, valid_loss: -4.667844295501709\n",
      "-------epoch  1206 -------\n",
      "epoch: 1206, train_loss: -8.437670707702637, valid_loss: -4.607998847961426\n",
      "-------epoch  1207 -------\n",
      "epoch: 1207, train_loss: -8.439871788024902, valid_loss: -4.686307430267334\n",
      "-------epoch  1208 -------\n",
      "epoch: 1208, train_loss: -8.447800636291504, valid_loss: -4.616057395935059\n",
      "-------epoch  1209 -------\n",
      "epoch: 1209, train_loss: -8.453776359558105, valid_loss: -4.681955337524414\n",
      "-------epoch  1210 -------\n",
      "epoch: 1210, train_loss: -8.452025413513184, valid_loss: -4.595463752746582\n",
      "-------epoch  1211 -------\n",
      "epoch: 1211, train_loss: -8.423651695251465, valid_loss: -4.616683006286621\n",
      "-------epoch  1212 -------\n",
      "epoch: 1212, train_loss: -8.310418128967285, valid_loss: -4.215999126434326\n",
      "-------epoch  1213 -------\n",
      "epoch: 1213, train_loss: -7.958510398864746, valid_loss: -3.9374210834503174\n",
      "-------epoch  1214 -------\n",
      "epoch: 1214, train_loss: -7.219128608703613, valid_loss: -4.497259140014648\n",
      "-------epoch  1215 -------\n",
      "epoch: 1215, train_loss: -7.198269844055176, valid_loss: -4.705312252044678\n",
      "-------epoch  1216 -------\n",
      "epoch: 1216, train_loss: -8.324564933776855, valid_loss: -4.484542369842529\n",
      "-------epoch  1217 -------\n",
      "epoch: 1217, train_loss: -7.87872838973999, valid_loss: -4.256444931030273\n",
      "-------epoch  1218 -------\n",
      "epoch: 1218, train_loss: -7.47157096862793, valid_loss: -4.5608954429626465\n",
      "-------epoch  1219 -------\n",
      "epoch: 1219, train_loss: -8.260461807250977, valid_loss: -4.569126129150391\n",
      "-------epoch  1220 -------\n",
      "epoch: 1220, train_loss: -8.096395492553711, valid_loss: -4.890283584594727\n",
      "-------epoch  1221 -------\n",
      "epoch: 1221, train_loss: -7.907454490661621, valid_loss: -4.613136291503906\n",
      "-------epoch  1222 -------\n",
      "epoch: 1222, train_loss: -8.294801712036133, valid_loss: -4.619957447052002\n",
      "-------epoch  1223 -------\n",
      "epoch: 1223, train_loss: -8.182146072387695, valid_loss: -5.019715785980225\n",
      "-------epoch  1224 -------\n",
      "epoch: 1224, train_loss: -8.07422161102295, valid_loss: -4.913452625274658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1225 -------\n",
      "epoch: 1225, train_loss: -8.359188079833984, valid_loss: -4.691816329956055\n",
      "-------epoch  1226 -------\n",
      "epoch: 1226, train_loss: -8.156049728393555, valid_loss: -4.871449947357178\n",
      "-------epoch  1227 -------\n",
      "epoch: 1227, train_loss: -8.235455513000488, valid_loss: -4.965599060058594\n",
      "-------epoch  1228 -------\n",
      "epoch: 1228, train_loss: -8.361861228942871, valid_loss: -4.829892158508301\n",
      "-------epoch  1229 -------\n",
      "epoch: 1229, train_loss: -8.190323829650879, valid_loss: -5.041962146759033\n",
      "-------epoch  1230 -------\n",
      "epoch: 1230, train_loss: -8.369715690612793, valid_loss: -5.039134502410889\n",
      "-------epoch  1231 -------\n",
      "epoch: 1231, train_loss: -8.300247192382812, valid_loss: -4.895375728607178\n",
      "-------epoch  1232 -------\n",
      "epoch: 1232, train_loss: -8.31666088104248, valid_loss: -4.902971267700195\n",
      "-------epoch  1233 -------\n",
      "epoch: 1233, train_loss: -8.38168716430664, valid_loss: -5.086517333984375\n",
      "-------epoch  1234 -------\n",
      "epoch: 1234, train_loss: -8.302773475646973, valid_loss: -5.020022392272949\n",
      "-------epoch  1235 -------\n",
      "epoch: 1235, train_loss: -8.40865421295166, valid_loss: -4.951507568359375\n",
      "-------epoch  1236 -------\n",
      "epoch: 1236, train_loss: -8.335844039916992, valid_loss: -5.002242088317871\n",
      "-------epoch  1237 -------\n",
      "epoch: 1237, train_loss: -8.399558067321777, valid_loss: -4.970897674560547\n",
      "-------epoch  1238 -------\n",
      "epoch: 1238, train_loss: -8.387152671813965, valid_loss: -4.9140496253967285\n",
      "-------epoch  1239 -------\n",
      "epoch: 1239, train_loss: -8.384014129638672, valid_loss: -4.995032787322998\n",
      "-------epoch  1240 -------\n",
      "epoch: 1240, train_loss: -8.423175811767578, valid_loss: -5.019992828369141\n",
      "-------epoch  1241 -------\n",
      "epoch: 1241, train_loss: -8.393739700317383, valid_loss: -4.9083099365234375\n",
      "-------epoch  1242 -------\n",
      "epoch: 1242, train_loss: -8.435508728027344, valid_loss: -4.898136615753174\n",
      "-------epoch  1243 -------\n",
      "epoch: 1243, train_loss: -8.422639846801758, valid_loss: -4.985132217407227\n",
      "-------epoch  1244 -------\n",
      "epoch: 1244, train_loss: -8.431577682495117, valid_loss: -4.961761951446533\n",
      "-------epoch  1245 -------\n",
      "epoch: 1245, train_loss: -8.451726913452148, valid_loss: -4.881577968597412\n",
      "-------epoch  1246 -------\n",
      "epoch: 1246, train_loss: -8.43467903137207, valid_loss: -4.887773036956787\n",
      "-------epoch  1247 -------\n",
      "epoch: 1247, train_loss: -8.462041854858398, valid_loss: -4.899435043334961\n",
      "-------epoch  1248 -------\n",
      "epoch: 1248, train_loss: -8.45650863647461, valid_loss: -4.892472267150879\n",
      "-------epoch  1249 -------\n",
      "epoch: 1249, train_loss: -8.457555770874023, valid_loss: -4.895253658294678\n",
      "-------epoch  1250 -------\n",
      "epoch: 1250, train_loss: -8.481395721435547, valid_loss: -4.851532936096191\n",
      "-------epoch  1251 -------\n",
      "epoch: 1251, train_loss: -8.462796211242676, valid_loss: -4.856459617614746\n",
      "-------epoch  1252 -------\n",
      "epoch: 1252, train_loss: -8.483177185058594, valid_loss: -4.857019424438477\n",
      "-------epoch  1253 -------\n",
      "epoch: 1253, train_loss: -8.488802909851074, valid_loss: -4.8492207527160645\n",
      "-------epoch  1254 -------\n",
      "epoch: 1254, train_loss: -8.478923797607422, valid_loss: -4.8295087814331055\n",
      "-------epoch  1255 -------\n",
      "epoch: 1255, train_loss: -8.498483657836914, valid_loss: -4.812106609344482\n",
      "-------epoch  1256 -------\n",
      "epoch: 1256, train_loss: -8.497586250305176, valid_loss: -4.834271430969238\n",
      "-------epoch  1257 -------\n",
      "epoch: 1257, train_loss: -8.497293472290039, valid_loss: -4.821403980255127\n",
      "-------epoch  1258 -------\n",
      "epoch: 1258, train_loss: -8.509313583374023, valid_loss: -4.768472671508789\n",
      "-------epoch  1259 -------\n",
      "epoch: 1259, train_loss: -8.508691787719727, valid_loss: -4.809833526611328\n",
      "-------epoch  1260 -------\n",
      "epoch: 1260, train_loss: -8.5099458694458, valid_loss: -4.803229331970215\n",
      "-------epoch  1261 -------\n",
      "epoch: 1261, train_loss: -8.52245044708252, valid_loss: -4.756106853485107\n",
      "-------epoch  1262 -------\n",
      "epoch: 1262, train_loss: -8.520350456237793, valid_loss: -4.7987823486328125\n",
      "-------epoch  1263 -------\n",
      "epoch: 1263, train_loss: -8.522162437438965, valid_loss: -4.776857852935791\n",
      "-------epoch  1264 -------\n",
      "epoch: 1264, train_loss: -8.527505874633789, valid_loss: -4.751466751098633\n",
      "-------epoch  1265 -------\n",
      "epoch: 1265, train_loss: -8.535515785217285, valid_loss: -4.780028820037842\n",
      "-------epoch  1266 -------\n",
      "epoch: 1266, train_loss: -8.536535263061523, valid_loss: -4.773635387420654\n",
      "-------epoch  1267 -------\n",
      "epoch: 1267, train_loss: -8.534784317016602, valid_loss: -4.755592346191406\n",
      "-------epoch  1268 -------\n",
      "epoch: 1268, train_loss: -8.540018081665039, valid_loss: -4.757264137268066\n",
      "-------epoch  1269 -------\n",
      "epoch: 1269, train_loss: -8.547642707824707, valid_loss: -4.773250102996826\n",
      "-------epoch  1270 -------\n",
      "epoch: 1270, train_loss: -8.550225257873535, valid_loss: -4.763864517211914\n",
      "-------epoch  1271 -------\n",
      "epoch: 1271, train_loss: -8.55174732208252, valid_loss: -4.741833686828613\n",
      "-------epoch  1272 -------\n",
      "epoch: 1272, train_loss: -8.551888465881348, valid_loss: -4.763993263244629\n",
      "-------epoch  1273 -------\n",
      "epoch: 1273, train_loss: -8.554601669311523, valid_loss: -4.758875370025635\n",
      "-------epoch  1274 -------\n",
      "epoch: 1274, train_loss: -8.557844161987305, valid_loss: -4.72031831741333\n",
      "-------epoch  1275 -------\n",
      "epoch: 1275, train_loss: -8.558805465698242, valid_loss: -4.784697532653809\n",
      "-------epoch  1276 -------\n",
      "epoch: 1276, train_loss: -8.55528450012207, valid_loss: -4.682234287261963\n",
      "-------epoch  1277 -------\n",
      "epoch: 1277, train_loss: -8.547454833984375, valid_loss: -4.778236389160156\n",
      "-------epoch  1278 -------\n",
      "epoch: 1278, train_loss: -8.52717399597168, valid_loss: -4.624584674835205\n",
      "-------epoch  1279 -------\n",
      "epoch: 1279, train_loss: -8.474530220031738, valid_loss: -4.699568271636963\n",
      "-------epoch  1280 -------\n",
      "epoch: 1280, train_loss: -8.338197708129883, valid_loss: -4.423731327056885\n",
      "-------epoch  1281 -------\n",
      "epoch: 1281, train_loss: -8.106900215148926, valid_loss: -4.417698860168457\n",
      "-------epoch  1282 -------\n",
      "epoch: 1282, train_loss: -7.858804702758789, valid_loss: -4.601017475128174\n",
      "-------epoch  1283 -------\n",
      "epoch: 1283, train_loss: -8.006783485412598, valid_loss: -4.695542335510254\n",
      "-------epoch  1284 -------\n",
      "epoch: 1284, train_loss: -8.387927055358887, valid_loss: -4.760848522186279\n",
      "-------epoch  1285 -------\n",
      "epoch: 1285, train_loss: -8.575239181518555, valid_loss: -4.743333339691162\n",
      "-------epoch  1286 -------\n",
      "epoch: 1286, train_loss: -8.400453567504883, valid_loss: -4.633516788482666\n",
      "-------epoch  1287 -------\n",
      "epoch: 1287, train_loss: -8.256049156188965, valid_loss: -4.830365180969238\n",
      "-------epoch  1288 -------\n",
      "epoch: 1288, train_loss: -8.447687149047852, valid_loss: -4.776494979858398\n",
      "-------epoch  1289 -------\n",
      "epoch: 1289, train_loss: -8.565502166748047, valid_loss: -4.728562831878662\n",
      "-------epoch  1290 -------\n",
      "epoch: 1290, train_loss: -8.448798179626465, valid_loss: -4.875165939331055\n",
      "-------epoch  1291 -------\n",
      "epoch: 1291, train_loss: -8.397411346435547, valid_loss: -4.746569633483887\n",
      "-------epoch  1292 -------\n",
      "epoch: 1292, train_loss: -8.527864456176758, valid_loss: -4.814891815185547\n",
      "-------epoch  1293 -------\n",
      "epoch: 1293, train_loss: -8.553427696228027, valid_loss: -4.891881465911865\n",
      "-------epoch  1294 -------\n",
      "epoch: 1294, train_loss: -8.460626602172852, valid_loss: -4.728128910064697\n",
      "-------epoch  1295 -------\n",
      "epoch: 1295, train_loss: -8.483741760253906, valid_loss: -4.8365678787231445\n",
      "-------epoch  1296 -------\n",
      "epoch: 1296, train_loss: -8.573545455932617, valid_loss: -4.859744071960449\n",
      "-------epoch  1297 -------\n",
      "epoch: 1297, train_loss: -8.529512405395508, valid_loss: -4.73051118850708\n",
      "-------epoch  1298 -------\n",
      "epoch: 1298, train_loss: -8.488786697387695, valid_loss: -4.865682601928711\n",
      "-------epoch  1299 -------\n",
      "epoch: 1299, train_loss: -8.5550537109375, valid_loss: -4.835677146911621\n",
      "-------epoch  1300 -------\n",
      "epoch: 1300, train_loss: -8.576070785522461, valid_loss: -4.730746269226074\n",
      "-------epoch  1301 -------\n",
      "epoch: 1301, train_loss: -8.524782180786133, valid_loss: -4.843958377838135\n",
      "-------epoch  1302 -------\n",
      "epoch: 1302, train_loss: -8.541194915771484, valid_loss: -4.805595397949219\n",
      "-------epoch  1303 -------\n",
      "epoch: 1303, train_loss: -8.585996627807617, valid_loss: -4.75518798828125\n",
      "-------epoch  1304 -------\n",
      "epoch: 1304, train_loss: -8.569679260253906, valid_loss: -4.815118312835693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1305 -------\n",
      "epoch: 1305, train_loss: -8.547465324401855, valid_loss: -4.75269889831543\n",
      "-------epoch  1306 -------\n",
      "epoch: 1306, train_loss: -8.575615882873535, valid_loss: -4.768737316131592\n",
      "-------epoch  1307 -------\n",
      "epoch: 1307, train_loss: -8.595674514770508, valid_loss: -4.7964677810668945\n",
      "-------epoch  1308 -------\n",
      "epoch: 1308, train_loss: -8.575328826904297, valid_loss: -4.713237285614014\n",
      "-------epoch  1309 -------\n",
      "epoch: 1309, train_loss: -8.569937705993652, valid_loss: -4.762498378753662\n",
      "-------epoch  1310 -------\n",
      "epoch: 1310, train_loss: -8.592806816101074, valid_loss: -4.766060829162598\n",
      "-------epoch  1311 -------\n",
      "epoch: 1311, train_loss: -8.601885795593262, valid_loss: -4.692379474639893\n",
      "-------epoch  1312 -------\n",
      "epoch: 1312, train_loss: -8.587974548339844, valid_loss: -4.755688190460205\n",
      "-------epoch  1313 -------\n",
      "epoch: 1313, train_loss: -8.586898803710938, valid_loss: -4.712989807128906\n",
      "-------epoch  1314 -------\n",
      "epoch: 1314, train_loss: -8.601487159729004, valid_loss: -4.695312023162842\n",
      "-------epoch  1315 -------\n",
      "epoch: 1315, train_loss: -8.610705375671387, valid_loss: -4.7313079833984375\n",
      "-------epoch  1316 -------\n",
      "epoch: 1316, train_loss: -8.603432655334473, valid_loss: -4.663846492767334\n",
      "-------epoch  1317 -------\n",
      "epoch: 1317, train_loss: -8.595240592956543, valid_loss: -4.720314979553223\n",
      "-------epoch  1318 -------\n",
      "epoch: 1318, train_loss: -8.602537155151367, valid_loss: -4.64926290512085\n",
      "-------epoch  1319 -------\n",
      "epoch: 1319, train_loss: -8.612086296081543, valid_loss: -4.705036163330078\n",
      "-------epoch  1320 -------\n",
      "epoch: 1320, train_loss: -8.61250114440918, valid_loss: -4.656948089599609\n",
      "-------epoch  1321 -------\n",
      "epoch: 1321, train_loss: -8.598213195800781, valid_loss: -4.677755355834961\n",
      "-------epoch  1322 -------\n",
      "epoch: 1322, train_loss: -8.567947387695312, valid_loss: -4.56620979309082\n",
      "-------epoch  1323 -------\n",
      "epoch: 1323, train_loss: -8.500795364379883, valid_loss: -4.674841403961182\n",
      "-------epoch  1324 -------\n",
      "epoch: 1324, train_loss: -8.335498809814453, valid_loss: -4.45163631439209\n",
      "-------epoch  1325 -------\n",
      "epoch: 1325, train_loss: -8.105243682861328, valid_loss: -4.512582302093506\n",
      "-------epoch  1326 -------\n",
      "epoch: 1326, train_loss: -8.050840377807617, valid_loss: -4.553812503814697\n",
      "-------epoch  1327 -------\n",
      "epoch: 1327, train_loss: -8.189261436462402, valid_loss: -4.580144882202148\n",
      "-------epoch  1328 -------\n",
      "epoch: 1328, train_loss: -8.352320671081543, valid_loss: -4.7761359214782715\n",
      "-------epoch  1329 -------\n",
      "epoch: 1329, train_loss: -8.576966285705566, valid_loss: -4.7021989822387695\n",
      "-------epoch  1330 -------\n",
      "epoch: 1330, train_loss: -8.571834564208984, valid_loss: -4.614357948303223\n",
      "-------epoch  1331 -------\n",
      "epoch: 1331, train_loss: -8.400666236877441, valid_loss: -4.785532474517822\n",
      "-------epoch  1332 -------\n",
      "epoch: 1332, train_loss: -8.394895553588867, valid_loss: -4.695267200469971\n",
      "-------epoch  1333 -------\n",
      "epoch: 1333, train_loss: -8.544068336486816, valid_loss: -4.737231731414795\n",
      "-------epoch  1334 -------\n",
      "epoch: 1334, train_loss: -8.608014106750488, valid_loss: -4.796955585479736\n",
      "-------epoch  1335 -------\n",
      "epoch: 1335, train_loss: -8.529574394226074, valid_loss: -4.670741081237793\n",
      "-------epoch  1336 -------\n",
      "epoch: 1336, train_loss: -8.470311164855957, valid_loss: -4.7663044929504395\n",
      "-------epoch  1337 -------\n",
      "epoch: 1337, train_loss: -8.559135437011719, valid_loss: -4.775790214538574\n",
      "-------epoch  1338 -------\n",
      "epoch: 1338, train_loss: -8.614763259887695, valid_loss: -4.712263584136963\n",
      "-------epoch  1339 -------\n",
      "epoch: 1339, train_loss: -8.561956405639648, valid_loss: -4.7815022468566895\n",
      "-------epoch  1340 -------\n",
      "epoch: 1340, train_loss: -8.523717880249023, valid_loss: -4.734241962432861\n",
      "-------epoch  1341 -------\n",
      "epoch: 1341, train_loss: -8.579355239868164, valid_loss: -4.770298004150391\n",
      "-------epoch  1342 -------\n",
      "epoch: 1342, train_loss: -8.621676445007324, valid_loss: -4.777292251586914\n",
      "-------epoch  1343 -------\n",
      "epoch: 1343, train_loss: -8.586060523986816, valid_loss: -4.7196855545043945\n",
      "-------epoch  1344 -------\n",
      "epoch: 1344, train_loss: -8.561697006225586, valid_loss: -4.782869338989258\n",
      "-------epoch  1345 -------\n",
      "epoch: 1345, train_loss: -8.595504760742188, valid_loss: -4.748188495635986\n",
      "-------epoch  1346 -------\n",
      "epoch: 1346, train_loss: -8.62564754486084, valid_loss: -4.755204200744629\n",
      "-------epoch  1347 -------\n",
      "epoch: 1347, train_loss: -8.6104097366333, valid_loss: -4.7786784172058105\n",
      "-------epoch  1348 -------\n",
      "epoch: 1348, train_loss: -8.587860107421875, valid_loss: -4.7077717781066895\n",
      "-------epoch  1349 -------\n",
      "epoch: 1349, train_loss: -8.604608535766602, valid_loss: -4.757590293884277\n",
      "-------epoch  1350 -------\n",
      "epoch: 1350, train_loss: -8.63169002532959, valid_loss: -4.766899585723877\n",
      "-------epoch  1351 -------\n",
      "epoch: 1351, train_loss: -8.628314971923828, valid_loss: -4.6959733963012695\n",
      "-------epoch  1352 -------\n",
      "epoch: 1352, train_loss: -8.611687660217285, valid_loss: -4.723540782928467\n",
      "-------epoch  1353 -------\n",
      "epoch: 1353, train_loss: -8.61158561706543, valid_loss: -4.728122711181641\n",
      "-------epoch  1354 -------\n",
      "epoch: 1354, train_loss: -8.626693725585938, valid_loss: -4.689650535583496\n",
      "-------epoch  1355 -------\n",
      "epoch: 1355, train_loss: -8.637646675109863, valid_loss: -4.713371753692627\n",
      "-------epoch  1356 -------\n",
      "epoch: 1356, train_loss: -8.635658264160156, valid_loss: -4.683636665344238\n",
      "-------epoch  1357 -------\n",
      "epoch: 1357, train_loss: -8.631421089172363, valid_loss: -4.682865142822266\n",
      "-------epoch  1358 -------\n",
      "epoch: 1358, train_loss: -8.630415916442871, valid_loss: -4.6922454833984375\n",
      "-------epoch  1359 -------\n",
      "epoch: 1359, train_loss: -8.626837730407715, valid_loss: -4.635154724121094\n",
      "-------epoch  1360 -------\n",
      "epoch: 1360, train_loss: -8.622720718383789, valid_loss: -4.71425724029541\n",
      "-------epoch  1361 -------\n",
      "epoch: 1361, train_loss: -8.622170448303223, valid_loss: -4.613419532775879\n",
      "-------epoch  1362 -------\n",
      "epoch: 1362, train_loss: -8.625017166137695, valid_loss: -4.688708305358887\n",
      "-------epoch  1363 -------\n",
      "epoch: 1363, train_loss: -8.629754066467285, valid_loss: -4.641827583312988\n",
      "-------epoch  1364 -------\n",
      "epoch: 1364, train_loss: -8.62926197052002, valid_loss: -4.654937267303467\n",
      "-------epoch  1365 -------\n",
      "epoch: 1365, train_loss: -8.622390747070312, valid_loss: -4.635904312133789\n",
      "-------epoch  1366 -------\n",
      "epoch: 1366, train_loss: -8.611757278442383, valid_loss: -4.656824111938477\n",
      "-------epoch  1367 -------\n",
      "epoch: 1367, train_loss: -8.591933250427246, valid_loss: -4.583003044128418\n",
      "-------epoch  1368 -------\n",
      "epoch: 1368, train_loss: -8.554593086242676, valid_loss: -4.655447006225586\n",
      "-------epoch  1369 -------\n",
      "epoch: 1369, train_loss: -8.48161506652832, valid_loss: -4.5334649085998535\n",
      "-------epoch  1370 -------\n",
      "epoch: 1370, train_loss: -8.387700080871582, valid_loss: -4.57630729675293\n",
      "-------epoch  1371 -------\n",
      "epoch: 1371, train_loss: -8.284235000610352, valid_loss: -4.5481038093566895\n",
      "-------epoch  1372 -------\n",
      "epoch: 1372, train_loss: -8.271747589111328, valid_loss: -4.556013107299805\n",
      "-------epoch  1373 -------\n",
      "epoch: 1373, train_loss: -8.341503143310547, valid_loss: -4.66227388381958\n",
      "-------epoch  1374 -------\n",
      "epoch: 1374, train_loss: -8.519363403320312, valid_loss: -4.662957191467285\n",
      "-------epoch  1375 -------\n",
      "epoch: 1375, train_loss: -8.636199951171875, valid_loss: -4.714631080627441\n",
      "-------epoch  1376 -------\n",
      "epoch: 1376, train_loss: -8.638422012329102, valid_loss: -4.688395977020264\n",
      "-------epoch  1377 -------\n",
      "epoch: 1377, train_loss: -8.565327644348145, valid_loss: -4.605545520782471\n",
      "-------epoch  1378 -------\n",
      "epoch: 1378, train_loss: -8.504383087158203, valid_loss: -4.735809803009033\n",
      "-------epoch  1379 -------\n",
      "epoch: 1379, train_loss: -8.550588607788086, valid_loss: -4.672317981719971\n",
      "-------epoch  1380 -------\n",
      "epoch: 1380, train_loss: -8.631559371948242, valid_loss: -4.699141502380371\n",
      "-------epoch  1381 -------\n",
      "epoch: 1381, train_loss: -8.652788162231445, valid_loss: -4.735358715057373\n",
      "-------epoch  1382 -------\n",
      "epoch: 1382, train_loss: -8.60722827911377, valid_loss: -4.634070873260498\n",
      "-------epoch  1383 -------\n",
      "epoch: 1383, train_loss: -8.568802833557129, valid_loss: -4.7157182693481445\n",
      "-------epoch  1384 -------\n",
      "epoch: 1384, train_loss: -8.598139762878418, valid_loss: -4.694148540496826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1385 -------\n",
      "epoch: 1385, train_loss: -8.644881248474121, valid_loss: -4.695674896240234\n",
      "-------epoch  1386 -------\n",
      "epoch: 1386, train_loss: -8.65501594543457, valid_loss: -4.713292598724365\n",
      "-------epoch  1387 -------\n",
      "epoch: 1387, train_loss: -8.627264022827148, valid_loss: -4.655310153961182\n",
      "-------epoch  1388 -------\n",
      "epoch: 1388, train_loss: -8.60675048828125, valid_loss: -4.716076374053955\n",
      "-------epoch  1389 -------\n",
      "epoch: 1389, train_loss: -8.623202323913574, valid_loss: -4.683560848236084\n",
      "-------epoch  1390 -------\n",
      "epoch: 1390, train_loss: -8.651713371276855, valid_loss: -4.68768835067749\n",
      "-------epoch  1391 -------\n",
      "epoch: 1391, train_loss: -8.661104202270508, valid_loss: -4.70943021774292\n",
      "-------epoch  1392 -------\n",
      "epoch: 1392, train_loss: -8.647028923034668, valid_loss: -4.658520698547363\n",
      "-------epoch  1393 -------\n",
      "epoch: 1393, train_loss: -8.632062911987305, valid_loss: -4.6858391761779785\n",
      "-------epoch  1394 -------\n",
      "epoch: 1394, train_loss: -8.63550090789795, valid_loss: -4.674164772033691\n",
      "-------epoch  1395 -------\n",
      "epoch: 1395, train_loss: -8.650596618652344, valid_loss: -4.6695051193237305\n",
      "-------epoch  1396 -------\n",
      "epoch: 1396, train_loss: -8.664233207702637, valid_loss: -4.675554275512695\n",
      "-------epoch  1397 -------\n",
      "epoch: 1397, train_loss: -8.66649055480957, valid_loss: -4.65431022644043\n",
      "-------epoch  1398 -------\n",
      "epoch: 1398, train_loss: -8.659197807312012, valid_loss: -4.655158996582031\n",
      "-------epoch  1399 -------\n",
      "epoch: 1399, train_loss: -8.650838851928711, valid_loss: -4.645197868347168\n",
      "-------epoch  1400 -------\n",
      "epoch: 1400, train_loss: -8.646039009094238, valid_loss: -4.634213924407959\n",
      "-------epoch  1401 -------\n",
      "epoch: 1401, train_loss: -8.646932601928711, valid_loss: -4.646251201629639\n",
      "-------epoch  1402 -------\n",
      "epoch: 1402, train_loss: -8.65117073059082, valid_loss: -4.617147922515869\n",
      "-------epoch  1403 -------\n",
      "epoch: 1403, train_loss: -8.65685749053955, valid_loss: -4.645491600036621\n",
      "-------epoch  1404 -------\n",
      "epoch: 1404, train_loss: -8.662214279174805, valid_loss: -4.613871097564697\n",
      "-------epoch  1405 -------\n",
      "epoch: 1405, train_loss: -8.666974067687988, valid_loss: -4.630862712860107\n",
      "-------epoch  1406 -------\n",
      "epoch: 1406, train_loss: -8.670246124267578, valid_loss: -4.6216816902160645\n",
      "-------epoch  1407 -------\n",
      "epoch: 1407, train_loss: -8.672148704528809, valid_loss: -4.61822509765625\n",
      "-------epoch  1408 -------\n",
      "epoch: 1408, train_loss: -8.672971725463867, valid_loss: -4.618265151977539\n",
      "-------epoch  1409 -------\n",
      "epoch: 1409, train_loss: -8.672745704650879, valid_loss: -4.616837501525879\n",
      "-------epoch  1410 -------\n",
      "epoch: 1410, train_loss: -8.670838356018066, valid_loss: -4.606047630310059\n",
      "-------epoch  1411 -------\n",
      "epoch: 1411, train_loss: -8.665752410888672, valid_loss: -4.617051601409912\n",
      "-------epoch  1412 -------\n",
      "epoch: 1412, train_loss: -8.652302742004395, valid_loss: -4.580776214599609\n",
      "-------epoch  1413 -------\n",
      "epoch: 1413, train_loss: -8.620327949523926, valid_loss: -4.597525119781494\n",
      "-------epoch  1414 -------\n",
      "epoch: 1414, train_loss: -8.541621208190918, valid_loss: -4.5031609535217285\n",
      "-------epoch  1415 -------\n",
      "epoch: 1415, train_loss: -8.38070011138916, valid_loss: -4.448111534118652\n",
      "-------epoch  1416 -------\n",
      "epoch: 1416, train_loss: -8.078585624694824, valid_loss: -4.4058332443237305\n",
      "-------epoch  1417 -------\n",
      "epoch: 1417, train_loss: -7.9071455001831055, valid_loss: -4.414205551147461\n",
      "-------epoch  1418 -------\n",
      "epoch: 1418, train_loss: -8.034770965576172, valid_loss: -4.607762813568115\n",
      "-------epoch  1419 -------\n",
      "epoch: 1419, train_loss: -8.48397159576416, valid_loss: -4.620731830596924\n",
      "-------epoch  1420 -------\n",
      "epoch: 1420, train_loss: -8.669798851013184, valid_loss: -4.655706882476807\n",
      "-------epoch  1421 -------\n",
      "epoch: 1421, train_loss: -8.552513122558594, valid_loss: -4.664913177490234\n",
      "-------epoch  1422 -------\n",
      "epoch: 1422, train_loss: -8.411953926086426, valid_loss: -4.543989181518555\n",
      "-------epoch  1423 -------\n",
      "epoch: 1423, train_loss: -8.46030330657959, valid_loss: -4.7349138259887695\n",
      "-------epoch  1424 -------\n",
      "epoch: 1424, train_loss: -8.625373840332031, valid_loss: -4.698821067810059\n",
      "-------epoch  1425 -------\n",
      "epoch: 1425, train_loss: -8.61355972290039, valid_loss: -4.604601860046387\n",
      "-------epoch  1426 -------\n",
      "epoch: 1426, train_loss: -8.54113483428955, valid_loss: -4.7223711013793945\n",
      "-------epoch  1427 -------\n",
      "epoch: 1427, train_loss: -8.555939674377441, valid_loss: -4.67218017578125\n",
      "-------epoch  1428 -------\n",
      "epoch: 1428, train_loss: -8.595036506652832, valid_loss: -4.697323799133301\n",
      "-------epoch  1429 -------\n",
      "epoch: 1429, train_loss: -8.634758949279785, valid_loss: -4.707120418548584\n",
      "-------epoch  1430 -------\n",
      "epoch: 1430, train_loss: -8.610600471496582, valid_loss: -4.639084339141846\n",
      "-------epoch  1431 -------\n",
      "epoch: 1431, train_loss: -8.564371109008789, valid_loss: -4.74346399307251\n",
      "-------epoch  1432 -------\n",
      "epoch: 1432, train_loss: -8.623608589172363, valid_loss: -4.702916622161865\n",
      "-------epoch  1433 -------\n",
      "epoch: 1433, train_loss: -8.664754867553711, valid_loss: -4.645782470703125\n",
      "-------epoch  1434 -------\n",
      "epoch: 1434, train_loss: -8.61860466003418, valid_loss: -4.747714996337891\n",
      "-------epoch  1435 -------\n",
      "epoch: 1435, train_loss: -8.605581283569336, valid_loss: -4.680760860443115\n",
      "-------epoch  1436 -------\n",
      "epoch: 1436, train_loss: -8.656904220581055, valid_loss: -4.679565906524658\n",
      "-------epoch  1437 -------\n",
      "epoch: 1437, train_loss: -8.67059326171875, valid_loss: -4.728027820587158\n",
      "-------epoch  1438 -------\n",
      "epoch: 1438, train_loss: -8.634297370910645, valid_loss: -4.650496959686279\n",
      "-------epoch  1439 -------\n",
      "epoch: 1439, train_loss: -8.632855415344238, valid_loss: -4.690647602081299\n",
      "-------epoch  1440 -------\n",
      "epoch: 1440, train_loss: -8.671016693115234, valid_loss: -4.6862640380859375\n",
      "-------epoch  1441 -------\n",
      "epoch: 1441, train_loss: -8.670494079589844, valid_loss: -4.639595985412598\n",
      "-------epoch  1442 -------\n",
      "epoch: 1442, train_loss: -8.650594711303711, valid_loss: -4.681548595428467\n",
      "-------epoch  1443 -------\n",
      "epoch: 1443, train_loss: -8.659124374389648, valid_loss: -4.629800796508789\n",
      "-------epoch  1444 -------\n",
      "epoch: 1444, train_loss: -8.674193382263184, valid_loss: -4.629803657531738\n",
      "-------epoch  1445 -------\n",
      "epoch: 1445, train_loss: -8.675724029541016, valid_loss: -4.662125587463379\n",
      "-------epoch  1446 -------\n",
      "epoch: 1446, train_loss: -8.671921730041504, valid_loss: -4.592086315155029\n",
      "-------epoch  1447 -------\n",
      "epoch: 1447, train_loss: -8.673388481140137, valid_loss: -4.614046573638916\n",
      "-------epoch  1448 -------\n",
      "epoch: 1448, train_loss: -8.676858901977539, valid_loss: -4.623507022857666\n",
      "-------epoch  1449 -------\n",
      "epoch: 1449, train_loss: -8.67717170715332, valid_loss: -4.575836658477783\n",
      "-------epoch  1450 -------\n",
      "epoch: 1450, train_loss: -8.680521965026855, valid_loss: -4.610362529754639\n",
      "-------epoch  1451 -------\n",
      "epoch: 1451, train_loss: -8.687139511108398, valid_loss: -4.587653160095215\n",
      "-------epoch  1452 -------\n",
      "epoch: 1452, train_loss: -8.687877655029297, valid_loss: -4.568531513214111\n",
      "-------epoch  1453 -------\n",
      "epoch: 1453, train_loss: -8.68172550201416, valid_loss: -4.59483003616333\n",
      "-------epoch  1454 -------\n",
      "epoch: 1454, train_loss: -8.677796363830566, valid_loss: -4.558267593383789\n",
      "-------epoch  1455 -------\n",
      "epoch: 1455, train_loss: -8.68211841583252, valid_loss: -4.582982540130615\n",
      "-------epoch  1456 -------\n",
      "epoch: 1456, train_loss: -8.690299034118652, valid_loss: -4.568859577178955\n",
      "-------epoch  1457 -------\n",
      "epoch: 1457, train_loss: -8.695058822631836, valid_loss: -4.553325176239014\n",
      "-------epoch  1458 -------\n",
      "epoch: 1458, train_loss: -8.695210456848145, valid_loss: -4.583390712738037\n",
      "-------epoch  1459 -------\n",
      "epoch: 1459, train_loss: -8.694433212280273, valid_loss: -4.544307708740234\n",
      "-------epoch  1460 -------\n",
      "epoch: 1460, train_loss: -8.69460678100586, valid_loss: -4.5646233558654785\n",
      "-------epoch  1461 -------\n",
      "epoch: 1461, train_loss: -8.69427490234375, valid_loss: -4.552586555480957\n",
      "-------epoch  1462 -------\n",
      "epoch: 1462, train_loss: -8.691261291503906, valid_loss: -4.540335655212402\n",
      "-------epoch  1463 -------\n",
      "epoch: 1463, train_loss: -8.684710502624512, valid_loss: -4.551267147064209\n",
      "-------epoch  1464 -------\n",
      "epoch: 1464, train_loss: -8.675228118896484, valid_loss: -4.526663303375244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1465 -------\n",
      "epoch: 1465, train_loss: -8.659345626831055, valid_loss: -4.5211920738220215\n",
      "-------epoch  1466 -------\n",
      "epoch: 1466, train_loss: -8.632694244384766, valid_loss: -4.515151023864746\n",
      "-------epoch  1467 -------\n",
      "epoch: 1467, train_loss: -8.578213691711426, valid_loss: -4.468926906585693\n",
      "-------epoch  1468 -------\n",
      "epoch: 1468, train_loss: -8.494592666625977, valid_loss: -4.455069065093994\n",
      "-------epoch  1469 -------\n",
      "epoch: 1469, train_loss: -8.359990119934082, valid_loss: -4.429174900054932\n",
      "-------epoch  1470 -------\n",
      "epoch: 1470, train_loss: -8.273530006408691, valid_loss: -4.414985656738281\n",
      "-------epoch  1471 -------\n",
      "epoch: 1471, train_loss: -8.261849403381348, valid_loss: -4.497490406036377\n",
      "-------epoch  1472 -------\n",
      "epoch: 1472, train_loss: -8.450798988342285, valid_loss: -4.524317741394043\n",
      "-------epoch  1473 -------\n",
      "epoch: 1473, train_loss: -8.626276016235352, valid_loss: -4.599985122680664\n",
      "-------epoch  1474 -------\n",
      "epoch: 1474, train_loss: -8.697260856628418, valid_loss: -4.552553653717041\n",
      "-------epoch  1475 -------\n",
      "epoch: 1475, train_loss: -8.654193878173828, valid_loss: -4.527222156524658\n",
      "-------epoch  1476 -------\n",
      "epoch: 1476, train_loss: -8.579976081848145, valid_loss: -4.599005222320557\n",
      "-------epoch  1477 -------\n",
      "epoch: 1477, train_loss: -8.569342613220215, valid_loss: -4.513068675994873\n",
      "-------epoch  1478 -------\n",
      "epoch: 1478, train_loss: -8.620811462402344, valid_loss: -4.60593843460083\n",
      "-------epoch  1479 -------\n",
      "epoch: 1479, train_loss: -8.6799955368042, valid_loss: -4.591712474822998\n",
      "-------epoch  1480 -------\n",
      "epoch: 1480, train_loss: -8.682497024536133, valid_loss: -4.542550563812256\n",
      "-------epoch  1481 -------\n",
      "epoch: 1481, train_loss: -8.64841079711914, valid_loss: -4.601415634155273\n",
      "-------epoch  1482 -------\n",
      "epoch: 1482, train_loss: -8.62778091430664, valid_loss: -4.532063007354736\n",
      "-------epoch  1483 -------\n",
      "epoch: 1483, train_loss: -8.645716667175293, valid_loss: -4.606966018676758\n",
      "-------epoch  1484 -------\n",
      "epoch: 1484, train_loss: -8.689004898071289, valid_loss: -4.583459377288818\n",
      "-------epoch  1485 -------\n",
      "epoch: 1485, train_loss: -8.696287155151367, valid_loss: -4.538104057312012\n",
      "-------epoch  1486 -------\n",
      "epoch: 1486, train_loss: -8.668182373046875, valid_loss: -4.61145544052124\n",
      "-------epoch  1487 -------\n",
      "epoch: 1487, train_loss: -8.653751373291016, valid_loss: -4.535046577453613\n",
      "-------epoch  1488 -------\n",
      "epoch: 1488, train_loss: -8.6741361618042, valid_loss: -4.579112529754639\n",
      "-------epoch  1489 -------\n",
      "epoch: 1489, train_loss: -8.702128410339355, valid_loss: -4.584151268005371\n",
      "-------epoch  1490 -------\n",
      "epoch: 1490, train_loss: -8.700390815734863, valid_loss: -4.528017044067383\n",
      "-------epoch  1491 -------\n",
      "epoch: 1491, train_loss: -8.680850982666016, valid_loss: -4.577132225036621\n",
      "-------epoch  1492 -------\n",
      "epoch: 1492, train_loss: -8.676706314086914, valid_loss: -4.5269269943237305\n",
      "-------epoch  1493 -------\n",
      "epoch: 1493, train_loss: -8.690226554870605, valid_loss: -4.549260139465332\n",
      "-------epoch  1494 -------\n",
      "epoch: 1494, train_loss: -8.704937934875488, valid_loss: -4.545594215393066\n",
      "-------epoch  1495 -------\n",
      "epoch: 1495, train_loss: -8.705543518066406, valid_loss: -4.506289482116699\n",
      "-------epoch  1496 -------\n",
      "epoch: 1496, train_loss: -8.69820785522461, valid_loss: -4.543594837188721\n",
      "-------epoch  1497 -------\n",
      "epoch: 1497, train_loss: -8.694774627685547, valid_loss: -4.494651794433594\n",
      "-------epoch  1498 -------\n",
      "epoch: 1498, train_loss: -8.696943283081055, valid_loss: -4.508255958557129\n",
      "-------epoch  1499 -------\n",
      "epoch: 1499, train_loss: -8.702194213867188, valid_loss: -4.512862682342529\n",
      "-------epoch  1500 -------\n",
      "epoch: 1500, train_loss: -8.705789566040039, valid_loss: -4.4810471534729\n",
      "-------epoch  1501 -------\n",
      "epoch: 1501, train_loss: -8.708495140075684, valid_loss: -4.505716323852539\n",
      "-------epoch  1502 -------\n",
      "epoch: 1502, train_loss: -8.711119651794434, valid_loss: -4.47857666015625\n",
      "-------epoch  1503 -------\n",
      "epoch: 1503, train_loss: -8.711989402770996, valid_loss: -4.482969284057617\n",
      "-------epoch  1504 -------\n",
      "epoch: 1504, train_loss: -8.710118293762207, valid_loss: -4.481057167053223\n",
      "-------epoch  1505 -------\n",
      "epoch: 1505, train_loss: -8.70590877532959, valid_loss: -4.465238094329834\n",
      "-------epoch  1506 -------\n",
      "epoch: 1506, train_loss: -8.701663970947266, valid_loss: -4.479403495788574\n",
      "-------epoch  1507 -------\n",
      "epoch: 1507, train_loss: -8.698951721191406, valid_loss: -4.456622123718262\n",
      "-------epoch  1508 -------\n",
      "epoch: 1508, train_loss: -8.697855949401855, valid_loss: -4.464997291564941\n",
      "-------epoch  1509 -------\n",
      "epoch: 1509, train_loss: -8.695627212524414, valid_loss: -4.460203647613525\n",
      "-------epoch  1510 -------\n",
      "epoch: 1510, train_loss: -8.6911039352417, valid_loss: -4.441855430603027\n",
      "-------epoch  1511 -------\n",
      "epoch: 1511, train_loss: -8.680953025817871, valid_loss: -4.4589009284973145\n",
      "-------epoch  1512 -------\n",
      "epoch: 1512, train_loss: -8.666244506835938, valid_loss: -4.420716762542725\n",
      "-------epoch  1513 -------\n",
      "epoch: 1513, train_loss: -8.640290260314941, valid_loss: -4.435009002685547\n",
      "-------epoch  1514 -------\n",
      "epoch: 1514, train_loss: -8.607060432434082, valid_loss: -4.399925708770752\n",
      "-------epoch  1515 -------\n",
      "epoch: 1515, train_loss: -8.549206733703613, valid_loss: -4.394941329956055\n",
      "-------epoch  1516 -------\n",
      "epoch: 1516, train_loss: -8.492687225341797, valid_loss: -4.374475955963135\n",
      "-------epoch  1517 -------\n",
      "epoch: 1517, train_loss: -8.424168586730957, valid_loss: -4.387832164764404\n",
      "-------epoch  1518 -------\n",
      "epoch: 1518, train_loss: -8.431933403015137, valid_loss: -4.391488075256348\n",
      "-------epoch  1519 -------\n",
      "epoch: 1519, train_loss: -8.484707832336426, valid_loss: -4.4482645988464355\n",
      "-------epoch  1520 -------\n",
      "epoch: 1520, train_loss: -8.600808143615723, valid_loss: -4.437691688537598\n",
      "-------epoch  1521 -------\n",
      "epoch: 1521, train_loss: -8.68687629699707, valid_loss: -4.493373870849609\n",
      "-------epoch  1522 -------\n",
      "epoch: 1522, train_loss: -8.715221405029297, valid_loss: -4.460216522216797\n",
      "-------epoch  1523 -------\n",
      "epoch: 1523, train_loss: -8.693178176879883, valid_loss: -4.4504594802856445\n",
      "-------epoch  1524 -------\n",
      "epoch: 1524, train_loss: -8.658989906311035, valid_loss: -4.482499599456787\n",
      "-------epoch  1525 -------\n",
      "epoch: 1525, train_loss: -8.647430419921875, valid_loss: -4.423564910888672\n",
      "-------epoch  1526 -------\n",
      "epoch: 1526, train_loss: -8.65576457977295, valid_loss: -4.495334625244141\n",
      "-------epoch  1527 -------\n",
      "epoch: 1527, train_loss: -8.681039810180664, valid_loss: -4.451176643371582\n",
      "-------epoch  1528 -------\n",
      "epoch: 1528, train_loss: -8.699823379516602, valid_loss: -4.484498500823975\n",
      "-------epoch  1529 -------\n",
      "epoch: 1529, train_loss: -8.711058616638184, valid_loss: -4.476222515106201\n",
      "-------epoch  1530 -------\n",
      "epoch: 1530, train_loss: -8.708515167236328, valid_loss: -4.438828468322754\n",
      "-------epoch  1531 -------\n",
      "epoch: 1531, train_loss: -8.693099975585938, valid_loss: -4.500003814697266\n",
      "-------epoch  1532 -------\n",
      "epoch: 1532, train_loss: -8.682563781738281, valid_loss: -4.429930210113525\n",
      "-------epoch  1533 -------\n",
      "epoch: 1533, train_loss: -8.687752723693848, valid_loss: -4.484447479248047\n",
      "-------epoch  1534 -------\n",
      "epoch: 1534, train_loss: -8.709929466247559, valid_loss: -4.460764408111572\n",
      "-------epoch  1535 -------\n",
      "epoch: 1535, train_loss: -8.724153518676758, valid_loss: -4.448280334472656\n",
      "-------epoch  1536 -------\n",
      "epoch: 1536, train_loss: -8.721829414367676, valid_loss: -4.472644329071045\n",
      "-------epoch  1537 -------\n",
      "epoch: 1537, train_loss: -8.710297584533691, valid_loss: -4.419600963592529\n",
      "-------epoch  1538 -------\n",
      "epoch: 1538, train_loss: -8.701956748962402, valid_loss: -4.463366508483887\n",
      "-------epoch  1539 -------\n",
      "epoch: 1539, train_loss: -8.705404281616211, valid_loss: -4.41385555267334\n",
      "-------epoch  1540 -------\n",
      "epoch: 1540, train_loss: -8.712337493896484, valid_loss: -4.435511589050293\n",
      "-------epoch  1541 -------\n",
      "epoch: 1541, train_loss: -8.718562126159668, valid_loss: -4.426135540008545\n",
      "-------epoch  1542 -------\n",
      "epoch: 1542, train_loss: -8.721760749816895, valid_loss: -4.4071431159973145\n",
      "-------epoch  1543 -------\n",
      "epoch: 1543, train_loss: -8.724189758300781, valid_loss: -4.423696994781494\n",
      "-------epoch  1544 -------\n",
      "epoch: 1544, train_loss: -8.726704597473145, valid_loss: -4.396073341369629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1545 -------\n",
      "epoch: 1545, train_loss: -8.728015899658203, valid_loss: -4.406219482421875\n",
      "-------epoch  1546 -------\n",
      "epoch: 1546, train_loss: -8.726683616638184, valid_loss: -4.389338970184326\n",
      "-------epoch  1547 -------\n",
      "epoch: 1547, train_loss: -8.722414016723633, valid_loss: -4.391294956207275\n",
      "-------epoch  1548 -------\n",
      "epoch: 1548, train_loss: -8.716642379760742, valid_loss: -4.379537105560303\n",
      "-------epoch  1549 -------\n",
      "epoch: 1549, train_loss: -8.709835052490234, valid_loss: -4.380655765533447\n",
      "-------epoch  1550 -------\n",
      "epoch: 1550, train_loss: -8.702479362487793, valid_loss: -4.361844539642334\n",
      "-------epoch  1551 -------\n",
      "epoch: 1551, train_loss: -8.691067695617676, valid_loss: -4.375561237335205\n",
      "-------epoch  1552 -------\n",
      "epoch: 1552, train_loss: -8.675211906433105, valid_loss: -4.330990314483643\n",
      "-------epoch  1553 -------\n",
      "epoch: 1553, train_loss: -8.646921157836914, valid_loss: -4.362924098968506\n",
      "-------epoch  1554 -------\n",
      "epoch: 1554, train_loss: -8.615103721618652, valid_loss: -4.300813674926758\n",
      "-------epoch  1555 -------\n",
      "epoch: 1555, train_loss: -8.566102981567383, valid_loss: -4.332417964935303\n",
      "-------epoch  1556 -------\n",
      "epoch: 1556, train_loss: -8.539506912231445, valid_loss: -4.296736717224121\n",
      "-------epoch  1557 -------\n",
      "epoch: 1557, train_loss: -8.505826950073242, valid_loss: -4.319328784942627\n",
      "-------epoch  1558 -------\n",
      "epoch: 1558, train_loss: -8.52267837524414, valid_loss: -4.325002670288086\n",
      "-------epoch  1559 -------\n",
      "epoch: 1559, train_loss: -8.55151653289795, valid_loss: -4.351977825164795\n",
      "-------epoch  1560 -------\n",
      "epoch: 1560, train_loss: -8.619348526000977, valid_loss: -4.357932090759277\n",
      "-------epoch  1561 -------\n",
      "epoch: 1561, train_loss: -8.683506965637207, valid_loss: -4.381153583526611\n",
      "-------epoch  1562 -------\n",
      "epoch: 1562, train_loss: -8.726961135864258, valid_loss: -4.369283676147461\n",
      "-------epoch  1563 -------\n",
      "epoch: 1563, train_loss: -8.733817100524902, valid_loss: -4.375777244567871\n",
      "-------epoch  1564 -------\n",
      "epoch: 1564, train_loss: -8.71328067779541, valid_loss: -4.370456695556641\n",
      "-------epoch  1565 -------\n",
      "epoch: 1565, train_loss: -8.689815521240234, valid_loss: -4.354052543640137\n",
      "-------epoch  1566 -------\n",
      "epoch: 1566, train_loss: -8.681866645812988, valid_loss: -4.37680196762085\n",
      "-------epoch  1567 -------\n",
      "epoch: 1567, train_loss: -8.694219589233398, valid_loss: -4.348118782043457\n",
      "-------epoch  1568 -------\n",
      "epoch: 1568, train_loss: -8.708390235900879, valid_loss: -4.395364761352539\n",
      "-------epoch  1569 -------\n",
      "epoch: 1569, train_loss: -8.71664810180664, valid_loss: -4.351603984832764\n",
      "-------epoch  1570 -------\n",
      "epoch: 1570, train_loss: -8.71754264831543, valid_loss: -4.390718936920166\n",
      "-------epoch  1571 -------\n",
      "epoch: 1571, train_loss: -8.720707893371582, valid_loss: -4.3702874183654785\n",
      "-------epoch  1572 -------\n",
      "epoch: 1572, train_loss: -8.727603912353516, valid_loss: -4.364099025726318\n",
      "-------epoch  1573 -------\n",
      "epoch: 1573, train_loss: -8.732012748718262, valid_loss: -4.384856700897217\n",
      "-------epoch  1574 -------\n",
      "epoch: 1574, train_loss: -8.72879695892334, valid_loss: -4.346822738647461\n",
      "-------epoch  1575 -------\n",
      "epoch: 1575, train_loss: -8.719696998596191, valid_loss: -4.388054847717285\n",
      "-------epoch  1576 -------\n",
      "epoch: 1576, train_loss: -8.714777946472168, valid_loss: -4.335320472717285\n",
      "-------epoch  1577 -------\n",
      "epoch: 1577, train_loss: -8.717032432556152, valid_loss: -4.37767219543457\n",
      "-------epoch  1578 -------\n",
      "epoch: 1578, train_loss: -8.726367950439453, valid_loss: -4.336674213409424\n",
      "-------epoch  1579 -------\n",
      "epoch: 1579, train_loss: -8.733773231506348, valid_loss: -4.350980281829834\n",
      "-------epoch  1580 -------\n",
      "epoch: 1580, train_loss: -8.736578941345215, valid_loss: -4.342925548553467\n",
      "-------epoch  1581 -------\n",
      "epoch: 1581, train_loss: -8.735591888427734, valid_loss: -4.330714702606201\n",
      "-------epoch  1582 -------\n",
      "epoch: 1582, train_loss: -8.73469066619873, valid_loss: -4.336119651794434\n",
      "-------epoch  1583 -------\n",
      "epoch: 1583, train_loss: -8.736013412475586, valid_loss: -4.322519302368164\n",
      "-------epoch  1584 -------\n",
      "epoch: 1584, train_loss: -8.739435195922852, valid_loss: -4.324654579162598\n",
      "-------epoch  1585 -------\n",
      "epoch: 1585, train_loss: -8.742918968200684, valid_loss: -4.317900657653809\n",
      "-------epoch  1586 -------\n",
      "epoch: 1586, train_loss: -8.74488353729248, valid_loss: -4.3107380867004395\n",
      "-------epoch  1587 -------\n",
      "epoch: 1587, train_loss: -8.744983673095703, valid_loss: -4.319643974304199\n",
      "-------epoch  1588 -------\n",
      "epoch: 1588, train_loss: -8.743986129760742, valid_loss: -4.295447826385498\n",
      "-------epoch  1589 -------\n",
      "epoch: 1589, train_loss: -8.742911338806152, valid_loss: -4.318748474121094\n",
      "-------epoch  1590 -------\n",
      "epoch: 1590, train_loss: -8.742599487304688, valid_loss: -4.2884345054626465\n",
      "-------epoch  1591 -------\n",
      "epoch: 1591, train_loss: -8.743012428283691, valid_loss: -4.311663627624512\n",
      "-------epoch  1592 -------\n",
      "epoch: 1592, train_loss: -8.743732452392578, valid_loss: -4.282618522644043\n",
      "-------epoch  1593 -------\n",
      "epoch: 1593, train_loss: -8.743437767028809, valid_loss: -4.305689811706543\n",
      "-------epoch  1594 -------\n",
      "epoch: 1594, train_loss: -8.740234375, valid_loss: -4.267117977142334\n",
      "-------epoch  1595 -------\n",
      "epoch: 1595, train_loss: -8.728860855102539, valid_loss: -4.29614782333374\n",
      "-------epoch  1596 -------\n",
      "epoch: 1596, train_loss: -8.698955535888672, valid_loss: -4.220029830932617\n",
      "-------epoch  1597 -------\n",
      "epoch: 1597, train_loss: -8.618657112121582, valid_loss: -4.240596771240234\n",
      "-------epoch  1598 -------\n",
      "epoch: 1598, train_loss: -8.466141700744629, valid_loss: -4.105738639831543\n",
      "-------epoch  1599 -------\n",
      "epoch: 1599, train_loss: -8.186361312866211, valid_loss: -4.110507488250732\n",
      "-------epoch  1600 -------\n",
      "epoch: 1600, train_loss: -8.164728164672852, valid_loss: -4.1430487632751465\n",
      "-------epoch  1601 -------\n",
      "epoch: 1601, train_loss: -8.185943603515625, valid_loss: -4.264145374298096\n",
      "-------epoch  1602 -------\n",
      "epoch: 1602, train_loss: -8.425108909606934, valid_loss: -4.2791595458984375\n",
      "-------epoch  1603 -------\n",
      "epoch: 1603, train_loss: -8.634649276733398, valid_loss: -4.338169097900391\n",
      "-------epoch  1604 -------\n",
      "epoch: 1604, train_loss: -8.740379333496094, valid_loss: -4.345755577087402\n",
      "-------epoch  1605 -------\n",
      "epoch: 1605, train_loss: -8.660323143005371, valid_loss: -4.267889022827148\n",
      "-------epoch  1606 -------\n",
      "epoch: 1606, train_loss: -8.534658432006836, valid_loss: -4.329952239990234\n",
      "-------epoch  1607 -------\n",
      "epoch: 1607, train_loss: -8.604351043701172, valid_loss: -4.326084136962891\n",
      "-------epoch  1608 -------\n",
      "epoch: 1608, train_loss: -8.710355758666992, valid_loss: -4.355493545532227\n",
      "-------epoch  1609 -------\n",
      "epoch: 1609, train_loss: -8.720243453979492, valid_loss: -4.3368916511535645\n",
      "-------epoch  1610 -------\n",
      "epoch: 1610, train_loss: -8.652236938476562, valid_loss: -4.316933631896973\n",
      "-------epoch  1611 -------\n",
      "epoch: 1611, train_loss: -8.654179573059082, valid_loss: -4.376938819885254\n",
      "-------epoch  1612 -------\n",
      "epoch: 1612, train_loss: -8.722810745239258, valid_loss: -4.339100360870361\n",
      "-------epoch  1613 -------\n",
      "epoch: 1613, train_loss: -8.719426155090332, valid_loss: -4.350073337554932\n",
      "-------epoch  1614 -------\n",
      "epoch: 1614, train_loss: -8.671137809753418, valid_loss: -4.382745265960693\n",
      "-------epoch  1615 -------\n",
      "epoch: 1615, train_loss: -8.689992904663086, valid_loss: -4.360519886016846\n",
      "-------epoch  1616 -------\n",
      "epoch: 1616, train_loss: -8.735237121582031, valid_loss: -4.334671974182129\n",
      "-------epoch  1617 -------\n",
      "epoch: 1617, train_loss: -8.730353355407715, valid_loss: -4.373778820037842\n",
      "-------epoch  1618 -------\n",
      "epoch: 1618, train_loss: -8.702936172485352, valid_loss: -4.361623287200928\n",
      "-------epoch  1619 -------\n",
      "epoch: 1619, train_loss: -8.714923858642578, valid_loss: -4.334892749786377\n",
      "-------epoch  1620 -------\n",
      "epoch: 1620, train_loss: -8.738880157470703, valid_loss: -4.351480007171631\n",
      "-------epoch  1621 -------\n",
      "epoch: 1621, train_loss: -8.72946834564209, valid_loss: -4.327192306518555\n",
      "-------epoch  1622 -------\n",
      "epoch: 1622, train_loss: -8.71377944946289, valid_loss: -4.338598728179932\n",
      "-------epoch  1623 -------\n",
      "epoch: 1623, train_loss: -8.727585792541504, valid_loss: -4.307475566864014\n",
      "-------epoch  1624 -------\n",
      "epoch: 1624, train_loss: -8.74592113494873, valid_loss: -4.317925930023193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1625 -------\n",
      "epoch: 1625, train_loss: -8.742476463317871, valid_loss: -4.317938327789307\n",
      "-------epoch  1626 -------\n",
      "epoch: 1626, train_loss: -8.730550765991211, valid_loss: -4.278451442718506\n",
      "-------epoch  1627 -------\n",
      "epoch: 1627, train_loss: -8.734813690185547, valid_loss: -4.290680408477783\n",
      "-------epoch  1628 -------\n",
      "epoch: 1628, train_loss: -8.747000694274902, valid_loss: -4.2848358154296875\n",
      "-------epoch  1629 -------\n",
      "epoch: 1629, train_loss: -8.74622917175293, valid_loss: -4.279679775238037\n",
      "-------epoch  1630 -------\n",
      "epoch: 1630, train_loss: -8.738181114196777, valid_loss: -4.256829261779785\n",
      "-------epoch  1631 -------\n",
      "epoch: 1631, train_loss: -8.73587703704834, valid_loss: -4.283151626586914\n",
      "-------epoch  1632 -------\n",
      "epoch: 1632, train_loss: -8.7427339553833, valid_loss: -4.249011039733887\n",
      "-------epoch  1633 -------\n",
      "epoch: 1633, train_loss: -8.75003433227539, valid_loss: -4.255484104156494\n",
      "-------epoch  1634 -------\n",
      "epoch: 1634, train_loss: -8.748555183410645, valid_loss: -4.247492790222168\n",
      "-------epoch  1635 -------\n",
      "epoch: 1635, train_loss: -8.742073059082031, valid_loss: -4.255692005157471\n",
      "-------epoch  1636 -------\n",
      "epoch: 1636, train_loss: -8.738102912902832, valid_loss: -4.224842548370361\n",
      "-------epoch  1637 -------\n",
      "epoch: 1637, train_loss: -8.736960411071777, valid_loss: -4.250285625457764\n",
      "-------epoch  1638 -------\n",
      "epoch: 1638, train_loss: -8.736455917358398, valid_loss: -4.214170455932617\n",
      "-------epoch  1639 -------\n",
      "epoch: 1639, train_loss: -8.728694915771484, valid_loss: -4.252952575683594\n",
      "-------epoch  1640 -------\n",
      "epoch: 1640, train_loss: -8.714592933654785, valid_loss: -4.183943271636963\n",
      "-------epoch  1641 -------\n",
      "epoch: 1641, train_loss: -8.690439224243164, valid_loss: -4.260191917419434\n",
      "-------epoch  1642 -------\n",
      "epoch: 1642, train_loss: -8.673544883728027, valid_loss: -4.154167652130127\n",
      "-------epoch  1643 -------\n",
      "epoch: 1643, train_loss: -8.646655082702637, valid_loss: -4.237621307373047\n",
      "-------epoch  1644 -------\n",
      "epoch: 1644, train_loss: -8.649836540222168, valid_loss: -4.160745143890381\n",
      "-------epoch  1645 -------\n",
      "epoch: 1645, train_loss: -8.638367652893066, valid_loss: -4.206737518310547\n",
      "-------epoch  1646 -------\n",
      "epoch: 1646, train_loss: -8.65179443359375, valid_loss: -4.170524597167969\n",
      "-------epoch  1647 -------\n",
      "epoch: 1647, train_loss: -8.647832870483398, valid_loss: -4.197537899017334\n",
      "-------epoch  1648 -------\n",
      "epoch: 1648, train_loss: -8.657915115356445, valid_loss: -4.182910442352295\n",
      "-------epoch  1649 -------\n",
      "epoch: 1649, train_loss: -8.664694786071777, valid_loss: -4.192126750946045\n",
      "-------epoch  1650 -------\n",
      "epoch: 1650, train_loss: -8.685593605041504, valid_loss: -4.195383071899414\n",
      "-------epoch  1651 -------\n",
      "epoch: 1651, train_loss: -8.70622730255127, valid_loss: -4.208042144775391\n",
      "-------epoch  1652 -------\n",
      "epoch: 1652, train_loss: -8.729110717773438, valid_loss: -4.19202184677124\n",
      "-------epoch  1653 -------\n",
      "epoch: 1653, train_loss: -8.744613647460938, valid_loss: -4.221981048583984\n",
      "-------epoch  1654 -------\n",
      "epoch: 1654, train_loss: -8.753303527832031, valid_loss: -4.193453788757324\n",
      "-------epoch  1655 -------\n",
      "epoch: 1655, train_loss: -8.755106925964355, valid_loss: -4.2222819328308105\n",
      "-------epoch  1656 -------\n",
      "epoch: 1656, train_loss: -8.753655433654785, valid_loss: -4.199224472045898\n",
      "-------epoch  1657 -------\n",
      "epoch: 1657, train_loss: -8.752118110656738, valid_loss: -4.219931125640869\n",
      "-------epoch  1658 -------\n",
      "epoch: 1658, train_loss: -8.752119064331055, valid_loss: -4.203484535217285\n",
      "-------epoch  1659 -------\n",
      "epoch: 1659, train_loss: -8.75373363494873, valid_loss: -4.209430694580078\n",
      "-------epoch  1660 -------\n",
      "epoch: 1660, train_loss: -8.755194664001465, valid_loss: -4.217883110046387\n",
      "-------epoch  1661 -------\n",
      "epoch: 1661, train_loss: -8.755101203918457, valid_loss: -4.193172931671143\n",
      "-------epoch  1662 -------\n",
      "epoch: 1662, train_loss: -8.752293586730957, valid_loss: -4.225466251373291\n",
      "-------epoch  1663 -------\n",
      "epoch: 1663, train_loss: -8.748257637023926, valid_loss: -4.182205677032471\n",
      "-------epoch  1664 -------\n",
      "epoch: 1664, train_loss: -8.742807388305664, valid_loss: -4.223082065582275\n",
      "-------epoch  1665 -------\n",
      "epoch: 1665, train_loss: -8.7398099899292, valid_loss: -4.171071529388428\n",
      "-------epoch  1666 -------\n",
      "epoch: 1666, train_loss: -8.736291885375977, valid_loss: -4.216093063354492\n",
      "-------epoch  1667 -------\n",
      "epoch: 1667, train_loss: -8.735747337341309, valid_loss: -4.160406589508057\n",
      "-------epoch  1668 -------\n",
      "epoch: 1668, train_loss: -8.731706619262695, valid_loss: -4.199631690979004\n",
      "-------epoch  1669 -------\n",
      "epoch: 1669, train_loss: -8.727598190307617, valid_loss: -4.153123378753662\n",
      "-------epoch  1670 -------\n",
      "epoch: 1670, train_loss: -8.715904235839844, valid_loss: -4.180910587310791\n",
      "-------epoch  1671 -------\n",
      "epoch: 1671, train_loss: -8.702201843261719, valid_loss: -4.136916637420654\n",
      "-------epoch  1672 -------\n",
      "epoch: 1672, train_loss: -8.677083969116211, valid_loss: -4.164670467376709\n",
      "-------epoch  1673 -------\n",
      "epoch: 1673, train_loss: -8.653851509094238, valid_loss: -4.119004726409912\n",
      "-------epoch  1674 -------\n",
      "epoch: 1674, train_loss: -8.619712829589844, valid_loss: -4.147195816040039\n",
      "-------epoch  1675 -------\n",
      "epoch: 1675, train_loss: -8.607026100158691, valid_loss: -4.116267681121826\n",
      "-------epoch  1676 -------\n",
      "epoch: 1676, train_loss: -8.59564208984375, valid_loss: -4.151479244232178\n",
      "-------epoch  1677 -------\n",
      "epoch: 1677, train_loss: -8.623885154724121, valid_loss: -4.133739471435547\n",
      "-------epoch  1678 -------\n",
      "epoch: 1678, train_loss: -8.658089637756348, valid_loss: -4.174758434295654\n",
      "-------epoch  1679 -------\n",
      "epoch: 1679, train_loss: -8.707015991210938, valid_loss: -4.158658981323242\n",
      "-------epoch  1680 -------\n",
      "epoch: 1680, train_loss: -8.744582176208496, valid_loss: -4.178499221801758\n",
      "-------epoch  1681 -------\n",
      "epoch: 1681, train_loss: -8.765894889831543, valid_loss: -4.175714015960693\n",
      "-------epoch  1682 -------\n",
      "epoch: 1682, train_loss: -8.767578125, valid_loss: -4.161022663116455\n",
      "-------epoch  1683 -------\n",
      "epoch: 1683, train_loss: -8.754705429077148, valid_loss: -4.174917697906494\n",
      "-------epoch  1684 -------\n",
      "epoch: 1684, train_loss: -8.737818717956543, valid_loss: -4.147680282592773\n",
      "-------epoch  1685 -------\n",
      "epoch: 1685, train_loss: -8.726128578186035, valid_loss: -4.168519020080566\n",
      "-------epoch  1686 -------\n",
      "epoch: 1686, train_loss: -8.728384971618652, valid_loss: -4.145275115966797\n",
      "-------epoch  1687 -------\n",
      "epoch: 1687, train_loss: -8.738984107971191, valid_loss: -4.173720359802246\n",
      "-------epoch  1688 -------\n",
      "epoch: 1688, train_loss: -8.753349304199219, valid_loss: -4.147249221801758\n",
      "-------epoch  1689 -------\n",
      "epoch: 1689, train_loss: -8.761972427368164, valid_loss: -4.173043251037598\n",
      "-------epoch  1690 -------\n",
      "epoch: 1690, train_loss: -8.762944221496582, valid_loss: -4.153523921966553\n",
      "-------epoch  1691 -------\n",
      "epoch: 1691, train_loss: -8.757780075073242, valid_loss: -4.164914131164551\n",
      "-------epoch  1692 -------\n",
      "epoch: 1692, train_loss: -8.750955581665039, valid_loss: -4.151793479919434\n",
      "-------epoch  1693 -------\n",
      "epoch: 1693, train_loss: -8.747864723205566, valid_loss: -4.16228723526001\n",
      "-------epoch  1694 -------\n",
      "epoch: 1694, train_loss: -8.748958587646484, valid_loss: -4.145762920379639\n",
      "-------epoch  1695 -------\n",
      "epoch: 1695, train_loss: -8.754765510559082, valid_loss: -4.157095909118652\n",
      "-------epoch  1696 -------\n",
      "epoch: 1696, train_loss: -8.761491775512695, valid_loss: -4.142136096954346\n",
      "-------epoch  1697 -------\n",
      "epoch: 1697, train_loss: -8.767107009887695, valid_loss: -4.150967597961426\n",
      "-------epoch  1698 -------\n",
      "epoch: 1698, train_loss: -8.769669532775879, valid_loss: -4.129655838012695\n",
      "-------epoch  1699 -------\n",
      "epoch: 1699, train_loss: -8.76868724822998, valid_loss: -4.148175239562988\n",
      "-------epoch  1700 -------\n",
      "epoch: 1700, train_loss: -8.764439582824707, valid_loss: -4.1115217208862305\n",
      "-------epoch  1701 -------\n",
      "epoch: 1701, train_loss: -8.756329536437988, valid_loss: -4.1419997215271\n",
      "-------epoch  1702 -------\n",
      "epoch: 1702, train_loss: -8.744528770446777, valid_loss: -4.089227676391602\n",
      "-------epoch  1703 -------\n",
      "epoch: 1703, train_loss: -8.723795890808105, valid_loss: -4.134405136108398\n",
      "-------epoch  1704 -------\n",
      "epoch: 1704, train_loss: -8.696775436401367, valid_loss: -4.054415225982666\n",
      "-------epoch  1705 -------\n",
      "epoch: 1705, train_loss: -8.649541854858398, valid_loss: -4.121650695800781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1706 -------\n",
      "epoch: 1706, train_loss: -8.622049331665039, valid_loss: -4.034895896911621\n",
      "-------epoch  1707 -------\n",
      "epoch: 1707, train_loss: -8.585198402404785, valid_loss: -4.108928680419922\n",
      "-------epoch  1708 -------\n",
      "epoch: 1708, train_loss: -8.629131317138672, valid_loss: -4.063742637634277\n",
      "-------epoch  1709 -------\n",
      "epoch: 1709, train_loss: -8.661632537841797, valid_loss: -4.116198539733887\n",
      "-------epoch  1710 -------\n",
      "epoch: 1710, train_loss: -8.712355613708496, valid_loss: -4.092911243438721\n",
      "-------epoch  1711 -------\n",
      "epoch: 1711, train_loss: -8.742587089538574, valid_loss: -4.113944053649902\n",
      "-------epoch  1712 -------\n",
      "epoch: 1712, train_loss: -8.761910438537598, valid_loss: -4.113283634185791\n",
      "-------epoch  1713 -------\n",
      "epoch: 1713, train_loss: -8.768271446228027, valid_loss: -4.092006683349609\n",
      "-------epoch  1714 -------\n",
      "epoch: 1714, train_loss: -8.763627052307129, valid_loss: -4.126086711883545\n",
      "-------epoch  1715 -------\n",
      "epoch: 1715, train_loss: -8.751001358032227, valid_loss: -4.072877407073975\n",
      "-------epoch  1716 -------\n",
      "epoch: 1716, train_loss: -8.732322692871094, valid_loss: -4.129740238189697\n",
      "-------epoch  1717 -------\n",
      "epoch: 1717, train_loss: -8.718040466308594, valid_loss: -4.064584732055664\n",
      "-------epoch  1718 -------\n",
      "epoch: 1718, train_loss: -8.707098960876465, valid_loss: -4.141016006469727\n",
      "-------epoch  1719 -------\n",
      "epoch: 1719, train_loss: -8.71423053741455, valid_loss: -4.06912088394165\n",
      "-------epoch  1720 -------\n",
      "epoch: 1720, train_loss: -8.725458145141602, valid_loss: -4.142398357391357\n",
      "-------epoch  1721 -------\n",
      "epoch: 1721, train_loss: -8.74118423461914, valid_loss: -4.092738151550293\n",
      "-------epoch  1722 -------\n",
      "epoch: 1722, train_loss: -8.751920700073242, valid_loss: -4.122256755828857\n",
      "-------epoch  1723 -------\n",
      "epoch: 1723, train_loss: -8.754844665527344, valid_loss: -4.112272262573242\n",
      "-------epoch  1724 -------\n",
      "epoch: 1724, train_loss: -8.750341415405273, valid_loss: -4.097050666809082\n",
      "-------epoch  1725 -------\n",
      "epoch: 1725, train_loss: -8.736940383911133, valid_loss: -4.108403205871582\n",
      "-------epoch  1726 -------\n",
      "epoch: 1726, train_loss: -8.718670845031738, valid_loss: -4.075750827789307\n",
      "-------epoch  1727 -------\n",
      "epoch: 1727, train_loss: -8.69281005859375, valid_loss: -4.093993663787842\n",
      "-------epoch  1728 -------\n",
      "epoch: 1728, train_loss: -8.6738920211792, valid_loss: -4.0610880851745605\n",
      "-------epoch  1729 -------\n",
      "epoch: 1729, train_loss: -8.652362823486328, valid_loss: -4.083235740661621\n",
      "-------epoch  1730 -------\n",
      "epoch: 1730, train_loss: -8.652200698852539, valid_loss: -4.0691986083984375\n",
      "-------epoch  1731 -------\n",
      "epoch: 1731, train_loss: -8.653458595275879, valid_loss: -4.086292743682861\n",
      "-------epoch  1732 -------\n",
      "epoch: 1732, train_loss: -8.673713684082031, valid_loss: -4.087774276733398\n",
      "-------epoch  1733 -------\n",
      "epoch: 1733, train_loss: -8.694738388061523, valid_loss: -4.102634429931641\n",
      "-------epoch  1734 -------\n",
      "epoch: 1734, train_loss: -8.723308563232422, valid_loss: -4.105601787567139\n",
      "-------epoch  1735 -------\n",
      "epoch: 1735, train_loss: -8.747076988220215, valid_loss: -4.103065013885498\n",
      "-------epoch  1736 -------\n",
      "epoch: 1736, train_loss: -8.765253067016602, valid_loss: -4.118405342102051\n",
      "-------epoch  1737 -------\n",
      "epoch: 1737, train_loss: -8.772661209106445, valid_loss: -4.0911545753479\n",
      "-------epoch  1738 -------\n",
      "epoch: 1738, train_loss: -8.76960563659668, valid_loss: -4.121155738830566\n",
      "-------epoch  1739 -------\n",
      "epoch: 1739, train_loss: -8.759864807128906, valid_loss: -4.0806145668029785\n",
      "-------epoch  1740 -------\n",
      "epoch: 1740, train_loss: -8.747271537780762, valid_loss: -4.1154561042785645\n",
      "-------epoch  1741 -------\n",
      "epoch: 1741, train_loss: -8.7402925491333, valid_loss: -4.076847076416016\n",
      "-------epoch  1742 -------\n",
      "epoch: 1742, train_loss: -8.738739967346191, valid_loss: -4.111529350280762\n",
      "-------epoch  1743 -------\n",
      "epoch: 1743, train_loss: -8.746390342712402, valid_loss: -4.076673984527588\n",
      "-------epoch  1744 -------\n",
      "epoch: 1744, train_loss: -8.756046295166016, valid_loss: -4.106078624725342\n",
      "-------epoch  1745 -------\n",
      "epoch: 1745, train_loss: -8.766209602355957, valid_loss: -4.079817771911621\n",
      "-------epoch  1746 -------\n",
      "epoch: 1746, train_loss: -8.771942138671875, valid_loss: -4.098106861114502\n",
      "-------epoch  1747 -------\n",
      "epoch: 1747, train_loss: -8.773581504821777, valid_loss: -4.077817440032959\n",
      "-------epoch  1748 -------\n",
      "epoch: 1748, train_loss: -8.771103858947754, valid_loss: -4.093526840209961\n",
      "-------epoch  1749 -------\n",
      "epoch: 1749, train_loss: -8.76668930053711, valid_loss: -4.069859981536865\n",
      "-------epoch  1750 -------\n",
      "epoch: 1750, train_loss: -8.76171875, valid_loss: -4.092947006225586\n",
      "-------epoch  1751 -------\n",
      "epoch: 1751, train_loss: -8.757267951965332, valid_loss: -4.057995319366455\n",
      "-------epoch  1752 -------\n",
      "epoch: 1752, train_loss: -8.753654479980469, valid_loss: -4.097535133361816\n",
      "-------epoch  1753 -------\n",
      "epoch: 1753, train_loss: -8.750675201416016, valid_loss: -4.040913105010986\n",
      "-------epoch  1754 -------\n",
      "epoch: 1754, train_loss: -8.7462158203125, valid_loss: -4.102463722229004\n",
      "-------epoch  1755 -------\n",
      "epoch: 1755, train_loss: -8.741320610046387, valid_loss: -4.024605751037598\n",
      "-------epoch  1756 -------\n",
      "epoch: 1756, train_loss: -8.729056358337402, valid_loss: -4.08461332321167\n",
      "-------epoch  1757 -------\n",
      "epoch: 1757, train_loss: -8.716745376586914, valid_loss: -4.0146074295043945\n",
      "-------epoch  1758 -------\n",
      "epoch: 1758, train_loss: -8.687905311584473, valid_loss: -4.0596089363098145\n",
      "-------epoch  1759 -------\n",
      "epoch: 1759, train_loss: -8.668996810913086, valid_loss: -3.9883599281311035\n",
      "-------epoch  1760 -------\n",
      "epoch: 1760, train_loss: -8.627933502197266, valid_loss: -4.0426025390625\n",
      "-------epoch  1761 -------\n",
      "epoch: 1761, train_loss: -8.618593215942383, valid_loss: -3.982606887817383\n",
      "-------epoch  1762 -------\n",
      "epoch: 1762, train_loss: -8.591200828552246, valid_loss: -4.028810024261475\n",
      "-------epoch  1763 -------\n",
      "epoch: 1763, train_loss: -8.60342025756836, valid_loss: -4.017912864685059\n",
      "-------epoch  1764 -------\n",
      "epoch: 1764, train_loss: -8.6117525100708, valid_loss: -4.0422892570495605\n",
      "-------epoch  1765 -------\n",
      "epoch: 1765, train_loss: -8.651217460632324, valid_loss: -4.058311939239502\n",
      "-------epoch  1766 -------\n",
      "epoch: 1766, train_loss: -8.689906120300293, valid_loss: -4.063436508178711\n",
      "-------epoch  1767 -------\n",
      "epoch: 1767, train_loss: -8.731586456298828, valid_loss: -4.076580047607422\n",
      "-------epoch  1768 -------\n",
      "epoch: 1768, train_loss: -8.755184173583984, valid_loss: -4.056273460388184\n",
      "-------epoch  1769 -------\n",
      "epoch: 1769, train_loss: -8.759166717529297, valid_loss: -4.083047866821289\n",
      "-------epoch  1770 -------\n",
      "epoch: 1770, train_loss: -8.749085426330566, valid_loss: -4.032763481140137\n",
      "-------epoch  1771 -------\n",
      "epoch: 1771, train_loss: -8.733192443847656, valid_loss: -4.0824151039123535\n",
      "-------epoch  1772 -------\n",
      "epoch: 1772, train_loss: -8.733267784118652, valid_loss: -4.0493483543396\n",
      "-------epoch  1773 -------\n",
      "epoch: 1773, train_loss: -8.74466323852539, valid_loss: -4.074865818023682\n",
      "-------epoch  1774 -------\n",
      "epoch: 1774, train_loss: -8.7655611038208, valid_loss: -4.073782920837402\n",
      "-------epoch  1775 -------\n",
      "epoch: 1775, train_loss: -8.780887603759766, valid_loss: -4.0677714347839355\n",
      "-------epoch  1776 -------\n",
      "epoch: 1776, train_loss: -8.784102439880371, valid_loss: -4.076712608337402\n",
      "-------epoch  1777 -------\n",
      "epoch: 1777, train_loss: -8.775665283203125, valid_loss: -4.046896934509277\n",
      "-------epoch  1778 -------\n",
      "epoch: 1778, train_loss: -8.762200355529785, valid_loss: -4.073067665100098\n",
      "-------epoch  1779 -------\n",
      "epoch: 1779, train_loss: -8.75522518157959, valid_loss: -4.035618305206299\n",
      "-------epoch  1780 -------\n",
      "epoch: 1780, train_loss: -8.755727767944336, valid_loss: -4.066751956939697\n",
      "-------epoch  1781 -------\n",
      "epoch: 1781, train_loss: -8.76620101928711, valid_loss: -4.0397539138793945\n",
      "-------epoch  1782 -------\n",
      "epoch: 1782, train_loss: -8.77669620513916, valid_loss: -4.056910037994385\n",
      "-------epoch  1783 -------\n",
      "epoch: 1783, train_loss: -8.784241676330566, valid_loss: -4.042548179626465\n",
      "-------epoch  1784 -------\n",
      "epoch: 1784, train_loss: -8.78620719909668, valid_loss: -4.04032039642334\n",
      "-------epoch  1785 -------\n",
      "epoch: 1785, train_loss: -8.78405475616455, valid_loss: -4.039610862731934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1786 -------\n",
      "epoch: 1786, train_loss: -8.780351638793945, valid_loss: -4.026236057281494\n",
      "-------epoch  1787 -------\n",
      "epoch: 1787, train_loss: -8.776975631713867, valid_loss: -4.03023099899292\n",
      "-------epoch  1788 -------\n",
      "epoch: 1788, train_loss: -8.775591850280762, valid_loss: -4.019068717956543\n",
      "-------epoch  1789 -------\n",
      "epoch: 1789, train_loss: -8.775611877441406, valid_loss: -4.022008895874023\n",
      "-------epoch  1790 -------\n",
      "epoch: 1790, train_loss: -8.777235984802246, valid_loss: -4.010639667510986\n",
      "-------epoch  1791 -------\n",
      "epoch: 1791, train_loss: -8.779045104980469, valid_loss: -4.021975517272949\n",
      "-------epoch  1792 -------\n",
      "epoch: 1792, train_loss: -8.780990600585938, valid_loss: -4.0017476081848145\n",
      "-------epoch  1793 -------\n",
      "epoch: 1793, train_loss: -8.782012939453125, valid_loss: -4.017903804779053\n",
      "-------epoch  1794 -------\n",
      "epoch: 1794, train_loss: -8.782206535339355, valid_loss: -3.997431993484497\n",
      "-------epoch  1795 -------\n",
      "epoch: 1795, train_loss: -8.780293464660645, valid_loss: -4.012852668762207\n",
      "-------epoch  1796 -------\n",
      "epoch: 1796, train_loss: -8.776076316833496, valid_loss: -3.9847865104675293\n",
      "-------epoch  1797 -------\n",
      "epoch: 1797, train_loss: -8.766105651855469, valid_loss: -4.013650417327881\n",
      "-------epoch  1798 -------\n",
      "epoch: 1798, train_loss: -8.750205039978027, valid_loss: -3.9602949619293213\n",
      "-------epoch  1799 -------\n",
      "epoch: 1799, train_loss: -8.718255043029785, valid_loss: -4.008815288543701\n",
      "-------epoch  1800 -------\n",
      "epoch: 1800, train_loss: -8.684362411499023, valid_loss: -3.934096097946167\n",
      "-------epoch  1801 -------\n",
      "epoch: 1801, train_loss: -8.62835693359375, valid_loss: -3.9924330711364746\n",
      "-------epoch  1802 -------\n",
      "epoch: 1802, train_loss: -8.63272476196289, valid_loss: -3.9364590644836426\n",
      "-------epoch  1803 -------\n",
      "epoch: 1803, train_loss: -8.627540588378906, valid_loss: -3.9886486530303955\n",
      "-------epoch  1804 -------\n",
      "epoch: 1804, train_loss: -8.683520317077637, valid_loss: -3.962728261947632\n",
      "-------epoch  1805 -------\n",
      "epoch: 1805, train_loss: -8.714973449707031, valid_loss: -3.9965450763702393\n",
      "-------epoch  1806 -------\n",
      "epoch: 1806, train_loss: -8.742149353027344, valid_loss: -3.986374855041504\n",
      "-------epoch  1807 -------\n",
      "epoch: 1807, train_loss: -8.746137619018555, valid_loss: -3.9892876148223877\n",
      "-------epoch  1808 -------\n",
      "epoch: 1808, train_loss: -8.730387687683105, valid_loss: -4.021703243255615\n",
      "-------epoch  1809 -------\n",
      "epoch: 1809, train_loss: -8.685453414916992, valid_loss: -3.9817614555358887\n",
      "-------epoch  1810 -------\n",
      "epoch: 1810, train_loss: -8.624993324279785, valid_loss: -4.055562496185303\n",
      "-------epoch  1811 -------\n",
      "epoch: 1811, train_loss: -8.562517166137695, valid_loss: -3.9880359172821045\n",
      "-------epoch  1812 -------\n",
      "epoch: 1812, train_loss: -8.564114570617676, valid_loss: -4.063287258148193\n",
      "-------epoch  1813 -------\n",
      "epoch: 1813, train_loss: -8.625722885131836, valid_loss: -4.038422584533691\n",
      "-------epoch  1814 -------\n",
      "epoch: 1814, train_loss: -8.722328186035156, valid_loss: -4.034071922302246\n",
      "-------epoch  1815 -------\n",
      "epoch: 1815, train_loss: -8.771143913269043, valid_loss: -4.085907459259033\n",
      "-------epoch  1816 -------\n",
      "epoch: 1816, train_loss: -8.762038230895996, valid_loss: -4.043056011199951\n",
      "-------epoch  1817 -------\n",
      "epoch: 1817, train_loss: -8.71718978881836, valid_loss: -4.104774475097656\n",
      "-------epoch  1818 -------\n",
      "epoch: 1818, train_loss: -8.70622444152832, valid_loss: -4.0576701164245605\n",
      "-------epoch  1819 -------\n",
      "epoch: 1819, train_loss: -8.747590065002441, valid_loss: -4.083349227905273\n",
      "-------epoch  1820 -------\n",
      "epoch: 1820, train_loss: -8.787374496459961, valid_loss: -4.066634654998779\n",
      "-------epoch  1821 -------\n",
      "epoch: 1821, train_loss: -8.78934383392334, valid_loss: -4.062058925628662\n",
      "-------epoch  1822 -------\n",
      "epoch: 1822, train_loss: -8.764747619628906, valid_loss: -4.085724830627441\n",
      "-------epoch  1823 -------\n",
      "epoch: 1823, train_loss: -8.751230239868164, valid_loss: -4.033881664276123\n",
      "-------epoch  1824 -------\n",
      "epoch: 1824, train_loss: -8.765690803527832, valid_loss: -4.0599541664123535\n",
      "-------epoch  1825 -------\n",
      "epoch: 1825, train_loss: -8.78361701965332, valid_loss: -4.034453868865967\n",
      "-------epoch  1826 -------\n",
      "epoch: 1826, train_loss: -8.779702186584473, valid_loss: -4.052101135253906\n",
      "-------epoch  1827 -------\n",
      "epoch: 1827, train_loss: -8.757652282714844, valid_loss: -4.010492324829102\n",
      "-------epoch  1828 -------\n",
      "epoch: 1828, train_loss: -8.736224174499512, valid_loss: -4.030437469482422\n",
      "-------epoch  1829 -------\n",
      "epoch: 1829, train_loss: -8.728137969970703, valid_loss: -3.990046739578247\n",
      "-------epoch  1830 -------\n",
      "epoch: 1830, train_loss: -8.710428237915039, valid_loss: -4.038986682891846\n",
      "-------epoch  1831 -------\n",
      "epoch: 1831, train_loss: -8.679645538330078, valid_loss: -3.9212124347686768\n",
      "-------epoch  1832 -------\n",
      "epoch: 1832, train_loss: -8.624490737915039, valid_loss: -4.02683162689209\n",
      "-------epoch  1833 -------\n",
      "epoch: 1833, train_loss: -8.638094902038574, valid_loss: -3.9529943466186523\n",
      "-------epoch  1834 -------\n",
      "epoch: 1834, train_loss: -8.649551391601562, valid_loss: -4.0136542320251465\n",
      "-------epoch  1835 -------\n",
      "epoch: 1835, train_loss: -8.69454574584961, valid_loss: -3.975785493850708\n",
      "-------epoch  1836 -------\n",
      "epoch: 1836, train_loss: -8.718981742858887, valid_loss: -4.027125358581543\n",
      "-------epoch  1837 -------\n",
      "epoch: 1837, train_loss: -8.745820045471191, valid_loss: -4.003971576690674\n",
      "-------epoch  1838 -------\n",
      "epoch: 1838, train_loss: -8.76990032196045, valid_loss: -4.008327007293701\n",
      "-------epoch  1839 -------\n",
      "epoch: 1839, train_loss: -8.790398597717285, valid_loss: -4.002605438232422\n",
      "-------epoch  1840 -------\n",
      "epoch: 1840, train_loss: -8.798871040344238, valid_loss: -3.986455202102661\n",
      "-------epoch  1841 -------\n",
      "epoch: 1841, train_loss: -8.79532241821289, valid_loss: -4.000007629394531\n",
      "-------epoch  1842 -------\n",
      "epoch: 1842, train_loss: -8.783610343933105, valid_loss: -3.9764864444732666\n",
      "-------epoch  1843 -------\n",
      "epoch: 1843, train_loss: -8.769867897033691, valid_loss: -3.9771430492401123\n",
      "-------epoch  1844 -------\n",
      "epoch: 1844, train_loss: -8.761991500854492, valid_loss: -3.9604527950286865\n",
      "-------epoch  1845 -------\n",
      "epoch: 1845, train_loss: -8.760007858276367, valid_loss: -3.9917469024658203\n",
      "-------epoch  1846 -------\n",
      "epoch: 1846, train_loss: -8.765897750854492, valid_loss: -3.9543604850769043\n",
      "-------epoch  1847 -------\n",
      "epoch: 1847, train_loss: -8.774197578430176, valid_loss: -3.976193428039551\n",
      "-------epoch  1848 -------\n",
      "epoch: 1848, train_loss: -8.783472061157227, valid_loss: -3.9639732837677\n",
      "-------epoch  1849 -------\n",
      "epoch: 1849, train_loss: -8.789851188659668, valid_loss: -3.9866607189178467\n",
      "-------epoch  1850 -------\n",
      "epoch: 1850, train_loss: -8.79422664642334, valid_loss: -3.9597833156585693\n",
      "-------epoch  1851 -------\n",
      "epoch: 1851, train_loss: -8.795862197875977, valid_loss: -3.9777066707611084\n",
      "-------epoch  1852 -------\n",
      "epoch: 1852, train_loss: -8.796199798583984, valid_loss: -3.9672393798828125\n",
      "-------epoch  1853 -------\n",
      "epoch: 1853, train_loss: -8.796018600463867, valid_loss: -3.968817710876465\n",
      "-------epoch  1854 -------\n",
      "epoch: 1854, train_loss: -8.795488357543945, valid_loss: -3.967651605606079\n",
      "-------epoch  1855 -------\n",
      "epoch: 1855, train_loss: -8.795062065124512, valid_loss: -3.9531009197235107\n",
      "-------epoch  1856 -------\n",
      "epoch: 1856, train_loss: -8.794254302978516, valid_loss: -3.9638919830322266\n",
      "-------epoch  1857 -------\n",
      "epoch: 1857, train_loss: -8.792740821838379, valid_loss: -3.9468994140625\n",
      "-------epoch  1858 -------\n",
      "epoch: 1858, train_loss: -8.789468765258789, valid_loss: -3.9599406719207764\n",
      "-------epoch  1859 -------\n",
      "epoch: 1859, train_loss: -8.784294128417969, valid_loss: -3.9255735874176025\n",
      "-------epoch  1860 -------\n",
      "epoch: 1860, train_loss: -8.774263381958008, valid_loss: -3.9638936519622803\n",
      "-------epoch  1861 -------\n",
      "epoch: 1861, train_loss: -8.760698318481445, valid_loss: -3.909533977508545\n",
      "-------epoch  1862 -------\n",
      "epoch: 1862, train_loss: -8.736392974853516, valid_loss: -3.950425863265991\n",
      "-------epoch  1863 -------\n",
      "epoch: 1863, train_loss: -8.71247673034668, valid_loss: -3.8887016773223877\n",
      "-------epoch  1864 -------\n",
      "epoch: 1864, train_loss: -8.673029899597168, valid_loss: -3.939311981201172\n",
      "-------epoch  1865 -------\n",
      "epoch: 1865, train_loss: -8.662930488586426, valid_loss: -3.8813114166259766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1866 -------\n",
      "epoch: 1866, train_loss: -8.644050598144531, valid_loss: -3.9316961765289307\n",
      "-------epoch  1867 -------\n",
      "epoch: 1867, train_loss: -8.673039436340332, valid_loss: -3.902794599533081\n",
      "-------epoch  1868 -------\n",
      "epoch: 1868, train_loss: -8.691627502441406, valid_loss: -3.9430055618286133\n",
      "-------epoch  1869 -------\n",
      "epoch: 1869, train_loss: -8.72351360321045, valid_loss: -3.9308624267578125\n",
      "-------epoch  1870 -------\n",
      "epoch: 1870, train_loss: -8.742186546325684, valid_loss: -3.948837995529175\n",
      "-------epoch  1871 -------\n",
      "epoch: 1871, train_loss: -8.753847122192383, valid_loss: -3.9601783752441406\n",
      "-------epoch  1872 -------\n",
      "epoch: 1872, train_loss: -8.750387191772461, valid_loss: -3.9387054443359375\n",
      "-------epoch  1873 -------\n",
      "epoch: 1873, train_loss: -8.737811088562012, valid_loss: -3.9773309230804443\n",
      "-------epoch  1874 -------\n",
      "epoch: 1874, train_loss: -8.721409797668457, valid_loss: -3.9314773082733154\n",
      "-------epoch  1875 -------\n",
      "epoch: 1875, train_loss: -8.712528228759766, valid_loss: -3.976222515106201\n",
      "-------epoch  1876 -------\n",
      "epoch: 1876, train_loss: -8.727836608886719, valid_loss: -3.9420790672302246\n",
      "-------epoch  1877 -------\n",
      "epoch: 1877, train_loss: -8.752723693847656, valid_loss: -3.9704058170318604\n",
      "-------epoch  1878 -------\n",
      "epoch: 1878, train_loss: -8.78138256072998, valid_loss: -3.9542012214660645\n",
      "-------epoch  1879 -------\n",
      "epoch: 1879, train_loss: -8.799856185913086, valid_loss: -3.9631237983703613\n",
      "-------epoch  1880 -------\n",
      "epoch: 1880, train_loss: -8.806116104125977, valid_loss: -3.963397741317749\n",
      "-------epoch  1881 -------\n",
      "epoch: 1881, train_loss: -8.80159854888916, valid_loss: -3.951336145401001\n",
      "-------epoch  1882 -------\n",
      "epoch: 1882, train_loss: -8.790794372558594, valid_loss: -3.970155715942383\n",
      "-------epoch  1883 -------\n",
      "epoch: 1883, train_loss: -8.779952049255371, valid_loss: -3.9367473125457764\n",
      "-------epoch  1884 -------\n",
      "epoch: 1884, train_loss: -8.774017333984375, valid_loss: -3.9645774364471436\n",
      "-------epoch  1885 -------\n",
      "epoch: 1885, train_loss: -8.77685260772705, valid_loss: -3.9334495067596436\n",
      "-------epoch  1886 -------\n",
      "epoch: 1886, train_loss: -8.784618377685547, valid_loss: -3.950087070465088\n",
      "-------epoch  1887 -------\n",
      "epoch: 1887, train_loss: -8.794901847839355, valid_loss: -3.9293785095214844\n",
      "-------epoch  1888 -------\n",
      "epoch: 1888, train_loss: -8.802478790283203, valid_loss: -3.9400107860565186\n",
      "-------epoch  1889 -------\n",
      "epoch: 1889, train_loss: -8.806111335754395, valid_loss: -3.926920175552368\n",
      "-------epoch  1890 -------\n",
      "epoch: 1890, train_loss: -8.80545425415039, valid_loss: -3.9260666370391846\n",
      "-------epoch  1891 -------\n",
      "epoch: 1891, train_loss: -8.801713943481445, valid_loss: -3.9292361736297607\n",
      "-------epoch  1892 -------\n",
      "epoch: 1892, train_loss: -8.796013832092285, valid_loss: -3.917475938796997\n",
      "-------epoch  1893 -------\n",
      "epoch: 1893, train_loss: -8.789828300476074, valid_loss: -3.922908067703247\n",
      "-------epoch  1894 -------\n",
      "epoch: 1894, train_loss: -8.782441139221191, valid_loss: -3.9184792041778564\n",
      "-------epoch  1895 -------\n",
      "epoch: 1895, train_loss: -8.773788452148438, valid_loss: -3.9084599018096924\n",
      "-------epoch  1896 -------\n",
      "epoch: 1896, train_loss: -8.75755500793457, valid_loss: -3.9148709774017334\n",
      "-------epoch  1897 -------\n",
      "epoch: 1897, train_loss: -8.729239463806152, valid_loss: -3.8712122440338135\n",
      "-------epoch  1898 -------\n",
      "epoch: 1898, train_loss: -8.667054176330566, valid_loss: -3.885694742202759\n",
      "-------epoch  1899 -------\n",
      "epoch: 1899, train_loss: -8.574094772338867, valid_loss: -3.7987945079803467\n",
      "-------epoch  1900 -------\n",
      "epoch: 1900, train_loss: -8.422934532165527, valid_loss: -3.8325448036193848\n",
      "-------epoch  1901 -------\n",
      "epoch: 1901, train_loss: -8.420251846313477, valid_loss: -3.8123512268066406\n",
      "-------epoch  1902 -------\n",
      "epoch: 1902, train_loss: -8.461174964904785, valid_loss: -3.921748399734497\n",
      "-------epoch  1903 -------\n",
      "epoch: 1903, train_loss: -8.64795207977295, valid_loss: -3.900463819503784\n",
      "-------epoch  1904 -------\n",
      "epoch: 1904, train_loss: -8.76545238494873, valid_loss: -3.9495718479156494\n",
      "-------epoch  1905 -------\n",
      "epoch: 1905, train_loss: -8.800212860107422, valid_loss: -3.948784828186035\n",
      "-------epoch  1906 -------\n",
      "epoch: 1906, train_loss: -8.759340286254883, valid_loss: -3.861103057861328\n",
      "-------epoch  1907 -------\n",
      "epoch: 1907, train_loss: -8.68373966217041, valid_loss: -3.940469980239868\n",
      "-------epoch  1908 -------\n",
      "epoch: 1908, train_loss: -8.703753471374512, valid_loss: -3.9077465534210205\n",
      "-------epoch  1909 -------\n",
      "epoch: 1909, train_loss: -8.764074325561523, valid_loss: -3.9195644855499268\n",
      "-------epoch  1910 -------\n",
      "epoch: 1910, train_loss: -8.803770065307617, valid_loss: -3.9269816875457764\n",
      "-------epoch  1911 -------\n",
      "epoch: 1911, train_loss: -8.787346839904785, valid_loss: -3.8877525329589844\n",
      "-------epoch  1912 -------\n",
      "epoch: 1912, train_loss: -8.74894905090332, valid_loss: -3.9220197200775146\n",
      "-------epoch  1913 -------\n",
      "epoch: 1913, train_loss: -8.754377365112305, valid_loss: -3.891761541366577\n",
      "-------epoch  1914 -------\n",
      "epoch: 1914, train_loss: -8.788345336914062, valid_loss: -3.9122793674468994\n",
      "-------epoch  1915 -------\n",
      "epoch: 1915, train_loss: -8.805980682373047, valid_loss: -3.9165072441101074\n",
      "-------epoch  1916 -------\n",
      "epoch: 1916, train_loss: -8.789423942565918, valid_loss: -3.877838611602783\n",
      "-------epoch  1917 -------\n",
      "epoch: 1917, train_loss: -8.768396377563477, valid_loss: -3.917750835418701\n",
      "-------epoch  1918 -------\n",
      "epoch: 1918, train_loss: -8.77767562866211, valid_loss: -3.9006807804107666\n",
      "-------epoch  1919 -------\n",
      "epoch: 1919, train_loss: -8.796276092529297, valid_loss: -3.87770676612854\n",
      "-------epoch  1920 -------\n",
      "epoch: 1920, train_loss: -8.797444343566895, valid_loss: -3.9061756134033203\n",
      "-------epoch  1921 -------\n",
      "epoch: 1921, train_loss: -8.780752182006836, valid_loss: -3.875821828842163\n",
      "-------epoch  1922 -------\n",
      "epoch: 1922, train_loss: -8.76395034790039, valid_loss: -3.8841497898101807\n",
      "-------epoch  1923 -------\n",
      "epoch: 1923, train_loss: -8.766578674316406, valid_loss: -3.873106002807617\n",
      "-------epoch  1924 -------\n",
      "epoch: 1924, train_loss: -8.7644681930542, valid_loss: -3.893094301223755\n",
      "-------epoch  1925 -------\n",
      "epoch: 1925, train_loss: -8.746585845947266, valid_loss: -3.8830559253692627\n",
      "-------epoch  1926 -------\n",
      "epoch: 1926, train_loss: -8.714836120605469, valid_loss: -3.8856029510498047\n",
      "-------epoch  1927 -------\n",
      "epoch: 1927, train_loss: -8.684198379516602, valid_loss: -3.8831064701080322\n",
      "-------epoch  1928 -------\n",
      "epoch: 1928, train_loss: -8.68382740020752, valid_loss: -3.9050559997558594\n",
      "-------epoch  1929 -------\n",
      "epoch: 1929, train_loss: -8.700606346130371, valid_loss: -3.8566133975982666\n",
      "-------epoch  1930 -------\n",
      "epoch: 1930, train_loss: -8.72747802734375, valid_loss: -3.8914434909820557\n",
      "-------epoch  1931 -------\n",
      "epoch: 1931, train_loss: -8.758289337158203, valid_loss: -3.870689630508423\n",
      "-------epoch  1932 -------\n",
      "epoch: 1932, train_loss: -8.778063774108887, valid_loss: -3.8760719299316406\n",
      "-------epoch  1933 -------\n",
      "epoch: 1933, train_loss: -8.792643547058105, valid_loss: -3.8683156967163086\n",
      "-------epoch  1934 -------\n",
      "epoch: 1934, train_loss: -8.798887252807617, valid_loss: -3.8806917667388916\n",
      "-------epoch  1935 -------\n",
      "epoch: 1935, train_loss: -8.800634384155273, valid_loss: -3.8763084411621094\n",
      "-------epoch  1936 -------\n",
      "epoch: 1936, train_loss: -8.799198150634766, valid_loss: -3.8584582805633545\n",
      "-------epoch  1937 -------\n",
      "epoch: 1937, train_loss: -8.795581817626953, valid_loss: -3.876194477081299\n",
      "-------epoch  1938 -------\n",
      "epoch: 1938, train_loss: -8.790421485900879, valid_loss: -3.8422789573669434\n",
      "-------epoch  1939 -------\n",
      "epoch: 1939, train_loss: -8.783167839050293, valid_loss: -3.856794595718384\n",
      "-------epoch  1940 -------\n",
      "epoch: 1940, train_loss: -8.777813911437988, valid_loss: -3.8284060955047607\n",
      "-------epoch  1941 -------\n",
      "epoch: 1941, train_loss: -8.769562721252441, valid_loss: -3.8494679927825928\n",
      "-------epoch  1942 -------\n",
      "epoch: 1942, train_loss: -8.766355514526367, valid_loss: -3.8119893074035645\n",
      "-------epoch  1943 -------\n",
      "epoch: 1943, train_loss: -8.760835647583008, valid_loss: -3.841780662536621\n",
      "-------epoch  1944 -------\n",
      "epoch: 1944, train_loss: -8.76233959197998, valid_loss: -3.8130879402160645\n",
      "-------epoch  1945 -------\n",
      "epoch: 1945, train_loss: -8.762495040893555, valid_loss: -3.8311398029327393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  1946 -------\n",
      "epoch: 1946, train_loss: -8.768383026123047, valid_loss: -3.808587074279785\n",
      "-------epoch  1947 -------\n",
      "epoch: 1947, train_loss: -8.771875381469727, valid_loss: -3.8298380374908447\n",
      "-------epoch  1948 -------\n",
      "epoch: 1948, train_loss: -8.778352737426758, valid_loss: -3.799001932144165\n",
      "-------epoch  1949 -------\n",
      "epoch: 1949, train_loss: -8.781380653381348, valid_loss: -3.828031301498413\n",
      "-------epoch  1950 -------\n",
      "epoch: 1950, train_loss: -8.786065101623535, valid_loss: -3.7919092178344727\n",
      "-------epoch  1951 -------\n",
      "epoch: 1951, train_loss: -8.787446022033691, valid_loss: -3.82025408744812\n",
      "-------epoch  1952 -------\n",
      "epoch: 1952, train_loss: -8.790510177612305, valid_loss: -3.7851803302764893\n",
      "-------epoch  1953 -------\n",
      "epoch: 1953, train_loss: -8.7909574508667, valid_loss: -3.810955762863159\n",
      "-------epoch  1954 -------\n",
      "epoch: 1954, train_loss: -8.79295539855957, valid_loss: -3.775780439376831\n",
      "-------epoch  1955 -------\n",
      "epoch: 1955, train_loss: -8.792686462402344, valid_loss: -3.8017234802246094\n",
      "-------epoch  1956 -------\n",
      "epoch: 1956, train_loss: -8.793851852416992, valid_loss: -3.7675046920776367\n",
      "-------epoch  1957 -------\n",
      "epoch: 1957, train_loss: -8.792682647705078, valid_loss: -3.7913320064544678\n",
      "-------epoch  1958 -------\n",
      "epoch: 1958, train_loss: -8.792844772338867, valid_loss: -3.7543785572052\n",
      "-------epoch  1959 -------\n",
      "epoch: 1959, train_loss: -8.790240287780762, valid_loss: -3.784191370010376\n",
      "-------epoch  1960 -------\n",
      "epoch: 1960, train_loss: -8.788896560668945, valid_loss: -3.741384267807007\n",
      "-------epoch  1961 -------\n",
      "epoch: 1961, train_loss: -8.784104347229004, valid_loss: -3.771430253982544\n",
      "-------epoch  1962 -------\n",
      "epoch: 1962, train_loss: -8.780879020690918, valid_loss: -3.7312352657318115\n",
      "-------epoch  1963 -------\n",
      "epoch: 1963, train_loss: -8.773161888122559, valid_loss: -3.7588369846343994\n",
      "-------epoch  1964 -------\n",
      "epoch: 1964, train_loss: -8.767956733703613, valid_loss: -3.7163426876068115\n",
      "-------epoch  1965 -------\n",
      "epoch: 1965, train_loss: -8.756937026977539, valid_loss: -3.747683048248291\n",
      "-------epoch  1966 -------\n",
      "epoch: 1966, train_loss: -8.749813079833984, valid_loss: -3.706691265106201\n",
      "-------epoch  1967 -------\n",
      "epoch: 1967, train_loss: -8.735756874084473, valid_loss: -3.7378101348876953\n",
      "-------epoch  1968 -------\n",
      "epoch: 1968, train_loss: -8.727163314819336, valid_loss: -3.704617500305176\n",
      "-------epoch  1969 -------\n",
      "epoch: 1969, train_loss: -8.710763931274414, valid_loss: -3.737821340560913\n",
      "-------epoch  1970 -------\n",
      "epoch: 1970, train_loss: -8.701089859008789, valid_loss: -3.720468759536743\n",
      "-------epoch  1971 -------\n",
      "epoch: 1971, train_loss: -8.683423042297363, valid_loss: -3.7442736625671387\n",
      "-------epoch  1972 -------\n",
      "epoch: 1972, train_loss: -8.677960395812988, valid_loss: -3.7579545974731445\n",
      "-------epoch  1973 -------\n",
      "epoch: 1973, train_loss: -8.668854713439941, valid_loss: -3.7522547245025635\n",
      "-------epoch  1974 -------\n",
      "epoch: 1974, train_loss: -8.683591842651367, valid_loss: -3.785046100616455\n",
      "-------epoch  1975 -------\n",
      "epoch: 1975, train_loss: -8.703149795532227, valid_loss: -3.746699094772339\n",
      "-------epoch  1976 -------\n",
      "epoch: 1976, train_loss: -8.730708122253418, valid_loss: -3.779672384262085\n",
      "-------epoch  1977 -------\n",
      "epoch: 1977, train_loss: -8.752251625061035, valid_loss: -3.726930618286133\n",
      "-------epoch  1978 -------\n",
      "epoch: 1978, train_loss: -8.749242782592773, valid_loss: -3.7698423862457275\n",
      "-------epoch  1979 -------\n",
      "epoch: 1979, train_loss: -8.744439125061035, valid_loss: -3.7164671421051025\n",
      "-------epoch  1980 -------\n",
      "epoch: 1980, train_loss: -8.738109588623047, valid_loss: -3.7679898738861084\n",
      "-------epoch  1981 -------\n",
      "epoch: 1981, train_loss: -8.757973670959473, valid_loss: -3.7312746047973633\n",
      "-------epoch  1982 -------\n",
      "epoch: 1982, train_loss: -8.786352157592773, valid_loss: -3.7529215812683105\n",
      "-------epoch  1983 -------\n",
      "epoch: 1983, train_loss: -8.812493324279785, valid_loss: -3.7385995388031006\n",
      "-------epoch  1984 -------\n",
      "epoch: 1984, train_loss: -8.82160472869873, valid_loss: -3.728060245513916\n",
      "-------epoch  1985 -------\n",
      "epoch: 1985, train_loss: -8.81408977508545, valid_loss: -3.7333295345306396\n",
      "-------epoch  1986 -------\n",
      "epoch: 1986, train_loss: -8.800093650817871, valid_loss: -3.697577714920044\n",
      "-------epoch  1987 -------\n",
      "epoch: 1987, train_loss: -8.790090560913086, valid_loss: -3.7253408432006836\n",
      "-------epoch  1988 -------\n",
      "epoch: 1988, train_loss: -8.790692329406738, valid_loss: -3.6828558444976807\n",
      "-------epoch  1989 -------\n",
      "epoch: 1989, train_loss: -8.79477310180664, valid_loss: -3.71101713180542\n",
      "-------epoch  1990 -------\n",
      "epoch: 1990, train_loss: -8.79723072052002, valid_loss: -3.6829710006713867\n",
      "-------epoch  1991 -------\n",
      "epoch: 1991, train_loss: -8.789852142333984, valid_loss: -3.710855484008789\n",
      "-------epoch  1992 -------\n",
      "epoch: 1992, train_loss: -8.778470993041992, valid_loss: -3.675935983657837\n",
      "-------epoch  1993 -------\n",
      "epoch: 1993, train_loss: -8.763803482055664, valid_loss: -3.7095584869384766\n",
      "-------epoch  1994 -------\n",
      "epoch: 1994, train_loss: -8.759635925292969, valid_loss: -3.6702778339385986\n",
      "-------epoch  1995 -------\n",
      "epoch: 1995, train_loss: -8.759601593017578, valid_loss: -3.6988015174865723\n",
      "-------epoch  1996 -------\n",
      "epoch: 1996, train_loss: -8.769712448120117, valid_loss: -3.6521852016448975\n",
      "-------epoch  1997 -------\n",
      "epoch: 1997, train_loss: -8.7733793258667, valid_loss: -3.688361167907715\n",
      "-------epoch  1998 -------\n",
      "epoch: 1998, train_loss: -8.777495384216309, valid_loss: -3.635016441345215\n",
      "-------epoch  1999 -------\n",
      "epoch: 1999, train_loss: -8.770858764648438, valid_loss: -3.668318748474121\n",
      "-------epoch  2000 -------\n",
      "epoch: 2000, train_loss: -8.76654052734375, valid_loss: -3.618227005004883\n",
      "-------epoch  2001 -------\n",
      "epoch: 2001, train_loss: -8.756340980529785, valid_loss: -3.6529088020324707\n",
      "-------epoch  2002 -------\n",
      "epoch: 2002, train_loss: -8.754731178283691, valid_loss: -3.6029067039489746\n",
      "-------epoch  2003 -------\n",
      "epoch: 2003, train_loss: -8.7503662109375, valid_loss: -3.6374526023864746\n",
      "-------epoch  2004 -------\n",
      "epoch: 2004, train_loss: -8.757040023803711, valid_loss: -3.595243453979492\n",
      "-------epoch  2005 -------\n",
      "epoch: 2005, train_loss: -8.76127815246582, valid_loss: -3.6316843032836914\n",
      "-------epoch  2006 -------\n",
      "epoch: 2006, train_loss: -8.774351119995117, valid_loss: -3.5910048484802246\n",
      "-------epoch  2007 -------\n",
      "epoch: 2007, train_loss: -8.784193992614746, valid_loss: -3.6240386962890625\n",
      "-------epoch  2008 -------\n",
      "epoch: 2008, train_loss: -8.797301292419434, valid_loss: -3.5966975688934326\n",
      "-------epoch  2009 -------\n",
      "epoch: 2009, train_loss: -8.806205749511719, valid_loss: -3.6161866188049316\n",
      "-------epoch  2010 -------\n",
      "epoch: 2010, train_loss: -8.81374740600586, valid_loss: -3.5965635776519775\n",
      "-------epoch  2011 -------\n",
      "epoch: 2011, train_loss: -8.817277908325195, valid_loss: -3.6093239784240723\n",
      "-------epoch  2012 -------\n",
      "epoch: 2012, train_loss: -8.817917823791504, valid_loss: -3.597482204437256\n",
      "-------epoch  2013 -------\n",
      "epoch: 2013, train_loss: -8.815518379211426, valid_loss: -3.5972394943237305\n",
      "-------epoch  2014 -------\n",
      "epoch: 2014, train_loss: -8.810279846191406, valid_loss: -3.595365524291992\n",
      "-------epoch  2015 -------\n",
      "epoch: 2015, train_loss: -8.802922248840332, valid_loss: -3.5913567543029785\n",
      "-------epoch  2016 -------\n",
      "epoch: 2016, train_loss: -8.792323112487793, valid_loss: -3.5901660919189453\n",
      "-------epoch  2017 -------\n",
      "epoch: 2017, train_loss: -8.780572891235352, valid_loss: -3.585376262664795\n",
      "-------epoch  2018 -------\n",
      "epoch: 2018, train_loss: -8.76462459564209, valid_loss: -3.589340925216675\n",
      "-------epoch  2019 -------\n",
      "epoch: 2019, train_loss: -8.751070022583008, valid_loss: inf\n",
      "-------epoch  2020 -------\n",
      "epoch: 2020, train_loss: -8.735057830810547, valid_loss: -3.5874733924865723\n",
      "-------epoch  2021 -------\n",
      "epoch: 2021, train_loss: -8.730870246887207, valid_loss: inf\n",
      "-------epoch  2022 -------\n",
      "epoch: 2022, train_loss: -8.730010032653809, valid_loss: -3.5903594493865967\n",
      "-------epoch  2023 -------\n",
      "epoch: 2023, train_loss: -8.74635124206543, valid_loss: inf\n",
      "-------epoch  2024 -------\n",
      "epoch: 2024, train_loss: -8.764166831970215, valid_loss: inf\n",
      "-------epoch  2025 -------\n",
      "epoch: 2025, train_loss: -8.7850341796875, valid_loss: inf\n",
      "-------epoch  2026 -------\n",
      "epoch: 2026, train_loss: -8.794318199157715, valid_loss: inf\n",
      "-------epoch  2027 -------\n",
      "epoch: 2027, train_loss: -8.789312362670898, valid_loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  2028 -------\n",
      "epoch: 2028, train_loss: -8.759842872619629, valid_loss: inf\n",
      "-------epoch  2029 -------\n",
      "epoch: 2029, train_loss: -8.722448348999023, valid_loss: inf\n",
      "-------epoch  2030 -------\n",
      "epoch: 2030, train_loss: -8.666813850402832, valid_loss: inf\n",
      "-------epoch  2031 -------\n",
      "epoch: 2031, train_loss: -8.676840782165527, valid_loss: inf\n",
      "-------epoch  2032 -------\n",
      "epoch: 2032, train_loss: -8.678327560424805, valid_loss: inf\n",
      "-------epoch  2033 -------\n",
      "epoch: 2033, train_loss: -8.72639274597168, valid_loss: inf\n",
      "-------epoch  2034 -------\n",
      "epoch: 2034, train_loss: -8.745866775512695, valid_loss: inf\n",
      "early stop at: 2034; optimal epoch at: 1233\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "min_loss = 100.0 # starts from large number\n",
    "min_epoch = 0\n",
    "early_stop_epoch_size = 800\n",
    "\n",
    "f = open(PATH_LOG, \"w\")\n",
    "\n",
    "for epoch in range(4000):\n",
    "    optimizer.zero_grad()\n",
    "    pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi = model_mdn(pd_thetaphi_label_tensor)\n",
    "    # print(\"xx\", correlation_theta_phi)\n",
    "    loss = mdn_loss_fn(gt_thetaphi_label_tensor, pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.data.item())\n",
    "    \n",
    "    # Calculating validation data loss\n",
    "    total_valid_loss = 0.0\n",
    "    valid_pred = []\n",
    "    pi_valid, mu_theta_valid, mu_phi_valid, sigma_theta_valid, sigma_phi_valid, correlation_theta_phi_valid = model_mdn(pd_thetaphi_valid_label_tensor)\n",
    "    loss_valid = mdn_loss_fn(gt_thetaphi_valid_label_tensor,\n",
    "                       pi_valid, mu_theta_valid,\n",
    "                       mu_phi_valid,\n",
    "                       sigma_theta_valid,\n",
    "                       sigma_phi_valid,\n",
    "                       correlation_theta_phi_valid)\n",
    "    valid_loss.append(loss_valid.data.item())\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(\"-------epoch \", epoch, \"-------\")\n",
    "        # print(loss.data.tolist())\n",
    "        print(\"epoch: {}, train_loss: {}, valid_loss: {}\".format(epoch,\n",
    "                                                                 loss.data.item(),\n",
    "                                                                 loss_valid.data.item()))\n",
    "    # check early stop condition\n",
    "    if (min_loss > loss_valid.data.item()):\n",
    "        min_loss = loss_valid.data.item()\n",
    "        min_epoch = epoch\n",
    "        torch.save(model_mdn.state_dict(), PATH)\n",
    "    if (epoch - min_epoch > early_stop_epoch_size):\n",
    "        log = \"early stop at: \" + str(epoch) + \"; optimal epoch at: \" + str(min_epoch)\n",
    "        print(log)\n",
    "        f.write(log)\n",
    "        f.close()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 1529\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "valid_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), valid_loss[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, valid_loss_curve], ['Train Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch: 728, train_loss: -7.462584972381592, valid_loss: -4.700007915496826\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDN(\n",
       "  (l_h): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=20, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (l_pi): Linear(in_features=20, out_features=6, bias=True)\n",
       "  (l_mu_theta): Linear(in_features=20, out_features=6, bias=True)\n",
       "  (l_sigma_theta): Linear(in_features=20, out_features=6, bias=True)\n",
       "  (l_mu_phi): Linear(in_features=20, out_features=6, bias=True)\n",
       "  (l_sigma_phi): Linear(in_features=20, out_features=6, bias=True)\n",
       "  (l_correlation_theta_phi): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=6, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_mdn = MDN(2, n_hidden=20, n_gaussians=num_gaussians)\n",
    "load_model_mdn.load_state_dict(torch.load(PATH))\n",
    "load_model_mdn.eval()\n",
    "# mmodel = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.700007915496826\n"
     ]
    }
   ],
   "source": [
    "# Calculating validation data loss\n",
    "pi_valid, mu_theta_valid, mu_phi_valid, sigma_theta_valid, sigma_phi_valid, correlation_theta_phi_valid = load_model_mdn(pd_thetaphi_valid_label_tensor)\n",
    "loss_valid = mdn_loss_fn(gt_thetaphi_valid_label_tensor,\n",
    "                         pi_valid, mu_theta_valid,\n",
    "                         mu_phi_valid,\n",
    "                         sigma_theta_valid,\n",
    "                         sigma_phi_valid,\n",
    "                         correlation_theta_phi_valid)\n",
    "print(loss_valid.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('4000/without_normalization_with_earlystop/ng_6/train_loss.npy', np.asarray(train_loss))\n",
    "np.save('4000/without_normalization_with_earlystop/ng_6/valid_loss.npy', np.asarray(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.repeat(1, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi = model_mdn(pd_thetaphi_label_tensor)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# print(\"sample size: \", size)\n",
    "# print(\"num of gaus: \", n_gaussians)\n",
    "# print(\"correlation size: \", correlation_x_y.shape)\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_theta, mu_phi), dim=2)\n",
    "# build covariance matrix with standard bivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 2, 2])\n",
    "cov[:, :, 0, 0] = sigma_theta**2\n",
    "cov[:, :, 1, 1] = sigma_phi**2\n",
    "cov[:, :, 1, 0] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "cov[:, :, 0, 1] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "\n",
    "m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = m.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "PPD = torch.zeros([size, 2])\n",
    "for i in range(pd.shape[0]):\n",
    "    PPD[i, :] = pd[i,k[i],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPD = PPD.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = mu_theta.detach().numpy()\n",
    "mu_phi = mu_phi.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_thetaphi_label = gt_thetaphi_label_tensor.numpy()\n",
    "pd_thetaphi_label = pd_thetaphi_label_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gt_thetaphi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_theta_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_thetaphi_label[:,0], label='gt', linestyle='--')\n",
    "plt.plot(pd_thetaphi_label[:,0], label='pd', linestyle='--')\n",
    "plt.plot(PPD[:,0], label='pd_mdn', linestyle='--')\n",
    "plt.plot(mu_theta[:,0], label='mu 1')\n",
    "plt.plot(mu_theta[:,1], label='mu 2')\n",
    "# plt.plot(mu_x[:,2], label='mu 3')\n",
    "# plt.plot(mu_x[:,3], label='mu 4')\n",
    "# plt.plot(mu_x[:,4], label='mu 5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_thetaphi_label[:,1], label='gt', linestyle='--')\n",
    "plt.plot(pd_thetaphi_label[:,1], label='pd', linestyle='--')\n",
    "plt.plot(PPD[:,1], label='pd_mdn', linestyle='--')\n",
    "plt.plot(mu_phi[:,0], label='mu 1')\n",
    "plt.plot(mu_phi[:,1], label='mu 2')\n",
    "# plt.plot(mu_x[:,2], label='mu 3')\n",
    "# plt.plot(mu_x[:,3], label='mu 4')\n",
    "# plt.plot(mu_x[:,4], label='mu 5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mu_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = mu_theta*180\n",
    "mu_phi = mu_phi*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_theta = sigma_theta*180\n",
    "sigma_phi = sigma_phi*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label\n",
    "pd_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXYZ(theta, phi, d):\n",
    "    cos_theta = math.cos(theta * math.pi / 180.0 );\n",
    "    sin_theta = math.sin(theta * math.pi / 180.0 );\n",
    "    cos_phi   = math.cos(phi   * math.pi / 180.0 );\n",
    "    sin_phi   = math.sin(phi   * math.pi / 180.0 );\n",
    "\n",
    "    x = d * sin_theta * cos_phi;\n",
    "    y = d * sin_theta * sin_phi;\n",
    "    z = d * cos_theta;\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCircle(theta_list, phi_list, d):\n",
    "    size = len(theta_list)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    for i in range(size):\n",
    "        x, y, z = getXYZ(theta_list[i], phi_list[i], d)\n",
    "        # print(math.sqrt(x**2 + y**2 + z**2))\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        z_list.append(z)\n",
    "        \n",
    "        \n",
    "    return x_list, y_list, z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhi(theta, sigma_theta, sigma_phi):\n",
    "    return math.sqrt((1-theta*theta/sigma_theta/sigma_theta)*sigma_phi*sigma_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# add origin\n",
    "ax.scatter3D([0], [0], [0], cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_x = []\n",
    "cam_y = []\n",
    "cam_z = []\n",
    "\n",
    "for i in range(10):\n",
    "    index = i\n",
    "    selected_mean_theta = mu_theta[index, 1]\n",
    "    selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "    selected_mean_phi = mu_phi[index, 1]\n",
    "    selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "    half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "    theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "                  selected_mean_theta + selected_sigma_theta/2,\n",
    "                  selected_mean_theta,\n",
    "                  selected_mean_theta - selected_sigma_theta/2,\n",
    "                  selected_mean_theta - selected_sigma_theta,\n",
    "                  selected_mean_theta - selected_sigma_theta/2,\n",
    "                  selected_mean_theta,\n",
    "                  selected_mean_theta + selected_sigma_theta/2,\n",
    "                  selected_mean_theta + selected_sigma_theta]\n",
    "    phi_list   = [selected_mean_phi,\n",
    "                  selected_mean_phi - half_phi,\n",
    "                  selected_mean_phi - selected_sigma_phi,\n",
    "                  selected_mean_phi - half_phi,\n",
    "                  selected_mean_phi,\n",
    "                  selected_mean_phi + half_phi,\n",
    "                  selected_mean_phi + selected_sigma_phi,\n",
    "                  selected_mean_phi + half_phi,\n",
    "                  selected_mean_phi]\n",
    "\n",
    "    x = pd_label[index, 0]\n",
    "    y = pd_label[index, 1]\n",
    "    z = pd_label[index, 2]\n",
    "    \n",
    "    cam_x.append(x)\n",
    "    cam_y.append(y)\n",
    "    cam_z.append(z)\n",
    "\n",
    "    d = math.sqrt(x**2 + y**2 + z**2)\n",
    "    # print(\"d\", d)\n",
    "\n",
    "    x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "    ax.plot3D(x_list, y_list, z_list, 'red')\n",
    "    # ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "    # ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot3D(cam_x, cam_y, cam_z, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "selected_mean_theta = mu_theta[index, 1]\n",
    "selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "selected_mean_phi = mu_phi[index, 1]\n",
    "selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta - selected_sigma_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta + selected_sigma_theta]\n",
    "phi_list   = [selected_mean_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi - selected_sigma_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi + selected_sigma_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi]\n",
    "\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "selected_mean_theta = mu_theta[index, 1]\n",
    "selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "selected_mean_phi = mu_phi[index, 1]\n",
    "selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta - selected_sigma_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta + selected_sigma_theta]\n",
    "phi_list   = [selected_mean_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi - selected_sigma_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi + selected_sigma_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi]\n",
    "\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = math.sqrt(gt_label[0, 0]**2 + gt_label[0, 0]**2 + gt_label[0, 0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = -0.18\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "pdf = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            # pdf[i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            pdf[i, j, k] = likelihood[0, 0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = 0.01\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "pdf_1 = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            # pdf_![i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            pdf_1[i, j, k] = likelihood[0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "# radius = 0.01\n",
    "# density = 20 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)dd\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = -0.18\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "        \n",
    "pdf_all = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            pdf_all[i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            # pdf_4[i, j, k] = likelihood[0, 4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_label_tensor[0])\n",
    "print(pi[0])\n",
    "print(mu_x[0])\n",
    "print(mu_y[0])\n",
    "print(mu_z[0])\n",
    "print(sigma_x[0])\n",
    "print(sigma_y[0])\n",
    "print(sigma_z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "src = mlab.pipeline.scalar_field(pdf)\n",
    "mlab.pipeline.iso_surface(src, contours=[0.1, ], opacity=0.3, color = (0.8,0.0,0.0))\n",
    "# mlab.pipeline.iso_surface(src, contours=[pdf.max()-0.1*pdf.ptp(), ],)\n",
    "\n",
    "src_1 = mlab.pipeline.scalar_field(pdf_1)\n",
    "mlab.pipeline.iso_surface(src_1, contours=[0.1, ], opacity=0.3, color = (0.8,0.0,0.0))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)\n",
    "\n",
    "# src_half = mlab.pipeline.scalar_field(pdf_half)\n",
    "# mlab.pipeline.iso_surface(src_half, contours=[0.1, ], opacity=0.3, color = (0.0,0.8,0.0))\n",
    "# mlab.pipeline.iso_surface(src, contours=[pdf.max()-0.1*pdf.ptp(), ],)\n",
    "\n",
    "# src_1_half = mlab.pipeline.scalar_field(pdf_1_half)\n",
    "# mlab.pipeline.iso_surface(src_1_half, contours=[0.1, ], opacity=0.3, color = (0.0,0.8,0.0))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)\n",
    "\n",
    "src_all = mlab.pipeline.scalar_field(pdf_all)\n",
    "mlab.pipeline.iso_surface(src_all, contours=[0.1, ], opacity=0.3, color = (0.0,0.0,0.8))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.1318e-02 + 3.6224e-01 + 9.2028e-07 + 1.2253e-02 + 1.7447e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(s_x.flatten(), s_y.flatten(), s_z.flatten(), c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.1585, 0.1787, 0.1571, 0.3795, 0.1261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.0165, 0.0245, 0.1137, 2.8152, 0.0082])\n",
    "b = torch.tensor([0.0339, 0.0643, 0.1532, 0.5262, 0.2225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a*b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:, [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.stack((a, b), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(7) / (math.exp(1) + math.exp(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:, :, 0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gaussians = 5\n",
    "size = 14000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.unsqueeze(0).repeat(n_gaussians, 1, 1).unsqueeze(0).repeat(size, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.repeat(5, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.repeat(10, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[0, :, 0], input_tensor[0, :, 1], input_tensor[0, :, 2], 'gray')\n",
    "ax.scatter(gt_label_tensor[0,0], gt_label_tensor[0,1], gt_label_tensor[0,2], 'green')\n",
    "ax.scatter(pd_label_tensor[0,0], pd_label_tensor[0,1], pd_label_tensor[0,2], 'red')\n",
    "# ax.plot3D(gt_label_tensor[0:2,0], gt_label_tensor[0:2,1], gt_label_tensor[0:2,2], 'green')\n",
    "# ax.plot3D(pd_label_tensor[0:2,0], pd_label_tensor[0:2,1], pd_label_tensor[0:2,2], 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], 'gray')\n",
    "ax.scatter(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], c = 'gray')\n",
    "\n",
    "ax.plot3D(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], 'green')\n",
    "ax.scatter(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], c = 'green')\n",
    "\n",
    "ax.plot3D(gt_label_tensor[0:3,0], gt_label_tensor[0:3,1], gt_label_tensor[0:3,2], 'black')\n",
    "ax.plot3D(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], 'red')\n",
    "ax.scatter(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], c = 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pd = pd_label_tensor[:, 0].reshape(-1,1)\n",
    "y_pd = pd_label_tensor[:, 1].reshape(-1,1)\n",
    "z_pd = pd_label_tensor[:, 2].reshape(-1,1)\n",
    "\n",
    "x_gt = gt_label_tensor[:, 0].reshape(-1,1)\n",
    "y_gt = gt_label_tensor[:, 1].reshape(-1,1)\n",
    "z_gt = gt_label_tensor[:, 2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt = x_gt.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_gt)\n",
    "plt.plot(x_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply MDN on x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 14985\n",
    "\n",
    "x_data = torch.Tensor(x_pd)\n",
    "y_data = torch.Tensor(x_gt)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_data, y_data, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.z_h = nn.Sequential(\n",
    "            nn.Linear(1, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_mu = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(n_hidden, n_gaussians)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_h = self.z_h(x)\n",
    "        pi = F.softmax(self.z_pi(z_h), -1)\n",
    "        mu = self.z_mu(z_h)\n",
    "        sigma = torch.exp(self.z_sigma(z_h))\n",
    "        return pi, mu, sigma\n",
    "\n",
    "model = MDN(n_hidden=20, n_gaussians=5)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, mu, sigma, pi):\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    loss = torch.exp(m.log_prob(y))\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "    pi, mu, sigma = model(x_data)\n",
    "    loss = mdn_loss_fn(y_data, mu, sigma, pi)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, sigma = model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the highest index of the 5 elements in each row\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "# lt.scatter(x_data, y_data, alpha=0.4)\n",
    "# plt.scatter(x_data, y_pred, alpha=0.8, color='red')\n",
    "# plt.scatter(x_test[0], y_pred[0], alpha=0.4, color='yellow')\n",
    "plt.scatter(x_data, mu[:,0].detach(), label='mean 0')\n",
    "plt.scatter(x_data, mu[:,1].detach(), label='mean 1')\n",
    "plt.scatter(x_data, mu[:,2].detach(), label='mean 2')\n",
    "plt.scatter(x_data, mu[:,3].detach(), label='mean 3')\n",
    "plt.scatter(x_data, mu[:,4].detach(), label='mean 4')\n",
    "# plt.plot(x_test.numpy(), mu[:,0].detach().numpy())\n",
    "# plt.scatter(x_data[:20], y_data[:20], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pi.detach().numpy()\n",
    "mu = mu.detach().numpy()\n",
    "sigma = sigma.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros((n_samples, 100))\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    each = []\n",
    "    for j in range(5):\n",
    "        if (j == 0):\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = y\n",
    "        else:\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = [p + q for p, q in zip(cumulated_y, y)]\n",
    "    p[i, :] = cumulated_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "for i in range(0,n_samples):\n",
    "    ax.plot3D([x_data[i, 0]]*100, x, p[i, :])\n",
    "\n",
    "ax.set_xlabel('index')\n",
    "ax.set_ylabel('x')\n",
    "ax.set_zlabel('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply MDN on 3D projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1_1 = torch.Tensor([1, 1, 1])\n",
    "mean_1_2 = torch.Tensor([2, 2, 2])\n",
    "mean_1_3 = torch.Tensor([3, 3, 3])\n",
    "mean_1_4 = torch.Tensor([4, 4, 4])\n",
    "mean_1_5 = torch.Tensor([5, 5, 5])\n",
    "\n",
    "mean_2_1 = torch.Tensor([6, 6, 6])\n",
    "mean_2_2 = torch.Tensor([7, 7, 7])\n",
    "mean_2_3 = torch.Tensor([8, 8, 8])\n",
    "mean_2_4 = torch.Tensor([9, 9, 9])\n",
    "mean_2_5 = torch.Tensor([10, 10, 10])\n",
    "\n",
    "sigma_1 = 1\n",
    "sigma_2 = 1\n",
    "sigma_3 = 1\n",
    "cov_1_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 3\n",
    "sigma_2 = 3\n",
    "sigma_3 = 3\n",
    "cov_1_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov_1_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 5\n",
    "sigma_2 = 5\n",
    "sigma_3 = 5\n",
    "cov_1_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 6\n",
    "sigma_2 = 6\n",
    "sigma_3 = 6\n",
    "cov_2_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 7\n",
    "sigma_2 = 7\n",
    "sigma_3 = 7\n",
    "cov_2_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 8\n",
    "sigma_2 = 8\n",
    "sigma_3 = 8\n",
    "cov_2_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 9\n",
    "sigma_2 = 9\n",
    "sigma_3 = 9\n",
    "cov_2_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 10\n",
    "sigma_2 = 10\n",
    "sigma_3 = 10\n",
    "cov_2_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[1, 2] = rho_2_3*sigma_2*sigma_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = torch.stack((mean_1_1, mean_1_2, mean_1_3, mean_1_4, mean_1_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2 = torch.stack((mean_2_1, mean_2_2, mean_2_3, mean_2_4, mean_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mean_1, mean_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1 = torch.stack((cov_1_1, cov_1_2, cov_1_3, cov_1_4, cov_1_5), dim=0)\n",
    "cov_2 = torch.stack((cov_2_1, cov_2_2, cov_2_3, cov_2_4, cov_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov_1, cov_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack((y, y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = mean_1.unsqueeze(0)\n",
    "cov_1 = cov_1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,2,3]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov, cov_1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov_1)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "s = np.zeros([21, 21, 21])\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1001)\n",
    "y = np.linspace(0, 10, 1001)\n",
    "z = np.linspace(0, 10, 1001)\n",
    "x_p = x + 0.5 * np.random.randn(1001)\n",
    "y_p = y + 0.5 * np.random.randn(1001)\n",
    "z_p = z + 0.5 * np.random.randn(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "ax.scatter(x_p, y_p, z_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        # self.l_correlation_x_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_x_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_y_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_correlation_x_y = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_x_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_y_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_x = self.l_mu_x(h)\n",
    "        # print(\"mu_x\", pi.shape)\n",
    "        mu_y = self.l_mu_y(h)\n",
    "        # print(\"mu_y\", pi.shape)\n",
    "        mu_z = self.l_mu_z(h)\n",
    "        # print(\"mu_z\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_x = torch.exp(self.l_sigma_x(h))\n",
    "        # print(\"sigma_x\", pi.shape)\n",
    "        sigma_y = torch.exp(self.l_sigma_y(h))\n",
    "        # print(\"sigma_y\", pi.shape)\n",
    "        sigma_z = torch.exp(self.l_sigma_z(h))\n",
    "        # print(\"sigma_z\", pi.shape)\n",
    "        \n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_x_y = self.l_correlation_x_y(h)\n",
    "        # print(\"correlation_x_y\", pi.shape)\n",
    "        # print(\"correlation_x_y[0]\", correlation_x_y[0, :])\n",
    "        correlation_x_z = self.l_correlation_x_z(h)\n",
    "        # print(\"correlation_x_z\", pi.shape)\n",
    "        # print(\"correlation_x_z[0]\", correlation_x_z[0, :])\n",
    "        correlation_y_z = self.l_correlation_y_z(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z\n",
    "\n",
    "model = MDN(3, n_hidden=20, n_gaussians=5)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z):\n",
    "    # print(\"mu_x shape\", mu_x.shape)\n",
    "    # mu = torch.stack((mu_x, mu_y, mu_z), 2)\n",
    "    # print(\"mu shape \", mu.shape)\n",
    "    # print(\"mu[0]\", mu[0])\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    size = y.shape[0]\n",
    "    n_gaussians = pi.shape[1]\n",
    "    # print(\"sample size: \", size)\n",
    "    # print(\"num of gaus: \", n_gaussians)\n",
    "    # print(\"correlation size: \", correlation_x_y.shape)\n",
    "    # build mean matrix\n",
    "    mean = torch.zeros([size, n_gaussians, 3])\n",
    "    # build covariance matrix with standard trivariate normal distribution\n",
    "    cov = torch.ones([size, n_gaussians, 3, 3])\n",
    "    cov[:, :, 0, 1] = correlation_x_y\n",
    "    cov[:, :, 1, 0] = correlation_x_y\n",
    "    cov[:, :, 0, 2] = correlation_x_z\n",
    "    cov[:, :, 2, 0] = correlation_x_z\n",
    "    cov[:, :, 1, 2] = correlation_y_z\n",
    "    cov[:, :, 2, 1] = correlation_y_z\n",
    "    \n",
    "    print(\"mean: \", mean.shape)\n",
    "    print(\"cov: \", cov.shape)\n",
    "    \n",
    "    print(\"mean is cuda: \", mean.is_cuda)\n",
    "    print(\" cov is cuda: \", cov.is_cuda)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    w = y[0]**2*(correlation_y_z**2 - 1) +\\\n",
    "        y[1]**2*(correlation_x_z**2 - 1) +\\\n",
    "        y[2]**2*(correlation_x_y**2 - 1) +\\\n",
    "        2*(y[0]*y[:1]*(correlation_x_y - correlation_x_z*correlation_y_z) +\\\n",
    "           y[0]*y[:2]*(correlation_x_z - correlation_x_y*correlation_y_z) +\\\n",
    "           y[1]*y[:2]*(correlation_y_z - correlation_x_y*correlation_x_z))\n",
    "    \n",
    "    return w\n",
    "    '''\n",
    "    '''\n",
    "    cov = torch.Tensor([[sigma_x**2, 0, 0], [0, sigma_y**2, 0], [0, 0, sigma_z**2]])\n",
    "    cov[1, 0] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[2, 0] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[2, 1] = correlation_y_z*sigma_y*sigma_z\n",
    "    cov[0, 1] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[0, 2] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[1, 2] = correlation_y_z*sigma_y*sigma_z\n",
    "    '''\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "    # normalize input y\n",
    "    y_repeat = y.unsqueeze(1).repeat(1, n_gaussians, 1)\n",
    "    # print(y_repeat.shape)\n",
    "    # print(y_repeat[0])\n",
    "    mu_all = torch.stack((mu_x, mu_y, mu_z), dim = 2)\n",
    "    print(\"mu_all shape: \", mu_all.shape)\n",
    "    y_sub = y_repeat - mu_all\n",
    "    print(\"y_sub shape\", y_sub.shape)\n",
    "    sigma_all = torch.stack((sigma_x, sigma_y, sigma_z), dim = 2)\n",
    "    print(\"sigma\", sigma_x.shape)\n",
    "    print(\"sigma all\", sigma_all.shape)\n",
    "    \n",
    "    y_normal = torch.div(y_sub, sigma_all)\n",
    "    \n",
    "    print(\"y_normal size\", y_normal.shape)\n",
    "    \n",
    "    # y: [x, y, z]; row: x, y, z; column: N, number of samples\n",
    "    # y: N x 3\n",
    "    # loss: N x 1\n",
    "    print(y_normal.is_cuda)\n",
    "    loss = torch.exp(m.log_prob(y_normal))\n",
    "    print(\"loss shape: \", loss.shape)\n",
    "    print(\"pi shape: \", pi.shape)\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z = model(pd_label_tensor)\n",
    "    loss = mdn_loss_fn(gt_label_tensor, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z)\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # if epoch % 1000 == 0:\n",
    "    #     print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2,3)\n",
    "print(t)\n",
    "tr = t.repeat(2, 3)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2],[4,5],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.Tensor([[7,8],[6,4],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[0,0,0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0, 0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_y_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2939**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(gt_label_tensor[:,0], gt_label_tensor[:,1], gt_label_tensor[:,2], 'gray')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_label_tensor[:,2])\n",
    "plt.plot(pd_label_tensor[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model/with_early_stop_after_400/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "\n",
    "\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_1_loss.append(total_test_loss/te    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)st_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_1, test_set_y_1, test_set_z_1, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find window size 6 is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_1_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input window size\", fontsize=14)\n",
    "plt.ylabel(\"MSE loss\", fontsize=14)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.ylim([0.017,0.0255])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot result without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model_v3/with_early_stop_after_400_without_normalization/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    '''\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "    '''\n",
    "    '''\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "    '''\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_3):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_3_loss.append(total_test_loss/test_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_3, test_set_y_3, test_set_z_3, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test_3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_3_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input Window Size\", fontsize=12)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "# plt.ylim([0.017,0.0255])\n",
    "plt.ylim([0.025,0.039])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
