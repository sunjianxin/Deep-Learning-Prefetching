{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to analyze the gt points' distribution with respect to predicted points location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, min_max_scaling, min_max_scaling_radius, normalize_one, construct_train_valid_tensor, construct_test_tensor, show_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = np.load(\"input_tensor.npy\")\n",
    "gt_label = np.load(\"gt_label_tensor.npy\")\n",
    "pd_label = np.load(\"pd_label_tensor.npy\")\n",
    "\n",
    "input_valid_train = np.load(\"input_valid_tensor.npy\")\n",
    "gt_valid_label = np.load(\"gt_valid_label_tensor.npy\")\n",
    "pd_valid_label = np.load(\"pd_valid_label_tensor.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to sperical coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistanceThetaPhi(xyz): # N x 3 -> N x 2, xyz_tensor must be two dimension\n",
    "    size = xyz.shape[0]\n",
    "    dtp = np.zeros([size, 3])\n",
    "    for i in range(size):\n",
    "        x = xyz[i, 0]\n",
    "        y = xyz[i, 1]\n",
    "        z = xyz[i, 2]\n",
    "        # print(z)\n",
    "        d = math.sqrt(x*x + y*y + z*z)\n",
    "        theta = math.atan(math.sqrt(x*x + y*y)/z)/math.pi*180\n",
    "        phi = math.atan(y/x)/math.pi*180\n",
    "        \n",
    "        if (d >= 2.0):\n",
    "            d= 2.0;\n",
    "        elif (d <= 1.0):\n",
    "            d= 1.0;\n",
    "        \n",
    "        if (theta < 0):\n",
    "            theta = 180 + theta;\n",
    "        \n",
    "        if (phi > 0):\n",
    "            if (x > 0):\n",
    "                phi = phi;\n",
    "            else:\n",
    "                phi = 180 + phi;\n",
    "        else:\n",
    "            if (x < 0):\n",
    "                phi = 180 + phi;\n",
    "            else:\n",
    "                phi = 360 + phi;\n",
    "        \n",
    "        \n",
    "        dtp[i, 0] = d\n",
    "        dtp[i, 1] = theta\n",
    "        dtp[i, 2] = phi\n",
    "    \n",
    "    return dtp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function's validity\n",
    "pd_dthetaphi_label_tensor = getDistanceThetaPhi(pd_label_tensor)\n",
    "gt_dthetaphi_label_tensor = getDistanceThetaPhi(gt_label_tensor)\n",
    "\n",
    "test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('test/test_1.csv')\n",
    "test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('test/test_2.csv')\n",
    "test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('test/test_3.csv')\n",
    "test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('test/test_4.csv')\n",
    "test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('test/test_5.csv')\n",
    "\n",
    "test_set_x_1_direct, test_set_y_1_direct, test_set_z_1_direct = load_test_data('test/test_1_direct.csv')\n",
    "test_set_x_2_direct, test_set_y_2_direct, test_set_z_2_direct = load_test_data('test/test_2_direct.csv')\n",
    "test_set_x_3_direct, test_set_y_3_direct, test_set_z_3_direct = load_test_data('test/test_3_direct.csv')\n",
    "test_set_x_4_direct, test_set_y_4_direct, test_set_z_4_direct = load_test_data('test/test_4_direct.csv')\n",
    "test_set_x_5_direct, test_set_y_5_direct, test_set_z_5_direct = load_test_data('test/test_5_direct.csv')\n",
    "\n",
    "input_xyz = np.zeros([400, 3])\n",
    "for i in range(400):\n",
    "    input_xyz[i, 0] = test_set_x_1[i]\n",
    "    input_xyz[i, 1] = test_set_y_1[i]\n",
    "    input_xyz[i, 2] = test_set_z_1[i]\n",
    "    \n",
    "input_dthetaphi_tensor = getDistanceThetaPhi(input_xyz)\n",
    "\n",
    "x_diff = [i - j for i, j in zip(test_set_x_1_direct, input_dthetaphi_tensor[:,0].tolist())]\n",
    "y_diff = [i - j for i, j in zip(test_set_y_1_direct, input_dthetaphi_tensor[:,1].tolist())]\n",
    "z_diff = [i - j for i, j in zip(test_set_z_1_direct, input_dthetaphi_tensor[:,2].tolist())]\n",
    "\n",
    "# plt.plot(test_set_x_1_direct, label='d')\n",
    "# plt.plot(test_set_y_1_direct, label='theta')\n",
    "plt.plot(test_set_z_1_direct, label='pi')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_thetaphi_label_tensor shape: torch.Size([14985, 2])\n",
      "pd_thetaphi_label_tensor shape: torch.Size([14985, 2])\n",
      "gt_thetaphi_valid_label_tensor shape: torch.Size([397, 2])\n",
      "pd_thetaphi_valid_label_tensor shape: torch.Size([397, 2])\n"
     ]
    }
   ],
   "source": [
    "# construct 2d theta phi training tensor\n",
    "gt_dthetaphi_label = getDistanceThetaPhi(gt_label)\n",
    "pd_dthetaphi_label = getDistanceThetaPhi(pd_label)\n",
    "gt_thetaphi_label_tensor = torch.Tensor(gt_dthetaphi_label[:,1:])\n",
    "pd_thetaphi_label_tensor = torch.Tensor(pd_dthetaphi_label[:,1:])\n",
    "print(\"gt_thetaphi_label_tensor shape:\", gt_thetaphi_label_tensor.shape)\n",
    "print(\"pd_thetaphi_label_tensor shape:\", pd_thetaphi_label_tensor.shape)\n",
    "\n",
    "# construct 2d theta phi valid tensor\n",
    "gt_dthetaphi_valid_label = getDistanceThetaPhi(gt_valid_label)\n",
    "pd_dthetaphi_valid_label = getDistanceThetaPhi(pd_valid_label)\n",
    "gt_thetaphi_valid_label_tensor = torch.Tensor(gt_dthetaphi_valid_label[:,1:])\n",
    "pd_thetaphi_valid_label_tensor = torch.Tensor(pd_dthetaphi_valid_label[:,1:])\n",
    "print(\"gt_thetaphi_valid_label_tensor shape:\", gt_thetaphi_valid_label_tensor.shape)\n",
    "print(\"pd_thetaphi_valid_label_tensor shape:\", pd_thetaphi_valid_label_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_thetaphi_label_tensor = torch.matmul(gt_thetaphi_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "pd_thetaphi_label_tensor = torch.matmul(pd_thetaphi_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "\n",
    "gt_thetaphi_valid_label_tensor = torch.matmul(gt_thetaphi_valid_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "pd_thetaphi_valid_label_tensor = torch.matmul(pd_thetaphi_valid_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_theta = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_theta = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_phi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_phi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        \n",
    "        self.l_correlation_theta_phi = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        \n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        \n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_theta = self.l_mu_theta(h)\n",
    "        # print(\"mu_theta\", pi.shape)\n",
    "        # print(\"mu_theta out\", mu_theta[0])\n",
    "        mu_phi = self.l_mu_phi(h)\n",
    "        # print(\"mu_phi\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_theta = torch.exp(self.l_sigma_theta(h))\n",
    "        # print(\"sigma_theta\", sigma_theta.shape)\n",
    "        sigma_phi = torch.exp(self.l_sigma_phi(h))\n",
    "        # print(\"sigma_phi\", sigma_phi.shape)\n",
    "\n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_theta_phi = self.l_correlation_theta_phi(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi):\n",
    "    size = y.shape[0] # N\n",
    "    n_gaussians = pi.shape[1] # G\n",
    "    '''\n",
    "    print(\"sample size: \", size)\n",
    "    print(\"num of gaus: \", n_gaussians)\n",
    "    print(\"mu_theta size: \", mu_theta.shape)\n",
    "    print(\"mu_phi size: \", mu_phi.shape)\n",
    "    print(\"sigma_theta size: \", sigma_theta.shape)\n",
    "    print(\"sigma_phi size: \", sigma_phi.shape)\n",
    "    print(\"correlation_theta_phi size: \", correlation_theta_phi.shape)\n",
    "    '''\n",
    "    x_theta = y[:, 0].unsqueeze(1) # N x 1\n",
    "    # print(\"x_theta shape: \", x_theta.shape)\n",
    "    x_theta = x_theta.repeat(1, n_gaussians) #  N x G\n",
    "    # print(\"x_theta shape: \", x_theta.shape)\n",
    "    x_phi = y[:, 1].unsqueeze(1)   # N X 1\n",
    "    # print(\"x_phi shape: \", x_phi.shape)\n",
    "    x_phi = x_phi.repeat(1, n_gaussians) #  N x G\n",
    "    # print(\"x_phi shape: \", x_phi.shape)\n",
    "    \n",
    "    \n",
    "    # mu_theta: N x G\n",
    "    # sigma_theta: N x G\n",
    "    # correlation_theta_phi: N x G\n",
    "\n",
    "    z = (x_theta - mu_theta)**2/(sigma_theta**2) + \\\n",
    "        (x_phi - mu_phi)**2/(sigma_phi**2) - \\\n",
    "        2*correlation_theta_phi*(x_theta - mu_theta)*(x_phi - mu_phi)/sigma_theta/sigma_phi\n",
    "    '''\n",
    "    print(\"=======\")\n",
    "    print(\"(x_theta - mu_theta)**2/(sigma_theta**2)\", ((x_theta - mu_theta)**2/(sigma_theta**2))[0])\n",
    "    print(\"x_theta: \", x_theta[0])\n",
    "    print(\"mu_theta: \", mu_theta[0])\n",
    "    print(\"sigma_theta: \", sigma_theta[0])\n",
    "    '''\n",
    "\n",
    "    # print(\"(x_phi - mu_phi)**2/(sigma_phi**2)\", ((x_phi - mu_phi)**2/(sigma_phi**2))[0])\n",
    "    # print(\"2*correlation_theta_phi*(x_theta - mu_theta)*(x_phi - mu_phi)/sigma_theta/sigma_phi\", (2*correlation_theta_phi*(x_theta - mu_theta)*(x_phi - mu_phi)/sigma_theta/sigma_phi)[0])\n",
    "    # print(\"z shape: \", z.shape)\n",
    "    \n",
    "    PI = np.pi\n",
    "    \n",
    "    likelihood = 1/(2*PI*sigma_theta*sigma_phi*torch.sqrt(1 - correlation_theta_phi**2))*\\\n",
    "                 torch.exp(-z/(2*(1-correlation_theta_phi**2))) # N X G\n",
    "    # print(\"likelihood\", likelihood[0:10])\n",
    "    \n",
    "    # print(\"likelihood shape: \", likelihood.shape)\n",
    "    \n",
    "    \n",
    "    # print(\"pi shape: \", pi.shape) # N x 2\n",
    "    loss = torch.sum(likelihood * pi, dim=1) # N X 1\n",
    "    # print(\"loss sum\", loss[0:10])\n",
    "    # print(likelihood.max())\n",
    "    '''\n",
    "    for i in range(loss.shape[0]):\n",
    "        if loss[i] > 1.5:\n",
    "            print(\"loss: \", loss[i])\n",
    "            print(\"likelihood: \", likelihood[i])\n",
    "            print(\"mu array  : \", pi[i])\n",
    "            # print(\"likelihood shape: \", likelihood.shape) # 14985 x 5\n",
    "    '''\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gaussians = 1\n",
    "PATH = \"model_v2/with_early_stop_after_400_without_normalization/model_ng_1.pt\"\n",
    "PATH_LOG = \"model_v2/with_early_stop_after_400_without_normalization/model_ng_1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mdn = MDN(2, n_hidden=20, n_gaussians=num_gaussians)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mdn.parameters())\n",
    "\n",
    "# learning_rate = 0.0001\n",
    "# optimizer = torch.optim.SGD(model_mdn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 2.962233066558838, valid_loss: 2.826390504837036\n",
      "epoch: 1, train_loss: 2.926262378692627, valid_loss: 2.7972495555877686\n",
      "epoch: 2, train_loss: 2.8920674324035645, valid_loss: 2.7687835693359375\n",
      "epoch: 3, train_loss: 2.8591856956481934, valid_loss: 2.740734577178955\n",
      "epoch: 4, train_loss: 2.8272252082824707, valid_loss: 2.7129974365234375\n",
      "epoch: 5, train_loss: 2.795943260192871, valid_loss: 2.6854968070983887\n",
      "epoch: 6, train_loss: 2.7651777267456055, valid_loss: 2.6581737995147705\n",
      "epoch: 7, train_loss: 2.7348105907440186, valid_loss: 2.630988121032715\n",
      "epoch: 8, train_loss: 2.704756498336792, valid_loss: 2.603904962539673\n",
      "epoch: 9, train_loss: 2.6749539375305176, valid_loss: 2.576894998550415\n",
      "epoch: 10, train_loss: 2.6453473567962646, valid_loss: 2.5499327182769775\n",
      "epoch: 11, train_loss: 2.615895986557007, valid_loss: 2.5229945182800293\n",
      "epoch: 12, train_loss: 2.5865609645843506, valid_loss: 2.496058225631714\n",
      "epoch: 13, train_loss: 2.557307004928589, valid_loss: 2.4691050052642822\n",
      "epoch: 14, train_loss: 2.5281052589416504, valid_loss: 2.442117691040039\n",
      "epoch: 15, train_loss: 2.4989285469055176, valid_loss: 2.4150795936584473\n",
      "epoch: 16, train_loss: 2.4697539806365967, valid_loss: 2.387977361679077\n",
      "epoch: 17, train_loss: 2.440556764602661, valid_loss: 2.3607988357543945\n",
      "epoch: 18, train_loss: 2.411318778991699, valid_loss: 2.333531379699707\n",
      "epoch: 19, train_loss: 2.382024049758911, valid_loss: 2.3061671257019043\n",
      "epoch: 20, train_loss: 2.352653980255127, valid_loss: 2.278696060180664\n",
      "epoch: 21, train_loss: 2.3231964111328125, valid_loss: 2.2511117458343506\n",
      "epoch: 22, train_loss: 2.293637990951538, valid_loss: 2.22340726852417\n",
      "epoch: 23, train_loss: 2.2639670372009277, valid_loss: 2.195577383041382\n",
      "epoch: 24, train_loss: 2.2341744899749756, valid_loss: 2.1676173210144043\n",
      "epoch: 25, train_loss: 2.204249858856201, valid_loss: 2.1395227909088135\n",
      "epoch: 26, train_loss: 2.1741864681243896, valid_loss: 2.111290454864502\n",
      "epoch: 27, train_loss: 2.1439759731292725, valid_loss: 2.0829179286956787\n",
      "epoch: 28, train_loss: 2.113612413406372, valid_loss: 2.0544023513793945\n",
      "epoch: 29, train_loss: 2.0830893516540527, valid_loss: 2.025743007659912\n",
      "epoch: 30, train_loss: 2.0524027347564697, valid_loss: 1.9969381093978882\n",
      "epoch: 31, train_loss: 2.021547794342041, valid_loss: 1.9679880142211914\n",
      "epoch: 32, train_loss: 1.9905200004577637, valid_loss: 1.9388904571533203\n",
      "epoch: 33, train_loss: 1.959316611289978, valid_loss: 1.9096477031707764\n",
      "epoch: 34, train_loss: 1.927935242652893, valid_loss: 1.8802602291107178\n",
      "epoch: 35, train_loss: 1.8963730335235596, valid_loss: 1.8507287502288818\n",
      "epoch: 36, train_loss: 1.8646292686462402, valid_loss: 1.8210560083389282\n",
      "epoch: 37, train_loss: 1.83270263671875, valid_loss: 1.7912437915802002\n",
      "epoch: 38, train_loss: 1.8005927801132202, valid_loss: 1.7612953186035156\n",
      "epoch: 39, train_loss: 1.76829993724823, valid_loss: 1.7312145233154297\n",
      "epoch: 40, train_loss: 1.7358245849609375, valid_loss: 1.7010048627853394\n",
      "epoch: 41, train_loss: 1.703169345855713, valid_loss: 1.6706725358963013\n",
      "epoch: 42, train_loss: 1.670336365699768, valid_loss: 1.6402225494384766\n",
      "epoch: 43, train_loss: 1.6373279094696045, valid_loss: 1.6096614599227905\n",
      "epoch: 44, train_loss: 1.6041486263275146, valid_loss: 1.5789968967437744\n",
      "epoch: 45, train_loss: 1.5708034038543701, valid_loss: 1.5482374429702759\n",
      "epoch: 46, train_loss: 1.5372968912124634, valid_loss: 1.5173916816711426\n",
      "epoch: 47, train_loss: 1.5036364793777466, valid_loss: 1.4864702224731445\n",
      "epoch: 48, train_loss: 1.4698295593261719, valid_loss: 1.45548415184021\n",
      "epoch: 49, train_loss: 1.4358843564987183, valid_loss: 1.4244464635849\n",
      "epoch: 50, train_loss: 1.4018118381500244, valid_loss: 1.3933706283569336\n",
      "epoch: 51, train_loss: 1.367621660232544, valid_loss: 1.3622711896896362\n",
      "epoch: 52, train_loss: 1.3333263397216797, valid_loss: 1.3311647176742554\n",
      "epoch: 53, train_loss: 1.2989394664764404, valid_loss: 1.3000681400299072\n",
      "epoch: 54, train_loss: 1.264475703239441, valid_loss: 1.2690010070800781\n",
      "epoch: 55, train_loss: 1.2299513816833496, valid_loss: 1.2379828691482544\n",
      "epoch: 56, train_loss: 1.19538414478302, valid_loss: 1.2070358991622925\n",
      "epoch: 57, train_loss: 1.160792589187622, valid_loss: 1.1761823892593384\n",
      "epoch: 58, train_loss: 1.1261975765228271, valid_loss: 1.1454468965530396\n",
      "epoch: 59, train_loss: 1.0916210412979126, valid_loss: 1.1148544549942017\n",
      "epoch: 60, train_loss: 1.05708646774292, valid_loss: 1.0844314098358154\n",
      "epoch: 61, train_loss: 1.0226181745529175, valid_loss: 1.0542044639587402\n",
      "epoch: 62, train_loss: 0.988241970539093, valid_loss: 1.0242005586624146\n",
      "epoch: 63, train_loss: 0.9539842009544373, valid_loss: 0.994446337223053\n",
      "epoch: 64, train_loss: 0.9198710918426514, valid_loss: 0.9649674892425537\n",
      "epoch: 65, train_loss: 0.8859288096427917, valid_loss: 0.9357876181602478\n",
      "epoch: 66, train_loss: 0.85218346118927, valid_loss: 0.9069283604621887\n",
      "epoch: 67, train_loss: 0.8186578750610352, valid_loss: 0.8784058094024658\n",
      "epoch: 68, train_loss: 0.7853729128837585, valid_loss: 0.8502321243286133\n",
      "epoch: 69, train_loss: 0.7523460388183594, valid_loss: 0.8224121332168579\n",
      "epoch: 70, train_loss: 0.7195879220962524, valid_loss: 0.7949426174163818\n",
      "epoch: 71, train_loss: 0.6871034502983093, valid_loss: 0.7678111791610718\n",
      "epoch: 72, train_loss: 0.6548905968666077, valid_loss: 0.7409940361976624\n",
      "epoch: 73, train_loss: 0.6229375600814819, valid_loss: 0.7144578695297241\n",
      "epoch: 74, train_loss: 0.5912233591079712, valid_loss: 0.688157320022583\n",
      "epoch: 75, train_loss: 0.5597185492515564, valid_loss: 0.66203773021698\n",
      "epoch: 76, train_loss: 0.5283849835395813, valid_loss: 0.6360362768173218\n",
      "epoch: 77, train_loss: 0.49717816710472107, valid_loss: 0.6100854277610779\n",
      "epoch: 78, train_loss: 0.46605023741722107, valid_loss: 0.5841174125671387\n",
      "epoch: 79, train_loss: 0.4349536597728729, valid_loss: 0.5580689311027527\n",
      "epoch: 80, train_loss: 0.4038464426994324, valid_loss: 0.5318871140480042\n",
      "epoch: 81, train_loss: 0.37269678711891174, valid_loss: 0.5055351853370667\n",
      "epoch: 82, train_loss: 0.3414880633354187, valid_loss: 0.4789976477622986\n",
      "epoch: 83, train_loss: 0.3102239966392517, valid_loss: 0.45228469371795654\n",
      "epoch: 84, train_loss: 0.2789303660392761, valid_loss: 0.425434410572052\n",
      "epoch: 85, train_loss: 0.24765637516975403, valid_loss: 0.3985128402709961\n",
      "epoch: 86, train_loss: 0.21647213399410248, valid_loss: 0.3716121315956116\n",
      "epoch: 87, train_loss: 0.18546374142169952, valid_loss: 0.34484460949897766\n",
      "epoch: 88, train_loss: 0.15472470223903656, valid_loss: 0.31833580136299133\n",
      "epoch: 89, train_loss: 0.12434472143650055, valid_loss: 0.2922149896621704\n",
      "epoch: 90, train_loss: 0.09439840912818909, valid_loss: 0.2666056752204895\n",
      "epoch: 91, train_loss: 0.06493251025676727, valid_loss: 0.24161706864833832\n",
      "epoch: 92, train_loss: 0.03595809265971184, valid_loss: 0.2173381894826889\n",
      "epoch: 93, train_loss: 0.007447017822414637, valid_loss: 0.1938362419605255\n",
      "epoch: 94, train_loss: -0.020663324743509293, valid_loss: 0.17115958034992218\n",
      "epoch: 95, train_loss: -0.048457071185112, valid_loss: 0.14934363961219788\n",
      "epoch: 96, train_loss: -0.0760207325220108, valid_loss: 0.1284169852733612\n",
      "epoch: 97, train_loss: -0.10342124849557877, valid_loss: 0.10840462893247604\n",
      "epoch: 98, train_loss: -0.13068775832653046, valid_loss: 0.0893237292766571\n",
      "epoch: 99, train_loss: -0.1578037142753601, valid_loss: 0.07116977870464325\n",
      "epoch: 100, train_loss: -0.18471287190914154, valid_loss: 0.053896013647317886\n",
      "epoch: 101, train_loss: -0.21134008467197418, valid_loss: 0.0373910628259182\n",
      "epoch: 102, train_loss: -0.23762096464633942, valid_loss: 0.02146412990987301\n",
      "epoch: 103, train_loss: -0.26353147625923157, valid_loss: 0.005848132539540529\n",
      "epoch: 104, train_loss: -0.28910544514656067, valid_loss: -0.009774096310138702\n",
      "epoch: 105, train_loss: -0.3144317865371704, valid_loss: -0.025720814242959023\n",
      "epoch: 106, train_loss: -0.33963003754615784, valid_loss: -0.04225043207406998\n",
      "epoch: 107, train_loss: -0.36481162905693054, valid_loss: -0.05950532481074333\n",
      "epoch: 108, train_loss: -0.3900444507598877, valid_loss: -0.07748007774353027\n",
      "epoch: 109, train_loss: -0.4153357744216919, valid_loss: -0.09602545946836472\n",
      "epoch: 110, train_loss: -0.4406455159187317, valid_loss: -0.11488832533359528\n",
      "epoch: 111, train_loss: -0.4659230709075928, valid_loss: -0.13377133011817932\n",
      "epoch: 112, train_loss: -0.49115023016929626, valid_loss: -0.15239469707012177\n",
      "epoch: 113, train_loss: -0.5163652300834656, valid_loss: -0.1705380231142044\n",
      "epoch: 114, train_loss: -0.541651725769043, valid_loss: -0.18806107342243195\n",
      "epoch: 115, train_loss: -0.5670986175537109, valid_loss: -0.20490847527980804\n",
      "epoch: 116, train_loss: -0.592755138874054, valid_loss: -0.22111524641513824\n",
      "epoch: 117, train_loss: -0.6186092495918274, valid_loss: -0.23681561648845673\n",
      "epoch: 118, train_loss: -0.644606351852417, valid_loss: -0.25224578380584717\n",
      "epoch: 119, train_loss: -0.6706938743591309, valid_loss: -0.2677185535430908\n",
      "epoch: 120, train_loss: -0.69685959815979, valid_loss: -0.28355613350868225\n",
      "epoch: 121, train_loss: -0.7231400609016418, valid_loss: -0.2999936044216156\n",
      "epoch: 122, train_loss: -0.7495875954627991, valid_loss: -0.3170883059501648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 123, train_loss: -0.7762276530265808, valid_loss: -0.33468180894851685\n",
      "epoch: 124, train_loss: -0.8030382394790649, valid_loss: -0.352436363697052\n",
      "epoch: 125, train_loss: -0.8299712538719177, valid_loss: -0.3699263632297516\n",
      "epoch: 126, train_loss: -0.8569954633712769, valid_loss: -0.3867480456829071\n",
      "epoch: 127, train_loss: -0.8841233849525452, valid_loss: -0.40260156989097595\n",
      "epoch: 128, train_loss: -0.9113966226577759, valid_loss: -0.4173350930213928\n",
      "epoch: 129, train_loss: -0.9388452172279358, valid_loss: -0.4309602975845337\n",
      "epoch: 130, train_loss: -0.9664604663848877, valid_loss: -0.44364309310913086\n",
      "epoch: 131, train_loss: -0.9942112565040588, valid_loss: -0.45566326379776\n",
      "epoch: 132, train_loss: -1.022080659866333, valid_loss: -0.4673212170600891\n",
      "epoch: 133, train_loss: -1.0500913858413696, valid_loss: -0.47880446910858154\n",
      "epoch: 134, train_loss: -1.0782790184020996, valid_loss: -0.49007436633110046\n",
      "epoch: 135, train_loss: -1.1066581010818481, valid_loss: -0.5008403062820435\n",
      "epoch: 136, train_loss: -1.135208249092102, valid_loss: -0.5106490254402161\n",
      "epoch: 137, train_loss: -1.1639090776443481, valid_loss: -0.5190348029136658\n",
      "epoch: 138, train_loss: -1.1927684545516968, valid_loss: -0.5256597995758057\n",
      "epoch: 139, train_loss: -1.2218105792999268, valid_loss: -0.5303942561149597\n",
      "epoch: 140, train_loss: -1.2510325908660889, valid_loss: nan\n",
      "early stop at: 140; optimal epoch at: 139\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "min_loss = 100.0 # starts from large number\n",
    "min_epoch = 0\n",
    "early_stop_epoch_size = 800\n",
    "\n",
    "f = open(PATH_LOG, \"w\")\n",
    "\n",
    "for epoch in range(4000):\n",
    "    optimizer.zero_grad()\n",
    "    pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi = model_mdn(pd_thetaphi_label_tensor)\n",
    "    # print(\"xx\", correlation_theta_phi)\n",
    "    loss = mdn_loss_fn (gt_thetaphi_label_tensor, pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.data.item())\n",
    "    \n",
    "    # Calculating validation data loss\n",
    "    total_valid_loss = 0.0\n",
    "    valid_pred = []\n",
    "    pi_valid, mu_theta_valid, mu_phi_valid, sigma_theta_valid, sigma_phi_valid, correlation_theta_phi_valid = model_mdn(pd_thetaphi_valid_label_tensor)\n",
    "    loss_valid = mdn_loss_fn(gt_thetaphi_valid_label_tensor,\n",
    "                       pi_valid, mu_theta_valid,\n",
    "                       mu_phi_valid,\n",
    "                       sigma_theta_valid,\n",
    "                       sigma_phi_valid,\n",
    "                       correlation_theta_phi_valid)\n",
    "    valid_loss.append(loss_valid.data.item())\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        # print(loss.data.tolist())\n",
    "        print(\"epoch: {}, train_loss: {}, valid_loss: {}\".format(epoch,\n",
    "                                                                 loss.data.item(),\n",
    "                                                                 loss_valid.data.item()))\n",
    "    if (math.isnan(loss.data.item()) or math.isnan(loss_valid.data.item())):\n",
    "        log = \"early stop at: \" + str(epoch) + \"; optimal epoch at: \" + str(epoch - 1)\n",
    "        print(log)\n",
    "        f.write(log)\n",
    "        f.close()\n",
    "        break\n",
    "    \n",
    "    \n",
    "    torch.save(model_mdn.state_dict(), PATH)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE Loss')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 140\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "valid_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), valid_loss[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, valid_loss_curve], ['Train Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch: 139, train_loss: -1.2218105792999268, valid_loss: -0.5303942561149597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDN(\n",
       "  (l_h): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=20, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (l_pi): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (l_mu_theta): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (l_sigma_theta): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (l_mu_phi): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (l_sigma_phi): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (l_correlation_theta_phi): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=1, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_mdn = MDN(2, n_hidden=20, n_gaussians=num_gaussians)\n",
    "load_model_mdn.load_state_dict(torch.load(PATH))\n",
    "load_model_mdn.eval()\n",
    "# mmodel = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5303942561149597\n"
     ]
    }
   ],
   "source": [
    "# Calculating validation data loss\n",
    "pi_valid, mu_theta_valid, mu_phi_valid, sigma_theta_valid, sigma_phi_valid, correlation_theta_phi_valid = load_model_mdn(pd_thetaphi_valid_label_tensor)\n",
    "loss_valid = mdn_loss_fn(gt_thetaphi_valid_label_tensor,\n",
    "                         pi_valid, mu_theta_valid,\n",
    "                         mu_phi_valid,\n",
    "                         sigma_theta_valid,\n",
    "                         sigma_phi_valid,\n",
    "                         correlation_theta_phi_valid)\n",
    "print(loss_valid.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('4000_v2/without_normalization_with_earlystop/ng_1/train_loss.npy', np.asarray(train_loss))\n",
    "np.save('4000_v2/without_normalization_with_earlystop/ng_1/valid_loss.npy', np.asarray(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_mdn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.repeat(1, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi = model_mdn(pd_thetaphi_label_tensor)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# print(\"sample size: \", size)\n",
    "# print(\"num of gaus: \", n_gaussians)\n",
    "# print(\"correlation size: \", correlation_x_y.shape)\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_theta, mu_phi), dim=2)\n",
    "# build covariance matrix with standard bivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 2, 2])\n",
    "cov[:, :, 0, 0] = sigma_theta**2\n",
    "cov[:, :, 1, 1] = sigma_phi**2\n",
    "cov[:, :, 1, 0] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "cov[:, :, 0, 1] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "\n",
    "m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = m.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "PPD = torch.zeros([size, 2])\n",
    "for i in range(pd.shape[0]):\n",
    "    PPD[i, :] = pd[i,k[i],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPD = PPD.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = mu_theta.detach().numpy()\n",
    "mu_phi = mu_phi.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_thetaphi_label = gt_thetaphi_label_tensor.numpy()\n",
    "pd_thetaphi_label = pd_thetaphi_label_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gt_thetaphi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_theta_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_thetaphi_label[:,0], label='gt', linestyle='--')\n",
    "plt.plot(pd_thetaphi_label[:,0], label='pd', linestyle='--')\n",
    "plt.plot(PPD[:,0], label='pd_mdn', linestyle='--')\n",
    "plt.plot(mu_theta[:,0], label='mu 1')\n",
    "plt.plot(mu_theta[:,1], label='mu 2')\n",
    "# plt.plot(mu_x[:,2], label='mu 3')\n",
    "# plt.plot(mu_x[:,3], label='mu 4')\n",
    "# plt.plot(mu_x[:,4], label='mu 5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_thetaphi_label[:,1], label='gt', linestyle='--')\n",
    "plt.plot(pd_thetaphi_label[:,1], label='pd', linestyle='--')\n",
    "plt.plot(PPD[:,1], label='pd_mdn', linestyle='--')\n",
    "plt.plot(mu_phi[:,0], label='mu 1')\n",
    "plt.plot(mu_phi[:,1], label='mu 2')\n",
    "# plt.plot(mu_x[:,2], label='mu 3')\n",
    "# plt.plot(mu_x[:,3], label='mu 4')\n",
    "# plt.plot(mu_x[:,4], label='mu 5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mu_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = mu_theta*180\n",
    "mu_phi = mu_phi*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_theta = sigma_theta*180\n",
    "sigma_phi = sigma_phi*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label\n",
    "pd_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXYZ(theta, phi, d):\n",
    "    cos_theta = math.cos(theta * math.pi / 180.0 );\n",
    "    sin_theta = math.sin(theta * math.pi / 180.0 );\n",
    "    cos_phi   = math.cos(phi   * math.pi / 180.0 );\n",
    "    sin_phi   = math.sin(phi   * math.pi / 180.0 );\n",
    "\n",
    "    x = d * sin_theta * cos_phi;\n",
    "    y = d * sin_theta * sin_phi;\n",
    "    z = d * cos_theta;\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCircle(theta_list, phi_list, d):\n",
    "    size = len(theta_list)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    for i in range(size):\n",
    "        x, y, z = getXYZ(theta_list[i], phi_list[i], d)\n",
    "        # print(math.sqrt(x**2 + y**2 + z**2))\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        z_list.append(z)\n",
    "        \n",
    "        \n",
    "    return x_list, y_list, z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhi(theta, sigma_theta, sigma_phi):\n",
    "    return math.sqrt((1-theta*theta/sigma_theta/sigma_theta)*sigma_phi*sigma_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# add origin\n",
    "ax.scatter3D([0], [0], [0], cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_x = []\n",
    "cam_y = []\n",
    "cam_z = []\n",
    "\n",
    "for i in range(10):\n",
    "    index = i\n",
    "    selected_mean_theta = mu_theta[index, 1]\n",
    "    selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "    selected_mean_phi = mu_phi[index, 1]\n",
    "    selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "    half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "    theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "                  selected_mean_theta + selected_sigma_theta/2,\n",
    "                  selected_mean_theta,\n",
    "                  selected_mean_theta - selected_sigma_theta/2,\n",
    "                  selected_mean_theta - selected_sigma_theta,\n",
    "                  selected_mean_theta - selected_sigma_theta/2,\n",
    "                  selected_mean_theta,\n",
    "                  selected_mean_theta + selected_sigma_theta/2,\n",
    "                  selected_mean_theta + selected_sigma_theta]\n",
    "    phi_list   = [selected_mean_phi,\n",
    "                  selected_mean_phi - half_phi,\n",
    "                  selected_mean_phi - selected_sigma_phi,\n",
    "                  selected_mean_phi - half_phi,\n",
    "                  selected_mean_phi,\n",
    "                  selected_mean_phi + half_phi,\n",
    "                  selected_mean_phi + selected_sigma_phi,\n",
    "                  selected_mean_phi + half_phi,\n",
    "                  selected_mean_phi]\n",
    "\n",
    "    x = pd_label[index, 0]\n",
    "    y = pd_label[index, 1]\n",
    "    z = pd_label[index, 2]\n",
    "    \n",
    "    cam_x.append(x)\n",
    "    cam_y.append(y)\n",
    "    cam_z.append(z)\n",
    "\n",
    "    d = math.sqrt(x**2 + y**2 + z**2)\n",
    "    # print(\"d\", d)\n",
    "\n",
    "    x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "    ax.plot3D(x_list, y_list, z_list, 'red')\n",
    "    # ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "    # ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot3D(cam_x, cam_y, cam_z, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "selected_mean_theta = mu_theta[index, 1]\n",
    "selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "selected_mean_phi = mu_phi[index, 1]\n",
    "selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta - selected_sigma_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta + selected_sigma_theta]\n",
    "phi_list   = [selected_mean_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi - selected_sigma_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi + selected_sigma_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi]\n",
    "\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "selected_mean_theta = mu_theta[index, 1]\n",
    "selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "selected_mean_phi = mu_phi[index, 1]\n",
    "selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta - selected_sigma_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta + selected_sigma_theta]\n",
    "phi_list   = [selected_mean_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi - selected_sigma_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi + selected_sigma_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi]\n",
    "\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = math.sqrt(gt_label[0, 0]**2 + gt_label[0, 0]**2 + gt_label[0, 0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = -0.18\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "pdf = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            # pdf[i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            pdf[i, j, k] = likelihood[0, 0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = 0.01\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "pdf_1 = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            # pdf_![i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            pdf_1[i, j, k] = likelihood[0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "# radius = 0.01\n",
    "# density = 20 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)dd\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = -0.18\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "        \n",
    "pdf_all = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            pdf_all[i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            # pdf_4[i, j, k] = likelihood[0, 4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_label_tensor[0])\n",
    "print(pi[0])\n",
    "print(mu_x[0])\n",
    "print(mu_y[0])\n",
    "print(mu_z[0])\n",
    "print(sigma_x[0])\n",
    "print(sigma_y[0])\n",
    "print(sigma_z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "src = mlab.pipeline.scalar_field(pdf)\n",
    "mlab.pipeline.iso_surface(src, contours=[0.1, ], opacity=0.3, color = (0.8,0.0,0.0))\n",
    "# mlab.pipeline.iso_surface(src, contours=[pdf.max()-0.1*pdf.ptp(), ],)\n",
    "\n",
    "src_1 = mlab.pipeline.scalar_field(pdf_1)\n",
    "mlab.pipeline.iso_surface(src_1, contours=[0.1, ], opacity=0.3, color = (0.8,0.0,0.0))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)\n",
    "\n",
    "# src_half = mlab.pipeline.scalar_field(pdf_half)\n",
    "# mlab.pipeline.iso_surface(src_half, contours=[0.1, ], opacity=0.3, color = (0.0,0.8,0.0))\n",
    "# mlab.pipeline.iso_surface(src, contours=[pdf.max()-0.1*pdf.ptp(), ],)\n",
    "\n",
    "# src_1_half = mlab.pipeline.scalar_field(pdf_1_half)\n",
    "# mlab.pipeline.iso_surface(src_1_half, contours=[0.1, ], opacity=0.3, color = (0.0,0.8,0.0))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)\n",
    "\n",
    "src_all = mlab.pipeline.scalar_field(pdf_all)\n",
    "mlab.pipeline.iso_surface(src_all, contours=[0.1, ], opacity=0.3, color = (0.0,0.0,0.8))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.1318e-02 + 3.6224e-01 + 9.2028e-07 + 1.2253e-02 + 1.7447e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(s_x.flatten(), s_y.flatten(), s_z.flatten(), c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.1585, 0.1787, 0.1571, 0.3795, 0.1261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.0165, 0.0245, 0.1137, 2.8152, 0.0082])\n",
    "b = torch.tensor([0.0339, 0.0643, 0.1532, 0.5262, 0.2225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a*b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:, [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.stack((a, b), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(7) / (math.exp(1) + math.exp(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:, :, 0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gaussians = 5\n",
    "size = 14000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.unsqueeze(0).repeat(n_gaussians, 1, 1).unsqueeze(0).repeat(size, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.repeat(5, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.repeat(10, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[0, :, 0], input_tensor[0, :, 1], input_tensor[0, :, 2], 'gray')\n",
    "ax.scatter(gt_label_tensor[0,0], gt_label_tensor[0,1], gt_label_tensor[0,2], 'green')\n",
    "ax.scatter(pd_label_tensor[0,0], pd_label_tensor[0,1], pd_label_tensor[0,2], 'red')\n",
    "# ax.plot3D(gt_label_tensor[0:2,0], gt_label_tensor[0:2,1], gt_label_tensor[0:2,2], 'green')\n",
    "# ax.plot3D(pd_label_tensor[0:2,0], pd_label_tensor[0:2,1], pd_label_tensor[0:2,2], 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], 'gray')\n",
    "ax.scatter(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], c = 'gray')\n",
    "\n",
    "ax.plot3D(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], 'green')\n",
    "ax.scatter(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], c = 'green')\n",
    "\n",
    "ax.plot3D(gt_label_tensor[0:3,0], gt_label_tensor[0:3,1], gt_label_tensor[0:3,2], 'black')\n",
    "ax.plot3D(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], 'red')\n",
    "ax.scatter(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], c = 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pd = pd_label_tensor[:, 0].reshape(-1,1)\n",
    "y_pd = pd_label_tensor[:, 1].reshape(-1,1)\n",
    "z_pd = pd_label_tensor[:, 2].reshape(-1,1)\n",
    "\n",
    "x_gt = gt_label_tensor[:, 0].reshape(-1,1)\n",
    "y_gt = gt_label_tensor[:, 1].reshape(-1,1)\n",
    "z_gt = gt_label_tensor[:, 2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt = x_gt.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_gt)\n",
    "plt.plot(x_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply MDN on x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 14985\n",
    "\n",
    "x_data = torch.Tensor(x_pd)\n",
    "y_data = torch.Tensor(x_gt)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_data, y_data, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.z_h = nn.Sequential(\n",
    "            nn.Linear(1, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_mu = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(n_hidden, n_gaussians)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_h = self.z_h(x)\n",
    "        pi = F.softmax(self.z_pi(z_h), -1)\n",
    "        mu = self.z_mu(z_h)\n",
    "        sigma = torch.exp(self.z_sigma(z_h))\n",
    "        return pi, mu, sigma\n",
    "\n",
    "model = MDN(n_hidden=20, n_gaussians=5)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, mu, sigma, pi):\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    loss = torch.exp(m.log_prob(y))\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "    pi, mu, sigma = model(x_data)\n",
    "    loss = mdn_loss_fn(y_data, mu, sigma, pi)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, sigma = model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the highest index of the 5 elements in each row\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "# lt.scatter(x_data, y_data, alpha=0.4)\n",
    "# plt.scatter(x_data, y_pred, alpha=0.8, color='red')\n",
    "# plt.scatter(x_test[0], y_pred[0], alpha=0.4, color='yellow')\n",
    "plt.scatter(x_data, mu[:,0].detach(), label='mean 0')\n",
    "plt.scatter(x_data, mu[:,1].detach(), label='mean 1')\n",
    "plt.scatter(x_data, mu[:,2].detach(), label='mean 2')\n",
    "plt.scatter(x_data, mu[:,3].detach(), label='mean 3')\n",
    "plt.scatter(x_data, mu[:,4].detach(), label='mean 4')\n",
    "# plt.plot(x_test.numpy(), mu[:,0].detach().numpy())\n",
    "# plt.scatter(x_data[:20], y_data[:20], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pi.detach().numpy()\n",
    "mu = mu.detach().numpy()\n",
    "sigma = sigma.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros((n_samples, 100))\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    each = []\n",
    "    for j in range(5):\n",
    "        if (j == 0):\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = y\n",
    "        else:\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = [p + q for p, q in zip(cumulated_y, y)]\n",
    "    p[i, :] = cumulated_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "for i in range(0,n_samples):\n",
    "    ax.plot3D([x_data[i, 0]]*100, x, p[i, :])\n",
    "\n",
    "ax.set_xlabel('index')\n",
    "ax.set_ylabel('x')\n",
    "ax.set_zlabel('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply MDN on 3D projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1_1 = torch.Tensor([1, 1, 1])\n",
    "mean_1_2 = torch.Tensor([2, 2, 2])\n",
    "mean_1_3 = torch.Tensor([3, 3, 3])\n",
    "mean_1_4 = torch.Tensor([4, 4, 4])\n",
    "mean_1_5 = torch.Tensor([5, 5, 5])\n",
    "\n",
    "mean_2_1 = torch.Tensor([6, 6, 6])\n",
    "mean_2_2 = torch.Tensor([7, 7, 7])\n",
    "mean_2_3 = torch.Tensor([8, 8, 8])\n",
    "mean_2_4 = torch.Tensor([9, 9, 9])\n",
    "mean_2_5 = torch.Tensor([10, 10, 10])\n",
    "\n",
    "sigma_1 = 1\n",
    "sigma_2 = 1\n",
    "sigma_3 = 1\n",
    "cov_1_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 3\n",
    "sigma_2 = 3\n",
    "sigma_3 = 3\n",
    "cov_1_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov_1_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 5\n",
    "sigma_2 = 5\n",
    "sigma_3 = 5\n",
    "cov_1_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 6\n",
    "sigma_2 = 6\n",
    "sigma_3 = 6\n",
    "cov_2_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 7\n",
    "sigma_2 = 7\n",
    "sigma_3 = 7\n",
    "cov_2_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 8\n",
    "sigma_2 = 8\n",
    "sigma_3 = 8\n",
    "cov_2_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 9\n",
    "sigma_2 = 9\n",
    "sigma_3 = 9\n",
    "cov_2_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 10\n",
    "sigma_2 = 10\n",
    "sigma_3 = 10\n",
    "cov_2_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[1, 2] = rho_2_3*sigma_2*sigma_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = torch.stack((mean_1_1, mean_1_2, mean_1_3, mean_1_4, mean_1_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2 = torch.stack((mean_2_1, mean_2_2, mean_2_3, mean_2_4, mean_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mean_1, mean_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1 = torch.stack((cov_1_1, cov_1_2, cov_1_3, cov_1_4, cov_1_5), dim=0)\n",
    "cov_2 = torch.stack((cov_2_1, cov_2_2, cov_2_3, cov_2_4, cov_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov_1, cov_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack((y, y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = mean_1.unsqueeze(0)\n",
    "cov_1 = cov_1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,2,3]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov, cov_1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov_1)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "s = np.zeros([21, 21, 21])\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1001)\n",
    "y = np.linspace(0, 10, 1001)\n",
    "z = np.linspace(0, 10, 1001)\n",
    "x_p = x + 0.5 * np.random.randn(1001)\n",
    "y_p = y + 0.5 * np.random.randn(1001)\n",
    "z_p = z + 0.5 * np.random.randn(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "ax.scatter(x_p, y_p, z_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        # self.l_correlation_x_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_x_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_y_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_correlation_x_y = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_x_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_y_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_x = self.l_mu_x(h)\n",
    "        # print(\"mu_x\", pi.shape)\n",
    "        mu_y = self.l_mu_y(h)\n",
    "        # print(\"mu_y\", pi.shape)\n",
    "        mu_z = self.l_mu_z(h)\n",
    "        # print(\"mu_z\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_x = torch.exp(self.l_sigma_x(h))\n",
    "        # print(\"sigma_x\", pi.shape)\n",
    "        sigma_y = torch.exp(self.l_sigma_y(h))\n",
    "        # print(\"sigma_y\", pi.shape)\n",
    "        sigma_z = torch.exp(self.l_sigma_z(h))\n",
    "        # print(\"sigma_z\", pi.shape)\n",
    "        \n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_x_y = self.l_correlation_x_y(h)\n",
    "        # print(\"correlation_x_y\", pi.shape)\n",
    "        # print(\"correlation_x_y[0]\", correlation_x_y[0, :])\n",
    "        correlation_x_z = self.l_correlation_x_z(h)\n",
    "        # print(\"correlation_x_z\", pi.shape)\n",
    "        # print(\"correlation_x_z[0]\", correlation_x_z[0, :])\n",
    "        correlation_y_z = self.l_correlation_y_z(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z\n",
    "\n",
    "model = MDN(3, n_hidden=20, n_gaussians=5)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z):\n",
    "    # print(\"mu_x shape\", mu_x.shape)\n",
    "    # mu = torch.stack((mu_x, mu_y, mu_z), 2)\n",
    "    # print(\"mu shape \", mu.shape)\n",
    "    # print(\"mu[0]\", mu[0])\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    size = y.shape[0]\n",
    "    n_gaussians = pi.shape[1]\n",
    "    # print(\"sample size: \", size)\n",
    "    # print(\"num of gaus: \", n_gaussians)\n",
    "    # print(\"correlation size: \", correlation_x_y.shape)\n",
    "    # build mean matrix\n",
    "    mean = torch.zeros([size, n_gaussians, 3])\n",
    "    # build covariance matrix with standard trivariate normal distribution\n",
    "    cov = torch.ones([size, n_gaussians, 3, 3])\n",
    "    cov[:, :, 0, 1] = correlation_x_y\n",
    "    cov[:, :, 1, 0] = correlation_x_y\n",
    "    cov[:, :, 0, 2] = correlation_x_z\n",
    "    cov[:, :, 2, 0] = correlation_x_z\n",
    "    cov[:, :, 1, 2] = correlation_y_z\n",
    "    cov[:, :, 2, 1] = correlation_y_z\n",
    "    \n",
    "    print(\"mean: \", mean.shape)\n",
    "    print(\"cov: \", cov.shape)\n",
    "    \n",
    "    print(\"mean is cuda: \", mean.is_cuda)\n",
    "    print(\" cov is cuda: \", cov.is_cuda)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    w = y[0]**2*(correlation_y_z**2 - 1) +\\\n",
    "        y[1]**2*(correlation_x_z**2 - 1) +\\\n",
    "        y[2]**2*(correlation_x_y**2 - 1) +\\\n",
    "        2*(y[0]*y[:1]*(correlation_x_y - correlation_x_z*correlation_y_z) +\\\n",
    "           y[0]*y[:2]*(correlation_x_z - correlation_x_y*correlation_y_z) +\\\n",
    "           y[1]*y[:2]*(correlation_y_z - correlation_x_y*correlation_x_z))\n",
    "    \n",
    "    return w\n",
    "    '''\n",
    "    '''\n",
    "    cov = torch.Tensor([[sigma_x**2, 0, 0], [0, sigma_y**2, 0], [0, 0, sigma_z**2]])\n",
    "    cov[1, 0] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[2, 0] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[2, 1] = correlation_y_z*sigma_y*sigma_z\n",
    "    cov[0, 1] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[0, 2] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[1, 2] = correlation_y_z*sigma_y*sigma_z\n",
    "    '''\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "    # normalize input y\n",
    "    y_repeat = y.unsqueeze(1).repeat(1, n_gaussians, 1)\n",
    "    # print(y_repeat.shape)\n",
    "    # print(y_repeat[0])\n",
    "    mu_all = torch.stack((mu_x, mu_y, mu_z), dim = 2)\n",
    "    print(\"mu_all shape: \", mu_all.shape)\n",
    "    y_sub = y_repeat - mu_all\n",
    "    print(\"y_sub shape\", y_sub.shape)\n",
    "    sigma_all = torch.stack((sigma_x, sigma_y, sigma_z), dim = 2)\n",
    "    print(\"sigma\", sigma_x.shape)\n",
    "    print(\"sigma all\", sigma_all.shape)\n",
    "    \n",
    "    y_normal = torch.div(y_sub, sigma_all)\n",
    "    \n",
    "    print(\"y_normal size\", y_normal.shape)\n",
    "    \n",
    "    # y: [x, y, z]; row: x, y, z; column: N, number of samples\n",
    "    # y: N x 3\n",
    "    # loss: N x 1\n",
    "    print(y_normal.is_cuda)\n",
    "    loss = torch.exp(m.log_prob(y_normal))\n",
    "    print(\"loss shape: \", loss.shape)\n",
    "    print(\"pi shape: \", pi.shape)\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z = model(pd_label_tensor)\n",
    "    loss = mdn_loss_fn(gt_label_tensor, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z)\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # if epoch % 1000 == 0:\n",
    "    #     print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2,3)\n",
    "print(t)\n",
    "tr = t.repeat(2, 3)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2],[4,5],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.Tensor([[7,8],[6,4],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[0,0,0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0, 0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_y_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2939**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(gt_label_tensor[:,0], gt_label_tensor[:,1], gt_label_tensor[:,2], 'gray')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_label_tensor[:,2])\n",
    "plt.plot(pd_label_tensor[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model/with_early_stop_after_400/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "\n",
    "\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_1_loss.append(total_test_loss/te    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)st_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_1, test_set_y_1, test_set_z_1, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find window size 6 is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_1_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input window size\", fontsize=14)\n",
    "plt.ylabel(\"MSE loss\", fontsize=14)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.ylim([0.017,0.0255])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot result without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model_v3/with_early_stop_after_400_without_normalization/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    '''\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "    '''\n",
    "    '''\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "    '''\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_3):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_3_loss.append(total_test_loss/test_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_3, test_set_y_3, test_set_z_3, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test_3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_3_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input Window Size\", fontsize=12)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "# plt.ylim([0.017,0.0255])\n",
    "plt.ylim([0.025,0.039])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
