{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to analyze the gt points' distribution with respect to predicted points location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, min_max_scaling, min_max_scaling_radius, normalize_one, construct_train_valid_tensor, construct_test_tensor, show_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = np.load(\"input_tensor.npy\")\n",
    "gt_label_tensor = np.load(\"gt_label_tensor.npy\")\n",
    "pd_label_tensor = np.load(\"pd_label_tensor.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14985, 3, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.Tensor(input_tensor)\n",
    "gt_label_tensor = torch.Tensor(gt_label_tensor)\n",
    "pd_label_tensor = torch.Tensor(pd_label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        # self.l_correlation_x_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_x_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_y_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_correlation_x_y = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_x_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_y_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_x = self.l_mu_x(h)\n",
    "        # print(\"mu_x\", pi.shape)\n",
    "        mu_y = self.l_mu_y(h)\n",
    "        # print(\"mu_y\", pi.shape)\n",
    "        mu_z = self.l_mu_z(h)\n",
    "        # print(\"mu_z\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_x = torch.exp(self.l_sigma_x(h))\n",
    "        # print(\"sigma_x\", pi.shape)\n",
    "        sigma_y = torch.exp(self.l_sigma_y(h))\n",
    "        # print(\"sigma_y\", pi.shape)\n",
    "        sigma_z = torch.exp(self.l_sigma_z(h))\n",
    "        # print(\"sigma_z\", pi.shape)\n",
    "        \n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_x_y = self.l_correlation_x_y(h)\n",
    "        # print(\"correlation_x_y\", pi.shape)\n",
    "        # print(\"correlation_x_y[0]\", correlation_x_y[0, :])\n",
    "        correlation_x_z = self.l_correlation_x_z(h)\n",
    "        # print(\"correlation_x_z\", pi.shape)\n",
    "        # print(\"correlation_x_z[0]\", correlation_x_z[0, :])\n",
    "        correlation_y_z = self.l_correlation_y_z(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z):\n",
    "    # print(\"mu_x shape\", mu_x.shape)\n",
    "    # mu = torch.stack((mu_x, mu_y, mu_z), 2)\n",
    "    # print(\"mu shape \", mu.shape)\n",
    "    # print(\"mu[0]\", mu[0])\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    size = y.shape[0]\n",
    "    n_gaussians = pi.shape[1]\n",
    "    # print(\"sample size: \", size)\n",
    "    # print(\"num of gaus: \", n_gaussians)\n",
    "    # print(\"correlation size: \", correlation_x_y.shape)\n",
    "    # build mean matrix\n",
    "    mean = torch.zeros([size, n_gaussians, 3])\n",
    "    # build covariance matrix with standard trivariate normal distribution\n",
    "    cov = torch.ones([size, n_gaussians, 3, 3])\n",
    "    cov[:, :, 0, 1] = correlation_x_y\n",
    "    cov[:, :, 1, 0] = correlation_x_y\n",
    "    cov[:, :, 0, 2] = correlation_x_z\n",
    "    cov[:, :, 2, 0] = correlation_x_z\n",
    "    cov[:, :, 1, 2] = correlation_y_z\n",
    "    cov[:, :, 2, 1] = correlation_y_z\n",
    "    \n",
    "    # print(\"mean: \", mean.shape)\n",
    "    # print(\"cov: \", cov.shape)\n",
    "    \n",
    "    # print(\"mean is cuda: \", mean.is_cuda)\n",
    "    # print(\" cov is cuda: \", cov.is_cuda)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    w = y[0]**2*(correlation_y_z**2 - 1) +\\\n",
    "        y[1]**2*(correlation_x_z**2 - 1) +\\\n",
    "        y[2]**2*(correlation_x_y**2 - 1) +\\\n",
    "        2*(y[0]*y[:1]*(correlation_x_y - correlation_x_z*correlation_y_z) +\\\n",
    "           y[0]*y[:2]*(correlation_x_z - correlation_x_y*correlation_y_z) +\\\n",
    "           y[1]*y[:2]*(correlation_y_z - correlation_x_y*correlation_x_z))\n",
    "    \n",
    "    return w\n",
    "    '''\n",
    "    '''\n",
    "    cov = torch.Tensor([[sigma_x**2, 0, 0], [0, sigma_y**2, 0], [0, 0, sigma_z**2]])\n",
    "    cov[1, 0] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[2, 0] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[2, 1] = correlation_y_z*sigma_y*sigma_z\n",
    "    cov[0, 1] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[0, 2] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[1, 2] = correlation_y_z*sigma_y*sigma_z\n",
    "    '''\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "    # normalize input y\n",
    "    y_repeat = y.unsqueeze(1).repeat(1, n_gaussians, 1)\n",
    "    # print(y_repeat.shape)\n",
    "    # print(y_repeat[0])\n",
    "    mu_all = torch.stack((mu_x, mu_y, mu_z), dim = 2)\n",
    "    # print(\"mu_all shape: \", mu_all.shape)\n",
    "    y_sub = y_repeat - mu_all\n",
    "    # print(\"y_sub shape\", y_sub.shape)\n",
    "    sigma_all = torch.stack((sigma_x, sigma_y, sigma_z), dim = 2)\n",
    "    # print(\"sigma\", sigma_x.shape)\n",
    "    # print(\"sigma all\", sigma_all.shape)\n",
    "    \n",
    "    y_normal = torch.div(y_sub, sigma_all)\n",
    "    \n",
    "    # print(\"y_normal size\", y_normal.shape)\n",
    "    \n",
    "    # y: [x, y, z]; row: x, y, z; column: N, number of samples\n",
    "    # y: N x 3\n",
    "    # loss: N x 1\n",
    "    # print(y_normal.is_cuda)\n",
    "    loss = torch.exp(m.log_prob(y_normal))\n",
    "    # print(\"loss shape: \", loss.shape)\n",
    "    # print(\"pi shape: \", pi.shape)\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mdn = MDN(3, n_hidden=20, n_gaussians=1)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mdn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------epoch  0 -------\n",
      "4.6784844398498535\n",
      "-------epoch  10 -------\n",
      "4.124535083770752\n",
      "-------epoch  20 -------\n",
      "3.7611539363861084\n",
      "-------epoch  30 -------\n",
      "3.5046498775482178\n",
      "-------epoch  40 -------\n",
      "3.3056724071502686\n",
      "-------epoch  50 -------\n",
      "3.1235764026641846\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cholesky_cpu: For batch 2173: U(3,3) is zero, singular U.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-bdc911e78c8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_x_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_x_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_y_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_mdn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd_label_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdn_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_label_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_x_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_x_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelation_y_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-910a7f716528>\u001b[0m in \u001b[0;36mmdn_loss_fn\u001b[0;34m(y, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcov\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrelation_y_z\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma_y\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma_z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     '''\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m# normalize input y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0my_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gaussians\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprecision_matrix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cholesky_cpu: For batch 2173: U(3,3) is zero, singular U."
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z = model_mdn(pd_label_tensor)\n",
    "    loss = mdn_loss_fn(gt_label_tensor, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"-------epoch \", epoch, \"-------\")\n",
    "        print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[0, :, 0], input_tensor[0, :, 1], input_tensor[0, :, 2], 'gray')\n",
    "ax.scatter(gt_label_tensor[0,0], gt_label_tensor[0,1], gt_label_tensor[0,2], 'green')\n",
    "ax.scatter(pd_label_tensor[0,0], pd_label_tensor[0,1], pd_label_tensor[0,2], 'red')\n",
    "# ax.plot3D(gt_label_tensor[0:2,0], gt_label_tensor[0:2,1], gt_label_tensor[0:2,2], 'green')\n",
    "# ax.plot3D(pd_label_tensor[0:2,0], pd_label_tensor[0:2,1], pd_label_tensor[0:2,2], 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], 'gray')\n",
    "ax.scatter(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], c = 'gray')\n",
    "\n",
    "ax.plot3D(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], 'green')\n",
    "ax.scatter(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], c = 'green')\n",
    "\n",
    "ax.plot3D(gt_label_tensor[0:3,0], gt_label_tensor[0:3,1], gt_label_tensor[0:3,2], 'black')\n",
    "ax.plot3D(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], 'red')\n",
    "ax.scatter(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], c = 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pd = pd_label_tensor[:, 0].reshape(-1,1)\n",
    "y_pd = pd_label_tensor[:, 1].reshape(-1,1)\n",
    "z_pd = pd_label_tensor[:, 2].reshape(-1,1)\n",
    "\n",
    "x_gt = gt_label_tensor[:, 0].reshape(-1,1)\n",
    "y_gt = gt_label_tensor[:, 1].reshape(-1,1)\n",
    "z_gt = gt_label_tensor[:, 2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt = x_gt.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_gt)\n",
    "plt.plot(x_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply MDN on x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 14985\n",
    "\n",
    "x_data = torch.Tensor(x_pd)\n",
    "y_data = torch.Tensor(x_gt)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_data, y_data, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.z_h = nn.Sequential(\n",
    "            nn.Linear(1, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_mu = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(n_hidden, n_gaussians)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_h = self.z_h(x)\n",
    "        pi = F.softmax(self.z_pi(z_h), -1)\n",
    "        mu = self.z_mu(z_h)\n",
    "        sigma = torch.exp(self.z_sigma(z_h))\n",
    "        return pi, mu, sigma\n",
    "\n",
    "model = MDN(n_hidden=20, n_gaussians=5)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, mu, sigma, pi):\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    loss = torch.exp(m.log_prob(y))\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "    pi, mu, sigma = model(x_data)\n",
    "    loss = mdn_loss_fn(y_data, mu, sigma, pi)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, sigma = model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the highest index of the 5 elements in each row\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "# lt.scatter(x_data, y_data, alpha=0.4)\n",
    "# plt.scatter(x_data, y_pred, alpha=0.8, color='red')\n",
    "# plt.scatter(x_test[0], y_pred[0], alpha=0.4, color='yellow')\n",
    "plt.scatter(x_data, mu[:,0].detach(), label='mean 0')\n",
    "plt.scatter(x_data, mu[:,1].detach(), label='mean 1')\n",
    "plt.scatter(x_data, mu[:,2].detach(), label='mean 2')\n",
    "plt.scatter(x_data, mu[:,3].detach(), label='mean 3')\n",
    "plt.scatter(x_data, mu[:,4].detach(), label='mean 4')\n",
    "# plt.plot(x_test.numpy(), mu[:,0].detach().numpy())\n",
    "# plt.scatter(x_data[:20], y_data[:20], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pi.detach().numpy()\n",
    "mu = mu.detach().numpy()\n",
    "sigma = sigma.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros((n_samples, 100))\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    each = []\n",
    "    for j in range(5):\n",
    "        if (j == 0):\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = y\n",
    "        else:\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = [p + q for p, q in zip(cumulated_y, y)]\n",
    "    p[i, :] = cumulated_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "for i in range(0,n_samples):\n",
    "    ax.plot3D([x_data[i, 0]]*100, x, p[i, :])\n",
    "\n",
    "ax.set_xlabel('index')\n",
    "ax.set_ylabel('x')\n",
    "ax.set_zlabel('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply MDN on 3D projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1_1 = torch.Tensor([1, 1, 1])\n",
    "mean_1_2 = torch.Tensor([2, 2, 2])\n",
    "mean_1_3 = torch.Tensor([3, 3, 3])\n",
    "mean_1_4 = torch.Tensor([4, 4, 4])\n",
    "mean_1_5 = torch.Tensor([5, 5, 5])\n",
    "\n",
    "mean_2_1 = torch.Tensor([6, 6, 6])\n",
    "mean_2_2 = torch.Tensor([7, 7, 7])\n",
    "mean_2_3 = torch.Tensor([8, 8, 8])\n",
    "mean_2_4 = torch.Tensor([9, 9, 9])\n",
    "mean_2_5 = torch.Tensor([10, 10, 10])\n",
    "\n",
    "sigma_1 = 1\n",
    "sigma_2 = 1\n",
    "sigma_3 = 1\n",
    "cov_1_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 3\n",
    "sigma_2 = 3\n",
    "sigma_3 = 3\n",
    "cov_1_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov_1_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 5\n",
    "sigma_2 = 5\n",
    "sigma_3 = 5\n",
    "cov_1_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 6\n",
    "sigma_2 = 6\n",
    "sigma_3 = 6\n",
    "cov_2_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 7\n",
    "sigma_2 = 7\n",
    "sigma_3 = 7\n",
    "cov_2_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 8\n",
    "sigma_2 = 8\n",
    "sigma_3 = 8\n",
    "cov_2_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 9\n",
    "sigma_2 = 9\n",
    "sigma_3 = 9\n",
    "cov_2_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 10\n",
    "sigma_2 = 10\n",
    "sigma_3 = 10\n",
    "cov_2_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[1, 2] = rho_2_3*sigma_2*sigma_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = torch.stack((mean_1_1, mean_1_2, mean_1_3, mean_1_4, mean_1_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2 = torch.stack((mean_2_1, mean_2_2, mean_2_3, mean_2_4, mean_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mean_1, mean_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1 = torch.stack((cov_1_1, cov_1_2, cov_1_3, cov_1_4, cov_1_5), dim=0)\n",
    "cov_2 = torch.stack((cov_2_1, cov_2_2, cov_2_3, cov_2_4, cov_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov_1, cov_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack((y, y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = mean_1.unsqueeze(0)\n",
    "cov_1 = cov_1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,2,3]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov, cov_1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov_1)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "s = np.zeros([21, 21, 21])\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1001)\n",
    "y = np.linspace(0, 10, 1001)\n",
    "z = np.linspace(0, 10, 1001)\n",
    "x_p = x + 0.5 * np.random.randn(1001)\n",
    "y_p = y + 0.5 * np.random.randn(1001)\n",
    "z_p = z + 0.5 * np.random.randn(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "ax.scatter(x_p, y_p, z_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        # self.l_correlation_x_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_x_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_y_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_correlation_x_y = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_x_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_y_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_x = self.l_mu_x(h)\n",
    "        # print(\"mu_x\", pi.shape)\n",
    "        mu_y = self.l_mu_y(h)\n",
    "        # print(\"mu_y\", pi.shape)\n",
    "        mu_z = self.l_mu_z(h)\n",
    "        # print(\"mu_z\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_x = torch.exp(self.l_sigma_x(h))\n",
    "        # print(\"sigma_x\", pi.shape)\n",
    "        sigma_y = torch.exp(self.l_sigma_y(h))\n",
    "        # print(\"sigma_y\", pi.shape)\n",
    "        sigma_z = torch.exp(self.l_sigma_z(h))\n",
    "        # print(\"sigma_z\", pi.shape)\n",
    "        \n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_x_y = self.l_correlation_x_y(h)\n",
    "        # print(\"correlation_x_y\", pi.shape)\n",
    "        # print(\"correlation_x_y[0]\", correlation_x_y[0, :])\n",
    "        correlation_x_z = self.l_correlation_x_z(h)\n",
    "        # print(\"correlation_x_z\", pi.shape)\n",
    "        # print(\"correlation_x_z[0]\", correlation_x_z[0, :])\n",
    "        correlation_y_z = self.l_correlation_y_z(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z\n",
    "\n",
    "model = MDN(3, n_hidden=20, n_gaussians=5)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z):\n",
    "    # print(\"mu_x shape\", mu_x.shape)\n",
    "    # mu = torch.stack((mu_x, mu_y, mu_z), 2)\n",
    "    # print(\"mu shape \", mu.shape)\n",
    "    # print(\"mu[0]\", mu[0])\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    size = y.shape[0]\n",
    "    n_gaussians = pi.shape[1]\n",
    "    # print(\"sample size: \", size)\n",
    "    # print(\"num of gaus: \", n_gaussians)\n",
    "    # print(\"correlation size: \", correlation_x_y.shape)\n",
    "    # build mean matrix\n",
    "    mean = torch.zeros([size, n_gaussians, 3])\n",
    "    # build covariance matrix with standard trivariate normal distribution\n",
    "    cov = torch.ones([size, n_gaussians, 3, 3])\n",
    "    cov[:, :, 0, 1] = correlation_x_y\n",
    "    cov[:, :, 1, 0] = correlation_x_y\n",
    "    cov[:, :, 0, 2] = correlation_x_z\n",
    "    cov[:, :, 2, 0] = correlation_x_z\n",
    "    cov[:, :, 1, 2] = correlation_y_z\n",
    "    cov[:, :, 2, 1] = correlation_y_z\n",
    "    \n",
    "    print(\"mean: \", mean.shape)\n",
    "    print(\"cov: \", cov.shape)\n",
    "    \n",
    "    print(\"mean is cuda: \", mean.is_cuda)\n",
    "    print(\" cov is cuda: \", cov.is_cuda)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    w = y[0]**2*(correlation_y_z**2 - 1) +\\\n",
    "        y[1]**2*(correlation_x_z**2 - 1) +\\\n",
    "        y[2]**2*(correlation_x_y**2 - 1) +\\\n",
    "        2*(y[0]*y[:1]*(correlation_x_y - correlation_x_z*correlation_y_z) +\\\n",
    "           y[0]*y[:2]*(correlation_x_z - correlation_x_y*correlation_y_z) +\\\n",
    "           y[1]*y[:2]*(correlation_y_z - correlation_x_y*correlation_x_z))\n",
    "    \n",
    "    return w\n",
    "    '''\n",
    "    '''\n",
    "    cov = torch.Tensor([[sigma_x**2, 0, 0], [0, sigma_y**2, 0], [0, 0, sigma_z**2]])\n",
    "    cov[1, 0] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[2, 0] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[2, 1] = correlation_y_z*sigma_y*sigma_z\n",
    "    cov[0, 1] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[0, 2] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[1, 2] = correlation_y_z*sigma_y*sigma_z\n",
    "    '''\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "    # normalize input y\n",
    "    y_repeat = y.unsqueeze(1).repeat(1, n_gaussians, 1)\n",
    "    # print(y_repeat.shape)\n",
    "    # print(y_repeat[0])\n",
    "    mu_all = torch.stack((mu_x, mu_y, mu_z), dim = 2)\n",
    "    print(\"mu_all shape: \", mu_all.shape)\n",
    "    y_sub = y_repeat - mu_all\n",
    "    print(\"y_sub shape\", y_sub.shape)\n",
    "    sigma_all = torch.stack((sigma_x, sigma_y, sigma_z), dim = 2)\n",
    "    print(\"sigma\", sigma_x.shape)\n",
    "    print(\"sigma all\", sigma_all.shape)\n",
    "    \n",
    "    y_normal = torch.div(y_sub, sigma_all)\n",
    "    \n",
    "    print(\"y_normal size\", y_normal.shape)\n",
    "    \n",
    "    # y: [x, y, z]; row: x, y, z; column: N, number of samples\n",
    "    # y: N x 3\n",
    "    # loss: N x 1\n",
    "    print(y_normal.is_cuda)\n",
    "    loss = torch.exp(m.log_prob(y_normal))\n",
    "    print(\"loss shape: \", loss.shape)\n",
    "    print(\"pi shape: \", pi.shape)\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z = model(pd_label_tensor)\n",
    "    loss = mdn_loss_fn(gt_label_tensor, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z)\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # if epoch % 1000 == 0:\n",
    "    #     print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2,3)\n",
    "print(t)\n",
    "tr = t.repeat(2, 3)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2],[4,5],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.Tensor([[7,8],[6,4],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[0,0,0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0, 0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_y_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2939**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(gt_label_tensor[:,0], gt_label_tensor[:,1], gt_label_tensor[:,2], 'gray')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_label_tensor[:,2])\n",
    "plt.plot(pd_label_tensor[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model/with_early_stop_after_400/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "\n",
    "\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_1_loss.append(total_test_loss/te    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)st_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_1, test_set_y_1, test_set_z_1, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find window size 6 is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_1_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input window size\", fontsize=14)\n",
    "plt.ylabel(\"MSE loss\", fontsize=14)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.ylim([0.017,0.0255])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot result without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model_v3/with_early_stop_after_400_without_normalization/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    '''\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "    '''\n",
    "    '''\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "    '''\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_3):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_3_loss.append(total_test_loss/test_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_3, test_set_y_3, test_set_z_3, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test_3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_3_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input Window Size\", fontsize=12)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "# plt.ylim([0.017,0.0255])\n",
    "plt.ylim([0.025,0.039])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
