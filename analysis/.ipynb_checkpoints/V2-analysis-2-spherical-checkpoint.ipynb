{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to analyze the gt points' distribution with respect to predicted points location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "%matplotlib qt\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import my_model\n",
    "from utilities import MyTrainDataSet, MyTestDataSet, load_data_2, load_test_data, min_max_scaling, min_max_scaling_radius, normalize_one, construct_train_valid_tensor, construct_test_tensor, show_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = np.load(\"input_tensor.npy\")\n",
    "gt_label = np.load(\"gt_label_tensor.npy\")\n",
    "pd_label = np.load(\"pd_label_tensor.npy\")\n",
    "\n",
    "input_valid_train = np.load(\"input_valid_tensor.npy\")\n",
    "gt_valid_label = np.load(\"gt_valid_label_tensor.npy\")\n",
    "pd_valid_label = np.load(\"pd_valid_label_tensor.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0f6d8c1f28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(pd_label[:, 0], label=\"pd\")\n",
    "plt.plot(gt_label[:, 0], label=\"gt\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to sperical coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDistanceThetaPhi(xyz): # N x 3 -> N x 2, xyz_tensor must be two dimension\n",
    "    size = xyz.shape[0]\n",
    "    dtp = np.zeros([size, 3])\n",
    "    for i in range(size):\n",
    "        x = xyz[i, 0]\n",
    "        y = xyz[i, 1]\n",
    "        z = xyz[i, 2]\n",
    "        # print(z)\n",
    "        d = math.sqrt(x*x + y*y + z*z)\n",
    "        theta = math.atan(math.sqrt(x*x + y*y)/z)/math.pi*180\n",
    "        phi = math.atan(y/x)/math.pi*180\n",
    "        \n",
    "        if (d >= 2.0):\n",
    "            d= 2.0;\n",
    "        elif (d <= 1.0):\n",
    "            d= 1.0;\n",
    "        \n",
    "        if (theta < 0):\n",
    "            theta = 180 + theta;\n",
    "        \n",
    "        if (phi > 0):\n",
    "            if (x > 0):\n",
    "                phi = phi;\n",
    "            else:\n",
    "                phi = 180 + phi;\n",
    "        else:\n",
    "            if (x < 0):\n",
    "                phi = 180 + phi;\n",
    "            else:\n",
    "                phi = 360 + phi;\n",
    "        \n",
    "        \n",
    "        dtp[i, 0] = d\n",
    "        dtp[i, 1] = theta\n",
    "        dtp[i, 2] = phi\n",
    "    \n",
    "    return dtp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function's validity\n",
    "pd_dthetaphi_label_tensor = getDistanceThetaPhi(pd_label_tensor)\n",
    "gt_dthetaphi_label_tensor = getDistanceThetaPhi(gt_label_tensor)\n",
    "\n",
    "test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('test/test_1.csv')\n",
    "test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('test/test_2.csv')\n",
    "test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('test/test_3.csv')\n",
    "test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('test/test_4.csv')\n",
    "test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('test/test_5.csv')\n",
    "\n",
    "test_set_x_1_direct, test_set_y_1_direct, test_set_z_1_direct = load_test_data('test/test_1_direct.csv')\n",
    "test_set_x_2_direct, test_set_y_2_direct, test_set_z_2_direct = load_test_data('test/test_2_direct.csv')\n",
    "test_set_x_3_direct, test_set_y_3_direct, test_set_z_3_direct = load_test_data('test/test_3_direct.csv')\n",
    "test_set_x_4_direct, test_set_y_4_direct, test_set_z_4_direct = load_test_data('test/test_4_direct.csv')\n",
    "test_set_x_5_direct, test_set_y_5_direct, test_set_z_5_direct = load_test_data('test/test_5_direct.csv')\n",
    "\n",
    "input_xyz = np.zeros([400, 3])\n",
    "for i in range(400):\n",
    "    input_xyz[i, 0] = test_set_x_1[i]\n",
    "    input_xyz[i, 1] = test_set_y_1[i]\n",
    "    input_xyz[i, 2] = test_set_z_1[i]\n",
    "    \n",
    "input_dthetaphi_tensor = getDistanceThetaPhi(input_xyz)\n",
    "\n",
    "x_diff = [i - j for i, j in zip(test_set_x_1_direct, input_dthetaphi_tensor[:,0].tolist())]\n",
    "y_diff = [i - j for i, j in zip(test_set_y_1_direct, input_dthetaphi_tensor[:,1].tolist())]\n",
    "z_diff = [i - j for i, j in zip(test_set_z_1_direct, input_dthetaphi_tensor[:,2].tolist())]\n",
    "\n",
    "# plt.plot(test_set_x_1_direct, label='d')\n",
    "# plt.plot(test_set_y_1_direct, label='theta')\n",
    "plt.plot(test_set_z_1_direct, label='pi')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_thetaphi_label_tensor shape: torch.Size([14985, 2])\n",
      "pd_thetaphi_label_tensor shape: torch.Size([14985, 2])\n",
      "gt_thetaphi_valid_label_tensor shape: torch.Size([397, 2])\n",
      "pd_thetaphi_valid_label_tensor shape: torch.Size([397, 2])\n"
     ]
    }
   ],
   "source": [
    "# construct 2d theta phi training tensor\n",
    "gt_dthetaphi_label = getDistanceThetaPhi(gt_label)\n",
    "pd_dthetaphi_label = getDistanceThetaPhi(pd_label)\n",
    "gt_thetaphi_label_tensor = torch.Tensor(gt_dthetaphi_label[:,1:])\n",
    "pd_thetaphi_label_tensor = torch.Tensor(pd_dthetaphi_label[:,1:])\n",
    "print(\"gt_thetaphi_label_tensor shape:\", gt_thetaphi_label_tensor.shape)\n",
    "print(\"pd_thetaphi_label_tensor shape:\", pd_thetaphi_label_tensor.shape)\n",
    "\n",
    "# construct 2d theta phi valid tensor\n",
    "gt_dthetaphi_valid_label = getDistanceThetaPhi(gt_valid_label)\n",
    "pd_dthetaphi_valid_label = getDistanceThetaPhi(pd_valid_label)\n",
    "gt_thetaphi_valid_label_tensor = torch.Tensor(gt_dthetaphi_valid_label[:,1:])\n",
    "pd_thetaphi_valid_label_tensor = torch.Tensor(pd_dthetaphi_valid_label[:,1:])\n",
    "print(\"gt_thetaphi_valid_label_tensor shape:\", gt_thetaphi_valid_label_tensor.shape)\n",
    "print(\"pd_thetaphi_valid_label_tensor shape:\", pd_thetaphi_valid_label_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_dthetaphi_label[:, 2], label=\"pd\")\n",
    "plt.plot(gt_dthetaphi_label[:, 2], label=\"gt\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_dthetaphi_valid_label[:, 0], label=\"pd\")\n",
    "plt.plot(gt_dthetaphi_valid_label[:, 0], label=\"gt\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_dthetaphi_valid_label[:, 2], label=\"pd\")\n",
    "plt.plot(gt_dthetaphi_valid_label[:, 2], label=\"gt\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_thetaphi_label_tensor = torch.matmul(gt_thetaphi_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "pd_thetaphi_label_tensor = torch.matmul(pd_thetaphi_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "\n",
    "gt_thetaphi_valid_label_tensor = torch.matmul(gt_thetaphi_valid_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))\n",
    "pd_thetaphi_valid_label_tensor = torch.matmul(pd_thetaphi_valid_label_tensor, torch.Tensor([[1/180, 0],[0, 1/360]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_theta = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_theta = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_phi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_phi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        \n",
    "        self.l_correlation_theta_phi = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        \n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        \n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_theta = self.l_mu_theta(h)\n",
    "        # print(\"mu_theta\", pi.shape)\n",
    "        # print(\"mu_theta out\", mu_theta[0])\n",
    "        mu_phi = self.l_mu_phi(h)\n",
    "        # print(\"mu_phi\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_theta = torch.exp(self.l_sigma_theta(h))\n",
    "        # print(\"sigma_theta\", sigma_theta.shape)\n",
    "        sigma_phi = torch.exp(self.l_sigma_phi(h))\n",
    "        # print(\"sigma_phi\", sigma_phi.shape)\n",
    "\n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_theta_phi = self.l_correlation_theta_phi(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi):\n",
    "    size = y.shape[0] # N\n",
    "    n_gaussians = pi.shape[1] # G\n",
    "    '''\n",
    "    print(\"sample size: \", size)\n",
    "    print(\"num of gaus: \", n_gaussians)\n",
    "    print(\"mu_theta size: \", mu_theta.shape)\n",
    "    print(\"mu_phi size: \", mu_phi.shape)\n",
    "    print(\"sigma_theta size: \", sigma_theta.shape)\n",
    "    print(\"sigma_phi size: \", sigma_phi.shape)\n",
    "    print(\"correlation_theta_phi size: \", correlation_theta_phi.shape)\n",
    "    '''\n",
    "    x_theta = y[:, 0].unsqueeze(1) # N x 1\n",
    "    # print(\"x_theta shape: \", x_theta.shape)\n",
    "    x_theta = x_theta.repeat(1, n_gaussians) #  N x G\n",
    "    # print(\"x_theta shape: \", x_theta.shape)\n",
    "    x_phi = y[:, 1].unsqueeze(1)   # N X 1\n",
    "    # print(\"x_phi shape: \", x_phi.shape)\n",
    "    x_phi = x_phi.repeat(1, n_gaussians) #  N x G\n",
    "    # print(\"x_phi shape: \", x_phi.shape)\n",
    "    \n",
    "    \n",
    "    # mu_theta: N x G\n",
    "    # sigma_theta: N x G\n",
    "    # correlation_theta_phi: N x G\n",
    "\n",
    "    z = (x_theta - mu_theta)**2/(sigma_theta**2) + \\\n",
    "        (x_phi - mu_phi)**2/(sigma_phi**2) - \\\n",
    "        2*correlation_theta_phi*(x_theta - mu_theta)*(x_phi - mu_phi)/sigma_theta/sigma_phi\n",
    "    '''\n",
    "    print(\"=======\")\n",
    "    print(\"(x_theta - mu_theta)**2/(sigma_theta**2)\", ((x_theta - mu_theta)**2/(sigma_theta**2))[0])\n",
    "    print(\"x_theta: \", x_theta[0])\n",
    "    print(\"mu_theta: \", mu_theta[0])\n",
    "    print(\"sigma_theta: \", sigma_theta[0])\n",
    "    '''\n",
    "\n",
    "    # print(\"(x_phi - mu_phi)**2/(sigma_phi**2)\", ((x_phi - mu_phi)**2/(sigma_phi**2))[0])\n",
    "    # print(\"2*correlation_theta_phi*(x_theta - mu_theta)*(x_phi - mu_phi)/sigma_theta/sigma_phi\", (2*correlation_theta_phi*(x_theta - mu_theta)*(x_phi - mu_phi)/sigma_theta/sigma_phi)[0])\n",
    "    # print(\"z shape: \", z.shape)\n",
    "    \n",
    "    PI = np.pi\n",
    "    \n",
    "    likelihood = 1/(2*PI*sigma_theta*sigma_phi*torch.sqrt(1 - correlation_theta_phi**2))*\\\n",
    "                 torch.exp(-z/(2*(1-correlation_theta_phi**2))) # N X G\n",
    "    # print(\"likelihood\", likelihood[0:10])\n",
    "    \n",
    "    # print(\"likelihood shape: \", likelihood.shape)\n",
    "    \n",
    "    \n",
    "    # print(\"pi shape: \", pi.shape) # N x 2\n",
    "    loss = torch.sum(likelihood * pi, dim=1) # N X 1\n",
    "    # print(\"loss sum\", loss[0:10])\n",
    "    # print(likelihood.max())\n",
    "    '''\n",
    "    for i in range(loss.shape[0]):\n",
    "        if loss[i] > 1.5:\n",
    "            print(\"loss: \", loss[i])\n",
    "            print(\"likelihood: \", likelihood[i])\n",
    "            print(\"mu array  : \", pi[i])\n",
    "            # print(\"likelihood shape: \", likelihood.shape) # 14985 x 5\n",
    "    '''\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gaussians = 3\n",
    "PATH = \"model_v2/with_early_stop_after_400_without_normalization/model_ng_3.pt\"\n",
    "PATH_LOG = \"model_v2/with_early_stop_after_400_without_normalization/model_ng_3.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mdn = MDN(2, n_hidden=20, n_gaussians=num_gaussians)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_mdn.parameters())\n",
    "\n",
    "# learning_rate = 0.0001\n",
    "# optimizer = torch.optim.SGD(model_mdn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 2.0692973136901855, valid_loss: 2.00131893157959\n",
      "epoch: 1, train_loss: 2.040661573410034, valid_loss: 1.9749443531036377\n",
      "epoch: 2, train_loss: 2.011890172958374, valid_loss: 1.9483784437179565\n",
      "epoch: 3, train_loss: 1.982950210571289, valid_loss: 1.9216314554214478\n",
      "epoch: 4, train_loss: 1.953822135925293, valid_loss: 1.8947089910507202\n",
      "epoch: 5, train_loss: 1.9244906902313232, valid_loss: 1.8676140308380127\n",
      "epoch: 6, train_loss: 1.8949483633041382, valid_loss: 1.8403483629226685\n",
      "epoch: 7, train_loss: 1.865188717842102, valid_loss: 1.8129165172576904\n",
      "epoch: 8, train_loss: 1.8352093696594238, valid_loss: 1.7853214740753174\n",
      "epoch: 9, train_loss: 1.8050090074539185, valid_loss: 1.757569432258606\n",
      "epoch: 10, train_loss: 1.7745882272720337, valid_loss: 1.7296653985977173\n",
      "epoch: 11, train_loss: 1.7439467906951904, valid_loss: 1.70161771774292\n",
      "epoch: 12, train_loss: 1.7130889892578125, valid_loss: 1.6734333038330078\n",
      "epoch: 13, train_loss: 1.6820192337036133, valid_loss: 1.6451226472854614\n",
      "epoch: 14, train_loss: 1.6507434844970703, valid_loss: 1.6166964769363403\n",
      "epoch: 15, train_loss: 1.6192702054977417, valid_loss: 1.588167667388916\n",
      "epoch: 16, train_loss: 1.5876116752624512, valid_loss: 1.5595524311065674\n",
      "epoch: 17, train_loss: 1.5557804107666016, valid_loss: 1.5308680534362793\n",
      "epoch: 18, train_loss: 1.5237919092178345, valid_loss: 1.5021355152130127\n",
      "epoch: 19, train_loss: 1.4916666746139526, valid_loss: 1.4733766317367554\n",
      "epoch: 20, train_loss: 1.4594264030456543, valid_loss: 1.4446184635162354\n",
      "epoch: 21, train_loss: 1.4270976781845093, valid_loss: 1.4158892631530762\n",
      "epoch: 22, train_loss: 1.3947086334228516, valid_loss: 1.3872196674346924\n",
      "epoch: 23, train_loss: 1.3622926473617554, valid_loss: 1.3586421012878418\n",
      "epoch: 24, train_loss: 1.32988440990448, valid_loss: 1.3301903009414673\n",
      "epoch: 25, train_loss: 1.2975215911865234, valid_loss: 1.3018988370895386\n",
      "epoch: 26, train_loss: 1.2652429342269897, valid_loss: 1.2737993001937866\n",
      "epoch: 27, train_loss: 1.2330868244171143, valid_loss: 1.2459224462509155\n",
      "epoch: 28, train_loss: 1.20109224319458, valid_loss: 1.2182945013046265\n",
      "epoch: 29, train_loss: 1.1692934036254883, valid_loss: 1.1909345388412476\n",
      "epoch: 30, train_loss: 1.1377192735671997, valid_loss: 1.1638541221618652\n",
      "epoch: 31, train_loss: 1.1063916683197021, valid_loss: 1.1370532512664795\n",
      "epoch: 32, train_loss: 1.075321912765503, valid_loss: 1.1105188131332397\n",
      "epoch: 33, train_loss: 1.0445079803466797, valid_loss: 1.0842229127883911\n",
      "epoch: 34, train_loss: 1.013930320739746, valid_loss: 1.0581213235855103\n",
      "epoch: 35, train_loss: 0.9835538268089294, valid_loss: 1.0321532487869263\n",
      "epoch: 36, train_loss: 0.9533222317695618, valid_loss: 1.0062425136566162\n",
      "epoch: 37, train_loss: 0.9231622219085693, valid_loss: 0.9802998900413513\n",
      "epoch: 38, train_loss: 0.8929839134216309, valid_loss: 0.9542288780212402\n",
      "epoch: 39, train_loss: 0.8626877069473267, valid_loss: 0.9279313087463379\n",
      "epoch: 40, train_loss: 0.8321690559387207, valid_loss: 0.9013165831565857\n",
      "epoch: 41, train_loss: 0.8013294339179993, valid_loss: 0.8743081092834473\n",
      "epoch: 42, train_loss: 0.7700839638710022, valid_loss: 0.8468514084815979\n",
      "epoch: 43, train_loss: 0.7383720874786377, valid_loss: 0.818919837474823\n",
      "epoch: 44, train_loss: 0.7061631679534912, valid_loss: 0.790518045425415\n",
      "epoch: 45, train_loss: 0.673461377620697, valid_loss: 0.761680543422699\n",
      "epoch: 46, train_loss: 0.6403041481971741, valid_loss: 0.7324686646461487\n",
      "epoch: 47, train_loss: 0.6067575216293335, valid_loss: 0.7029613256454468\n",
      "epoch: 48, train_loss: 0.5729063153266907, valid_loss: 0.6732448935508728\n",
      "epoch: 49, train_loss: 0.5388381481170654, valid_loss: 0.6433998942375183\n",
      "epoch: 50, train_loss: 0.5046257972717285, valid_loss: 0.6134862899780273\n",
      "epoch: 51, train_loss: 0.4703075885772705, valid_loss: 0.5835312008857727\n",
      "epoch: 52, train_loss: 0.43587154150009155, valid_loss: 0.5535219311714172\n",
      "epoch: 53, train_loss: 0.401247501373291, valid_loss: 0.5234089493751526\n",
      "epoch: 54, train_loss: 0.3663139045238495, valid_loss: 0.4931172728538513\n",
      "epoch: 55, train_loss: 0.33092135190963745, valid_loss: 0.4625689685344696\n",
      "epoch: 56, train_loss: 0.29492732882499695, valid_loss: 0.4317038059234619\n",
      "epoch: 57, train_loss: 0.2582356035709381, valid_loss: 0.40049606561660767\n",
      "epoch: 58, train_loss: 0.22082461416721344, valid_loss: 0.3689568340778351\n",
      "epoch: 59, train_loss: 0.18275509774684906, valid_loss: 0.337117999792099\n",
      "epoch: 60, train_loss: 0.14414876699447632, valid_loss: 0.30500075221061707\n",
      "epoch: 61, train_loss: 0.1051395982503891, valid_loss: 0.27257513999938965\n",
      "epoch: 62, train_loss: 0.06580961495637894, valid_loss: 0.2397313416004181\n",
      "epoch: 63, train_loss: 0.026135554537177086, valid_loss: 0.20628328621387482\n",
      "epoch: 64, train_loss: -0.014020693488419056, valid_loss: 0.1720229685306549\n",
      "epoch: 65, train_loss: -0.05485576391220093, valid_loss: 0.1368093192577362\n",
      "epoch: 66, train_loss: -0.09651897102594376, valid_loss: 0.10064628720283508\n",
      "epoch: 67, train_loss: -0.1390174776315689, valid_loss: 0.06369675695896149\n",
      "epoch: 68, train_loss: -0.18220113217830658, valid_loss: 0.026211578398942947\n",
      "epoch: 69, train_loss: -0.22585482895374298, valid_loss: -0.011594453826546669\n",
      "epoch: 70, train_loss: -0.2698522210121155, valid_loss: -0.049640338867902756\n",
      "epoch: 71, train_loss: -0.3142605721950531, valid_loss: -0.08796876668930054\n",
      "epoch: 72, train_loss: -0.3592849671840668, valid_loss: -0.12662570178508759\n",
      "epoch: 73, train_loss: -0.4050605893135071, valid_loss: -0.16555249691009521\n",
      "epoch: 74, train_loss: -0.45147719979286194, valid_loss: -0.20463761687278748\n",
      "epoch: 75, train_loss: -0.49825209379196167, valid_loss: -0.24390970170497894\n",
      "epoch: 76, train_loss: -0.5452337861061096, valid_loss: -0.2836156189441681\n",
      "epoch: 77, train_loss: -0.5925906896591187, valid_loss: -0.3239741027355194\n",
      "epoch: 78, train_loss: -0.6405693292617798, valid_loss: -0.3648388683795929\n",
      "epoch: 79, train_loss: -0.6890822649002075, valid_loss: -0.40576228499412537\n",
      "epoch: 80, train_loss: -0.7377628684043884, valid_loss: -0.44646933674812317\n",
      "epoch: 81, train_loss: -0.7865113615989685, valid_loss: -0.4870632290840149\n",
      "epoch: 82, train_loss: -0.8356295228004456, valid_loss: -0.5276151299476624\n",
      "epoch: 83, train_loss: -0.8851579427719116, valid_loss: -0.5679965615272522\n",
      "epoch: 84, train_loss: -0.9346950650215149, valid_loss: -0.60836261510849\n",
      "epoch: 85, train_loss: -0.9842617511749268, valid_loss: -0.648971676826477\n",
      "epoch: 86, train_loss: -1.034247875213623, valid_loss: -0.6894579529762268\n",
      "epoch: 87, train_loss: -1.0843570232391357, valid_loss: -0.7295812964439392\n",
      "epoch: 88, train_loss: -1.134344458580017, valid_loss: -0.7698249816894531\n",
      "epoch: 89, train_loss: -1.1846516132354736, valid_loss: -0.8101143836975098\n",
      "epoch: 90, train_loss: -1.2349379062652588, valid_loss: -0.8501990437507629\n",
      "epoch: 91, train_loss: -1.2851446866989136, valid_loss: -0.8899217844009399\n",
      "epoch: 92, train_loss: -1.3355934619903564, valid_loss: -0.9291185736656189\n",
      "epoch: 93, train_loss: -1.3856971263885498, valid_loss: -0.9689502716064453\n",
      "epoch: 94, train_loss: -1.4358911514282227, valid_loss: -1.0087271928787231\n",
      "epoch: 95, train_loss: -1.4856125116348267, valid_loss: -1.0471757650375366\n",
      "epoch: 96, train_loss: -1.5352344512939453, valid_loss: -1.084485411643982\n",
      "epoch: 97, train_loss: -1.5841701030731201, valid_loss: -1.1232247352600098\n",
      "epoch: 98, train_loss: -1.6327836513519287, valid_loss: -1.160515308380127\n",
      "epoch: 99, train_loss: -1.6805870532989502, valid_loss: -1.1948155164718628\n",
      "epoch: 100, train_loss: -1.7276852130889893, valid_loss: -1.231774926185608\n",
      "epoch: 101, train_loss: -1.7741973400115967, valid_loss: -1.2654560804367065\n",
      "epoch: 102, train_loss: -1.819938063621521, valid_loss: -1.2976261377334595\n",
      "epoch: 103, train_loss: -1.8648388385772705, valid_loss: -1.332698941230774\n",
      "epoch: 104, train_loss: -1.908815860748291, valid_loss: -1.3567934036254883\n",
      "epoch: 105, train_loss: -1.9513263702392578, valid_loss: -1.3975932598114014\n",
      "epoch: 106, train_loss: -1.9892772436141968, valid_loss: -1.4008151292800903\n",
      "epoch: 107, train_loss: -2.0260961055755615, valid_loss: -1.4531223773956299\n",
      "epoch: 108, train_loss: -2.0719213485717773, valid_loss: -1.4771677255630493\n",
      "epoch: 109, train_loss: -2.1125378608703613, valid_loss: -1.4791158437728882\n",
      "epoch: 110, train_loss: -2.1427671909332275, valid_loss: -1.5278663635253906\n",
      "epoch: 111, train_loss: -2.1835520267486572, valid_loss: -1.5477867126464844\n",
      "epoch: 112, train_loss: -2.2208476066589355, valid_loss: -1.5465502738952637\n",
      "epoch: 113, train_loss: -2.2480101585388184, valid_loss: -1.5938451290130615\n",
      "epoch: 114, train_loss: -2.284017562866211, valid_loss: -1.6075092554092407\n",
      "epoch: 115, train_loss: -2.3200230598449707, valid_loss: -1.6098036766052246\n",
      "epoch: 116, train_loss: -2.3451437950134277, valid_loss: -1.6528388261795044\n",
      "epoch: 117, train_loss: -2.3732876777648926, valid_loss: -1.655084490776062\n",
      "epoch: 118, train_loss: -2.40799617767334, valid_loss: -1.6723675727844238\n",
      "epoch: 119, train_loss: -2.435945749282837, valid_loss: -1.7034640312194824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120, train_loss: -2.456894636154175, valid_loss: -1.688666582107544\n",
      "epoch: 121, train_loss: -2.4810101985931396, valid_loss: -1.730186939239502\n",
      "epoch: 122, train_loss: -2.511335611343384, valid_loss: -1.7360270023345947\n",
      "epoch: 123, train_loss: -2.538449764251709, valid_loss: -1.735674262046814\n",
      "epoch: 124, train_loss: -2.557835102081299, valid_loss: -1.7683502435684204\n",
      "epoch: 125, train_loss: -2.5725481510162354, valid_loss: -1.7404539585113525\n",
      "epoch: 126, train_loss: -2.5884103775024414, valid_loss: -1.7863765954971313\n",
      "epoch: 127, train_loss: -2.614509344100952, valid_loss: -1.780908226966858\n",
      "epoch: 128, train_loss: -2.642390489578247, valid_loss: -1.7849066257476807\n",
      "epoch: 129, train_loss: -2.658740997314453, valid_loss: -1.8092753887176514\n",
      "epoch: 130, train_loss: -2.666624069213867, valid_loss: -1.780744194984436\n",
      "epoch: 131, train_loss: -2.6771159172058105, valid_loss: -1.822632908821106\n",
      "epoch: 132, train_loss: -2.7009449005126953, valid_loss: -1.8191689252853394\n",
      "epoch: 133, train_loss: -2.7254772186279297, valid_loss: -1.8193236589431763\n",
      "epoch: 134, train_loss: -2.736699104309082, valid_loss: -1.8438457250595093\n",
      "epoch: 135, train_loss: -2.740755796432495, valid_loss: -1.8177555799484253\n",
      "epoch: 136, train_loss: -2.7510313987731934, valid_loss: -1.8581137657165527\n",
      "epoch: 137, train_loss: -2.774437427520752, valid_loss: -1.8589280843734741\n",
      "epoch: 138, train_loss: -2.79437518119812, valid_loss: -1.8548604249954224\n",
      "epoch: 139, train_loss: -2.8014986515045166, valid_loss: -1.88217294216156\n",
      "epoch: 140, train_loss: -2.805069923400879, valid_loss: -1.8603190183639526\n",
      "epoch: 141, train_loss: -2.8166401386260986, valid_loss: -1.89785897731781\n",
      "epoch: 142, train_loss: -2.838322401046753, valid_loss: -1.9018330574035645\n",
      "epoch: 143, train_loss: -2.8543479442596436, valid_loss: -1.8969042301177979\n",
      "epoch: 144, train_loss: -2.8598392009735107, valid_loss: -1.9248054027557373\n",
      "epoch: 145, train_loss: -2.864259719848633, valid_loss: -1.9079564809799194\n",
      "epoch: 146, train_loss: -2.876034736633301, valid_loss: -1.9414535760879517\n",
      "epoch: 147, train_loss: -2.8949344158172607, valid_loss: -1.9464257955551147\n",
      "epoch: 148, train_loss: -2.908801317214966, valid_loss: -1.9434994459152222\n",
      "epoch: 149, train_loss: -2.914729356765747, valid_loss: -1.9692258834838867\n",
      "epoch: 150, train_loss: -2.919795513153076, valid_loss: -1.9540421962738037\n",
      "epoch: 151, train_loss: -2.9300484657287598, valid_loss: -1.9850897789001465\n",
      "epoch: 152, train_loss: -2.946258544921875, valid_loss: -1.9870367050170898\n",
      "epoch: 153, train_loss: -2.9601855278015137, valid_loss: -1.9894319772720337\n",
      "epoch: 154, train_loss: -2.968303680419922, valid_loss: -2.009716749191284\n",
      "epoch: 155, train_loss: -2.973926067352295, valid_loss: -1.9953333139419556\n",
      "epoch: 156, train_loss: -2.9815833568573, valid_loss: -2.0248959064483643\n",
      "epoch: 157, train_loss: -2.9941818714141846, valid_loss: -2.020529270172119\n",
      "epoch: 158, train_loss: -3.00810170173645, valid_loss: -2.0337538719177246\n",
      "epoch: 159, train_loss: -3.0194902420043945, valid_loss: -2.0457723140716553\n",
      "epoch: 160, train_loss: -3.027355432510376, valid_loss: -2.0378024578094482\n",
      "epoch: 161, train_loss: -3.0337626934051514, valid_loss: -2.063291549682617\n",
      "epoch: 162, train_loss: -3.041522741317749, valid_loss: -2.0516183376312256\n",
      "epoch: 163, train_loss: -3.0514628887176514, valid_loss: -2.0781543254852295\n",
      "epoch: 164, train_loss: -3.0636019706726074, valid_loss: -2.0770111083984375\n",
      "epoch: 165, train_loss: -3.075437307357788, valid_loss: -2.088953971862793\n",
      "epoch: 166, train_loss: -3.0855281352996826, valid_loss: -2.101078510284424\n",
      "epoch: 167, train_loss: -3.093791961669922, valid_loss: -2.097804069519043\n",
      "epoch: 168, train_loss: -3.1010303497314453, valid_loss: -2.1206507682800293\n",
      "epoch: 169, train_loss: -3.1081998348236084, valid_loss: -2.1096153259277344\n",
      "epoch: 170, train_loss: -3.1156809329986572, valid_loss: -2.1381442546844482\n",
      "epoch: 171, train_loss: -3.124556303024292, valid_loss: -2.1277575492858887\n",
      "epoch: 172, train_loss: -3.1344542503356934, valid_loss: -2.154128074645996\n",
      "epoch: 173, train_loss: -3.1455626487731934, valid_loss: -2.1513659954071045\n",
      "epoch: 174, train_loss: -3.1565351486206055, valid_loss: -2.1672275066375732\n",
      "epoch: 175, train_loss: -3.166745185852051, valid_loss: -2.173785924911499\n",
      "epoch: 176, train_loss: -3.1759088039398193, valid_loss: -2.177806854248047\n",
      "epoch: 177, train_loss: -3.184218645095825, valid_loss: -2.192723512649536\n",
      "epoch: 178, train_loss: -3.1918716430664062, valid_loss: -2.1865122318267822\n",
      "epoch: 179, train_loss: -3.1987311840057373, valid_loss: -2.209357738494873\n",
      "epoch: 180, train_loss: -3.2046103477478027, valid_loss: -2.190948247909546\n",
      "epoch: 181, train_loss: -3.2086100578308105, valid_loss: -2.223336935043335\n",
      "epoch: 182, train_loss: -3.212297201156616, valid_loss: -2.1951584815979004\n",
      "epoch: 183, train_loss: -3.2167224884033203, valid_loss: -2.2371203899383545\n",
      "epoch: 184, train_loss: -3.229570150375366, valid_loss: -2.2241265773773193\n",
      "epoch: 185, train_loss: -3.247002124786377, valid_loss: -2.2479164600372314\n",
      "epoch: 186, train_loss: -3.2623703479766846, valid_loss: -2.256622314453125\n",
      "epoch: 187, train_loss: -3.2693960666656494, valid_loss: -2.243990898132324\n",
      "epoch: 188, train_loss: -3.2712438106536865, valid_loss: -2.2726891040802\n",
      "epoch: 189, train_loss: -3.275639533996582, valid_loss: -2.255519390106201\n",
      "epoch: 190, train_loss: -3.2858076095581055, valid_loss: -2.2860891819000244\n",
      "epoch: 191, train_loss: -3.3008954524993896, valid_loss: -2.2872281074523926\n",
      "epoch: 192, train_loss: -3.313049077987671, valid_loss: -2.2889885902404785\n",
      "epoch: 193, train_loss: -3.3196053504943848, valid_loss: -2.3075551986694336\n",
      "epoch: 194, train_loss: -3.3236026763916016, valid_loss: -2.2924132347106934\n",
      "epoch: 195, train_loss: -3.3291072845458984, valid_loss: -2.3213839530944824\n",
      "epoch: 196, train_loss: -3.3393282890319824, valid_loss: -2.313920021057129\n",
      "epoch: 197, train_loss: -3.351696252822876, valid_loss: -2.3316736221313477\n",
      "epoch: 198, train_loss: -3.363258123397827, valid_loss: -2.3383755683898926\n",
      "epoch: 199, train_loss: -3.371823310852051, valid_loss: -2.335742950439453\n",
      "epoch: 200, train_loss: -3.3780517578125, valid_loss: -2.3552417755126953\n",
      "epoch: 201, train_loss: -3.38379168510437, valid_loss: -2.342108726501465\n",
      "epoch: 202, train_loss: -3.390105962753296, valid_loss: -2.3690989017486572\n",
      "epoch: 203, train_loss: -3.3985912799835205, valid_loss: -2.3571791648864746\n",
      "epoch: 204, train_loss: -3.408377170562744, valid_loss: -2.3822598457336426\n",
      "epoch: 205, train_loss: -3.4196178913116455, valid_loss: -2.378397226333618\n",
      "epoch: 206, train_loss: -3.4306092262268066, valid_loss: -2.3935558795928955\n",
      "epoch: 207, train_loss: -3.440852403640747, valid_loss: -2.398674964904785\n",
      "epoch: 208, train_loss: -3.450094223022461, valid_loss: -2.4032387733459473\n",
      "epoch: 209, train_loss: -3.458540201187134, valid_loss: -2.4163565635681152\n",
      "epoch: 210, train_loss: -3.4663448333740234, valid_loss: -2.411367416381836\n",
      "epoch: 211, train_loss: -3.4732532501220703, valid_loss: -2.4323971271514893\n",
      "epoch: 212, train_loss: -3.4787442684173584, valid_loss: -2.4131405353546143\n",
      "epoch: 213, train_loss: -3.4808034896850586, valid_loss: -2.4441514015197754\n",
      "epoch: 214, train_loss: -3.4795889854431152, valid_loss: -2.4041786193847656\n",
      "epoch: 215, train_loss: -3.473543405532837, valid_loss: -2.453327178955078\n",
      "epoch: 216, train_loss: -3.48169207572937, valid_loss: -2.431490182876587\n",
      "epoch: 217, train_loss: -3.5048763751983643, valid_loss: -2.474377155303955\n",
      "epoch: 218, train_loss: -3.535625457763672, valid_loss: -2.4814836978912354\n",
      "epoch: 219, train_loss: -3.544658899307251, valid_loss: -2.4592647552490234\n",
      "epoch: 220, train_loss: -3.5372562408447266, valid_loss: -2.493577241897583\n",
      "epoch: 221, train_loss: -3.5412087440490723, valid_loss: -2.4804036617279053\n",
      "epoch: 222, train_loss: -3.561664342880249, valid_loss: -2.5077569484710693\n",
      "epoch: 223, train_loss: -3.5825090408325195, valid_loss: -2.517315626144409\n",
      "epoch: 224, train_loss: -3.586090087890625, valid_loss: -2.4973413944244385\n",
      "epoch: 225, train_loss: -3.5839195251464844, valid_loss: -2.530353307723999\n",
      "epoch: 226, train_loss: -3.5955817699432373, valid_loss: -2.526111125946045\n",
      "epoch: 227, train_loss: -3.6149001121520996, valid_loss: -2.539170026779175\n",
      "epoch: 228, train_loss: -3.6271886825561523, valid_loss: -2.5522758960723877\n",
      "epoch: 229, train_loss: -3.6296300888061523, valid_loss: -2.538106918334961\n",
      "epoch: 230, train_loss: -3.63334584236145, valid_loss: -2.5669705867767334\n",
      "epoch: 231, train_loss: -3.646920919418335, valid_loss: -2.5665247440338135\n",
      "epoch: 232, train_loss: -3.6628592014312744, valid_loss: -2.57623291015625\n",
      "epoch: 233, train_loss: -3.673208475112915, valid_loss: -2.589609384536743\n",
      "epoch: 234, train_loss: -3.678161144256592, valid_loss: -2.580249786376953\n",
      "epoch: 235, train_loss: -3.6837241649627686, valid_loss: -2.6046462059020996\n",
      "epoch: 236, train_loss: -3.6947524547576904, valid_loss: -2.602553606033325\n",
      "epoch: 237, train_loss: -3.70857834815979, valid_loss: -2.61842942237854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 238, train_loss: -3.721369981765747, valid_loss: -2.626934051513672\n",
      "epoch: 239, train_loss: -3.7307684421539307, valid_loss: -2.6264779567718506\n",
      "epoch: 240, train_loss: -3.737863302230835, valid_loss: -2.6438775062561035\n",
      "epoch: 241, train_loss: -3.7450482845306396, valid_loss: -2.637913227081299\n",
      "epoch: 242, train_loss: -3.7534127235412598, valid_loss: -2.6600265502929688\n",
      "epoch: 243, train_loss: -3.7640953063964844, valid_loss: -2.6568338871002197\n",
      "epoch: 244, train_loss: -3.775789260864258, valid_loss: -2.6768875122070312\n",
      "epoch: 245, train_loss: -3.7882957458496094, valid_loss: -2.6785178184509277\n",
      "epoch: 246, train_loss: -3.800448179244995, valid_loss: -2.692996025085449\n",
      "epoch: 247, train_loss: -3.8121471405029297, valid_loss: -2.698775291442871\n",
      "epoch: 248, train_loss: -3.8233554363250732, valid_loss: -2.7086799144744873\n",
      "epoch: 249, train_loss: -3.8342769145965576, valid_loss: -2.7176616191864014\n",
      "epoch: 250, train_loss: -3.8450565338134766, valid_loss: -2.7244913578033447\n",
      "epoch: 251, train_loss: -3.8557217121124268, valid_loss: -2.7363829612731934\n",
      "epoch: 252, train_loss: -3.8660922050476074, valid_loss: -2.738664388656616\n",
      "epoch: 253, train_loss: -3.8754634857177734, valid_loss: -2.753924608230591\n",
      "epoch: 254, train_loss: -3.881868600845337, valid_loss: -2.7391774654388428\n",
      "epoch: 255, train_loss: -3.8784983158111572, valid_loss: -2.749871253967285\n",
      "epoch: 256, train_loss: -3.852593183517456, valid_loss: -2.6706809997558594\n",
      "epoch: 257, train_loss: -3.780670166015625, valid_loss: -2.714289903640747\n",
      "epoch: 258, train_loss: -3.7741246223449707, valid_loss: -2.745100259780884\n",
      "epoch: 259, train_loss: -3.877943754196167, valid_loss: -2.8041188716888428\n",
      "epoch: 260, train_loss: -3.951918840408325, valid_loss: -2.7762677669525146\n",
      "epoch: 261, train_loss: -3.873182773590088, valid_loss: -2.7556991577148438\n",
      "epoch: 262, train_loss: -3.886355400085449, valid_loss: -2.835212469100952\n",
      "epoch: 263, train_loss: -3.9862399101257324, valid_loss: -2.8130898475646973\n",
      "epoch: 264, train_loss: -3.924149751663208, valid_loss: -2.7944495677948\n",
      "epoch: 265, train_loss: -3.9365532398223877, valid_loss: -2.8588173389434814\n",
      "epoch: 266, train_loss: -4.016834259033203, valid_loss: -2.8391709327697754\n",
      "epoch: 267, train_loss: -3.9560046195983887, valid_loss: -2.8416218757629395\n",
      "epoch: 268, train_loss: -3.9969372749328613, valid_loss: -2.876600742340088\n",
      "epoch: 269, train_loss: -4.041354656219482, valid_loss: -2.8669614791870117\n",
      "epoch: 270, train_loss: -3.994432210922241, valid_loss: -2.886748790740967\n",
      "epoch: 271, train_loss: -4.055853366851807, valid_loss: -2.8912224769592285\n",
      "epoch: 272, train_loss: -4.062206745147705, valid_loss: -2.902282238006592\n",
      "epoch: 273, train_loss: -4.047037124633789, valid_loss: -2.924945116043091\n",
      "epoch: 274, train_loss: -4.102698802947998, valid_loss: -2.9123945236206055\n",
      "epoch: 275, train_loss: -4.089061737060547, valid_loss: -2.9397218227386475\n",
      "epoch: 276, train_loss: -4.1027021408081055, valid_loss: -2.9565014839172363\n",
      "epoch: 277, train_loss: -4.138288497924805, valid_loss: -2.9413766860961914\n",
      "epoch: 278, train_loss: -4.125125408172607, valid_loss: -2.972076416015625\n",
      "epoch: 279, train_loss: -4.15165901184082, valid_loss: -2.983192205429077\n",
      "epoch: 280, train_loss: -4.171543598175049, valid_loss: -2.972316265106201\n",
      "epoch: 281, train_loss: -4.166204452514648, valid_loss: -3.0004630088806152\n",
      "epoch: 282, train_loss: -4.193965911865234, valid_loss: -3.010161876678467\n",
      "epoch: 283, train_loss: -4.208078384399414, valid_loss: -3.0041587352752686\n",
      "epoch: 284, train_loss: -4.208372592926025, valid_loss: -3.028947591781616\n",
      "epoch: 285, train_loss: -4.233119010925293, valid_loss: -3.0389492511749268\n",
      "epoch: 286, train_loss: -4.248534202575684, valid_loss: -3.035858154296875\n",
      "epoch: 287, train_loss: -4.2513861656188965, valid_loss: -3.0561647415161133\n",
      "epoch: 288, train_loss: -4.2708420753479, valid_loss: -3.0664076805114746\n",
      "epoch: 289, train_loss: -4.2908196449279785, valid_loss: -3.0685107707977295\n",
      "epoch: 290, train_loss: -4.297738075256348, valid_loss: -3.082892894744873\n",
      "epoch: 291, train_loss: -4.309125900268555, valid_loss: -3.091531753540039\n",
      "epoch: 292, train_loss: -4.329717636108398, valid_loss: -3.1041336059570312\n",
      "epoch: 293, train_loss: -4.346429347991943, valid_loss: -3.1138739585876465\n",
      "epoch: 294, train_loss: -4.355851650238037, valid_loss: -3.1173691749572754\n",
      "epoch: 295, train_loss: -4.367006301879883, valid_loss: -3.1336746215820312\n",
      "epoch: 296, train_loss: -4.384714603424072, valid_loss: -3.143825054168701\n",
      "epoch: 297, train_loss: -4.403969764709473, valid_loss: -3.1555325984954834\n",
      "epoch: 298, train_loss: -4.420215606689453, valid_loss: -3.1657023429870605\n",
      "epoch: 299, train_loss: -4.4330925941467285, valid_loss: -3.171998977661133\n",
      "epoch: 300, train_loss: -4.444436550140381, valid_loss: -3.1821775436401367\n",
      "epoch: 301, train_loss: -4.4556498527526855, valid_loss: -3.187955617904663\n",
      "epoch: 302, train_loss: -4.4657392501831055, valid_loss: -3.1953811645507812\n",
      "epoch: 303, train_loss: -4.473826885223389, valid_loss: -3.1982581615448\n",
      "epoch: 304, train_loss: -4.4745402336120605, valid_loss: -3.1930062770843506\n",
      "epoch: 305, train_loss: -4.466279983520508, valid_loss: -3.186152935028076\n",
      "epoch: 306, train_loss: -4.438568115234375, valid_loss: -3.1690850257873535\n",
      "epoch: 307, train_loss: -4.424543380737305, valid_loss: -3.1932687759399414\n",
      "epoch: 308, train_loss: -4.436091899871826, valid_loss: -3.235405445098877\n",
      "epoch: 309, train_loss: -4.5290985107421875, valid_loss: -3.2863705158233643\n",
      "epoch: 310, train_loss: -4.603003025054932, valid_loss: -3.2922489643096924\n",
      "epoch: 311, train_loss: -4.6054205894470215, valid_loss: -3.261312961578369\n",
      "epoch: 312, train_loss: -4.567145347595215, valid_loss: -3.2742600440979004\n",
      "epoch: 313, train_loss: -4.551936149597168, valid_loss: -3.2896811962127686\n",
      "epoch: 314, train_loss: -4.610260963439941, valid_loss: -3.3404300212860107\n",
      "epoch: 315, train_loss: -4.675123691558838, valid_loss: -3.351771593093872\n",
      "epoch: 316, train_loss: -4.691397190093994, valid_loss: -3.328146457672119\n",
      "epoch: 317, train_loss: -4.6688127517700195, valid_loss: -3.343165397644043\n",
      "epoch: 318, train_loss: -4.653181076049805, valid_loss: -3.3371870517730713\n",
      "epoch: 319, train_loss: -4.684394359588623, valid_loss: -3.389263391494751\n",
      "epoch: 320, train_loss: -4.73616361618042, valid_loss: -3.4029107093811035\n",
      "epoch: 321, train_loss: -4.774261474609375, valid_loss: -3.4032235145568848\n",
      "epoch: 322, train_loss: -4.781169414520264, valid_loss: -3.415496349334717\n",
      "epoch: 323, train_loss: -4.768393039703369, valid_loss: -3.377305507659912\n",
      "epoch: 324, train_loss: -4.755161285400391, valid_loss: -3.4139349460601807\n",
      "epoch: 325, train_loss: -4.748142242431641, valid_loss: -3.3821520805358887\n",
      "epoch: 326, train_loss: -4.768674850463867, valid_loss: -3.444932699203491\n",
      "epoch: 327, train_loss: -4.802972793579102, valid_loss: -3.437532901763916\n",
      "epoch: 328, train_loss: -4.849452972412109, valid_loss: -3.4835450649261475\n",
      "epoch: 329, train_loss: -4.88730001449585, valid_loss: -3.4867520332336426\n",
      "epoch: 330, train_loss: -4.9117841720581055, valid_loss: -3.490518569946289\n",
      "epoch: 331, train_loss: -4.924485206604004, valid_loss: -3.5066514015197754\n",
      "epoch: 332, train_loss: -4.926525115966797, valid_loss: -3.4630234241485596\n",
      "epoch: 333, train_loss: -4.909896373748779, valid_loss: -3.4757301807403564\n",
      "epoch: 334, train_loss: -4.843011379241943, valid_loss: -3.3016042709350586\n",
      "epoch: 335, train_loss: -4.689599990844727, valid_loss: -3.2855498790740967\n",
      "epoch: 336, train_loss: -4.4540815353393555, valid_loss: -3.316276788711548\n",
      "epoch: 337, train_loss: -4.715315818786621, valid_loss: -3.5516152381896973\n",
      "epoch: 338, train_loss: -5.002194881439209, valid_loss: -3.5314505100250244\n",
      "epoch: 339, train_loss: -4.938644886016846, valid_loss: -3.3478188514709473\n",
      "epoch: 340, train_loss: -4.771548271179199, valid_loss: -3.517742395401001\n",
      "epoch: 341, train_loss: -4.889355182647705, valid_loss: -3.5616466999053955\n",
      "epoch: 342, train_loss: -5.061192989349365, valid_loss: -3.4511682987213135\n",
      "epoch: 343, train_loss: -4.929005146026611, valid_loss: -3.51777982711792\n",
      "epoch: 344, train_loss: -4.880471229553223, valid_loss: -3.5556321144104004\n",
      "epoch: 345, train_loss: -5.072763919830322, valid_loss: -3.525726318359375\n",
      "epoch: 346, train_loss: -5.037112236022949, valid_loss: -3.552042007446289\n",
      "epoch: 347, train_loss: -4.940291404724121, valid_loss: -3.5627243518829346\n",
      "epoch: 348, train_loss: -5.079798221588135, valid_loss: -3.57468581199646\n",
      "epoch: 349, train_loss: -5.108260154724121, valid_loss: -3.587623357772827\n",
      "epoch: 350, train_loss: -5.017971515655518, valid_loss: -3.563062906265259\n",
      "epoch: 351, train_loss: -5.090250015258789, valid_loss: -3.607146978378296\n",
      "epoch: 352, train_loss: -5.158354759216309, valid_loss: -3.6135592460632324\n",
      "epoch: 353, train_loss: -5.100019454956055, valid_loss: -3.5585031509399414\n",
      "epoch: 354, train_loss: -5.10141658782959, valid_loss: -3.6313841342926025\n",
      "epoch: 355, train_loss: -5.179708480834961, valid_loss: -3.6335954666137695\n",
      "epoch: 356, train_loss: -5.1847028732299805, valid_loss: -3.578989267349243\n",
      "epoch: 357, train_loss: -5.142816543579102, valid_loss: -3.6406798362731934\n",
      "epoch: 358, train_loss: -5.163323402404785, valid_loss: -3.6200926303863525\n",
      "epoch: 359, train_loss: -5.220885753631592, valid_loss: -3.6398048400878906\n",
      "epoch: 360, train_loss: -5.233245372772217, valid_loss: -3.6567535400390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 361, train_loss: -5.205129623413086, valid_loss: -3.571277618408203\n",
      "epoch: 362, train_loss: -5.195204734802246, valid_loss: -3.659266710281372\n",
      "epoch: 363, train_loss: -5.223092079162598, valid_loss: -3.627700090408325\n",
      "epoch: 364, train_loss: -5.2643232345581055, valid_loss: -3.6437878608703613\n",
      "epoch: 365, train_loss: -5.288336277008057, valid_loss: -3.6704089641571045\n",
      "epoch: 366, train_loss: -5.2908711433410645, valid_loss: -3.6029274463653564\n",
      "epoch: 367, train_loss: -5.278376579284668, valid_loss: -3.657809257507324\n",
      "epoch: 368, train_loss: -5.250888347625732, valid_loss: -3.5409841537475586\n",
      "epoch: 369, train_loss: -5.202107906341553, valid_loss: -3.5988852977752686\n",
      "epoch: 370, train_loss: -5.114238739013672, valid_loss: -3.4245667457580566\n",
      "epoch: 371, train_loss: -5.033231735229492, valid_loss: -3.5764713287353516\n",
      "epoch: 372, train_loss: -5.002532958984375, valid_loss: -3.4799063205718994\n",
      "epoch: 373, train_loss: -5.179914474487305, valid_loss: -3.6866211891174316\n",
      "epoch: 374, train_loss: -5.337258815765381, valid_loss: -3.6888492107391357\n",
      "epoch: 375, train_loss: -5.359116554260254, valid_loss: -3.535429000854492\n",
      "epoch: 376, train_loss: -5.268184661865234, valid_loss: -3.6413869857788086\n",
      "epoch: 377, train_loss: -5.180313587188721, valid_loss: -3.5357868671417236\n",
      "epoch: 378, train_loss: -5.2352519035339355, valid_loss: -3.6833271980285645\n",
      "epoch: 379, train_loss: -5.354254245758057, valid_loss: -3.6782426834106445\n",
      "epoch: 380, train_loss: -5.404898166656494, valid_loss: -3.6272470951080322\n",
      "epoch: 381, train_loss: -5.3575310707092285, valid_loss: -3.69022274017334\n",
      "epoch: 382, train_loss: -5.29986047744751, valid_loss: -3.5742692947387695\n",
      "epoch: 383, train_loss: -5.3252997398376465, valid_loss: -3.71614933013916\n",
      "epoch: 384, train_loss: -5.3970770835876465, valid_loss: -3.6916568279266357\n",
      "epoch: 385, train_loss: -5.439793586730957, valid_loss: -3.635725498199463\n",
      "epoch: 386, train_loss: -5.418795585632324, valid_loss: -3.7126429080963135\n",
      "epoch: 387, train_loss: -5.377564907073975, valid_loss: -3.5985140800476074\n",
      "epoch: 388, train_loss: -5.378388404846191, valid_loss: -3.703467845916748\n",
      "epoch: 389, train_loss: -5.412213325500488, valid_loss: -3.676955461502075\n",
      "epoch: 390, train_loss: -5.455504894256592, valid_loss: -3.6850011348724365\n",
      "epoch: 391, train_loss: -5.478250980377197, valid_loss: -3.716310739517212\n",
      "epoch: 392, train_loss: -5.474177360534668, valid_loss: -3.649195432662964\n",
      "epoch: 393, train_loss: -5.4523210525512695, valid_loss: -3.695618152618408\n",
      "epoch: 394, train_loss: -5.426832675933838, valid_loss: -3.607774257659912\n",
      "epoch: 395, train_loss: -5.415218830108643, valid_loss: -3.708202838897705\n",
      "epoch: 396, train_loss: -5.414129734039307, valid_loss: -3.5999741554260254\n",
      "epoch: 397, train_loss: -5.438065528869629, valid_loss: -3.728850841522217\n",
      "epoch: 398, train_loss: -5.462421894073486, valid_loss: -3.637348175048828\n",
      "epoch: 399, train_loss: -5.498157501220703, valid_loss: -3.715700387954712\n",
      "epoch: 400, train_loss: -5.523895263671875, valid_loss: -3.6885738372802734\n",
      "epoch: 401, train_loss: -5.540188789367676, valid_loss: -3.6839334964752197\n",
      "epoch: 402, train_loss: -5.548835277557373, valid_loss: -3.7119557857513428\n",
      "epoch: 403, train_loss: -5.555074691772461, valid_loss: -3.6772775650024414\n",
      "epoch: 404, train_loss: -5.560133934020996, valid_loss: -3.714850425720215\n",
      "epoch: 405, train_loss: -5.560340881347656, valid_loss: -3.6623904705047607\n",
      "epoch: 406, train_loss: -5.548037528991699, valid_loss: -3.699470281600952\n",
      "epoch: 407, train_loss: -5.501096725463867, valid_loss: -3.5321052074432373\n",
      "epoch: 408, train_loss: -5.374154567718506, valid_loss: -3.575774908065796\n",
      "epoch: 409, train_loss: -5.078680038452148, valid_loss: -3.285520076751709\n",
      "epoch: 410, train_loss: -4.972344398498535, valid_loss: -3.641669750213623\n",
      "epoch: 411, train_loss: -5.147663116455078, valid_loss: -3.6333227157592773\n",
      "epoch: 412, train_loss: -5.57181453704834, valid_loss: -3.545142412185669\n",
      "epoch: 413, train_loss: -5.425841331481934, valid_loss: -3.645566940307617\n",
      "epoch: 414, train_loss: -5.068408489227295, valid_loss: -3.61018705368042\n",
      "epoch: 415, train_loss: -5.521084308624268, valid_loss: -3.634894371032715\n",
      "epoch: 416, train_loss: -5.537551403045654, valid_loss: -3.722423791885376\n",
      "epoch: 417, train_loss: -5.250479221343994, valid_loss: -3.6785686016082764\n",
      "epoch: 418, train_loss: -5.541794776916504, valid_loss: -3.6932735443115234\n",
      "epoch: 419, train_loss: -5.576639652252197, valid_loss: -3.7609596252441406\n",
      "epoch: 420, train_loss: -5.391684055328369, valid_loss: -3.739285707473755\n",
      "epoch: 421, train_loss: -5.571389675140381, valid_loss: -3.7350001335144043\n",
      "epoch: 422, train_loss: -5.592294216156006, valid_loss: -3.7549171447753906\n",
      "epoch: 423, train_loss: -5.456691265106201, valid_loss: -3.7679696083068848\n",
      "epoch: 424, train_loss: -5.612060070037842, valid_loss: -3.752939224243164\n",
      "epoch: 425, train_loss: -5.592544078826904, valid_loss: -3.795567274093628\n",
      "epoch: 426, train_loss: -5.526405334472656, valid_loss: -3.7751882076263428\n",
      "epoch: 427, train_loss: -5.646753311157227, valid_loss: -3.7495861053466797\n",
      "epoch: 428, train_loss: -5.59940767288208, valid_loss: -3.8471672534942627\n",
      "epoch: 429, train_loss: -5.574838161468506, valid_loss: -3.8020870685577393\n",
      "epoch: 430, train_loss: -5.667233943939209, valid_loss: -3.7353718280792236\n",
      "epoch: 431, train_loss: -5.602884769439697, valid_loss: -3.8509774208068848\n",
      "epoch: 432, train_loss: -5.610475540161133, valid_loss: -3.835540294647217\n",
      "epoch: 433, train_loss: -5.682092666625977, valid_loss: -3.746204376220703\n",
      "epoch: 434, train_loss: -5.621452808380127, valid_loss: -3.8285791873931885\n",
      "epoch: 435, train_loss: -5.634975433349609, valid_loss: -3.8353238105773926\n",
      "epoch: 436, train_loss: -5.696657657623291, valid_loss: -3.766749143600464\n",
      "epoch: 437, train_loss: -5.650694847106934, valid_loss: -3.8267271518707275\n",
      "epoch: 438, train_loss: -5.652018070220947, valid_loss: -3.8101208209991455\n",
      "epoch: 439, train_loss: -5.713522911071777, valid_loss: -3.7768149375915527\n",
      "epoch: 440, train_loss: -5.684257984161377, valid_loss: -3.8383524417877197\n",
      "epoch: 441, train_loss: -5.66901159286499, valid_loss: -3.785963773727417\n",
      "epoch: 442, train_loss: -5.721297264099121, valid_loss: -3.7848525047302246\n",
      "epoch: 443, train_loss: -5.7227373123168945, valid_loss: -3.839024305343628\n",
      "epoch: 444, train_loss: -5.692050457000732, valid_loss: -3.7711641788482666\n",
      "epoch: 445, train_loss: -5.720194339752197, valid_loss: -3.801848888397217\n",
      "epoch: 446, train_loss: -5.749651908874512, valid_loss: -3.823605537414551\n",
      "epoch: 447, train_loss: -5.739236831665039, valid_loss: -3.7619197368621826\n",
      "epoch: 448, train_loss: -5.72622013092041, valid_loss: -3.8149967193603516\n",
      "epoch: 449, train_loss: -5.742878437042236, valid_loss: -3.7843029499053955\n",
      "epoch: 450, train_loss: -5.767961025238037, valid_loss: -3.783191204071045\n",
      "epoch: 451, train_loss: -5.770901679992676, valid_loss: -3.8106114864349365\n",
      "epoch: 452, train_loss: -5.759120464324951, valid_loss: -3.748339891433716\n",
      "epoch: 453, train_loss: -5.756825923919678, valid_loss: -3.803395986557007\n",
      "epoch: 454, train_loss: -5.768434524536133, valid_loss: -3.768746852874756\n",
      "epoch: 455, train_loss: -5.786316394805908, valid_loss: -3.7824208736419678\n",
      "epoch: 456, train_loss: -5.79831075668335, valid_loss: -3.7836503982543945\n",
      "epoch: 457, train_loss: -5.800711154937744, valid_loss: -3.7586162090301514\n",
      "epoch: 458, train_loss: -5.79678201675415, valid_loss: -3.7919058799743652\n",
      "epoch: 459, train_loss: -5.789798736572266, valid_loss: -3.7318973541259766\n",
      "epoch: 460, train_loss: -5.782681941986084, valid_loss: -3.7926506996154785\n",
      "epoch: 461, train_loss: -5.771270751953125, valid_loss: -3.709115743637085\n",
      "epoch: 462, train_loss: -5.760988235473633, valid_loss: -3.784186363220215\n",
      "epoch: 463, train_loss: -5.737955093383789, valid_loss: -3.674626588821411\n",
      "epoch: 464, train_loss: -5.719369411468506, valid_loss: -3.7783796787261963\n",
      "epoch: 465, train_loss: -5.682174205780029, valid_loss: -3.6478848457336426\n",
      "epoch: 466, train_loss: -5.684228420257568, valid_loss: -3.777681589126587\n",
      "epoch: 467, train_loss: -5.682520866394043, valid_loss: -3.6772282123565674\n",
      "epoch: 468, train_loss: -5.737398147583008, valid_loss: -3.7887587547302246\n",
      "epoch: 469, train_loss: -5.786934852600098, valid_loss: -3.7461540699005127\n",
      "epoch: 470, train_loss: -5.832713603973389, valid_loss: -3.78952956199646\n",
      "epoch: 471, train_loss: -5.855551242828369, valid_loss: -3.7897400856018066\n",
      "epoch: 472, train_loss: -5.858377456665039, valid_loss: -3.766874313354492\n",
      "epoch: 473, train_loss: -5.846569538116455, valid_loss: -3.812943458557129\n",
      "epoch: 474, train_loss: -5.82513427734375, valid_loss: -3.73905611038208\n",
      "epoch: 475, train_loss: -5.808713436126709, valid_loss: -3.813732624053955\n",
      "epoch: 476, train_loss: -5.795652866363525, valid_loss: -3.732976198196411\n",
      "epoch: 477, train_loss: -5.8064069747924805, valid_loss: -3.806286334991455\n",
      "epoch: 478, train_loss: -5.825364589691162, valid_loss: -3.759061813354492\n",
      "epoch: 479, train_loss: -5.853683948516846, valid_loss: -3.8071205615997314\n",
      "epoch: 480, train_loss: -5.876936912536621, valid_loss: -3.788341999053955\n",
      "epoch: 481, train_loss: -5.890229225158691, valid_loss: -3.7965593338012695\n",
      "epoch: 482, train_loss: -5.893731117248535, valid_loss: -3.8146395683288574\n",
      "epoch: 483, train_loss: -5.889681816101074, valid_loss: -3.778186798095703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 484, train_loss: -5.881196975708008, valid_loss: -3.8271291255950928\n",
      "epoch: 485, train_loss: -5.870893955230713, valid_loss: -3.7660021781921387\n",
      "epoch: 486, train_loss: -5.865947723388672, valid_loss: -3.820861577987671\n",
      "epoch: 487, train_loss: -5.860224723815918, valid_loss: -3.756373405456543\n",
      "epoch: 488, train_loss: -5.860083103179932, valid_loss: -3.8166165351867676\n",
      "epoch: 489, train_loss: -5.8617706298828125, valid_loss: -3.753720283508301\n",
      "epoch: 490, train_loss: -5.870106220245361, valid_loss: -3.822561502456665\n",
      "epoch: 491, train_loss: -5.8785271644592285, valid_loss: -3.763909339904785\n",
      "epoch: 492, train_loss: -5.893767356872559, valid_loss: -3.822500228881836\n",
      "epoch: 493, train_loss: -5.906896591186523, valid_loss: -3.785184621810913\n",
      "epoch: 494, train_loss: -5.920103073120117, valid_loss: -3.8189163208007812\n",
      "epoch: 495, train_loss: -5.929853916168213, valid_loss: -3.802964448928833\n",
      "epoch: 496, train_loss: -5.937432765960693, valid_loss: -3.820702314376831\n",
      "epoch: 497, train_loss: -5.943337917327881, valid_loss: -3.8104841709136963\n",
      "epoch: 498, train_loss: -5.947842121124268, valid_loss: -3.8199968338012695\n",
      "epoch: 499, train_loss: -5.9516682624816895, valid_loss: -3.811706304550171\n",
      "epoch: 500, train_loss: -5.955417633056641, valid_loss: -3.812854528427124\n",
      "epoch: 501, train_loss: -5.958979606628418, valid_loss: -3.809840202331543\n",
      "epoch: 502, train_loss: -5.962229251861572, valid_loss: -3.8054280281066895\n",
      "epoch: 503, train_loss: -5.9654130935668945, valid_loss: -3.806745767593384\n",
      "epoch: 504, train_loss: -5.968703269958496, valid_loss: -3.802384853363037\n",
      "epoch: 505, train_loss: -5.971957683563232, valid_loss: -3.8057799339294434\n",
      "epoch: 506, train_loss: -5.974983215332031, valid_loss: -3.799722909927368\n",
      "epoch: 507, train_loss: -5.977615833282471, valid_loss: -3.8086235523223877\n",
      "epoch: 508, train_loss: -5.979306221008301, valid_loss: -3.7894318103790283\n",
      "epoch: 509, train_loss: -5.978094577789307, valid_loss: -3.814791440963745\n",
      "epoch: 510, train_loss: -5.967841625213623, valid_loss: -3.7448573112487793\n",
      "epoch: 511, train_loss: -5.931256294250488, valid_loss: -3.800055980682373\n",
      "epoch: 512, train_loss: -5.812883377075195, valid_loss: -3.518660545349121\n",
      "epoch: 513, train_loss: -5.567285060882568, valid_loss: -3.645306348800659\n",
      "epoch: 514, train_loss: -5.188351154327393, valid_loss: -3.4340550899505615\n",
      "epoch: 515, train_loss: -5.524374961853027, valid_loss: -3.8666577339172363\n",
      "epoch: 516, train_loss: -5.799270153045654, valid_loss: -3.7352774143218994\n",
      "epoch: 517, train_loss: -5.887861728668213, valid_loss: -3.609692096710205\n",
      "epoch: 518, train_loss: -5.698214054107666, valid_loss: -3.7902824878692627\n",
      "epoch: 519, train_loss: -5.569458484649658, valid_loss: -3.698998212814331\n",
      "epoch: 520, train_loss: -5.771287441253662, valid_loss: -3.8522703647613525\n",
      "epoch: 521, train_loss: -5.9133992195129395, valid_loss: -3.9568231105804443\n",
      "epoch: 522, train_loss: -5.890372276306152, valid_loss: -3.759915351867676\n",
      "epoch: 523, train_loss: -5.8371148109436035, valid_loss: -3.8668432235717773\n",
      "epoch: 524, train_loss: -5.9178595542907715, valid_loss: -3.9389853477478027\n",
      "epoch: 525, train_loss: -5.990067481994629, valid_loss: -3.8968124389648438\n",
      "epoch: 526, train_loss: -5.927966594696045, valid_loss: -3.9365384578704834\n",
      "epoch: 527, train_loss: -5.909233093261719, valid_loss: -3.945739507675171\n",
      "epoch: 528, train_loss: -5.9849772453308105, valid_loss: -3.9331040382385254\n",
      "epoch: 529, train_loss: -5.994439601898193, valid_loss: -3.9585673809051514\n",
      "epoch: 530, train_loss: -5.952709674835205, valid_loss: -3.9047932624816895\n",
      "epoch: 531, train_loss: -5.976535320281982, valid_loss: -3.925116777420044\n",
      "epoch: 532, train_loss: -5.997786998748779, valid_loss: -3.9825544357299805\n",
      "epoch: 533, train_loss: -5.989682674407959, valid_loss: -3.947730541229248\n",
      "epoch: 534, train_loss: -5.99730920791626, valid_loss: -3.956603765487671\n",
      "epoch: 535, train_loss: -6.00191593170166, valid_loss: -3.993182897567749\n",
      "epoch: 536, train_loss: -6.001877307891846, valid_loss: -3.9550769329071045\n",
      "epoch: 537, train_loss: -6.017226219177246, valid_loss: -3.965719699859619\n",
      "epoch: 538, train_loss: -6.022366523742676, valid_loss: -3.971386671066284\n",
      "epoch: 539, train_loss: -6.013702392578125, valid_loss: -3.938549757003784\n",
      "epoch: 540, train_loss: -6.022428512573242, valid_loss: -3.971168041229248\n",
      "epoch: 541, train_loss: -6.03884220123291, valid_loss: -3.9643802642822266\n",
      "epoch: 542, train_loss: -6.036986351013184, valid_loss: -3.941261053085327\n",
      "epoch: 543, train_loss: -6.031508922576904, valid_loss: -3.9717178344726562\n",
      "epoch: 544, train_loss: -6.041954517364502, valid_loss: -3.9483654499053955\n",
      "epoch: 545, train_loss: -6.053579807281494, valid_loss: -3.9334123134613037\n",
      "epoch: 546, train_loss: -6.051150798797607, valid_loss: -3.9498705863952637\n",
      "epoch: 547, train_loss: -6.050008296966553, valid_loss: -3.9226465225219727\n",
      "epoch: 548, train_loss: -6.059994220733643, valid_loss: -3.937804698944092\n",
      "epoch: 549, train_loss: -6.067132949829102, valid_loss: -3.9361486434936523\n",
      "epoch: 550, train_loss: -6.06675386428833, valid_loss: -3.9198784828186035\n",
      "epoch: 551, train_loss: -6.068403244018555, valid_loss: -3.9400084018707275\n",
      "epoch: 552, train_loss: -6.073762893676758, valid_loss: -3.9140186309814453\n",
      "epoch: 553, train_loss: -6.078199863433838, valid_loss: -3.9159741401672363\n",
      "epoch: 554, train_loss: -6.082122802734375, valid_loss: -3.9194130897521973\n",
      "epoch: 555, train_loss: -6.086438179016113, valid_loss: -3.8987464904785156\n",
      "epoch: 556, train_loss: -6.087825775146484, valid_loss: -3.9113399982452393\n",
      "epoch: 557, train_loss: -6.088696002960205, valid_loss: -3.8909952640533447\n",
      "epoch: 558, train_loss: -6.0943803787231445, valid_loss: -3.896070718765259\n",
      "epoch: 559, train_loss: -6.100624084472656, valid_loss: -3.89070987701416\n",
      "epoch: 560, train_loss: -6.102764129638672, valid_loss: -3.8805289268493652\n",
      "epoch: 561, train_loss: -6.104238986968994, valid_loss: -3.8955557346343994\n",
      "epoch: 562, train_loss: -6.106880187988281, valid_loss: -3.8757781982421875\n",
      "epoch: 563, train_loss: -6.109528064727783, valid_loss: -3.8860597610473633\n",
      "epoch: 564, train_loss: -6.113041400909424, valid_loss: -3.8695061206817627\n",
      "epoch: 565, train_loss: -6.116888999938965, valid_loss: -3.8717563152313232\n",
      "epoch: 566, train_loss: -6.120113849639893, valid_loss: -3.8699586391448975\n",
      "epoch: 567, train_loss: -6.1236090660095215, valid_loss: -3.86736798286438\n",
      "epoch: 568, train_loss: -6.125908374786377, valid_loss: -3.872450351715088\n",
      "epoch: 569, train_loss: -6.127687931060791, valid_loss: -3.856780529022217\n",
      "epoch: 570, train_loss: -6.130640983581543, valid_loss: -3.8625943660736084\n",
      "epoch: 571, train_loss: -6.132676601409912, valid_loss: -3.846240997314453\n",
      "epoch: 572, train_loss: -6.134800434112549, valid_loss: -3.8600218296051025\n",
      "epoch: 573, train_loss: -6.137541770935059, valid_loss: -3.844668388366699\n",
      "epoch: 574, train_loss: -6.1398186683654785, valid_loss: -3.8596410751342773\n",
      "epoch: 575, train_loss: -6.142337322235107, valid_loss: -3.8392505645751953\n",
      "epoch: 576, train_loss: -6.144541263580322, valid_loss: -3.8552756309509277\n",
      "epoch: 577, train_loss: -6.146769046783447, valid_loss: -3.8353512287139893\n",
      "epoch: 578, train_loss: -6.1489338874816895, valid_loss: -3.8549089431762695\n",
      "epoch: 579, train_loss: -6.149974822998047, valid_loss: -3.827932834625244\n",
      "epoch: 580, train_loss: -6.150893688201904, valid_loss: -3.854935646057129\n",
      "epoch: 581, train_loss: -6.149758815765381, valid_loss: -3.815516710281372\n",
      "epoch: 582, train_loss: -6.147000312805176, valid_loss: -3.8626081943511963\n",
      "epoch: 583, train_loss: -6.139247894287109, valid_loss: -3.7952234745025635\n",
      "epoch: 584, train_loss: -6.1293745040893555, valid_loss: -3.864652395248413\n",
      "epoch: 585, train_loss: -6.108336925506592, valid_loss: -3.7674977779388428\n",
      "epoch: 586, train_loss: -6.0905656814575195, valid_loss: -3.869492292404175\n",
      "epoch: 587, train_loss: -6.058310508728027, valid_loss: -3.7488300800323486\n",
      "epoch: 588, train_loss: -6.064381122589111, valid_loss: -3.8770382404327393\n",
      "epoch: 589, train_loss: -6.071461200714111, valid_loss: -3.783853530883789\n",
      "epoch: 590, train_loss: -6.122376441955566, valid_loss: -3.8717575073242188\n",
      "epoch: 591, train_loss: -6.160895824432373, valid_loss: -3.8464038372039795\n",
      "epoch: 592, train_loss: -6.183872699737549, valid_loss: -3.8472509384155273\n",
      "epoch: 593, train_loss: -6.183628082275391, valid_loss: -3.8840441703796387\n",
      "epoch: 594, train_loss: -6.1674418449401855, valid_loss: -3.8256185054779053\n",
      "epoch: 595, train_loss: -6.151435375213623, valid_loss: -3.9026010036468506\n",
      "epoch: 596, train_loss: -6.142285346984863, valid_loss: -3.833139657974243\n",
      "epoch: 597, train_loss: -6.1589789390563965, valid_loss: -3.8990910053253174\n",
      "epoch: 598, train_loss: -6.180466175079346, valid_loss: -3.8687987327575684\n",
      "epoch: 599, train_loss: -6.19850492477417, valid_loss: -3.8701391220092773\n",
      "epoch: 600, train_loss: -6.201864242553711, valid_loss: -3.890268564224243\n",
      "epoch: 601, train_loss: -6.194000720977783, valid_loss: -3.8436596393585205\n",
      "epoch: 602, train_loss: -6.186398029327393, valid_loss: -3.89870285987854\n",
      "epoch: 603, train_loss: -6.185113906860352, valid_loss: -3.8483097553253174\n",
      "epoch: 604, train_loss: -6.1960930824279785, valid_loss: -3.8901922702789307\n",
      "epoch: 605, train_loss: -6.208530426025391, valid_loss: -3.8754265308380127\n",
      "epoch: 606, train_loss: -6.216409206390381, valid_loss: -3.8673369884490967\n",
      "epoch: 607, train_loss: -6.2163825035095215, valid_loss: -3.8902411460876465\n",
      "epoch: 608, train_loss: -6.211771488189697, valid_loss: -3.8507349491119385\n",
      "epoch: 609, train_loss: -6.208741188049316, valid_loss: -3.8942534923553467\n",
      "epoch: 610, train_loss: -6.208813667297363, valid_loss: -3.8484251499176025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 611, train_loss: -6.214781761169434, valid_loss: -3.8857710361480713\n",
      "epoch: 612, train_loss: -6.222314834594727, valid_loss: -3.857339382171631\n",
      "epoch: 613, train_loss: -6.229948043823242, valid_loss: -3.865389585494995\n",
      "epoch: 614, train_loss: -6.234585762023926, valid_loss: -3.8648762702941895\n",
      "epoch: 615, train_loss: -6.236091136932373, valid_loss: -3.846390724182129\n",
      "epoch: 616, train_loss: -6.23577880859375, valid_loss: -3.8704559803009033\n",
      "epoch: 617, train_loss: -6.234642028808594, valid_loss: -3.8337340354919434\n",
      "epoch: 618, train_loss: -6.234067440032959, valid_loss: -3.8723344802856445\n",
      "epoch: 619, train_loss: -6.23380708694458, valid_loss: -3.8267617225646973\n",
      "epoch: 620, train_loss: -6.235685348510742, valid_loss: -3.8677287101745605\n",
      "epoch: 621, train_loss: -6.2373480796813965, valid_loss: -3.8226306438446045\n",
      "epoch: 622, train_loss: -6.240122318267822, valid_loss: -3.8627548217773438\n",
      "epoch: 623, train_loss: -6.241801738739014, valid_loss: -3.817192554473877\n",
      "epoch: 624, train_loss: -6.2440900802612305, valid_loss: -3.8609166145324707\n",
      "epoch: 625, train_loss: -6.2448272705078125, valid_loss: -3.8101859092712402\n",
      "epoch: 626, train_loss: -6.246194839477539, valid_loss: -3.8590986728668213\n",
      "epoch: 627, train_loss: -6.24567985534668, valid_loss: -3.801835536956787\n",
      "epoch: 628, train_loss: -6.245789051055908, valid_loss: -3.858476161956787\n",
      "epoch: 629, train_loss: -6.242405414581299, valid_loss: -3.7903077602386475\n",
      "epoch: 630, train_loss: -6.239622592926025, valid_loss: -3.863097906112671\n",
      "epoch: 631, train_loss: -6.230976104736328, valid_loss: -3.774681329727173\n",
      "epoch: 632, train_loss: -6.225649833679199, valid_loss: -3.8689444065093994\n",
      "epoch: 633, train_loss: -6.212930679321289, valid_loss: -3.761549711227417\n",
      "epoch: 634, train_loss: -6.212597846984863, valid_loss: -3.870717763900757\n",
      "epoch: 635, train_loss: -6.2072954177856445, valid_loss: -3.7653117179870605\n",
      "epoch: 636, train_loss: -6.221150875091553, valid_loss: -3.8703155517578125\n",
      "epoch: 637, train_loss: -6.232801914215088, valid_loss: -3.792370319366455\n",
      "epoch: 638, train_loss: -6.255309581756592, valid_loss: -3.8640103340148926\n",
      "epoch: 639, train_loss: -6.272830963134766, valid_loss: -3.829725742340088\n",
      "epoch: 640, train_loss: -6.286554336547852, valid_loss: -3.849276304244995\n",
      "epoch: 641, train_loss: -6.292917251586914, valid_loss: -3.8593497276306152\n",
      "epoch: 642, train_loss: -6.292934417724609, valid_loss: -3.8323426246643066\n",
      "epoch: 643, train_loss: -6.288943767547607, valid_loss: -3.8754525184631348\n",
      "epoch: 644, train_loss: -6.283084869384766, valid_loss: -3.8179802894592285\n",
      "epoch: 645, train_loss: -6.279660701751709, valid_loss: -3.879479169845581\n",
      "epoch: 646, train_loss: -6.277830600738525, valid_loss: -3.812654972076416\n",
      "epoch: 647, train_loss: -6.282384395599365, valid_loss: -3.8728249073028564\n",
      "epoch: 648, train_loss: -6.288626194000244, valid_loss: -3.8200652599334717\n",
      "epoch: 649, train_loss: -6.297828197479248, valid_loss: -3.8596384525299072\n",
      "epoch: 650, train_loss: -6.305858612060547, valid_loss: -3.8350307941436768\n",
      "epoch: 651, train_loss: -6.312168598175049, valid_loss: -3.8450140953063965\n",
      "epoch: 652, train_loss: -6.3159027099609375, valid_loss: -3.8497769832611084\n",
      "epoch: 653, train_loss: -6.317372798919678, valid_loss: -3.8318231105804443\n",
      "epoch: 654, train_loss: -6.317285537719727, valid_loss: -3.8589258193969727\n",
      "epoch: 655, train_loss: -6.316254138946533, valid_loss: -3.8199143409729004\n",
      "epoch: 656, train_loss: -6.315064907073975, valid_loss: -3.8622801303863525\n",
      "epoch: 657, train_loss: -6.313285827636719, valid_loss: -3.8070452213287354\n",
      "epoch: 658, train_loss: -6.311952114105225, valid_loss: -3.8626582622528076\n",
      "epoch: 659, train_loss: -6.309422016143799, valid_loss: -3.792389392852783\n",
      "epoch: 660, train_loss: -6.307740688323975, valid_loss: -3.8620975017547607\n",
      "epoch: 661, train_loss: -6.304222106933594, valid_loss: -3.7784504890441895\n",
      "epoch: 662, train_loss: -6.302357196807861, valid_loss: -3.8626413345336914\n",
      "epoch: 663, train_loss: -6.298506259918213, valid_loss: -3.769603729248047\n",
      "epoch: 664, train_loss: -6.297755718231201, valid_loss: -3.8658900260925293\n",
      "epoch: 665, train_loss: -6.295499324798584, valid_loss: -3.7697994709014893\n",
      "epoch: 666, train_loss: -6.298497676849365, valid_loss: -3.870025873184204\n",
      "epoch: 667, train_loss: -6.300802707672119, valid_loss: -3.7807528972625732\n",
      "epoch: 668, train_loss: -6.3092498779296875, valid_loss: -3.871443271636963\n",
      "epoch: 669, train_loss: -6.317192554473877, valid_loss: -3.8001744747161865\n",
      "epoch: 670, train_loss: -6.328653335571289, valid_loss: -3.8682808876037598\n",
      "epoch: 671, train_loss: -6.3387064933776855, valid_loss: -3.8223066329956055\n",
      "epoch: 672, train_loss: -6.348383903503418, valid_loss: -3.8602652549743652\n",
      "epoch: 673, train_loss: -6.35574197769165, valid_loss: -3.8415117263793945\n",
      "epoch: 674, train_loss: -6.360899448394775, valid_loss: -3.848665952682495\n",
      "epoch: 675, train_loss: -6.3638834953308105, valid_loss: -3.8547840118408203\n",
      "epoch: 676, train_loss: -6.365152359008789, valid_loss: -3.8359689712524414\n",
      "epoch: 677, train_loss: -6.365276336669922, valid_loss: -3.8622024059295654\n",
      "epoch: 678, train_loss: -6.364692687988281, valid_loss: -3.8238494396209717\n",
      "epoch: 679, train_loss: -6.363877296447754, valid_loss: -3.865751028060913\n",
      "epoch: 680, train_loss: -6.362756729125977, valid_loss: -3.812537431716919\n",
      "epoch: 681, train_loss: -6.3617167472839355, valid_loss: -3.867577075958252\n",
      "epoch: 682, train_loss: -6.360138416290283, valid_loss: -3.8013529777526855\n",
      "epoch: 683, train_loss: -6.358528137207031, valid_loss: -3.869525909423828\n",
      "epoch: 684, train_loss: -6.3558173179626465, valid_loss: -3.7896621227264404\n",
      "epoch: 685, train_loss: -6.352925777435303, valid_loss: -3.872842311859131\n",
      "epoch: 686, train_loss: -6.348515510559082, valid_loss: -3.7783496379852295\n",
      "epoch: 687, train_loss: -6.344577312469482, valid_loss: -3.877614736557007\n",
      "epoch: 688, train_loss: -6.339751243591309, valid_loss: -3.7721338272094727\n",
      "epoch: 689, train_loss: -6.338192939758301, valid_loss: -3.882847785949707\n",
      "epoch: 690, train_loss: -6.337957859039307, valid_loss: -3.7793710231781006\n",
      "epoch: 691, train_loss: -6.344862937927246, valid_loss: -3.8859972953796387\n",
      "epoch: 692, train_loss: -6.354242324829102, valid_loss: -3.8036882877349854\n",
      "epoch: 693, train_loss: -6.369266986846924, valid_loss: -3.8828721046447754\n",
      "epoch: 694, train_loss: -6.383491516113281, valid_loss: -3.836188793182373\n",
      "epoch: 695, train_loss: -6.396391868591309, valid_loss: -3.8722689151763916\n",
      "epoch: 696, train_loss: -6.405014991760254, valid_loss: -3.8641908168792725\n",
      "epoch: 697, train_loss: -6.409287929534912, valid_loss: -3.8575286865234375\n",
      "epoch: 698, train_loss: -6.409857749938965, valid_loss: -3.881972551345825\n",
      "epoch: 699, train_loss: -6.408008575439453, valid_loss: -3.842393636703491\n",
      "epoch: 700, train_loss: -6.405308723449707, valid_loss: -3.8895785808563232\n",
      "epoch: 701, train_loss: -6.402658462524414, valid_loss: -3.8304829597473145\n",
      "epoch: 702, train_loss: -6.401481628417969, valid_loss: -3.890092372894287\n",
      "epoch: 703, train_loss: -6.401560306549072, valid_loss: -3.825058698654175\n",
      "epoch: 704, train_loss: -6.403629302978516, valid_loss: -3.8873839378356934\n",
      "epoch: 705, train_loss: -6.406832695007324, valid_loss: -3.8268024921417236\n",
      "epoch: 706, train_loss: -6.4112629890441895, valid_loss: -3.883585214614868\n",
      "epoch: 707, train_loss: -6.415981292724609, valid_loss: -3.8331403732299805\n",
      "epoch: 708, train_loss: -6.420871257781982, valid_loss: -3.879664421081543\n",
      "epoch: 709, train_loss: -6.425370693206787, valid_loss: -3.8401358127593994\n",
      "epoch: 710, train_loss: -6.429424285888672, valid_loss: -3.876565456390381\n",
      "epoch: 711, train_loss: -6.432931423187256, valid_loss: -3.844571828842163\n",
      "epoch: 712, train_loss: -6.435924053192139, valid_loss: -3.8746495246887207\n",
      "epoch: 713, train_loss: -6.438465595245361, valid_loss: -3.844557046890259\n",
      "epoch: 714, train_loss: -6.440530776977539, valid_loss: -3.8740551471710205\n",
      "epoch: 715, train_loss: -6.442066669464111, valid_loss: -3.838855743408203\n",
      "epoch: 716, train_loss: -6.442775726318359, valid_loss: -3.8757874965667725\n",
      "epoch: 717, train_loss: -6.442266941070557, valid_loss: -3.825028896331787\n",
      "epoch: 718, train_loss: -6.4392924308776855, valid_loss: -3.881037950515747\n",
      "epoch: 719, train_loss: -6.432615756988525, valid_loss: -3.7957446575164795\n",
      "epoch: 720, train_loss: -6.417768955230713, valid_loss: -3.888324737548828\n",
      "epoch: 721, train_loss: -6.393971920013428, valid_loss: -3.7427992820739746\n",
      "epoch: 722, train_loss: -6.352019309997559, valid_loss: -3.899744987487793\n",
      "epoch: 723, train_loss: -6.310258865356445, valid_loss: -3.7132322788238525\n",
      "epoch: 724, train_loss: -6.284899711608887, valid_loss: -3.9262661933898926\n",
      "epoch: 725, train_loss: -6.295699119567871, valid_loss: -3.791400194168091\n",
      "epoch: 726, train_loss: -6.377983570098877, valid_loss: -3.9199700355529785\n",
      "epoch: 727, train_loss: -6.445055961608887, valid_loss: -3.8972294330596924\n",
      "epoch: 728, train_loss: -6.472348690032959, valid_loss: -3.868264675140381\n",
      "epoch: 729, train_loss: -6.450634956359863, valid_loss: -3.9525651931762695\n",
      "epoch: 730, train_loss: -6.4101667404174805, valid_loss: -3.8528287410736084\n",
      "epoch: 731, train_loss: -6.40686559677124, valid_loss: -3.9676547050476074\n",
      "epoch: 732, train_loss: -6.434412956237793, valid_loss: -3.9164483547210693\n",
      "epoch: 733, train_loss: -6.472809791564941, valid_loss: -3.933152914047241\n",
      "epoch: 734, train_loss: -6.481717109680176, valid_loss: -3.9678661823272705\n",
      "epoch: 735, train_loss: -6.462140083312988, valid_loss: -3.8954811096191406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 736, train_loss: -6.446999549865723, valid_loss: -3.9730939865112305\n",
      "epoch: 737, train_loss: -6.455977916717529, valid_loss: -3.9233028888702393\n",
      "epoch: 738, train_loss: -6.481416702270508, valid_loss: -3.9457547664642334\n",
      "epoch: 739, train_loss: -6.493607997894287, valid_loss: -3.964388608932495\n",
      "epoch: 740, train_loss: -6.486202239990234, valid_loss: -3.9068610668182373\n",
      "epoch: 741, train_loss: -6.4761176109313965, valid_loss: -3.9663889408111572\n",
      "epoch: 742, train_loss: -6.478243827819824, valid_loss: -3.9173145294189453\n",
      "epoch: 743, train_loss: -6.4926934242248535, valid_loss: -3.9449825286865234\n",
      "epoch: 744, train_loss: -6.503853797912598, valid_loss: -3.950098752975464\n",
      "epoch: 745, train_loss: -6.504318714141846, valid_loss: -3.914897918701172\n",
      "epoch: 746, train_loss: -6.49886417388916, valid_loss: -3.9591550827026367\n",
      "epoch: 747, train_loss: -6.49617862701416, valid_loss: -3.906634569168091\n",
      "epoch: 748, train_loss: -6.500295162200928, valid_loss: -3.946800947189331\n",
      "epoch: 749, train_loss: -6.508992671966553, valid_loss: -3.9236438274383545\n",
      "epoch: 750, train_loss: -6.516430377960205, valid_loss: -3.926877498626709\n",
      "epoch: 751, train_loss: -6.519798278808594, valid_loss: -3.9375669956207275\n",
      "epoch: 752, train_loss: -6.519251346588135, valid_loss: -3.904884099960327\n",
      "epoch: 753, train_loss: -6.516993045806885, valid_loss: -3.939687967300415\n",
      "epoch: 754, train_loss: -6.5157551765441895, valid_loss: -3.892127752304077\n",
      "epoch: 755, train_loss: -6.515668869018555, valid_loss: -3.936002016067505\n",
      "epoch: 756, train_loss: -6.518066883087158, valid_loss: -3.8900463581085205\n",
      "epoch: 757, train_loss: -6.520908832550049, valid_loss: -3.9322030544281006\n",
      "epoch: 758, train_loss: -6.524989604949951, valid_loss: -3.8926994800567627\n",
      "epoch: 759, train_loss: -6.528581619262695, valid_loss: -3.9284310340881348\n",
      "epoch: 760, train_loss: -6.532336711883545, valid_loss: -3.8947129249572754\n",
      "epoch: 761, train_loss: -6.535372734069824, valid_loss: -3.924590587615967\n",
      "epoch: 762, train_loss: -6.538200378417969, valid_loss: -3.893824338912964\n",
      "epoch: 763, train_loss: -6.540228843688965, valid_loss: -3.9234492778778076\n",
      "epoch: 764, train_loss: -6.541850566864014, valid_loss: -3.889199733734131\n",
      "epoch: 765, train_loss: -6.542255878448486, valid_loss: -3.926030397415161\n",
      "epoch: 766, train_loss: -6.5415754318237305, valid_loss: -3.877368450164795\n",
      "epoch: 767, train_loss: -6.537806510925293, valid_loss: -3.9302754402160645\n",
      "epoch: 768, train_loss: -6.531193256378174, valid_loss: -3.852254629135132\n",
      "epoch: 769, train_loss: -6.516303062438965, valid_loss: -3.934283494949341\n",
      "epoch: 770, train_loss: -6.498894691467285, valid_loss: -3.820391893386841\n",
      "epoch: 771, train_loss: -6.470495700836182, valid_loss: -3.947601318359375\n",
      "epoch: 772, train_loss: -6.455310344696045, valid_loss: -3.8323473930358887\n",
      "epoch: 773, train_loss: -6.454978942871094, valid_loss: -3.970022439956665\n",
      "epoch: 774, train_loss: -6.473424911499023, valid_loss: -3.8963000774383545\n",
      "epoch: 775, train_loss: -6.521623611450195, valid_loss: -3.9667885303497314\n",
      "epoch: 776, train_loss: -6.5589494705200195, valid_loss: -3.9586563110351562\n",
      "epoch: 777, train_loss: -6.574120044708252, valid_loss: -3.948143482208252\n",
      "epoch: 778, train_loss: -6.564120292663574, valid_loss: -3.994821310043335\n",
      "epoch: 779, train_loss: -6.543325901031494, valid_loss: -3.9365832805633545\n",
      "epoch: 780, train_loss: -6.5360002517700195, valid_loss: -4.007494926452637\n",
      "epoch: 781, train_loss: -6.544226169586182, valid_loss: -3.962440252304077\n",
      "epoch: 782, train_loss: -6.566472053527832, valid_loss: -3.9999518394470215\n",
      "epoch: 783, train_loss: -6.58394718170166, valid_loss: -4.000973701477051\n",
      "epoch: 784, train_loss: -6.5865983963012695, valid_loss: -3.9761219024658203\n",
      "epoch: 785, train_loss: -6.578029632568359, valid_loss: -4.013471603393555\n",
      "epoch: 786, train_loss: -6.569978713989258, valid_loss: -3.9676835536956787\n",
      "epoch: 787, train_loss: -6.571900367736816, valid_loss: -4.013104438781738\n",
      "epoch: 788, train_loss: -6.582214832305908, valid_loss: -3.9859068393707275\n",
      "epoch: 789, train_loss: -6.594743251800537, valid_loss: -3.997786521911621\n",
      "epoch: 790, train_loss: -6.601240158081055, valid_loss: -4.004045486450195\n",
      "epoch: 791, train_loss: -6.600400924682617, valid_loss: -3.979856252670288\n",
      "epoch: 792, train_loss: -6.596495628356934, valid_loss: -4.0109357833862305\n",
      "epoch: 793, train_loss: -6.593832015991211, valid_loss: -3.9687368869781494\n",
      "epoch: 794, train_loss: -6.5942816734313965, valid_loss: -4.01030969619751\n",
      "epoch: 795, train_loss: -6.597980499267578, valid_loss: -3.9722542762756348\n",
      "epoch: 796, train_loss: -6.6025896072387695, valid_loss: -4.00745153427124\n",
      "epoch: 797, train_loss: -6.608835697174072, valid_loss: -3.9840023517608643\n",
      "epoch: 798, train_loss: -6.614306926727295, valid_loss: -4.004603862762451\n",
      "epoch: 799, train_loss: -6.619044780731201, valid_loss: -3.993365526199341\n",
      "epoch: 800, train_loss: -6.622592449188232, valid_loss: -4.000598430633545\n",
      "epoch: 801, train_loss: -6.625362873077393, valid_loss: -3.998544454574585\n",
      "epoch: 802, train_loss: -6.627790451049805, valid_loss: -3.996081829071045\n",
      "epoch: 803, train_loss: -6.629941940307617, valid_loss: -4.0023274421691895\n",
      "epoch: 804, train_loss: -6.631616115570068, valid_loss: -3.9898974895477295\n",
      "epoch: 805, train_loss: -6.632529258728027, valid_loss: -4.006360054016113\n",
      "epoch: 806, train_loss: -6.631897926330566, valid_loss: -3.972346544265747\n",
      "epoch: 807, train_loss: -6.626649856567383, valid_loss: -4.007462978363037\n",
      "epoch: 808, train_loss: -6.610007286071777, valid_loss: -3.9018726348876953\n",
      "epoch: 809, train_loss: -6.5603461265563965, valid_loss: -3.9788942337036133\n",
      "epoch: 810, train_loss: -6.495400905609131, valid_loss: -3.809455394744873\n",
      "epoch: 811, train_loss: -6.407575607299805, valid_loss: -4.017259120941162\n",
      "epoch: 812, train_loss: -6.43581485748291, valid_loss: -3.967146396636963\n",
      "epoch: 813, train_loss: -6.503477573394775, valid_loss: -4.035554885864258\n",
      "epoch: 814, train_loss: -6.595036029815674, valid_loss: -4.0481743812561035\n",
      "epoch: 815, train_loss: -6.653010845184326, valid_loss: -4.038458824157715\n",
      "epoch: 816, train_loss: -6.599856853485107, valid_loss: -4.0737223625183105\n",
      "epoch: 817, train_loss: -6.543237209320068, valid_loss: -4.031803131103516\n",
      "epoch: 818, train_loss: -6.57505989074707, valid_loss: -4.102850437164307\n",
      "epoch: 819, train_loss: -6.622128009796143, valid_loss: -4.099466323852539\n",
      "epoch: 820, train_loss: -6.659669876098633, valid_loss: -4.086156368255615\n",
      "epoch: 821, train_loss: -6.64027738571167, valid_loss: -4.12489128112793\n",
      "epoch: 822, train_loss: -6.61067008972168, valid_loss: -4.099280834197998\n",
      "epoch: 823, train_loss: -6.626666069030762, valid_loss: -4.129876136779785\n",
      "epoch: 824, train_loss: -6.653926372528076, valid_loss: -4.133797645568848\n",
      "epoch: 825, train_loss: -6.6693854331970215, valid_loss: -4.116262435913086\n",
      "epoch: 826, train_loss: -6.655027389526367, valid_loss: -4.1398468017578125\n",
      "epoch: 827, train_loss: -6.640970230102539, valid_loss: -4.116048812866211\n",
      "epoch: 828, train_loss: -6.6619873046875, valid_loss: -4.13557243347168\n",
      "epoch: 829, train_loss: -6.677505970001221, valid_loss: -4.132092475891113\n",
      "epoch: 830, train_loss: -6.676212310791016, valid_loss: -4.113615036010742\n",
      "epoch: 831, train_loss: -6.66970157623291, valid_loss: -4.1377763748168945\n",
      "epoch: 832, train_loss: -6.668959617614746, valid_loss: -4.1131744384765625\n",
      "epoch: 833, train_loss: -6.68192720413208, valid_loss: -4.128417491912842\n",
      "epoch: 834, train_loss: -6.692479133605957, valid_loss: -4.135304927825928\n",
      "epoch: 835, train_loss: -6.68831205368042, valid_loss: -4.108118534088135\n",
      "epoch: 836, train_loss: -6.683104038238525, valid_loss: -4.1356587409973145\n",
      "epoch: 837, train_loss: -6.687607288360596, valid_loss: -4.1215434074401855\n",
      "epoch: 838, train_loss: -6.697179317474365, valid_loss: -4.125339508056641\n",
      "epoch: 839, train_loss: -6.70381498336792, valid_loss: -4.131192207336426\n",
      "epoch: 840, train_loss: -6.703729152679443, valid_loss: -4.112955570220947\n",
      "epoch: 841, train_loss: -6.7006120681762695, valid_loss: -4.131071090698242\n",
      "epoch: 842, train_loss: -6.701282024383545, valid_loss: -4.1108012199401855\n",
      "epoch: 843, train_loss: -6.707189559936523, valid_loss: -4.126436710357666\n",
      "epoch: 844, train_loss: -6.712817668914795, valid_loss: -4.113253116607666\n",
      "epoch: 845, train_loss: -6.716114521026611, valid_loss: -4.118194103240967\n",
      "epoch: 846, train_loss: -6.718958377838135, valid_loss: -4.117006301879883\n",
      "epoch: 847, train_loss: -6.722156524658203, valid_loss: -4.108893394470215\n",
      "epoch: 848, train_loss: -6.724179267883301, valid_loss: -4.118077754974365\n",
      "epoch: 849, train_loss: -6.724046230316162, valid_loss: -4.09595251083374\n",
      "epoch: 850, train_loss: -6.722121715545654, valid_loss: -4.118099689483643\n",
      "epoch: 851, train_loss: -6.718425273895264, valid_loss: -4.074214935302734\n",
      "epoch: 852, train_loss: -6.709055423736572, valid_loss: -4.115798473358154\n",
      "epoch: 853, train_loss: -6.692346096038818, valid_loss: -4.029514789581299\n",
      "epoch: 854, train_loss: -6.657541751861572, valid_loss: -4.106639385223389\n",
      "epoch: 855, train_loss: -6.631990432739258, valid_loss: -4.01019811630249\n",
      "epoch: 856, train_loss: -6.6110968589782715, valid_loss: -4.127188205718994\n",
      "epoch: 857, train_loss: -6.645492076873779, valid_loss: -4.103438854217529\n",
      "epoch: 858, train_loss: -6.6990647315979, valid_loss: -4.146957874298096\n",
      "epoch: 859, train_loss: -6.73444938659668, valid_loss: -4.1594672203063965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 860, train_loss: -6.7454657554626465, valid_loss: -4.143033027648926\n",
      "epoch: 861, train_loss: -6.729855060577393, valid_loss: -4.172906398773193\n",
      "epoch: 862, train_loss: -6.701876640319824, valid_loss: -4.153506278991699\n",
      "epoch: 863, train_loss: -6.707614898681641, valid_loss: -4.19211483001709\n",
      "epoch: 864, train_loss: -6.733476638793945, valid_loss: -4.182157039642334\n",
      "epoch: 865, train_loss: -6.755343914031982, valid_loss: -4.195656776428223\n",
      "epoch: 866, train_loss: -6.758683204650879, valid_loss: -4.206991672515869\n",
      "epoch: 867, train_loss: -6.747570514678955, valid_loss: -4.179599761962891\n",
      "epoch: 868, train_loss: -6.737016677856445, valid_loss: -4.212227821350098\n",
      "epoch: 869, train_loss: -6.74292516708374, valid_loss: -4.197847843170166\n",
      "epoch: 870, train_loss: -6.7610626220703125, valid_loss: -4.210281848907471\n",
      "epoch: 871, train_loss: -6.772282600402832, valid_loss: -4.215466022491455\n",
      "epoch: 872, train_loss: -6.772737979888916, valid_loss: -4.199642658233643\n",
      "epoch: 873, train_loss: -6.766955375671387, valid_loss: -4.216155052185059\n",
      "epoch: 874, train_loss: -6.762715816497803, valid_loss: -4.201339244842529\n",
      "epoch: 875, train_loss: -6.769383907318115, valid_loss: -4.217458248138428\n",
      "epoch: 876, train_loss: -6.780111789703369, valid_loss: -4.209851264953613\n",
      "epoch: 877, train_loss: -6.786942481994629, valid_loss: -4.213196277618408\n",
      "epoch: 878, train_loss: -6.788064956665039, valid_loss: -4.219179630279541\n",
      "epoch: 879, train_loss: -6.787168502807617, valid_loss: -4.204981803894043\n",
      "epoch: 880, train_loss: -6.7866597175598145, valid_loss: -4.223001480102539\n",
      "epoch: 881, train_loss: -6.786873817443848, valid_loss: -4.200525283813477\n",
      "epoch: 882, train_loss: -6.788936138153076, valid_loss: -4.223578929901123\n",
      "epoch: 883, train_loss: -6.793868541717529, valid_loss: -4.208322525024414\n",
      "epoch: 884, train_loss: -6.799598217010498, valid_loss: -4.222562789916992\n",
      "epoch: 885, train_loss: -6.804658889770508, valid_loss: -4.216630458831787\n",
      "epoch: 886, train_loss: -6.808298587799072, valid_loss: -4.2181901931762695\n",
      "epoch: 887, train_loss: -6.811009407043457, valid_loss: -4.2201409339904785\n",
      "epoch: 888, train_loss: -6.813144207000732, valid_loss: -4.212761402130127\n",
      "epoch: 889, train_loss: -6.814507007598877, valid_loss: -4.222072601318359\n",
      "epoch: 890, train_loss: -6.814810276031494, valid_loss: -4.204831123352051\n",
      "epoch: 891, train_loss: -6.813466548919678, valid_loss: -4.223853588104248\n",
      "epoch: 892, train_loss: -6.808918476104736, valid_loss: -4.183690547943115\n",
      "epoch: 893, train_loss: -6.796261310577393, valid_loss: -4.216662406921387\n",
      "epoch: 894, train_loss: -6.769967079162598, valid_loss: -4.118363380432129\n",
      "epoch: 895, train_loss: -6.714740753173828, valid_loss: -4.194765567779541\n",
      "epoch: 896, train_loss: -6.677663803100586, valid_loss: -4.103730201721191\n",
      "epoch: 897, train_loss: -6.654086589813232, valid_loss: -4.2245025634765625\n",
      "epoch: 898, train_loss: -6.689121723175049, valid_loss: -4.219785690307617\n",
      "epoch: 899, train_loss: -6.765970230102539, valid_loss: -4.256076335906982\n",
      "epoch: 900, train_loss: -6.81867790222168, valid_loss: -4.2782368659973145\n",
      "epoch: 901, train_loss: -6.833926677703857, valid_loss: -4.262515068054199\n",
      "epoch: 902, train_loss: -6.795186519622803, valid_loss: -4.28148078918457\n",
      "epoch: 903, train_loss: -6.755897045135498, valid_loss: -4.272975444793701\n",
      "epoch: 904, train_loss: -6.7920379638671875, valid_loss: -4.313918590545654\n",
      "epoch: 905, train_loss: -6.8304219245910645, valid_loss: -4.313287258148193\n",
      "epoch: 906, train_loss: -6.846665859222412, valid_loss: -4.3083271980285645\n",
      "epoch: 907, train_loss: -6.8329267501831055, valid_loss: -4.323962688446045\n",
      "epoch: 908, train_loss: -6.808566093444824, valid_loss: -4.3049468994140625\n",
      "epoch: 909, train_loss: -6.820387363433838, valid_loss: -4.334880828857422\n",
      "epoch: 910, train_loss: -6.847557544708252, valid_loss: -4.335278034210205\n",
      "epoch: 911, train_loss: -6.858402252197266, valid_loss: -4.32221794128418\n",
      "epoch: 912, train_loss: -6.85135555267334, valid_loss: -4.337179660797119\n",
      "epoch: 913, train_loss: -6.839365005493164, valid_loss: -4.325878620147705\n",
      "epoch: 914, train_loss: -6.847909450531006, valid_loss: -4.340121746063232\n",
      "epoch: 915, train_loss: -6.866421222686768, valid_loss: -4.336824417114258\n",
      "epoch: 916, train_loss: -6.871810436248779, valid_loss: -4.333177089691162\n",
      "epoch: 917, train_loss: -6.866120338439941, valid_loss: -4.343369960784912\n",
      "epoch: 918, train_loss: -6.863816738128662, valid_loss: -4.3266496658325195\n",
      "epoch: 919, train_loss: -6.868896007537842, valid_loss: -4.345648288726807\n",
      "epoch: 920, train_loss: -6.877424716949463, valid_loss: -4.3404107093811035\n",
      "epoch: 921, train_loss: -6.884849548339844, valid_loss: -4.337691307067871\n",
      "epoch: 922, train_loss: -6.885531902313232, valid_loss: -4.346606731414795\n",
      "epoch: 923, train_loss: -6.881723403930664, valid_loss: -4.325314998626709\n",
      "epoch: 924, train_loss: -6.881246566772461, valid_loss: -4.345461368560791\n",
      "epoch: 925, train_loss: -6.887266635894775, valid_loss: -4.333948612213135\n",
      "epoch: 926, train_loss: -6.894923686981201, valid_loss: -4.3395256996154785\n",
      "epoch: 927, train_loss: -6.899980545043945, valid_loss: -4.340460300445557\n",
      "epoch: 928, train_loss: -6.90158224105835, valid_loss: -4.328341960906982\n",
      "epoch: 929, train_loss: -6.9009623527526855, valid_loss: -4.340184211730957\n",
      "epoch: 930, train_loss: -6.899922847747803, valid_loss: -4.321224689483643\n",
      "epoch: 931, train_loss: -6.8999152183532715, valid_loss: -4.339910507202148\n",
      "epoch: 932, train_loss: -6.901644229888916, valid_loss: -4.320436000823975\n",
      "epoch: 933, train_loss: -6.904351711273193, valid_loss: -4.341348171234131\n",
      "epoch: 934, train_loss: -6.9075398445129395, valid_loss: -4.323387145996094\n",
      "epoch: 935, train_loss: -6.910434722900391, valid_loss: -4.344107151031494\n",
      "epoch: 936, train_loss: -6.913821220397949, valid_loss: -4.328784465789795\n",
      "epoch: 937, train_loss: -6.916960716247559, valid_loss: -4.347960472106934\n",
      "epoch: 938, train_loss: -6.920016765594482, valid_loss: -4.334656238555908\n",
      "epoch: 939, train_loss: -6.922530174255371, valid_loss: -4.352071285247803\n",
      "epoch: 940, train_loss: -6.924881458282471, valid_loss: -4.338977813720703\n",
      "epoch: 941, train_loss: -6.927065372467041, valid_loss: -4.35624361038208\n",
      "epoch: 942, train_loss: -6.929070949554443, valid_loss: -4.34099817276001\n",
      "epoch: 943, train_loss: -6.9305419921875, valid_loss: -4.36040735244751\n",
      "epoch: 944, train_loss: -6.931743144989014, valid_loss: -4.342190265655518\n",
      "epoch: 945, train_loss: -6.9322991371154785, valid_loss: -4.365002155303955\n",
      "epoch: 946, train_loss: -6.932827472686768, valid_loss: -4.34414529800415\n",
      "epoch: 947, train_loss: -6.932511329650879, valid_loss: -4.37009859085083\n",
      "epoch: 948, train_loss: -6.932055473327637, valid_loss: -4.346799373626709\n",
      "epoch: 949, train_loss: -6.93112850189209, valid_loss: -4.376077175140381\n",
      "epoch: 950, train_loss: -6.930625915527344, valid_loss: -4.351561069488525\n",
      "epoch: 951, train_loss: -6.931185722351074, valid_loss: -4.383816242218018\n",
      "epoch: 952, train_loss: -6.933340072631836, valid_loss: -4.362175464630127\n",
      "epoch: 953, train_loss: -6.937751770019531, valid_loss: -4.393559455871582\n",
      "epoch: 954, train_loss: -6.944047927856445, valid_loss: -4.379281044006348\n",
      "epoch: 955, train_loss: -6.951968669891357, valid_loss: -4.40306282043457\n",
      "epoch: 956, train_loss: -6.959874153137207, valid_loss: -4.39712381362915\n",
      "epoch: 957, train_loss: -6.967185974121094, valid_loss: -4.409144401550293\n",
      "epoch: 958, train_loss: -6.972813606262207, valid_loss: -4.410379886627197\n",
      "epoch: 959, train_loss: -6.976639747619629, valid_loss: -4.41200590133667\n",
      "epoch: 960, train_loss: -6.9787397384643555, valid_loss: -4.418658256530762\n",
      "epoch: 961, train_loss: -6.979617595672607, valid_loss: -4.412958145141602\n",
      "epoch: 962, train_loss: -6.980010032653809, valid_loss: -4.424354076385498\n",
      "epoch: 963, train_loss: -6.980366230010986, valid_loss: -4.413276672363281\n",
      "epoch: 964, train_loss: -6.981165409088135, valid_loss: -4.429131984710693\n",
      "epoch: 965, train_loss: -6.98214054107666, valid_loss: -4.414246559143066\n",
      "epoch: 966, train_loss: -6.983398914337158, valid_loss: -4.43355655670166\n",
      "epoch: 967, train_loss: -6.9847412109375, valid_loss: -4.416343688964844\n",
      "epoch: 968, train_loss: -6.985987186431885, valid_loss: -4.438053131103516\n",
      "epoch: 969, train_loss: -6.987004280090332, valid_loss: -4.418524265289307\n",
      "epoch: 970, train_loss: -6.987442493438721, valid_loss: -4.442552089691162\n",
      "epoch: 971, train_loss: -6.987068176269531, valid_loss: -4.418747425079346\n",
      "epoch: 972, train_loss: -6.985661029815674, valid_loss: -4.446616172790527\n",
      "epoch: 973, train_loss: -6.982891082763672, valid_loss: -4.415699481964111\n",
      "epoch: 974, train_loss: -6.97867488861084, valid_loss: -4.450095176696777\n",
      "epoch: 975, train_loss: -6.973651885986328, valid_loss: -4.413144588470459\n",
      "epoch: 976, train_loss: -6.9688215255737305, valid_loss: -4.455828666687012\n",
      "epoch: 977, train_loss: -6.966948509216309, valid_loss: -4.423551559448242\n",
      "epoch: 978, train_loss: -6.971195697784424, valid_loss: -4.468536376953125\n",
      "epoch: 979, train_loss: -6.9809250831604, valid_loss: -4.451749801635742\n",
      "epoch: 980, train_loss: -6.998298645019531, valid_loss: -4.483244895935059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 981, train_loss: -7.014945030212402, valid_loss: -4.4803876876831055\n",
      "epoch: 982, train_loss: -7.029188632965088, valid_loss: -4.489574432373047\n",
      "epoch: 983, train_loss: -7.036418914794922, valid_loss: -4.496129512786865\n",
      "epoch: 984, train_loss: -7.0367608070373535, valid_loss: -4.488877296447754\n",
      "epoch: 985, train_loss: -7.032980918884277, valid_loss: -4.502506256103516\n",
      "epoch: 986, train_loss: -7.028911590576172, valid_loss: -4.488602638244629\n",
      "epoch: 987, train_loss: -7.029485702514648, valid_loss: -4.508297443389893\n",
      "epoch: 988, train_loss: -7.03346586227417, valid_loss: -4.495143890380859\n",
      "epoch: 989, train_loss: -7.04099702835083, valid_loss: -4.513632297515869\n",
      "epoch: 990, train_loss: -7.048955917358398, valid_loss: -4.5075554847717285\n",
      "epoch: 991, train_loss: -7.055763244628906, valid_loss: -4.514533042907715\n",
      "epoch: 992, train_loss: -7.060222148895264, valid_loss: -4.517528533935547\n",
      "epoch: 993, train_loss: -7.062225818634033, valid_loss: -4.5109968185424805\n",
      "epoch: 994, train_loss: -7.0626115798950195, valid_loss: -4.522407054901123\n",
      "epoch: 995, train_loss: -7.062420845031738, valid_loss: -4.50828218460083\n",
      "epoch: 996, train_loss: -7.0623931884765625, valid_loss: -4.525360584259033\n",
      "epoch: 997, train_loss: -7.062734603881836, valid_loss: -4.507363319396973\n",
      "epoch: 998, train_loss: -7.063579559326172, valid_loss: -4.528332710266113\n",
      "epoch: 999, train_loss: -7.064476013183594, valid_loss: -4.505945682525635\n",
      "epoch: 1000, train_loss: -7.065180778503418, valid_loss: -4.531454563140869\n",
      "epoch: 1001, train_loss: -7.06524133682251, valid_loss: -4.502936840057373\n",
      "epoch: 1002, train_loss: -7.064058780670166, valid_loss: -4.534457206726074\n",
      "epoch: 1003, train_loss: -7.0620574951171875, valid_loss: -4.499495029449463\n",
      "epoch: 1004, train_loss: -7.058480739593506, valid_loss: -4.538055896759033\n",
      "epoch: 1005, train_loss: -7.054959774017334, valid_loss: -4.5004563331604\n",
      "epoch: 1006, train_loss: -7.052087306976318, valid_loss: -4.544712066650391\n",
      "epoch: 1007, train_loss: -7.051993370056152, valid_loss: -4.514133453369141\n",
      "epoch: 1008, train_loss: -7.058421611785889, valid_loss: -4.556807994842529\n",
      "epoch: 1009, train_loss: -7.0689311027526855, valid_loss: -4.541014194488525\n",
      "epoch: 1010, train_loss: -7.085556983947754, valid_loss: -4.568933010101318\n",
      "epoch: 1011, train_loss: -7.100530624389648, valid_loss: -4.566456317901611\n",
      "epoch: 1012, train_loss: -7.112117767333984, valid_loss: -4.572997093200684\n",
      "epoch: 1013, train_loss: -7.117504119873047, valid_loss: -4.580209732055664\n",
      "epoch: 1014, train_loss: -7.117461681365967, valid_loss: -4.571696758270264\n",
      "epoch: 1015, train_loss: -7.114651679992676, valid_loss: -4.586307525634766\n",
      "epoch: 1016, train_loss: -7.111879825592041, valid_loss: -4.571967124938965\n",
      "epoch: 1017, train_loss: -7.113000869750977, valid_loss: -4.591967582702637\n",
      "epoch: 1018, train_loss: -7.116783618927002, valid_loss: -4.578403949737549\n",
      "epoch: 1019, train_loss: -7.123814105987549, valid_loss: -4.597310543060303\n",
      "epoch: 1020, train_loss: -7.13124418258667, valid_loss: -4.589804172515869\n",
      "epoch: 1021, train_loss: -7.138053894042969, valid_loss: -4.5990118980407715\n",
      "epoch: 1022, train_loss: -7.143169403076172, valid_loss: -4.600007057189941\n",
      "epoch: 1023, train_loss: -7.146409034729004, valid_loss: -4.596818923950195\n",
      "epoch: 1024, train_loss: -7.14817476272583, valid_loss: -4.60601282119751\n",
      "epoch: 1025, train_loss: -7.14908504486084, valid_loss: -4.593973636627197\n",
      "epoch: 1026, train_loss: -7.149668216705322, valid_loss: -4.60964822769165\n",
      "epoch: 1027, train_loss: -7.150068759918213, valid_loss: -4.591250896453857\n",
      "epoch: 1028, train_loss: -7.150432586669922, valid_loss: -4.613025188446045\n",
      "epoch: 1029, train_loss: -7.1502275466918945, valid_loss: -4.58660888671875\n",
      "epoch: 1030, train_loss: -7.149231433868408, valid_loss: -4.616465091705322\n",
      "epoch: 1031, train_loss: -7.146461009979248, valid_loss: -4.577635288238525\n",
      "epoch: 1032, train_loss: -7.141179084777832, valid_loss: -4.61868953704834\n",
      "epoch: 1033, train_loss: -7.132857322692871, valid_loss: -4.5648698806762695\n",
      "epoch: 1034, train_loss: -7.121526718139648, valid_loss: -4.620457649230957\n",
      "epoch: 1035, train_loss: -7.109707832336426, valid_loss: -4.5637688636779785\n",
      "epoch: 1036, train_loss: -7.104254722595215, valid_loss: -4.63069486618042\n",
      "epoch: 1037, train_loss: -7.107540130615234, valid_loss: -4.598729610443115\n",
      "epoch: 1038, train_loss: -7.131414413452148, valid_loss: -4.649425029754639\n",
      "epoch: 1039, train_loss: -7.157482147216797, valid_loss: -4.645567893981934\n",
      "epoch: 1040, train_loss: -7.184764385223389, valid_loss: -4.6562581062316895\n",
      "epoch: 1041, train_loss: -7.198690891265869, valid_loss: -4.667506694793701\n",
      "epoch: 1042, train_loss: -7.199088096618652, valid_loss: -4.6565937995910645\n",
      "epoch: 1043, train_loss: -7.190257549285889, valid_loss: -4.672318458557129\n",
      "epoch: 1044, train_loss: -7.18042516708374, valid_loss: -4.658779144287109\n",
      "epoch: 1045, train_loss: -7.184368133544922, valid_loss: -4.682394027709961\n",
      "epoch: 1046, train_loss: -7.194858074188232, valid_loss: -4.670506477355957\n",
      "epoch: 1047, train_loss: -7.20968770980835, valid_loss: -4.689080715179443\n",
      "epoch: 1048, train_loss: -7.220041751861572, valid_loss: -4.687739849090576\n",
      "epoch: 1049, train_loss: -7.223898887634277, valid_loss: -4.681807041168213\n",
      "epoch: 1050, train_loss: -7.222279071807861, valid_loss: -4.696582794189453\n",
      "epoch: 1051, train_loss: -7.2190327644348145, valid_loss: -4.6747565269470215\n",
      "epoch: 1052, train_loss: -7.219437122344971, valid_loss: -4.699113368988037\n",
      "epoch: 1053, train_loss: -7.22407865524292, valid_loss: -4.683547019958496\n",
      "epoch: 1054, train_loss: -7.232407093048096, valid_loss: -4.6988139152526855\n",
      "epoch: 1055, train_loss: -7.240597724914551, valid_loss: -4.694643020629883\n",
      "epoch: 1056, train_loss: -7.24710750579834, valid_loss: -4.695755481719971\n",
      "epoch: 1057, train_loss: -7.250975131988525, valid_loss: -4.700764179229736\n",
      "epoch: 1058, train_loss: -7.252636909484863, valid_loss: -4.690864086151123\n",
      "epoch: 1059, train_loss: -7.253284454345703, valid_loss: -4.705076694488525\n",
      "epoch: 1060, train_loss: -7.253791332244873, valid_loss: -4.6848273277282715\n",
      "epoch: 1061, train_loss: -7.2548909187316895, valid_loss: -4.709247589111328\n",
      "epoch: 1062, train_loss: -7.255937576293945, valid_loss: -4.677446365356445\n",
      "epoch: 1063, train_loss: -7.256844520568848, valid_loss: -4.712589740753174\n",
      "epoch: 1064, train_loss: -7.257352352142334, valid_loss: -4.671261787414551\n",
      "epoch: 1065, train_loss: -7.257252216339111, valid_loss: -4.7156147956848145\n",
      "epoch: 1066, train_loss: -7.256711959838867, valid_loss: -4.668858528137207\n",
      "epoch: 1067, train_loss: -7.255415916442871, valid_loss: -4.7193603515625\n",
      "epoch: 1068, train_loss: -7.253478527069092, valid_loss: -4.670717716217041\n",
      "epoch: 1069, train_loss: -7.252710819244385, valid_loss: -4.725066184997559\n",
      "epoch: 1070, train_loss: -7.252697467803955, valid_loss: -4.680580139160156\n",
      "epoch: 1071, train_loss: -7.258449077606201, valid_loss: -4.732998371124268\n",
      "epoch: 1072, train_loss: -7.266458511352539, valid_loss: -4.7024455070495605\n",
      "epoch: 1073, train_loss: -7.280124187469482, valid_loss: -4.739097595214844\n",
      "epoch: 1074, train_loss: -7.293193340301514, valid_loss: -4.727343559265137\n",
      "epoch: 1075, train_loss: -7.305318355560303, valid_loss: -4.739062786102295\n",
      "epoch: 1076, train_loss: -7.3134589195251465, valid_loss: -4.743192195892334\n",
      "epoch: 1077, train_loss: -7.317722797393799, valid_loss: -4.737034320831299\n",
      "epoch: 1078, train_loss: -7.31884241104126, valid_loss: -4.750239849090576\n",
      "epoch: 1079, train_loss: -7.318198204040527, valid_loss: -4.736873149871826\n",
      "epoch: 1080, train_loss: -7.318005084991455, valid_loss: -4.75518274307251\n",
      "epoch: 1081, train_loss: -7.318746566772461, valid_loss: -4.737936973571777\n",
      "epoch: 1082, train_loss: -7.322577953338623, valid_loss: -4.760532379150391\n",
      "epoch: 1083, train_loss: -7.32741117477417, valid_loss: -4.741889476776123\n",
      "epoch: 1084, train_loss: -7.3338847160339355, valid_loss: -4.762894630432129\n",
      "epoch: 1085, train_loss: -7.340288162231445, valid_loss: -4.74953031539917\n",
      "epoch: 1086, train_loss: -7.346439361572266, valid_loss: -4.760514736175537\n",
      "epoch: 1087, train_loss: -7.35170316696167, valid_loss: -4.757067680358887\n",
      "epoch: 1088, train_loss: -7.3560380935668945, valid_loss: -4.756161212921143\n",
      "epoch: 1089, train_loss: -7.359567165374756, valid_loss: -4.7618408203125\n",
      "epoch: 1090, train_loss: -7.362523078918457, valid_loss: -4.7530903816223145\n",
      "epoch: 1091, train_loss: -7.36507511138916, valid_loss: -4.764636516571045\n",
      "epoch: 1092, train_loss: -7.367277145385742, valid_loss: -4.7498393058776855\n",
      "epoch: 1093, train_loss: -7.369231700897217, valid_loss: -4.767256259918213\n",
      "epoch: 1094, train_loss: -7.370672225952148, valid_loss: -4.7432050704956055\n",
      "epoch: 1095, train_loss: -7.371687889099121, valid_loss: -4.770747184753418\n",
      "epoch: 1096, train_loss: -7.371365547180176, valid_loss: -4.73145055770874\n",
      "epoch: 1097, train_loss: -7.369722366333008, valid_loss: -4.774824142456055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1098, train_loss: -7.3648881912231445, valid_loss: -4.713201522827148\n",
      "epoch: 1099, train_loss: -7.357191562652588, valid_loss: -4.778012752532959\n",
      "epoch: 1100, train_loss: -7.343412399291992, valid_loss: -4.690158367156982\n",
      "epoch: 1101, train_loss: -7.328186511993408, valid_loss: -4.779435157775879\n",
      "epoch: 1102, train_loss: -7.308361530303955, valid_loss: -4.683783054351807\n",
      "epoch: 1103, train_loss: -7.305368900299072, valid_loss: -4.786139011383057\n",
      "epoch: 1104, train_loss: -7.310622215270996, valid_loss: -4.729088306427002\n",
      "epoch: 1105, train_loss: -7.347941875457764, valid_loss: -4.7923197746276855\n",
      "epoch: 1106, train_loss: -7.38148307800293, valid_loss: -4.787481307983398\n",
      "epoch: 1107, train_loss: -7.410424709320068, valid_loss: -4.784327507019043\n",
      "epoch: 1108, train_loss: -7.4221062660217285, valid_loss: -4.806069374084473\n",
      "epoch: 1109, train_loss: -7.4193949699401855, valid_loss: -4.787257194519043\n",
      "epoch: 1110, train_loss: -7.407955646514893, valid_loss: -4.803240776062012\n",
      "epoch: 1111, train_loss: -7.396344184875488, valid_loss: -4.788998126983643\n",
      "epoch: 1112, train_loss: -7.4040703773498535, valid_loss: -4.813365936279297\n",
      "epoch: 1113, train_loss: -7.418090343475342, valid_loss: -4.792507171630859\n",
      "epoch: 1114, train_loss: -7.4347991943359375, valid_loss: -4.8146748542785645\n",
      "epoch: 1115, train_loss: -7.444706916809082, valid_loss: -4.806850433349609\n",
      "epoch: 1116, train_loss: -7.446804523468018, valid_loss: -4.792105197906494\n",
      "epoch: 1117, train_loss: -7.443108558654785, valid_loss: -4.814282417297363\n",
      "epoch: 1118, train_loss: -7.438770771026611, valid_loss: -4.778566360473633\n",
      "epoch: 1119, train_loss: -7.440910816192627, valid_loss: -4.807979583740234\n",
      "epoch: 1120, train_loss: -7.447769641876221, valid_loss: -4.790013313293457\n",
      "epoch: 1121, train_loss: -7.457675457000732, valid_loss: -4.79491662979126\n",
      "epoch: 1122, train_loss: -7.465560436248779, valid_loss: -4.7953267097473145\n",
      "epoch: 1123, train_loss: -7.470244407653809, valid_loss: -4.783448696136475\n",
      "epoch: 1124, train_loss: -7.47153902053833, valid_loss: -4.790671348571777\n",
      "epoch: 1125, train_loss: -7.470837593078613, valid_loss: -4.7731099128723145\n",
      "epoch: 1126, train_loss: -7.470788478851318, valid_loss: -4.787492275238037\n",
      "epoch: 1127, train_loss: -7.47208309173584, valid_loss: -4.761641979217529\n",
      "epoch: 1128, train_loss: -7.475818157196045, valid_loss: -4.7854437828063965\n",
      "epoch: 1129, train_loss: -7.480069160461426, valid_loss: -4.752528190612793\n",
      "epoch: 1130, train_loss: -7.484797477722168, valid_loss: -4.777705192565918\n",
      "epoch: 1131, train_loss: -7.489412784576416, valid_loss: -4.752176284790039\n",
      "epoch: 1132, train_loss: -7.493770122528076, valid_loss: -4.76409387588501\n",
      "epoch: 1133, train_loss: -7.49753475189209, valid_loss: -4.755995750427246\n",
      "epoch: 1134, train_loss: -7.500540733337402, valid_loss: -4.748774528503418\n",
      "epoch: 1135, train_loss: -7.502923011779785, valid_loss: -4.756109714508057\n",
      "epoch: 1136, train_loss: -7.504953384399414, valid_loss: -4.73673152923584\n",
      "epoch: 1137, train_loss: -7.506794452667236, valid_loss: -4.752109527587891\n",
      "epoch: 1138, train_loss: -7.50839900970459, valid_loss: -4.728316307067871\n",
      "epoch: 1139, train_loss: -7.509641647338867, valid_loss: -4.747430324554443\n",
      "epoch: 1140, train_loss: -7.510288715362549, valid_loss: -4.7185468673706055\n",
      "epoch: 1141, train_loss: -7.510409832000732, valid_loss: -4.745433807373047\n",
      "epoch: 1142, train_loss: -7.5093255043029785, valid_loss: -4.703253269195557\n",
      "epoch: 1143, train_loss: -7.507381439208984, valid_loss: -4.746984004974365\n",
      "epoch: 1144, train_loss: -7.502750873565674, valid_loss: -4.681727886199951\n",
      "epoch: 1145, train_loss: -7.496678352355957, valid_loss: -4.749589920043945\n",
      "epoch: 1146, train_loss: -7.485949993133545, valid_loss: -4.657750129699707\n",
      "epoch: 1147, train_loss: -7.475790500640869, valid_loss: -4.7505717277526855\n",
      "epoch: 1148, train_loss: -7.461849689483643, valid_loss: -4.648172855377197\n",
      "epoch: 1149, train_loss: -7.459723949432373, valid_loss: -4.751092433929443\n",
      "epoch: 1150, train_loss: -7.460411548614502, valid_loss: -4.676330089569092\n",
      "epoch: 1151, train_loss: -7.482558250427246, valid_loss: -4.748352527618408\n",
      "epoch: 1152, train_loss: -7.504019260406494, valid_loss: -4.7243332862854\n",
      "epoch: 1153, train_loss: -7.528413772583008, valid_loss: -4.7363362312316895\n",
      "epoch: 1154, train_loss: -7.542636394500732, valid_loss: -4.749234199523926\n",
      "epoch: 1155, train_loss: -7.547561168670654, valid_loss: -4.730109214782715\n",
      "epoch: 1156, train_loss: -7.54425048828125, valid_loss: -4.748196125030518\n",
      "epoch: 1157, train_loss: -7.536165714263916, valid_loss: -4.727400302886963\n",
      "epoch: 1158, train_loss: -7.531869888305664, valid_loss: -4.747353553771973\n",
      "epoch: 1159, train_loss: -7.532245635986328, valid_loss: -4.723517894744873\n",
      "epoch: 1160, train_loss: -7.542093276977539, valid_loss: -4.75180721282959\n",
      "epoch: 1161, train_loss: -7.552161693572998, valid_loss: -4.727169513702393\n",
      "epoch: 1162, train_loss: -7.560539722442627, valid_loss: -4.742362022399902\n",
      "epoch: 1163, train_loss: -7.56485652923584, valid_loss: -4.738168239593506\n",
      "epoch: 1164, train_loss: -7.565168857574463, valid_loss: -4.718539237976074\n",
      "epoch: 1165, train_loss: -7.563185691833496, valid_loss: -4.742708683013916\n",
      "epoch: 1166, train_loss: -7.561288356781006, valid_loss: -4.704084396362305\n",
      "epoch: 1167, train_loss: -7.562394618988037, valid_loss: -4.736456871032715\n",
      "epoch: 1168, train_loss: -7.5658721923828125, valid_loss: -4.706942558288574\n",
      "epoch: 1169, train_loss: -7.57144021987915, valid_loss: -4.723388671875\n",
      "epoch: 1170, train_loss: -7.576747417449951, valid_loss: -4.71238374710083\n",
      "epoch: 1171, train_loss: -7.580991744995117, valid_loss: -4.709161281585693\n",
      "epoch: 1172, train_loss: -7.583588123321533, valid_loss: -4.713166236877441\n",
      "epoch: 1173, train_loss: -7.584751605987549, valid_loss: -4.696680545806885\n",
      "epoch: 1174, train_loss: -7.585111141204834, valid_loss: -4.71071195602417\n",
      "epoch: 1175, train_loss: -7.585319995880127, valid_loss: -4.684103965759277\n",
      "epoch: 1176, train_loss: -7.586134433746338, valid_loss: -4.707818031311035\n",
      "epoch: 1177, train_loss: -7.587363243103027, valid_loss: -4.671496391296387\n",
      "epoch: 1178, train_loss: -7.589231967926025, valid_loss: -4.70494270324707\n",
      "epoch: 1179, train_loss: -7.591150760650635, valid_loss: -4.66149377822876\n",
      "epoch: 1180, train_loss: -7.593214988708496, valid_loss: -4.700802326202393\n",
      "epoch: 1181, train_loss: -7.595221042633057, valid_loss: -4.655170917510986\n",
      "epoch: 1182, train_loss: -7.597290515899658, valid_loss: -4.695006847381592\n",
      "epoch: 1183, train_loss: -7.599283218383789, valid_loss: -4.65189790725708\n",
      "epoch: 1184, train_loss: -7.601205348968506, valid_loss: -4.689306259155273\n",
      "epoch: 1185, train_loss: -7.602870941162109, valid_loss: -4.649326801300049\n",
      "epoch: 1186, train_loss: -7.604367733001709, valid_loss: -4.685576915740967\n",
      "epoch: 1187, train_loss: -7.605514049530029, valid_loss: -4.6442036628723145\n",
      "epoch: 1188, train_loss: -7.606541633605957, valid_loss: -4.684834003448486\n",
      "epoch: 1189, train_loss: -7.607113838195801, valid_loss: -4.635579586029053\n",
      "epoch: 1190, train_loss: -7.607515811920166, valid_loss: -4.687016487121582\n",
      "epoch: 1191, train_loss: -7.607237815856934, valid_loss: -4.624787330627441\n",
      "epoch: 1192, train_loss: -7.606710433959961, valid_loss: -4.690404415130615\n",
      "epoch: 1193, train_loss: -7.605433464050293, valid_loss: -4.614508628845215\n",
      "epoch: 1194, train_loss: -7.604275226593018, valid_loss: -4.692846298217773\n",
      "epoch: 1195, train_loss: -7.602806091308594, valid_loss: -4.6102213859558105\n",
      "epoch: 1196, train_loss: -7.602653503417969, valid_loss: -4.692612648010254\n",
      "epoch: 1197, train_loss: -7.6030755043029785, valid_loss: -4.617563724517822\n",
      "epoch: 1198, train_loss: -7.606287002563477, valid_loss: -4.688480854034424\n",
      "epoch: 1199, train_loss: -7.610367298126221, valid_loss: -4.635112285614014\n",
      "epoch: 1200, train_loss: -7.617153644561768, valid_loss: -4.680680751800537\n",
      "epoch: 1201, train_loss: -7.62379789352417, valid_loss: -4.654414653778076\n",
      "epoch: 1202, train_loss: -7.63084602355957, valid_loss: -4.672074794769287\n",
      "epoch: 1203, train_loss: -7.6363630294799805, valid_loss: -4.667718887329102\n",
      "epoch: 1204, train_loss: -7.64012336730957, valid_loss: -4.66489839553833\n",
      "epoch: 1205, train_loss: -7.641890525817871, valid_loss: -4.673862934112549\n",
      "epoch: 1206, train_loss: -7.6421051025390625, valid_loss: -4.6580915451049805\n",
      "epoch: 1207, train_loss: -7.641665935516357, valid_loss: -4.677181720733643\n",
      "epoch: 1208, train_loss: -7.641317367553711, valid_loss: -4.6511383056640625\n",
      "epoch: 1209, train_loss: -7.641971111297607, valid_loss: -4.680325031280518\n",
      "epoch: 1210, train_loss: -7.643307685852051, valid_loss: -4.645071506500244\n",
      "epoch: 1211, train_loss: -7.6455230712890625, valid_loss: -4.681708812713623\n",
      "epoch: 1212, train_loss: -7.647933006286621, valid_loss: -4.64181661605835\n",
      "epoch: 1213, train_loss: -7.650481224060059, valid_loss: -4.679529666900635\n",
      "epoch: 1214, train_loss: -7.652953147888184, valid_loss: -4.642635345458984\n",
      "epoch: 1215, train_loss: -7.655362606048584, valid_loss: -4.674172878265381\n",
      "epoch: 1216, train_loss: -7.657670497894287, valid_loss: -4.645899295806885\n",
      "epoch: 1217, train_loss: -7.65985631942749, valid_loss: -4.667324066162109\n",
      "epoch: 1218, train_loss: -7.661868095397949, valid_loss: -4.648690223693848\n",
      "epoch: 1219, train_loss: -7.6637067794799805, valid_loss: -4.6611480712890625\n",
      "epoch: 1220, train_loss: -7.665398597717285, valid_loss: -4.649143695831299\n",
      "epoch: 1221, train_loss: -7.666990756988525, valid_loss: -4.65692138671875\n",
      "epoch: 1222, train_loss: -7.668526649475098, valid_loss: -4.646529674530029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1223, train_loss: -7.670030117034912, valid_loss: -4.655028820037842\n",
      "epoch: 1224, train_loss: -7.671481609344482, valid_loss: -4.640748500823975\n",
      "epoch: 1225, train_loss: -7.6728339195251465, valid_loss: -4.656087875366211\n",
      "epoch: 1226, train_loss: -7.6739821434021, valid_loss: -4.630578517913818\n",
      "epoch: 1227, train_loss: -7.674727439880371, valid_loss: -4.661741733551025\n",
      "epoch: 1228, train_loss: -7.674620151519775, valid_loss: -4.61050271987915\n",
      "epoch: 1229, train_loss: -7.672706127166748, valid_loss: -4.674920082092285\n",
      "epoch: 1230, train_loss: -7.666744232177734, valid_loss: -4.564113140106201\n",
      "epoch: 1231, train_loss: -7.652523517608643, valid_loss: -4.692070007324219\n",
      "epoch: 1232, train_loss: -7.622233867645264, valid_loss: -4.467773914337158\n",
      "epoch: 1233, train_loss: -7.571843147277832, valid_loss: -4.691208362579346\n",
      "epoch: 1234, train_loss: -7.519853591918945, valid_loss: -4.4465413093566895\n",
      "epoch: 1235, train_loss: -7.510624885559082, valid_loss: -4.687361240386963\n",
      "epoch: 1236, train_loss: -7.550637245178223, valid_loss: -4.63075065612793\n",
      "epoch: 1237, train_loss: -7.601754188537598, valid_loss: -4.605529308319092\n",
      "epoch: 1238, train_loss: -7.614466190338135, valid_loss: -4.704195499420166\n",
      "epoch: 1239, train_loss: -7.656260967254639, valid_loss: -4.673180103302002\n",
      "epoch: 1240, train_loss: -7.6887898445129395, valid_loss: -4.643647193908691\n",
      "epoch: 1241, train_loss: -7.650522232055664, valid_loss: -4.695982933044434\n",
      "epoch: 1242, train_loss: -7.627199172973633, valid_loss: -4.697967529296875\n",
      "epoch: 1243, train_loss: -7.643920421600342, valid_loss: -4.634547233581543\n",
      "epoch: 1244, train_loss: -7.657767295837402, valid_loss: -4.729182720184326\n",
      "epoch: 1245, train_loss: -7.683457851409912, valid_loss: -4.7173542976379395\n",
      "epoch: 1246, train_loss: -7.693653106689453, valid_loss: -4.659347057342529\n",
      "epoch: 1247, train_loss: -7.668447494506836, valid_loss: -4.735930442810059\n",
      "epoch: 1248, train_loss: -7.667786121368408, valid_loss: -4.712574005126953\n",
      "epoch: 1249, train_loss: -7.680692672729492, valid_loss: -4.685051441192627\n",
      "epoch: 1250, train_loss: -7.688648223876953, valid_loss: -4.729866027832031\n",
      "epoch: 1251, train_loss: -7.702498435974121, valid_loss: -4.726846218109131\n",
      "epoch: 1252, train_loss: -7.696483135223389, valid_loss: -4.6944990158081055\n",
      "epoch: 1253, train_loss: -7.68593692779541, valid_loss: -4.715000152587891\n",
      "epoch: 1254, train_loss: -7.695837020874023, valid_loss: -4.730710506439209\n",
      "epoch: 1255, train_loss: -7.703988075256348, valid_loss: -4.68496561050415\n",
      "epoch: 1256, train_loss: -7.706052303314209, valid_loss: -4.722423076629639\n",
      "epoch: 1257, train_loss: -7.709770202636719, valid_loss: -4.724949836730957\n",
      "epoch: 1258, train_loss: -7.708076000213623, valid_loss: -4.673327445983887\n",
      "epoch: 1259, train_loss: -7.704520225524902, valid_loss: -4.726579666137695\n",
      "epoch: 1260, train_loss: -7.7098493576049805, valid_loss: -4.689373970031738\n",
      "epoch: 1261, train_loss: -7.718817234039307, valid_loss: -4.685603141784668\n",
      "epoch: 1262, train_loss: -7.720462799072266, valid_loss: -4.715432643890381\n",
      "epoch: 1263, train_loss: -7.716568946838379, valid_loss: -4.657558917999268\n",
      "epoch: 1264, train_loss: -7.715297698974609, valid_loss: -4.701889514923096\n",
      "epoch: 1265, train_loss: -7.718721389770508, valid_loss: -4.668900012969971\n",
      "epoch: 1266, train_loss: -7.723819255828857, valid_loss: -4.672489166259766\n",
      "epoch: 1267, train_loss: -7.7274088859558105, valid_loss: -4.684301376342773\n",
      "epoch: 1268, train_loss: -7.728397369384766, valid_loss: -4.648653507232666\n",
      "epoch: 1269, train_loss: -7.7274017333984375, valid_loss: -4.683053970336914\n",
      "epoch: 1270, train_loss: -7.726027011871338, valid_loss: -4.634012222290039\n",
      "epoch: 1271, train_loss: -7.7259602546691895, valid_loss: -4.676147937774658\n",
      "epoch: 1272, train_loss: -7.72767972946167, valid_loss: -4.629217624664307\n",
      "epoch: 1273, train_loss: -7.730565547943115, valid_loss: -4.67020320892334\n",
      "epoch: 1274, train_loss: -7.733181953430176, valid_loss: -4.625062465667725\n",
      "epoch: 1275, train_loss: -7.735000133514404, valid_loss: -4.661956787109375\n",
      "epoch: 1276, train_loss: -7.736202716827393, valid_loss: -4.621005058288574\n",
      "epoch: 1277, train_loss: -7.737486839294434, valid_loss: -4.656252384185791\n",
      "epoch: 1278, train_loss: -7.7390217781066895, valid_loss: -4.6194891929626465\n",
      "epoch: 1279, train_loss: -7.740666389465332, valid_loss: -4.652538299560547\n",
      "epoch: 1280, train_loss: -7.741936206817627, valid_loss: -4.614487648010254\n",
      "epoch: 1281, train_loss: -7.742629051208496, valid_loss: -4.65151309967041\n",
      "epoch: 1282, train_loss: -7.742704391479492, valid_loss: -4.605945587158203\n",
      "epoch: 1283, train_loss: -7.742318153381348, valid_loss: -4.655768394470215\n",
      "epoch: 1284, train_loss: -7.741366863250732, valid_loss: -4.590916156768799\n",
      "epoch: 1285, train_loss: -7.739678859710693, valid_loss: -4.66282844543457\n",
      "epoch: 1286, train_loss: -7.736746311187744, valid_loss: -4.5692667961120605\n",
      "epoch: 1287, train_loss: -7.732665538787842, valid_loss: -4.670540809631348\n",
      "epoch: 1288, train_loss: -7.727598667144775, valid_loss: -4.552210330963135\n",
      "epoch: 1289, train_loss: -7.723801612854004, valid_loss: -4.67413330078125\n",
      "epoch: 1290, train_loss: -7.722867965698242, valid_loss: -4.562087535858154\n",
      "epoch: 1291, train_loss: -7.727165699005127, valid_loss: -4.666646957397461\n",
      "epoch: 1292, train_loss: -7.734532356262207, valid_loss: -4.602210521697998\n",
      "epoch: 1293, train_loss: -7.7424635887146, valid_loss: -4.641658782958984\n",
      "epoch: 1294, train_loss: -7.747110366821289, valid_loss: -4.637803077697754\n",
      "epoch: 1295, train_loss: -7.750667572021484, valid_loss: -4.6218647956848145\n",
      "epoch: 1296, train_loss: -7.754659175872803, valid_loss: -4.647465229034424\n",
      "epoch: 1297, train_loss: -7.760066032409668, valid_loss: -4.630539417266846\n",
      "epoch: 1298, train_loss: -7.764050483703613, valid_loss: -4.636310577392578\n",
      "epoch: 1299, train_loss: -7.7642364501953125, valid_loss: -4.641489028930664\n",
      "epoch: 1300, train_loss: -7.761878967285156, valid_loss: -4.629868984222412\n",
      "epoch: 1301, train_loss: -7.7600836753845215, valid_loss: -4.635106563568115\n",
      "epoch: 1302, train_loss: -7.760653972625732, valid_loss: -4.643426895141602\n",
      "epoch: 1303, train_loss: -7.761981964111328, valid_loss: -4.617467880249023\n",
      "epoch: 1304, train_loss: -7.7633056640625, valid_loss: -4.655710220336914\n",
      "epoch: 1305, train_loss: -7.7641072273254395, valid_loss: -4.607786655426025\n",
      "epoch: 1306, train_loss: -7.765796661376953, valid_loss: -4.653905868530273\n",
      "epoch: 1307, train_loss: -7.768506050109863, valid_loss: -4.615869998931885\n",
      "epoch: 1308, train_loss: -7.771754741668701, valid_loss: -4.641113758087158\n",
      "epoch: 1309, train_loss: -7.774497985839844, valid_loss: -4.628831386566162\n",
      "epoch: 1310, train_loss: -7.776429653167725, valid_loss: -4.626646995544434\n",
      "epoch: 1311, train_loss: -7.777738094329834, valid_loss: -4.635415554046631\n",
      "epoch: 1312, train_loss: -7.7788801193237305, valid_loss: -4.618003845214844\n",
      "epoch: 1313, train_loss: -7.780083179473877, valid_loss: -4.634695053100586\n",
      "epoch: 1314, train_loss: -7.781289100646973, valid_loss: -4.614437580108643\n",
      "epoch: 1315, train_loss: -7.782345771789551, valid_loss: -4.630909442901611\n",
      "epoch: 1316, train_loss: -7.783111572265625, valid_loss: -4.611532211303711\n",
      "epoch: 1317, train_loss: -7.783560752868652, valid_loss: -4.6280131340026855\n",
      "epoch: 1318, train_loss: -7.7836713790893555, valid_loss: -4.605329990386963\n",
      "epoch: 1319, train_loss: -7.783515453338623, valid_loss: -4.628978729248047\n",
      "epoch: 1320, train_loss: -7.7828369140625, valid_loss: -4.593544960021973\n",
      "epoch: 1321, train_loss: -7.781609535217285, valid_loss: -4.636227607727051\n",
      "epoch: 1322, train_loss: -7.778814315795898, valid_loss: -4.570892333984375\n",
      "epoch: 1323, train_loss: -7.773928165435791, valid_loss: -4.651512622833252\n",
      "epoch: 1324, train_loss: -7.763822555541992, valid_loss: -4.524661064147949\n",
      "epoch: 1325, train_loss: -7.7464776039123535, valid_loss: -4.67157506942749\n",
      "epoch: 1326, train_loss: -7.714476585388184, valid_loss: -4.443077564239502\n",
      "epoch: 1327, train_loss: -7.671008586883545, valid_loss: -4.681369781494141\n",
      "epoch: 1328, train_loss: -7.624480724334717, valid_loss: -4.42198371887207\n",
      "epoch: 1329, train_loss: -7.621929168701172, valid_loss: -4.687257289886475\n",
      "epoch: 1330, train_loss: -7.663505554199219, valid_loss: -4.585996627807617\n",
      "epoch: 1331, train_loss: -7.724170207977295, valid_loss: -4.611731052398682\n",
      "epoch: 1332, train_loss: -7.736518859863281, valid_loss: -4.676219940185547\n",
      "epoch: 1333, train_loss: -7.7510833740234375, valid_loss: -4.630476474761963\n",
      "epoch: 1334, train_loss: -7.790695667266846, valid_loss: -4.655303955078125\n",
      "epoch: 1335, train_loss: -7.790660858154297, valid_loss: -4.681236267089844\n",
      "epoch: 1336, train_loss: -7.757328033447266, valid_loss: -4.656501770019531\n",
      "epoch: 1337, train_loss: -7.757555961608887, valid_loss: -4.638506889343262\n",
      "epoch: 1338, train_loss: -7.768425941467285, valid_loss: -4.720630168914795\n",
      "epoch: 1339, train_loss: -7.768603324890137, valid_loss: -4.654292583465576\n",
      "epoch: 1340, train_loss: -7.792251110076904, valid_loss: -4.681258201599121\n",
      "epoch: 1341, train_loss: -7.803521633148193, valid_loss: -4.722154140472412\n",
      "epoch: 1342, train_loss: -7.790764331817627, valid_loss: -4.657549858093262\n",
      "epoch: 1343, train_loss: -7.790186882019043, valid_loss: -4.6943206787109375\n",
      "epoch: 1344, train_loss: -7.789419174194336, valid_loss: -4.70182466506958\n",
      "epoch: 1345, train_loss: -7.787639617919922, valid_loss: -4.685001373291016\n",
      "epoch: 1346, train_loss: -7.801224708557129, valid_loss: -4.68440580368042\n",
      "epoch: 1347, train_loss: -7.8101277351379395, valid_loss: -4.708930492401123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1348, train_loss: -7.805557727813721, valid_loss: -4.680453300476074\n",
      "epoch: 1349, train_loss: -7.805394649505615, valid_loss: -4.6783037185668945\n",
      "epoch: 1350, train_loss: -7.807054042816162, valid_loss: -4.713723182678223\n",
      "epoch: 1351, train_loss: -7.804765701293945, valid_loss: -4.65308952331543\n",
      "epoch: 1352, train_loss: -7.808262348175049, valid_loss: -4.694151401519775\n",
      "epoch: 1353, train_loss: -7.816447734832764, valid_loss: -4.685671806335449\n",
      "epoch: 1354, train_loss: -7.8190741539001465, valid_loss: -4.650055408477783\n",
      "epoch: 1355, train_loss: -7.81613302230835, valid_loss: -4.69268274307251\n",
      "epoch: 1356, train_loss: -7.814942359924316, valid_loss: -4.644608497619629\n",
      "epoch: 1357, train_loss: -7.817103385925293, valid_loss: -4.665698528289795\n",
      "epoch: 1358, train_loss: -7.819178104400635, valid_loss: -4.65964937210083\n",
      "epoch: 1359, train_loss: -7.820591449737549, valid_loss: -4.638295650482178\n",
      "epoch: 1360, train_loss: -7.82244348526001, valid_loss: -4.659531593322754\n",
      "epoch: 1361, train_loss: -7.825018882751465, valid_loss: -4.631345272064209\n",
      "epoch: 1362, train_loss: -7.827018737792969, valid_loss: -4.6457133293151855\n",
      "epoch: 1363, train_loss: -7.827425003051758, valid_loss: -4.63273811340332\n",
      "epoch: 1364, train_loss: -7.826785564422607, valid_loss: -4.629729270935059\n",
      "epoch: 1365, train_loss: -7.826271057128906, valid_loss: -4.6305999755859375\n",
      "epoch: 1366, train_loss: -7.82700252532959, valid_loss: -4.619967937469482\n",
      "epoch: 1367, train_loss: -7.828815460205078, valid_loss: -4.626205921173096\n",
      "epoch: 1368, train_loss: -7.831286907196045, valid_loss: -4.615706443786621\n",
      "epoch: 1369, train_loss: -7.833617687225342, valid_loss: -4.620152473449707\n",
      "epoch: 1370, train_loss: -7.835373401641846, valid_loss: -4.612681865692139\n",
      "epoch: 1371, train_loss: -7.8364362716674805, valid_loss: -4.6146955490112305\n",
      "epoch: 1372, train_loss: -7.836935997009277, valid_loss: -4.609827518463135\n",
      "epoch: 1373, train_loss: -7.837116718292236, valid_loss: -4.610174179077148\n",
      "epoch: 1374, train_loss: -7.837172031402588, valid_loss: -4.6058268547058105\n",
      "epoch: 1375, train_loss: -7.83730936050415, valid_loss: -4.607509136199951\n",
      "epoch: 1376, train_loss: -7.837469577789307, valid_loss: -4.601639270782471\n",
      "epoch: 1377, train_loss: -7.837801456451416, valid_loss: -4.6071977615356445\n",
      "epoch: 1378, train_loss: -7.838004112243652, valid_loss: -4.59562873840332\n",
      "epoch: 1379, train_loss: -7.8382568359375, valid_loss: -4.609826564788818\n",
      "epoch: 1380, train_loss: -7.838014125823975, valid_loss: -4.587827682495117\n",
      "epoch: 1381, train_loss: -7.837531089782715, valid_loss: -4.616470813751221\n",
      "epoch: 1382, train_loss: -7.835781097412109, valid_loss: -4.574594974517822\n",
      "epoch: 1383, train_loss: -7.833188533782959, valid_loss: -4.6273193359375\n",
      "epoch: 1384, train_loss: -7.827581405639648, valid_loss: -4.552929401397705\n",
      "epoch: 1385, train_loss: -7.820099830627441, valid_loss: -4.643250942230225\n",
      "epoch: 1386, train_loss: -7.806059837341309, valid_loss: -4.518407821655273\n",
      "epoch: 1387, train_loss: -7.790149211883545, valid_loss: -4.661582946777344\n",
      "epoch: 1388, train_loss: -7.7642083168029785, valid_loss: -4.476424217224121\n",
      "epoch: 1389, train_loss: -7.745824337005615, valid_loss: -4.67757511138916\n",
      "epoch: 1390, train_loss: -7.725516319274902, valid_loss: -4.472151756286621\n",
      "epoch: 1391, train_loss: -7.737197399139404, valid_loss: -4.685460090637207\n",
      "epoch: 1392, train_loss: -7.762568473815918, valid_loss: -4.557960510253906\n",
      "epoch: 1393, train_loss: -7.808941841125488, valid_loss: -4.657670974731445\n",
      "epoch: 1394, train_loss: -7.841141700744629, valid_loss: -4.657779216766357\n",
      "epoch: 1395, train_loss: -7.850225448608398, valid_loss: -4.60230827331543\n",
      "epoch: 1396, train_loss: -7.84346342086792, valid_loss: -4.69191837310791\n",
      "epoch: 1397, train_loss: -7.838107585906982, valid_loss: -4.618991374969482\n",
      "epoch: 1398, train_loss: -7.838040828704834, valid_loss: -4.667250633239746\n",
      "epoch: 1399, train_loss: -7.833680152893066, valid_loss: -4.658753871917725\n",
      "epoch: 1400, train_loss: -7.8310041427612305, valid_loss: -4.646183490753174\n",
      "epoch: 1401, train_loss: -7.837963581085205, valid_loss: -4.669854640960693\n",
      "epoch: 1402, train_loss: -7.853416919708252, valid_loss: -4.6785054206848145\n",
      "epoch: 1403, train_loss: -7.861703872680664, valid_loss: -4.652790069580078\n",
      "epoch: 1404, train_loss: -7.8589582443237305, valid_loss: -4.680414199829102\n",
      "epoch: 1405, train_loss: -7.854658603668213, valid_loss: -4.667325973510742\n",
      "epoch: 1406, train_loss: -7.854465961456299, valid_loss: -4.653204917907715\n",
      "epoch: 1407, train_loss: -7.855432987213135, valid_loss: -4.695044994354248\n",
      "epoch: 1408, train_loss: -7.855481147766113, valid_loss: -4.63727331161499\n",
      "epoch: 1409, train_loss: -7.858829975128174, valid_loss: -4.686180114746094\n",
      "epoch: 1410, train_loss: -7.86496639251709, valid_loss: -4.661056995391846\n",
      "epoch: 1411, train_loss: -7.869549751281738, valid_loss: -4.652830600738525\n",
      "epoch: 1412, train_loss: -7.869436264038086, valid_loss: -4.68122673034668\n",
      "epoch: 1413, train_loss: -7.86665678024292, valid_loss: -4.629252910614014\n",
      "epoch: 1414, train_loss: -7.865132808685303, valid_loss: -4.675440788269043\n",
      "epoch: 1415, train_loss: -7.866125583648682, valid_loss: -4.635182857513428\n",
      "epoch: 1416, train_loss: -7.8687944412231445, valid_loss: -4.653509616851807\n",
      "epoch: 1417, train_loss: -7.871142864227295, valid_loss: -4.646721839904785\n",
      "epoch: 1418, train_loss: -7.872899055480957, valid_loss: -4.630758762359619\n",
      "epoch: 1419, train_loss: -7.874213695526123, valid_loss: -4.650145053863525\n",
      "epoch: 1420, train_loss: -7.8755035400390625, valid_loss: -4.620135307312012\n",
      "epoch: 1421, train_loss: -7.876843452453613, valid_loss: -4.64457893371582\n",
      "epoch: 1422, train_loss: -7.87799072265625, valid_loss: -4.617100715637207\n",
      "epoch: 1423, train_loss: -7.8787689208984375, valid_loss: -4.634343147277832\n",
      "epoch: 1424, train_loss: -7.879114151000977, valid_loss: -4.617025375366211\n",
      "epoch: 1425, train_loss: -7.879162788391113, valid_loss: -4.623705863952637\n",
      "epoch: 1426, train_loss: -7.878932952880859, valid_loss: -4.615458011627197\n",
      "epoch: 1427, train_loss: -7.878662586212158, valid_loss: -4.614375114440918\n",
      "epoch: 1428, train_loss: -7.878138065338135, valid_loss: -4.613338947296143\n",
      "epoch: 1429, train_loss: -7.877557754516602, valid_loss: -4.606752872467041\n",
      "epoch: 1430, train_loss: -7.876427173614502, valid_loss: -4.61023473739624\n",
      "epoch: 1431, train_loss: -7.874919414520264, valid_loss: -4.599583625793457\n",
      "epoch: 1432, train_loss: -7.872137546539307, valid_loss: -4.606905937194824\n",
      "epoch: 1433, train_loss: -7.868332386016846, valid_loss: -4.591653823852539\n",
      "epoch: 1434, train_loss: -7.8617682456970215, valid_loss: -4.601088523864746\n",
      "epoch: 1435, train_loss: -7.85325813293457, valid_loss: -4.581489086151123\n",
      "epoch: 1436, train_loss: -7.83956241607666, valid_loss: -4.591639518737793\n",
      "epoch: 1437, train_loss: -7.82460355758667, valid_loss: -4.571146488189697\n",
      "epoch: 1438, train_loss: -7.803758144378662, valid_loss: -4.5804057121276855\n",
      "epoch: 1439, train_loss: -7.791234016418457, valid_loss: -4.572412014007568\n",
      "epoch: 1440, train_loss: -7.781467914581299, valid_loss: -4.584004878997803\n",
      "epoch: 1441, train_loss: -7.797590255737305, valid_loss: -4.602462291717529\n",
      "epoch: 1442, train_loss: -7.822220325469971, valid_loss: -4.605741024017334\n",
      "epoch: 1443, train_loss: -7.859018325805664, valid_loss: -4.641664505004883\n",
      "epoch: 1444, train_loss: -7.884698867797852, valid_loss: -4.615439414978027\n",
      "epoch: 1445, train_loss: -7.894132614135742, valid_loss: -4.650322437286377\n",
      "epoch: 1446, train_loss: -7.888206481933594, valid_loss: -4.620941638946533\n",
      "epoch: 1447, train_loss: -7.87699031829834, valid_loss: -4.635519981384277\n",
      "epoch: 1448, train_loss: -7.8710808753967285, valid_loss: -4.649843215942383\n",
      "epoch: 1449, train_loss: -7.873086929321289, valid_loss: -4.622902870178223\n",
      "epoch: 1450, train_loss: -7.882394790649414, valid_loss: -4.675722122192383\n",
      "epoch: 1451, train_loss: -7.890237331390381, valid_loss: -4.621218681335449\n",
      "epoch: 1452, train_loss: -7.894766807556152, valid_loss: -4.670786380767822\n",
      "epoch: 1453, train_loss: -7.895977020263672, valid_loss: -4.643460750579834\n",
      "epoch: 1454, train_loss: -7.895781517028809, valid_loss: -4.646443843841553\n",
      "epoch: 1455, train_loss: -7.895063400268555, valid_loss: -4.673181533813477\n",
      "epoch: 1456, train_loss: -7.894231796264648, valid_loss: -4.62436056137085\n",
      "epoch: 1457, train_loss: -7.894999980926514, valid_loss: -4.682467460632324\n",
      "epoch: 1458, train_loss: -7.897280216217041, valid_loss: -4.62591552734375\n",
      "epoch: 1459, train_loss: -7.9009833335876465, valid_loss: -4.668168544769287\n",
      "epoch: 1460, train_loss: -7.9044365882873535, valid_loss: -4.6465888023376465\n",
      "epoch: 1461, train_loss: -7.906296253204346, valid_loss: -4.641578674316406\n",
      "epoch: 1462, train_loss: -7.906158924102783, valid_loss: -4.664642333984375\n",
      "epoch: 1463, train_loss: -7.904865741729736, valid_loss: -4.619266033172607\n",
      "epoch: 1464, train_loss: -7.903983116149902, valid_loss: -4.668414115905762\n",
      "epoch: 1465, train_loss: -7.904242992401123, valid_loss: -4.612714767456055\n",
      "epoch: 1466, train_loss: -7.905971050262451, valid_loss: -4.660125732421875\n",
      "epoch: 1467, train_loss: -7.9084858894348145, valid_loss: -4.619482040405273\n",
      "epoch: 1468, train_loss: -7.911113739013672, valid_loss: -4.644461154937744\n",
      "epoch: 1469, train_loss: -7.913200855255127, valid_loss: -4.630414009094238\n",
      "epoch: 1470, train_loss: -7.914482116699219, valid_loss: -4.626731872558594\n",
      "epoch: 1471, train_loss: -7.914984703063965, valid_loss: -4.638722896575928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1472, train_loss: -7.914928913116455, valid_loss: -4.610800743103027\n",
      "epoch: 1473, train_loss: -7.914604187011719, valid_loss: -4.642886638641357\n",
      "epoch: 1474, train_loss: -7.914163589477539, valid_loss: -4.598480224609375\n",
      "epoch: 1475, train_loss: -7.91378927230835, valid_loss: -4.6444244384765625\n",
      "epoch: 1476, train_loss: -7.913386344909668, valid_loss: -4.588940143585205\n",
      "epoch: 1477, train_loss: -7.913035869598389, valid_loss: -4.6454668045043945\n",
      "epoch: 1478, train_loss: -7.91244649887085, valid_loss: -4.580392837524414\n",
      "epoch: 1479, train_loss: -7.91170597076416, valid_loss: -4.64733362197876\n",
      "epoch: 1480, train_loss: -7.9102888107299805, valid_loss: -4.5706071853637695\n",
      "epoch: 1481, train_loss: -7.908353805541992, valid_loss: -4.650990962982178\n",
      "epoch: 1482, train_loss: -7.904827117919922, valid_loss: -4.558131217956543\n",
      "epoch: 1483, train_loss: -7.900147438049316, valid_loss: -4.656615734100342\n",
      "epoch: 1484, train_loss: -7.891767501831055, valid_loss: -4.541715621948242\n",
      "epoch: 1485, train_loss: -7.881663799285889, valid_loss: -4.662431716918945\n",
      "epoch: 1486, train_loss: -7.8638386726379395, valid_loss: -4.522942066192627\n",
      "epoch: 1487, train_loss: -7.84793758392334, valid_loss: -4.663580894470215\n",
      "epoch: 1488, train_loss: -7.822569370269775, valid_loss: -4.516913890838623\n",
      "epoch: 1489, train_loss: -7.818497657775879, valid_loss: -4.657635688781738\n",
      "epoch: 1490, train_loss: -7.814476013183594, valid_loss: -4.555784702301025\n",
      "epoch: 1491, train_loss: -7.845463752746582, valid_loss: -4.6411309242248535\n",
      "epoch: 1492, train_loss: -7.874173641204834, valid_loss: -4.627790927886963\n",
      "epoch: 1493, train_loss: -7.903752326965332, valid_loss: -4.605177879333496\n",
      "epoch: 1494, train_loss: -7.9166131019592285, valid_loss: -4.683364391326904\n",
      "epoch: 1495, train_loss: -7.912660598754883, valid_loss: -4.5695905685424805\n",
      "epoch: 1496, train_loss: -7.899864673614502, valid_loss: -4.705945014953613\n",
      "epoch: 1497, train_loss: -7.889179229736328, valid_loss: -4.57721471786499\n",
      "epoch: 1498, train_loss: -7.8931193351745605, valid_loss: -4.6989569664001465\n",
      "epoch: 1499, train_loss: -7.907653331756592, valid_loss: -4.634518146514893\n",
      "epoch: 1500, train_loss: -7.925544261932373, valid_loss: -4.663307189941406\n",
      "epoch: 1501, train_loss: -7.933983325958252, valid_loss: -4.688180923461914\n",
      "epoch: 1502, train_loss: -7.930488586425781, valid_loss: -4.622744083404541\n",
      "epoch: 1503, train_loss: -7.921640396118164, valid_loss: -4.7078118324279785\n",
      "epoch: 1504, train_loss: -7.9165754318237305, valid_loss: -4.623964786529541\n",
      "epoch: 1505, train_loss: -7.920806884765625, valid_loss: -4.6950764656066895\n",
      "epoch: 1506, train_loss: -7.929834365844727, valid_loss: -4.6623148918151855\n",
      "epoch: 1507, train_loss: -7.937130451202393, valid_loss: -4.6590776443481445\n",
      "epoch: 1508, train_loss: -7.938035488128662, valid_loss: -4.6929850578308105\n",
      "epoch: 1509, train_loss: -7.934177875518799, valid_loss: -4.630645275115967\n",
      "epoch: 1510, train_loss: -7.93064546585083, valid_loss: -4.695860385894775\n",
      "epoch: 1511, train_loss: -7.930817127227783, valid_loss: -4.635422229766846\n",
      "epoch: 1512, train_loss: -7.934962749481201, valid_loss: -4.675887584686279\n",
      "epoch: 1513, train_loss: -7.939819812774658, valid_loss: -4.657524585723877\n",
      "epoch: 1514, train_loss: -7.942746162414551, valid_loss: -4.645601272583008\n",
      "epoch: 1515, train_loss: -7.942833423614502, valid_loss: -4.673116207122803\n",
      "epoch: 1516, train_loss: -7.941194534301758, valid_loss: -4.623470306396484\n",
      "epoch: 1517, train_loss: -7.939660549163818, valid_loss: -4.674294471740723\n",
      "epoch: 1518, train_loss: -7.939399719238281, valid_loss: -4.618609428405762\n",
      "epoch: 1519, train_loss: -7.940774440765381, valid_loss: -4.664553642272949\n",
      "epoch: 1520, train_loss: -7.943100452423096, valid_loss: -4.625796794891357\n",
      "epoch: 1521, train_loss: -7.945662975311279, valid_loss: -4.647864818572998\n",
      "epoch: 1522, train_loss: -7.947728633880615, valid_loss: -4.63580322265625\n",
      "epoch: 1523, train_loss: -7.9490180015563965, valid_loss: -4.629368782043457\n",
      "epoch: 1524, train_loss: -7.949544906616211, valid_loss: -4.642629146575928\n",
      "epoch: 1525, train_loss: -7.949522018432617, valid_loss: -4.612667083740234\n",
      "epoch: 1526, train_loss: -7.949185848236084, valid_loss: -4.646281719207764\n",
      "epoch: 1527, train_loss: -7.948686599731445, valid_loss: -4.598764896392822\n",
      "epoch: 1528, train_loss: -7.9481000900268555, valid_loss: -4.648329734802246\n",
      "epoch: 1529, train_loss: -7.947328567504883, valid_loss: -4.585485458374023\n",
      "epoch: 1530, train_loss: -7.946274280548096, valid_loss: -4.651153087615967\n",
      "epoch: 1531, train_loss: -7.944597244262695, valid_loss: -4.569626808166504\n",
      "epoch: 1532, train_loss: -7.942124843597412, valid_loss: -4.655452251434326\n",
      "epoch: 1533, train_loss: -7.938287734985352, valid_loss: -4.549680233001709\n",
      "epoch: 1534, train_loss: -7.933322906494141, valid_loss: -4.659539222717285\n",
      "epoch: 1535, train_loss: -7.927018165588379, valid_loss: -4.532695770263672\n",
      "epoch: 1536, train_loss: -7.922279357910156, valid_loss: -4.658487796783447\n",
      "epoch: 1537, train_loss: -7.920786380767822, valid_loss: -4.546718597412109\n",
      "epoch: 1538, train_loss: -7.927922248840332, valid_loss: -4.645785808563232\n",
      "epoch: 1539, train_loss: -7.9403791427612305, valid_loss: -4.606207847595215\n",
      "epoch: 1540, train_loss: -7.952072620391846, valid_loss: -4.604193210601807\n",
      "epoch: 1541, train_loss: -7.95245361328125, valid_loss: -4.660728931427002\n",
      "epoch: 1542, train_loss: -7.938562870025635, valid_loss: -4.549670219421387\n",
      "epoch: 1543, train_loss: -7.915798187255859, valid_loss: -4.676126480102539\n",
      "epoch: 1544, train_loss: -7.879359722137451, valid_loss: -4.51005220413208\n",
      "epoch: 1545, train_loss: -7.828464508056641, valid_loss: -4.627680778503418\n",
      "epoch: 1546, train_loss: -7.723174095153809, valid_loss: -4.449862480163574\n",
      "epoch: 1547, train_loss: -7.661880970001221, valid_loss: -4.5691962242126465\n",
      "epoch: 1548, train_loss: -7.634491443634033, valid_loss: -4.539881229400635\n",
      "epoch: 1549, train_loss: -7.774289608001709, valid_loss: -4.633089542388916\n",
      "epoch: 1550, train_loss: -7.909444808959961, valid_loss: -4.670931816101074\n",
      "epoch: 1551, train_loss: -7.961713790893555, valid_loss: -4.62941837310791\n",
      "epoch: 1552, train_loss: -7.907891750335693, valid_loss: -4.6498212814331055\n",
      "epoch: 1553, train_loss: -7.839127063751221, valid_loss: -4.627340316772461\n",
      "epoch: 1554, train_loss: -7.874214172363281, valid_loss: -4.6828813552856445\n",
      "epoch: 1555, train_loss: -7.947362899780273, valid_loss: -4.702432155609131\n",
      "epoch: 1556, train_loss: -7.957678318023682, valid_loss: -4.66398286819458\n",
      "epoch: 1557, train_loss: -7.911482810974121, valid_loss: -4.685479640960693\n",
      "epoch: 1558, train_loss: -7.900905132293701, valid_loss: -4.701666831970215\n",
      "epoch: 1559, train_loss: -7.946682929992676, valid_loss: -4.693686008453369\n",
      "epoch: 1560, train_loss: -7.962182521820068, valid_loss: -4.719182968139648\n",
      "epoch: 1561, train_loss: -7.933453559875488, valid_loss: -4.693578243255615\n",
      "epoch: 1562, train_loss: -7.930285930633545, valid_loss: -4.700406074523926\n",
      "epoch: 1563, train_loss: -7.956045150756836, valid_loss: -4.741878986358643\n",
      "epoch: 1564, train_loss: -7.960865020751953, valid_loss: -4.683601379394531\n",
      "epoch: 1565, train_loss: -7.946280002593994, valid_loss: -4.726531028747559\n",
      "epoch: 1566, train_loss: -7.950068950653076, valid_loss: -4.726483345031738\n",
      "epoch: 1567, train_loss: -7.964719772338867, valid_loss: -4.676157474517822\n",
      "epoch: 1568, train_loss: -7.9595746994018555, valid_loss: -4.747158050537109\n",
      "epoch: 1569, train_loss: -7.950884819030762, valid_loss: -4.682966232299805\n",
      "epoch: 1570, train_loss: -7.961817741394043, valid_loss: -4.706359386444092\n",
      "epoch: 1571, train_loss: -7.971881866455078, valid_loss: -4.7217230796813965\n",
      "epoch: 1572, train_loss: -7.965651035308838, valid_loss: -4.662468433380127\n",
      "epoch: 1573, train_loss: -7.960108757019043, valid_loss: -4.722218990325928\n",
      "epoch: 1574, train_loss: -7.966238975524902, valid_loss: -4.673816680908203\n",
      "epoch: 1575, train_loss: -7.971804618835449, valid_loss: -4.687020778656006\n",
      "epoch: 1576, train_loss: -7.968532085418701, valid_loss: -4.688347339630127\n",
      "epoch: 1577, train_loss: -7.966475009918213, valid_loss: -4.67643404006958\n",
      "epoch: 1578, train_loss: -7.972144603729248, valid_loss: -4.677063465118408\n",
      "epoch: 1579, train_loss: -7.977100372314453, valid_loss: -4.68380880355835\n",
      "epoch: 1580, train_loss: -7.975549697875977, valid_loss: -4.662405967712402\n",
      "epoch: 1581, train_loss: -7.972939968109131, valid_loss: -4.671847820281982\n",
      "epoch: 1582, train_loss: -7.974590301513672, valid_loss: -4.671324729919434\n",
      "epoch: 1583, train_loss: -7.977965831756592, valid_loss: -4.644659519195557\n",
      "epoch: 1584, train_loss: -7.9781718254089355, valid_loss: -4.684698581695557\n",
      "epoch: 1585, train_loss: -7.9759602546691895, valid_loss: -4.625840663909912\n",
      "epoch: 1586, train_loss: -7.975425720214844, valid_loss: -4.678908348083496\n",
      "epoch: 1587, train_loss: -7.977377414703369, valid_loss: -4.629047870635986\n",
      "epoch: 1588, train_loss: -7.979390621185303, valid_loss: -4.661694049835205\n",
      "epoch: 1589, train_loss: -7.979316711425781, valid_loss: -4.632381439208984\n",
      "epoch: 1590, train_loss: -7.978042125701904, valid_loss: -4.651149272918701\n",
      "epoch: 1591, train_loss: -7.977410316467285, valid_loss: -4.625783920288086\n",
      "epoch: 1592, train_loss: -7.978316307067871, valid_loss: -4.6536126136779785\n",
      "epoch: 1593, train_loss: -7.979743957519531, valid_loss: -4.612504005432129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1594, train_loss: -7.980814456939697, valid_loss: -4.660736560821533\n",
      "epoch: 1595, train_loss: -7.981084823608398, valid_loss: -4.603011608123779\n",
      "epoch: 1596, train_loss: -7.9814252853393555, valid_loss: -4.659726619720459\n",
      "epoch: 1597, train_loss: -7.982375144958496, valid_loss: -4.605841159820557\n",
      "epoch: 1598, train_loss: -7.984280586242676, valid_loss: -4.650459289550781\n",
      "epoch: 1599, train_loss: -7.986572742462158, valid_loss: -4.617337703704834\n",
      "epoch: 1600, train_loss: -7.988687992095947, valid_loss: -4.634476184844971\n",
      "epoch: 1601, train_loss: -7.9900970458984375, valid_loss: -4.630174160003662\n",
      "epoch: 1602, train_loss: -7.990736961364746, valid_loss: -4.618719577789307\n",
      "epoch: 1603, train_loss: -7.990859031677246, valid_loss: -4.637217044830322\n",
      "epoch: 1604, train_loss: -7.990828037261963, valid_loss: -4.608768939971924\n",
      "epoch: 1605, train_loss: -7.990992069244385, valid_loss: -4.6371588706970215\n",
      "epoch: 1606, train_loss: -7.991444110870361, valid_loss: -4.605284690856934\n",
      "epoch: 1607, train_loss: -7.992217540740967, valid_loss: -4.6322855949401855\n",
      "epoch: 1608, train_loss: -7.9931464195251465, valid_loss: -4.607222557067871\n",
      "epoch: 1609, train_loss: -7.994116306304932, valid_loss: -4.624203205108643\n",
      "epoch: 1610, train_loss: -7.994996070861816, valid_loss: -4.610618591308594\n",
      "epoch: 1611, train_loss: -7.995731830596924, valid_loss: -4.616126537322998\n",
      "epoch: 1612, train_loss: -7.9962992668151855, valid_loss: -4.613640308380127\n",
      "epoch: 1613, train_loss: -7.996708869934082, valid_loss: -4.608318328857422\n",
      "epoch: 1614, train_loss: -7.996962070465088, valid_loss: -4.615528583526611\n",
      "epoch: 1615, train_loss: -7.997030735015869, valid_loss: -4.6017327308654785\n",
      "epoch: 1616, train_loss: -7.996857643127441, valid_loss: -4.616142272949219\n",
      "epoch: 1617, train_loss: -7.996214866638184, valid_loss: -4.595485210418701\n",
      "epoch: 1618, train_loss: -7.99481201171875, valid_loss: -4.615383625030518\n",
      "epoch: 1619, train_loss: -7.991684913635254, valid_loss: -4.586582183837891\n",
      "epoch: 1620, train_loss: -7.985696315765381, valid_loss: -4.6085944175720215\n",
      "epoch: 1621, train_loss: -7.972838401794434, valid_loss: -4.566954612731934\n",
      "epoch: 1622, train_loss: -7.9491496086120605, valid_loss: -4.574861526489258\n",
      "epoch: 1623, train_loss: -7.899609565734863, valid_loss: -4.508905410766602\n",
      "epoch: 1624, train_loss: -7.81960916519165, valid_loss: -4.458215236663818\n",
      "epoch: 1625, train_loss: -7.685747146606445, valid_loss: -4.404460906982422\n",
      "epoch: 1626, train_loss: -7.571686267852783, valid_loss: -4.390753746032715\n",
      "epoch: 1627, train_loss: -7.581626892089844, valid_loss: -4.515756130218506\n",
      "epoch: 1628, train_loss: -7.788873672485352, valid_loss: -4.65798807144165\n",
      "epoch: 1629, train_loss: -7.975193977355957, valid_loss: -4.574221134185791\n",
      "epoch: 1630, train_loss: -7.95033597946167, valid_loss: -4.583865642547607\n",
      "epoch: 1631, train_loss: -7.862087249755859, valid_loss: -4.676295757293701\n",
      "epoch: 1632, train_loss: -7.8980536460876465, valid_loss: -4.582779407501221\n",
      "epoch: 1633, train_loss: -7.930825233459473, valid_loss: -4.704237461090088\n",
      "epoch: 1634, train_loss: -7.9211835861206055, valid_loss: -4.695084095001221\n",
      "epoch: 1635, train_loss: -7.972999095916748, valid_loss: -4.646158218383789\n",
      "epoch: 1636, train_loss: -7.955451488494873, valid_loss: -4.744599342346191\n",
      "epoch: 1637, train_loss: -7.907401084899902, valid_loss: -4.689028263092041\n",
      "epoch: 1638, train_loss: -7.976300239562988, valid_loss: -4.7099432945251465\n",
      "epoch: 1639, train_loss: -7.980929851531982, valid_loss: -4.750410556793213\n",
      "epoch: 1640, train_loss: -7.945045471191406, valid_loss: -4.681702136993408\n",
      "epoch: 1641, train_loss: -7.97044038772583, valid_loss: -4.738848686218262\n",
      "epoch: 1642, train_loss: -7.973641872406006, valid_loss: -4.7411417961120605\n",
      "epoch: 1643, train_loss: -7.978677272796631, valid_loss: -4.700425148010254\n",
      "epoch: 1644, train_loss: -7.986907005310059, valid_loss: -4.742263317108154\n",
      "epoch: 1645, train_loss: -7.96845817565918, valid_loss: -4.725458145141602\n",
      "epoch: 1646, train_loss: -7.9830546379089355, valid_loss: -4.728363513946533\n",
      "epoch: 1647, train_loss: -8.002241134643555, valid_loss: -4.727634906768799\n",
      "epoch: 1648, train_loss: -7.982449531555176, valid_loss: -4.709639549255371\n",
      "epoch: 1649, train_loss: -7.983086585998535, valid_loss: -4.738617420196533\n",
      "epoch: 1650, train_loss: -8.002506256103516, valid_loss: -4.703052520751953\n",
      "epoch: 1651, train_loss: -7.997470855712891, valid_loss: -4.705770492553711\n",
      "epoch: 1652, train_loss: -7.991148948669434, valid_loss: -4.7241058349609375\n",
      "epoch: 1653, train_loss: -8.000490188598633, valid_loss: -4.682624340057373\n",
      "epoch: 1654, train_loss: -8.00342082977295, valid_loss: -4.711790561676025\n",
      "epoch: 1655, train_loss: -7.998870849609375, valid_loss: -4.685510158538818\n",
      "epoch: 1656, train_loss: -8.002076148986816, valid_loss: -4.678415298461914\n",
      "epoch: 1657, train_loss: -8.007691383361816, valid_loss: -4.705592632293701\n",
      "epoch: 1658, train_loss: -8.006606101989746, valid_loss: -4.645854473114014\n",
      "epoch: 1659, train_loss: -8.003188133239746, valid_loss: -4.685826301574707\n",
      "epoch: 1660, train_loss: -8.004694938659668, valid_loss: -4.66452169418335\n",
      "epoch: 1661, train_loss: -8.010541915893555, valid_loss: -4.649034023284912\n",
      "epoch: 1662, train_loss: -8.013365745544434, valid_loss: -4.676948547363281\n",
      "epoch: 1663, train_loss: -8.010965347290039, valid_loss: -4.62717342376709\n",
      "epoch: 1664, train_loss: -8.007475852966309, valid_loss: -4.663309097290039\n",
      "epoch: 1665, train_loss: -8.007271766662598, valid_loss: -4.628609657287598\n",
      "epoch: 1666, train_loss: -8.011327743530273, valid_loss: -4.650590896606445\n",
      "epoch: 1667, train_loss: -8.015832901000977, valid_loss: -4.640994071960449\n",
      "epoch: 1668, train_loss: -8.017779350280762, valid_loss: -4.628924369812012\n",
      "epoch: 1669, train_loss: -8.016671180725098, valid_loss: -4.646852970123291\n",
      "epoch: 1670, train_loss: -8.014490127563477, valid_loss: -4.613821506500244\n",
      "epoch: 1671, train_loss: -8.013510704040527, valid_loss: -4.645167827606201\n",
      "epoch: 1672, train_loss: -8.014309883117676, valid_loss: -4.610668182373047\n",
      "epoch: 1673, train_loss: -8.016613960266113, valid_loss: -4.641648769378662\n",
      "epoch: 1674, train_loss: -8.018991470336914, valid_loss: -4.616708278656006\n",
      "epoch: 1675, train_loss: -8.020618438720703, valid_loss: -4.628268241882324\n",
      "epoch: 1676, train_loss: -8.021158218383789, valid_loss: -4.627294540405273\n",
      "epoch: 1677, train_loss: -8.020915031433105, valid_loss: -4.613632678985596\n",
      "epoch: 1678, train_loss: -8.020544052124023, valid_loss: -4.631293773651123\n",
      "epoch: 1679, train_loss: -8.020565032958984, valid_loss: -4.6078386306762695\n",
      "epoch: 1680, train_loss: -8.021286010742188, valid_loss: -4.630361557006836\n",
      "epoch: 1681, train_loss: -8.022470474243164, valid_loss: -4.609626293182373\n",
      "epoch: 1682, train_loss: -8.023717880249023, valid_loss: -4.622470378875732\n",
      "epoch: 1683, train_loss: -8.024587631225586, valid_loss: -4.616781234741211\n",
      "epoch: 1684, train_loss: -8.024914741516113, valid_loss: -4.611697196960449\n",
      "epoch: 1685, train_loss: -8.024871826171875, valid_loss: -4.619519233703613\n",
      "epoch: 1686, train_loss: -8.02480697631836, valid_loss: -4.6071624755859375\n",
      "epoch: 1687, train_loss: -8.025017738342285, valid_loss: -4.616912841796875\n",
      "epoch: 1688, train_loss: -8.025567054748535, valid_loss: -4.608579635620117\n",
      "epoch: 1689, train_loss: -8.026328086853027, valid_loss: -4.611006736755371\n",
      "epoch: 1690, train_loss: -8.027052879333496, valid_loss: -4.614049911499023\n",
      "epoch: 1691, train_loss: -8.027580261230469, valid_loss: -4.602272033691406\n",
      "epoch: 1692, train_loss: -8.027887344360352, valid_loss: -4.618439197540283\n",
      "epoch: 1693, train_loss: -8.028066635131836, valid_loss: -4.596902370452881\n",
      "epoch: 1694, train_loss: -8.02825927734375, valid_loss: -4.618658542633057\n",
      "epoch: 1695, train_loss: -8.028509140014648, valid_loss: -4.595128536224365\n",
      "epoch: 1696, train_loss: -8.028828620910645, valid_loss: -4.617115497589111\n",
      "epoch: 1697, train_loss: -8.029065132141113, valid_loss: -4.594763278961182\n",
      "epoch: 1698, train_loss: -8.02910327911377, valid_loss: -4.6146063804626465\n",
      "epoch: 1699, train_loss: -8.028677940368652, valid_loss: -4.5929999351501465\n",
      "epoch: 1700, train_loss: -8.027588844299316, valid_loss: -4.61411714553833\n",
      "epoch: 1701, train_loss: -8.025181770324707, valid_loss: -4.583478927612305\n",
      "epoch: 1702, train_loss: -8.02084732055664, valid_loss: -4.61791467666626\n",
      "epoch: 1703, train_loss: -8.012158393859863, valid_loss: -4.559237480163574\n",
      "epoch: 1704, train_loss: -7.997673034667969, valid_loss: -4.621358394622803\n",
      "epoch: 1705, train_loss: -7.968878269195557, valid_loss: -4.507911682128906\n",
      "epoch: 1706, train_loss: -7.9294281005859375, valid_loss: -4.608704566955566\n",
      "epoch: 1707, train_loss: -7.860543251037598, valid_loss: -4.443263053894043\n",
      "epoch: 1708, train_loss: -7.818868637084961, valid_loss: -4.5772175788879395\n",
      "epoch: 1709, train_loss: -7.785090923309326, valid_loss: -4.489033222198486\n",
      "epoch: 1710, train_loss: -7.852089881896973, valid_loss: -4.5669965744018555\n",
      "epoch: 1711, train_loss: -7.93113899230957, valid_loss: -4.636879920959473\n",
      "epoch: 1712, train_loss: -7.996219158172607, valid_loss: -4.562771797180176\n",
      "epoch: 1713, train_loss: -8.010022163391113, valid_loss: -4.693657398223877\n",
      "epoch: 1714, train_loss: -7.978597164154053, valid_loss: -4.568721294403076\n",
      "epoch: 1715, train_loss: -7.964150905609131, valid_loss: -4.656710147857666\n",
      "epoch: 1716, train_loss: -7.982409954071045, valid_loss: -4.65778923034668\n",
      "epoch: 1717, train_loss: -8.0169677734375, valid_loss: -4.61732292175293\n",
      "epoch: 1718, train_loss: -8.021138191223145, valid_loss: -4.715766429901123\n",
      "epoch: 1719, train_loss: -7.998579502105713, valid_loss: -4.6148200035095215\n",
      "epoch: 1720, train_loss: -7.996644973754883, valid_loss: -4.685083866119385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1721, train_loss: -8.018292427062988, valid_loss: -4.695618152618408\n",
      "epoch: 1722, train_loss: -8.032397270202637, valid_loss: -4.639684200286865\n",
      "epoch: 1723, train_loss: -8.019756317138672, valid_loss: -4.724977970123291\n",
      "epoch: 1724, train_loss: -8.006900787353516, valid_loss: -4.654679775238037\n",
      "epoch: 1725, train_loss: -8.0211181640625, valid_loss: -4.691774845123291\n",
      "epoch: 1726, train_loss: -8.036285400390625, valid_loss: -4.712071895599365\n",
      "epoch: 1727, train_loss: -8.031012535095215, valid_loss: -4.648440837860107\n",
      "epoch: 1728, train_loss: -8.020245552062988, valid_loss: -4.720580101013184\n",
      "epoch: 1729, train_loss: -8.023825645446777, valid_loss: -4.668442249298096\n",
      "epoch: 1730, train_loss: -8.036261558532715, valid_loss: -4.6767168045043945\n",
      "epoch: 1731, train_loss: -8.037198066711426, valid_loss: -4.7033796310424805\n",
      "epoch: 1732, train_loss: -8.029606819152832, valid_loss: -4.647644519805908\n",
      "epoch: 1733, train_loss: -8.02968978881836, valid_loss: -4.694729328155518\n",
      "epoch: 1734, train_loss: -8.036591529846191, valid_loss: -4.66644287109375\n",
      "epoch: 1735, train_loss: -8.039581298828125, valid_loss: -4.661928653717041\n",
      "epoch: 1736, train_loss: -8.03579330444336, valid_loss: -4.677497863769531\n",
      "epoch: 1737, train_loss: -8.033629417419434, valid_loss: -4.653536319732666\n",
      "epoch: 1738, train_loss: -8.037827491760254, valid_loss: -4.665777206420898\n",
      "epoch: 1739, train_loss: -8.042243957519531, valid_loss: -4.664026260375977\n",
      "epoch: 1740, train_loss: -8.04184627532959, valid_loss: -4.64552640914917\n",
      "epoch: 1741, train_loss: -8.039008140563965, valid_loss: -4.660766124725342\n",
      "epoch: 1742, train_loss: -8.038619041442871, valid_loss: -4.646277904510498\n",
      "epoch: 1743, train_loss: -8.041370391845703, valid_loss: -4.639413356781006\n",
      "epoch: 1744, train_loss: -8.043538093566895, valid_loss: -4.661802291870117\n",
      "epoch: 1745, train_loss: -8.043109893798828, valid_loss: -4.618319034576416\n",
      "epoch: 1746, train_loss: -8.04173469543457, valid_loss: -4.66269588470459\n",
      "epoch: 1747, train_loss: -8.04173469543457, valid_loss: -4.617786884307861\n",
      "epoch: 1748, train_loss: -8.043652534484863, valid_loss: -4.648242950439453\n",
      "epoch: 1749, train_loss: -8.045560836791992, valid_loss: -4.628008842468262\n",
      "epoch: 1750, train_loss: -8.04616928100586, valid_loss: -4.631344795227051\n",
      "epoch: 1751, train_loss: -8.045625686645508, valid_loss: -4.632550239562988\n",
      "epoch: 1752, train_loss: -8.04512882232666, valid_loss: -4.624016761779785\n",
      "epoch: 1753, train_loss: -8.045513153076172, valid_loss: -4.626814842224121\n",
      "epoch: 1754, train_loss: -8.046611785888672, valid_loss: -4.6290669441223145\n",
      "epoch: 1755, train_loss: -8.047731399536133, valid_loss: -4.613674640655518\n",
      "epoch: 1756, train_loss: -8.04829216003418, valid_loss: -4.6375203132629395\n",
      "epoch: 1757, train_loss: -8.048210144042969, valid_loss: -4.6020355224609375\n",
      "epoch: 1758, train_loss: -8.047865867614746, valid_loss: -4.6411237716674805\n",
      "epoch: 1759, train_loss: -8.04758071899414, valid_loss: -4.596501350402832\n",
      "epoch: 1760, train_loss: -8.047661781311035, valid_loss: -4.639614582061768\n",
      "epoch: 1761, train_loss: -8.04790210723877, valid_loss: -4.595343589782715\n",
      "epoch: 1762, train_loss: -8.048253059387207, valid_loss: -4.634256839752197\n",
      "epoch: 1763, train_loss: -8.048334121704102, valid_loss: -4.596384525299072\n",
      "epoch: 1764, train_loss: -8.048151969909668, valid_loss: -4.627457618713379\n",
      "epoch: 1765, train_loss: -8.047445297241211, valid_loss: -4.5966715812683105\n",
      "epoch: 1766, train_loss: -8.04641056060791, valid_loss: -4.621227264404297\n",
      "epoch: 1767, train_loss: -8.044785499572754, valid_loss: -4.594696521759033\n",
      "epoch: 1768, train_loss: -8.042861938476562, valid_loss: -4.61550235748291\n",
      "epoch: 1769, train_loss: -8.040237426757812, valid_loss: -4.592770576477051\n",
      "epoch: 1770, train_loss: -8.037291526794434, valid_loss: -4.607737064361572\n",
      "epoch: 1771, train_loss: -8.0335111618042, valid_loss: -4.592472553253174\n",
      "epoch: 1772, train_loss: -8.02932357788086, valid_loss: -4.595714092254639\n",
      "epoch: 1773, train_loss: -8.024345397949219, valid_loss: -4.597914695739746\n",
      "epoch: 1774, train_loss: -8.018546104431152, valid_loss: -4.576905727386475\n",
      "epoch: 1775, train_loss: -8.012432098388672, valid_loss: -4.612337112426758\n",
      "epoch: 1776, train_loss: -8.0040864944458, valid_loss: -4.5526604652404785\n",
      "epoch: 1777, train_loss: -7.997698783874512, valid_loss: -4.633960247039795\n",
      "epoch: 1778, train_loss: -7.987802028656006, valid_loss: -4.5345964431762695\n",
      "epoch: 1779, train_loss: -7.987247467041016, valid_loss: -4.654757976531982\n",
      "epoch: 1780, train_loss: -7.98576021194458, valid_loss: -4.540534496307373\n",
      "epoch: 1781, train_loss: -8.000649452209473, valid_loss: -4.663627624511719\n",
      "epoch: 1782, train_loss: -8.0158052444458, valid_loss: -4.575714111328125\n",
      "epoch: 1783, train_loss: -8.036827087402344, valid_loss: -4.650333881378174\n",
      "epoch: 1784, train_loss: -8.051366806030273, valid_loss: -4.6178693771362305\n",
      "epoch: 1785, train_loss: -8.057860374450684, valid_loss: -4.620336055755615\n",
      "epoch: 1786, train_loss: -8.055642127990723, valid_loss: -4.6468024253845215\n",
      "epoch: 1787, train_loss: -8.048251152038574, valid_loss: -4.596181392669678\n",
      "epoch: 1788, train_loss: -8.041810989379883, valid_loss: -4.658298015594482\n",
      "epoch: 1789, train_loss: -8.039278030395508, valid_loss: -4.5977301597595215\n",
      "epoch: 1790, train_loss: -8.044139862060547, valid_loss: -4.653099536895752\n",
      "epoch: 1791, train_loss: -8.051412582397461, valid_loss: -4.621220588684082\n",
      "epoch: 1792, train_loss: -8.058229446411133, valid_loss: -4.633880615234375\n",
      "epoch: 1793, train_loss: -8.060914993286133, valid_loss: -4.644033908843994\n",
      "epoch: 1794, train_loss: -8.05938720703125, valid_loss: -4.614196300506592\n",
      "epoch: 1795, train_loss: -8.055946350097656, valid_loss: -4.652796745300293\n",
      "epoch: 1796, train_loss: -8.053215980529785, valid_loss: -4.609425067901611\n",
      "epoch: 1797, train_loss: -8.053566932678223, valid_loss: -4.646020412445068\n",
      "epoch: 1798, train_loss: -8.055990219116211, valid_loss: -4.621582508087158\n",
      "epoch: 1799, train_loss: -8.059420585632324, valid_loss: -4.6267547607421875\n",
      "epoch: 1800, train_loss: -8.061697006225586, valid_loss: -4.640671253204346\n",
      "epoch: 1801, train_loss: -8.061907768249512, valid_loss: -4.601994037628174\n",
      "epoch: 1802, train_loss: -8.060131072998047, valid_loss: -4.657471179962158\n",
      "epoch: 1803, train_loss: -8.05681037902832, valid_loss: -4.57785177230835\n",
      "epoch: 1804, train_loss: -8.052634239196777, valid_loss: -4.6698126792907715\n",
      "epoch: 1805, train_loss: -8.046130180358887, valid_loss: -4.549148082733154\n",
      "epoch: 1806, train_loss: -8.03622817993164, valid_loss: -4.674683094024658\n",
      "epoch: 1807, train_loss: -8.017023086547852, valid_loss: -4.501038551330566\n",
      "epoch: 1808, train_loss: -7.98858642578125, valid_loss: -4.6581315994262695\n",
      "epoch: 1809, train_loss: -7.9467010498046875, valid_loss: -4.466558933258057\n",
      "epoch: 1810, train_loss: -7.923784255981445, valid_loss: -4.632981777191162\n",
      "epoch: 1811, train_loss: -7.936370372772217, valid_loss: -4.582681179046631\n",
      "epoch: 1812, train_loss: -7.999819755554199, valid_loss: -4.606387615203857\n",
      "epoch: 1813, train_loss: -8.047362327575684, valid_loss: -4.704357147216797\n",
      "epoch: 1814, train_loss: -8.034865379333496, valid_loss: -4.5752763748168945\n",
      "epoch: 1815, train_loss: -8.001876831054688, valid_loss: -4.7041802406311035\n",
      "epoch: 1816, train_loss: -8.009861946105957, valid_loss: -4.629489421844482\n",
      "epoch: 1817, train_loss: -8.050138473510742, valid_loss: -4.661874771118164\n",
      "epoch: 1818, train_loss: -8.055006980895996, valid_loss: -4.656351566314697\n",
      "epoch: 1819, train_loss: -8.033299446105957, valid_loss: -4.647562026977539\n",
      "epoch: 1820, train_loss: -8.037285804748535, valid_loss: -4.6725616455078125\n",
      "epoch: 1821, train_loss: -8.061471939086914, valid_loss: -4.665400981903076\n",
      "epoch: 1822, train_loss: -8.060742378234863, valid_loss: -4.650298595428467\n",
      "epoch: 1823, train_loss: -8.044445037841797, valid_loss: -4.666676998138428\n",
      "epoch: 1824, train_loss: -8.047536849975586, valid_loss: -4.66864538192749\n",
      "epoch: 1825, train_loss: -8.061716079711914, valid_loss: -4.6363205909729\n",
      "epoch: 1826, train_loss: -8.059101104736328, valid_loss: -4.703948020935059\n",
      "epoch: 1827, train_loss: -8.04658317565918, valid_loss: -4.617705821990967\n",
      "epoch: 1828, train_loss: -8.04871654510498, valid_loss: -4.69228458404541\n",
      "epoch: 1829, train_loss: -8.057276725769043, valid_loss: -4.630919456481934\n",
      "epoch: 1830, train_loss: -8.055782318115234, valid_loss: -4.655653476715088\n",
      "epoch: 1831, train_loss: -8.044947624206543, valid_loss: -4.6218695640563965\n",
      "epoch: 1832, train_loss: -8.040328979492188, valid_loss: -4.659567832946777\n",
      "epoch: 1833, train_loss: -8.041648864746094, valid_loss: -4.595781326293945\n",
      "epoch: 1834, train_loss: -8.040915489196777, valid_loss: -4.694382190704346\n",
      "epoch: 1835, train_loss: -8.030905723571777, valid_loss: -4.553915023803711\n",
      "epoch: 1836, train_loss: -8.02464771270752, valid_loss: -4.703152179718018\n",
      "epoch: 1837, train_loss: -8.019601821899414, valid_loss: -4.551322937011719\n",
      "epoch: 1838, train_loss: -8.027351379394531, valid_loss: -4.673238277435303\n",
      "epoch: 1839, train_loss: -8.033143997192383, valid_loss: -4.587944984436035\n",
      "epoch: 1840, train_loss: -8.041167259216309, valid_loss: -4.625557899475098\n",
      "epoch: 1841, train_loss: -8.044340133666992, valid_loss: -4.62136173248291\n",
      "epoch: 1842, train_loss: -8.04627513885498, valid_loss: -4.590667247772217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1843, train_loss: -8.048065185546875, valid_loss: -4.647043228149414\n",
      "epoch: 1844, train_loss: -8.050050735473633, valid_loss: -4.577998161315918\n",
      "epoch: 1845, train_loss: -8.054679870605469, valid_loss: -4.658926963806152\n",
      "epoch: 1846, train_loss: -8.060160636901855, valid_loss: -4.595049858093262\n",
      "epoch: 1847, train_loss: -8.067030906677246, valid_loss: -4.650938987731934\n",
      "epoch: 1848, train_loss: -8.07266902923584, valid_loss: -4.624066352844238\n",
      "epoch: 1849, train_loss: -8.075925827026367, valid_loss: -4.631284236907959\n",
      "epoch: 1850, train_loss: -8.07604694366455, valid_loss: -4.643243312835693\n",
      "epoch: 1851, train_loss: -8.0737943649292, valid_loss: -4.613871097564697\n",
      "epoch: 1852, train_loss: -8.071100234985352, valid_loss: -4.650363445281982\n",
      "epoch: 1853, train_loss: -8.06958293914795, valid_loss: -4.612003803253174\n",
      "epoch: 1854, train_loss: -8.070428848266602, valid_loss: -4.6440534591674805\n",
      "epoch: 1855, train_loss: -8.072762489318848, valid_loss: -4.629603385925293\n",
      "epoch: 1856, train_loss: -8.075486183166504, valid_loss: -4.626905918121338\n",
      "epoch: 1857, train_loss: -8.077152252197266, valid_loss: -4.64718770980835\n",
      "epoch: 1858, train_loss: -8.07735824584961, valid_loss: -4.612056732177734\n",
      "epoch: 1859, train_loss: -8.076723098754883, valid_loss: -4.652921676635742\n",
      "epoch: 1860, train_loss: -8.076123237609863, valid_loss: -4.606727123260498\n",
      "epoch: 1861, train_loss: -8.07648754119873, valid_loss: -4.648281097412109\n",
      "epoch: 1862, train_loss: -8.077601432800293, valid_loss: -4.611780643463135\n",
      "epoch: 1863, train_loss: -8.0791015625, valid_loss: -4.634790420532227\n",
      "epoch: 1864, train_loss: -8.080248832702637, valid_loss: -4.622238636016846\n",
      "epoch: 1865, train_loss: -8.08073902130127, valid_loss: -4.619485855102539\n",
      "epoch: 1866, train_loss: -8.08064079284668, valid_loss: -4.627434253692627\n",
      "epoch: 1867, train_loss: -8.080307006835938, valid_loss: -4.611581325531006\n",
      "epoch: 1868, train_loss: -8.080097198486328, valid_loss: -4.624821662902832\n",
      "epoch: 1869, train_loss: -8.080132484436035, valid_loss: -4.611037731170654\n",
      "epoch: 1870, train_loss: -8.080378532409668, valid_loss: -4.61630392074585\n",
      "epoch: 1871, train_loss: -8.080533981323242, valid_loss: -4.6164751052856445\n",
      "epoch: 1872, train_loss: -8.080248832702637, valid_loss: -4.601377964019775\n",
      "epoch: 1873, train_loss: -8.079068183898926, valid_loss: -4.6265058517456055\n",
      "epoch: 1874, train_loss: -8.076153755187988, valid_loss: -4.579203128814697\n",
      "epoch: 1875, train_loss: -8.07060432434082, valid_loss: -4.636601448059082\n",
      "epoch: 1876, train_loss: -8.059113502502441, valid_loss: -4.543034076690674\n",
      "epoch: 1877, train_loss: -8.039868354797363, valid_loss: -4.637369155883789\n",
      "epoch: 1878, train_loss: -8.00045108795166, valid_loss: -4.472771644592285\n",
      "epoch: 1879, train_loss: -7.946833610534668, valid_loss: -4.596573352813721\n",
      "epoch: 1880, train_loss: -7.853856086730957, valid_loss: -4.394225120544434\n",
      "epoch: 1881, train_loss: -7.801414489746094, valid_loss: -4.514463424682617\n",
      "epoch: 1882, train_loss: -7.769744396209717, valid_loss: -4.475831031799316\n",
      "epoch: 1883, train_loss: -7.848264694213867, valid_loss: -4.508109092712402\n",
      "epoch: 1884, train_loss: -7.959169387817383, valid_loss: -4.673551559448242\n",
      "epoch: 1885, train_loss: -8.036136627197266, valid_loss: -4.563333988189697\n",
      "epoch: 1886, train_loss: -8.03044605255127, valid_loss: -4.678488254547119\n",
      "epoch: 1887, train_loss: -7.975038051605225, valid_loss: -4.572568416595459\n",
      "epoch: 1888, train_loss: -7.99068021774292, valid_loss: -4.658401966094971\n",
      "epoch: 1889, train_loss: -8.049928665161133, valid_loss: -4.703261852264404\n",
      "epoch: 1890, train_loss: -8.067668914794922, valid_loss: -4.600526809692383\n",
      "epoch: 1891, train_loss: -8.02608585357666, valid_loss: -4.71932315826416\n",
      "epoch: 1892, train_loss: -8.013294219970703, valid_loss: -4.653043746948242\n",
      "epoch: 1893, train_loss: -8.066205978393555, valid_loss: -4.66620397567749\n",
      "epoch: 1894, train_loss: -8.07711124420166, valid_loss: -4.7349371910095215\n",
      "epoch: 1895, train_loss: -8.039953231811523, valid_loss: -4.6430206298828125\n",
      "epoch: 1896, train_loss: -8.04586124420166, valid_loss: -4.714883327484131\n",
      "epoch: 1897, train_loss: -8.077089309692383, valid_loss: -4.712618350982666\n",
      "epoch: 1898, train_loss: -8.073556900024414, valid_loss: -4.668826103210449\n",
      "epoch: 1899, train_loss: -8.055951118469238, valid_loss: -4.721990585327148\n",
      "epoch: 1900, train_loss: -8.067232131958008, valid_loss: -4.691466331481934\n",
      "epoch: 1901, train_loss: -8.081613540649414, valid_loss: -4.694301605224609\n",
      "epoch: 1902, train_loss: -8.070130348205566, valid_loss: -4.691737174987793\n",
      "epoch: 1903, train_loss: -8.06529426574707, valid_loss: -4.6861186027526855\n",
      "epoch: 1904, train_loss: -8.0805025100708, valid_loss: -4.693336486816406\n",
      "epoch: 1905, train_loss: -8.08519458770752, valid_loss: -4.677577972412109\n",
      "epoch: 1906, train_loss: -8.074084281921387, valid_loss: -4.676080226898193\n",
      "epoch: 1907, train_loss: -8.072396278381348, valid_loss: -4.668301582336426\n",
      "epoch: 1908, train_loss: -8.082590103149414, valid_loss: -4.692107200622559\n",
      "epoch: 1909, train_loss: -8.085714340209961, valid_loss: -4.653207778930664\n",
      "epoch: 1910, train_loss: -8.08121109008789, valid_loss: -4.673671245574951\n",
      "epoch: 1911, train_loss: -8.08143424987793, valid_loss: -4.670088291168213\n",
      "epoch: 1912, train_loss: -8.086291313171387, valid_loss: -4.637940406799316\n",
      "epoch: 1913, train_loss: -8.086225509643555, valid_loss: -4.68731164932251\n",
      "epoch: 1914, train_loss: -8.08227252960205, valid_loss: -4.624143123626709\n",
      "epoch: 1915, train_loss: -8.083203315734863, valid_loss: -4.666504383087158\n",
      "early stop at: 1915; optimal epoch at: 1114\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "min_loss = 100.0 # starts from large number\n",
    "min_epoch = 0\n",
    "early_stop_epoch_size = 800\n",
    "\n",
    "f = open(PATH_LOG, \"w\")\n",
    "\n",
    "for epoch in range(4000):\n",
    "    optimizer.zero_grad()\n",
    "    pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi = model_mdn(pd_thetaphi_label_tensor)\n",
    "    # print(\"xx\", correlation_theta_phi)\n",
    "    loss = mdn_loss_fn (gt_thetaphi_label_tensor, pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.data.item())\n",
    "    \n",
    "    # Calculating validation data loss\n",
    "    total_valid_loss = 0.0\n",
    "    valid_pred = []\n",
    "    pi_valid, mu_theta_valid, mu_phi_valid, sigma_theta_valid, sigma_phi_valid, correlation_theta_phi_valid = model_mdn(pd_thetaphi_valid_label_tensor)\n",
    "    loss_valid = mdn_loss_fn(gt_thetaphi_valid_label_tensor,\n",
    "                       pi_valid, mu_theta_valid,\n",
    "                       mu_phi_valid,\n",
    "                       sigma_theta_valid,\n",
    "                       sigma_phi_valid,\n",
    "                       correlation_theta_phi_valid)\n",
    "    valid_loss.append(loss_valid.data.item())\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        # print(loss.data.tolist())\n",
    "        print(\"epoch: {}, train_loss: {}, valid_loss: {}\".format(epoch,\n",
    "                                                                 loss.data.item(),\n",
    "                                                                 loss_valid.data.item()))\n",
    "    \n",
    "    # check early stop condition\n",
    "    if (min_loss > loss_valid.data.item()):\n",
    "        min_loss = loss_valid.data.item()\n",
    "        min_epoch = epoch\n",
    "        torch.save(model_mdn.state_dict(), PATH)\n",
    "    if (epoch - min_epoch > early_stop_epoch_size):\n",
    "        log = \"early stop at: \" + str(epoch) + \"; optimal epoch at: \" + str(min_epoch)\n",
    "        print(log)\n",
    "        f.write(log)\n",
    "        f.close()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch = 30\n",
    "end_epoch = 1430\n",
    "train_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), train_loss[starting_epoch:end_epoch])\n",
    "valid_loss_curve, = plt.plot(list(range(starting_epoch, end_epoch)), valid_loss[starting_epoch:end_epoch])\n",
    "plt.legend([train_loss_curve, valid_loss_curve], ['Train Loss', 'Validation Loss'])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch: 629, train_loss: -7.17511510848999, valid_loss: -4.869997501373291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_mdn = MDN(2, n_hidden=20, n_gaussians=num_gaussians)\n",
    "load_model_mdn.load_state_dict(torch.load(PATH))\n",
    "load_model_mdn.eval()\n",
    "# mmodel = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating validation data loss\n",
    "pi_valid, mu_theta_valid, mu_phi_valid, sigma_theta_valid, sigma_phi_valid, correlation_theta_phi_valid = load_model_mdn(pd_thetaphi_valid_label_tensor)\n",
    "loss_valid = mdn_loss_fn(gt_thetaphi_valid_label_tensor,\n",
    "                         pi_valid, mu_theta_valid,\n",
    "                         mu_phi_valid,\n",
    "                         sigma_theta_valid,\n",
    "                         sigma_phi_valid,\n",
    "                         correlation_theta_phi_valid)\n",
    "print(loss_valid.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('4000_v2/without_normalization_with_earlystop/ng_10/train_loss.npy', np.asarray(train_loss))\n",
    "np.save('4000_v2/without_normalization_with_earlystop/ng_10/valid_loss.npy', np.asarray(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_valid = pi_valid.detach().numpy()\n",
    "mu_theta_valid = mu_theta_valid.detach().numpy()\n",
    "mu_phi_valid = mu_phi_valid.detach().numpy()\n",
    "sigma_theta_valid = sigma_theta_valid.detach().numpy()\n",
    "sigma_phi_valid = sigma_phi_valid.detach().numpy()\n",
    "correlation_theta_phi_valid = correlation_theta_phi_valid.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_theta_valid_label = pd_thetaphi_valid_label_tensor[:, 0].detach().numpy().tolist()\n",
    "pd_phi_valid_label = pd_thetaphi_valid_label_tensor[:, 1].detach().numpy().tolist()\n",
    "\n",
    "gt_theta_valid_label = gt_thetaphi_valid_label_tensor[:, 0].detach().numpy().tolist()\n",
    "gt_phi_valid_label = gt_thetaphi_valid_label_tensor[:, 1].detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_theta_valid_label)\n",
    "plt.plot(gt_theta_valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_theta_valid_label, label='pd')\n",
    "plt.plot(gt_theta_valid_label, label='gt')\n",
    "plt.plot(mu_theta_valid[:, 0], label='mu 1')\n",
    "plt.plot(mu_theta_valid[:, 1], label='mu 2')\n",
    "plt.plot(mu_theta_valid[:, 2], label='mu 3')\n",
    "plt.plot(mu_theta_valid[:, 3], label='mu 4')\n",
    "plt.plot(mu_theta_valid[:, 4], label='mu 5')\n",
    "plt.plot(mu_theta_valid[:, 5], label='mu 6')\n",
    "plt.plot(mu_theta_valid[:, 6], label='mu 7')\n",
    "plt.plot(mu_theta_valid[:, 7], label='mu 8')\n",
    "plt.plot(mu_theta_valid[:, 8], label='mu 9')\n",
    "plt.plot(mu_theta_valid[:, 9], label='mu 10')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(pd_theta_valid_label)\n",
    "np.argmax(pi_valid, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(size):\n",
    "    plt.plot(pi_valid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_theta_valid_label, label='pd')\n",
    "plt.plot(gt_theta_valid_label, label='gt')\n",
    "plt.plot(mu_theta_valid[:, 3], label='mu 4')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_phi_valid_label, label='pd')\n",
    "plt.plot(gt_phi_valid_label, label='gt')\n",
    "plt.plot(mu_phi_valid[:, 3], label='mu 4')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_valid_upper = mu_theta_valid[:, 3] + sigma_theta_valid[:, 3]\n",
    "theta_valid_lower = mu_theta_valid[:, 3] - sigma_theta_valid[:, 3]\n",
    "phi_valid_upper = mu_phi_valid[:, 3] + sigma_phi_valid[:, 3]\n",
    "phi_valid_lower = mu_phi_valid[:, 3] - sigma_phi_valid[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd_theta_valid_label, label='pd')\n",
    "plt.plot(gt_theta_valid_label, label='gt')\n",
    "plt.plot(mu_theta_valid[:, 6], label='mu 7')\n",
    "plt.plot(theta_valid_upper, label='upper')\n",
    "plt.plot(theta_valid_lower, label='lower')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu_theta, mu_phi, sigma_theta, sigma_phi, correlation_theta_phi = model_mdn(pd_thetaphi_label_tensor)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# print(\"sample size: \", size)\n",
    "# print(\"num of gaus: \", n_gaussians)\n",
    "# print(\"correlation size: \", correlation_x_y.shape)\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_theta, mu_phi), dim=2)\n",
    "# build covariance matrix with standard bivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 2, 2])\n",
    "cov[:, :, 0, 0] = sigma_theta**2\n",
    "cov[:, :, 1, 1] = sigma_phi**2\n",
    "cov[:, :, 1, 0] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "cov[:, :, 0, 1] = correlation_theta_phi*sigma_theta*sigma_phi\n",
    "\n",
    "m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = m.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "PPD = torch.zeros([size, 2])\n",
    "for i in range(pd.shape[0]):\n",
    "    PPD[i, :] = pd[i,k[i],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPD = PPD.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = mu_theta.detach().numpy()\n",
    "mu_phi = mu_phi.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_thetaphi_label = gt_thetaphi_label_tensor.numpy()\n",
    "pd_thetaphi_label = pd_thetaphi_label_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gt_thetaphi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_theta_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_thetaphi_label[:,0], label='gt', linestyle='--')\n",
    "plt.plot(pd_thetaphi_label[:,0], label='pd', linestyle='--')\n",
    "plt.plot(PPD[:,0], label='pd_mdn', linestyle='--')\n",
    "plt.plot(mu_theta[:,0], label='mu 1')\n",
    "plt.plot(mu_theta[:,1], label='mu 2')\n",
    "# plt.plot(mu_x[:,2], label='mu 3')\n",
    "# plt.plot(mu_x[:,3], label='mu 4')\n",
    "# plt.plot(mu_x[:,4], label='mu 5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_thetaphi_label[:,1], label='gt', linestyle='--')\n",
    "plt.plot(pd_thetaphi_label[:,1], label='pd', linestyle='--')\n",
    "plt.plot(PPD[:,1], label='pd_mdn', linestyle='--')\n",
    "plt.plot(mu_phi[:,0], label='mu 1')\n",
    "plt.plot(mu_phi[:,1], label='mu 2')\n",
    "# plt.plot(mu_x[:,2], label='mu 3')\n",
    "# plt.plot(mu_x[:,3], label='mu 4')\n",
    "# plt.plot(mu_x[:,4], label='mu 5')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mu_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta = mu_theta*180\n",
    "mu_phi = mu_phi*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_theta = sigma_theta*180\n",
    "sigma_phi = sigma_phi*360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label\n",
    "pd_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXYZ(theta, phi, d):\n",
    "    cos_theta = math.cos(theta * math.pi / 180.0 );\n",
    "    sin_theta = math.sin(theta * math.pi / 180.0 );\n",
    "    cos_phi   = math.cos(phi   * math.pi / 180.0 );\n",
    "    sin_phi   = math.sin(phi   * math.pi / 180.0 );\n",
    "\n",
    "    x = d * sin_theta * cos_phi;\n",
    "    y = d * sin_theta * sin_phi;\n",
    "    z = d * cos_theta;\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCircle(theta_list, phi_list, d):\n",
    "    size = len(theta_list)\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    for i in range(size):\n",
    "        x, y, z = getXYZ(theta_list[i], phi_list[i], d)\n",
    "        # print(math.sqrt(x**2 + y**2 + z**2))\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "        z_list.append(z)\n",
    "        \n",
    "        \n",
    "    return x_list, y_list, z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhi(theta, sigma_theta, sigma_phi):\n",
    "    return math.sqrt((1-theta*theta/sigma_theta/sigma_theta)*sigma_phi*sigma_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# add origin\n",
    "ax.scatter3D([0], [0], [0], cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_x = []\n",
    "cam_y = []\n",
    "cam_z = []\n",
    "\n",
    "for i in range(10):\n",
    "    index = i\n",
    "    selected_mean_theta = mu_theta[index, 1]\n",
    "    selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "    selected_mean_phi = mu_phi[index, 1]\n",
    "    selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "    half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "    theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "                  selected_mean_theta + selected_sigma_theta/2,\n",
    "                  selected_mean_theta,\n",
    "                  selected_mean_theta - selected_sigma_theta/2,\n",
    "                  selected_mean_theta - selected_sigma_theta,\n",
    "                  selected_mean_theta - selected_sigma_theta/2,\n",
    "                  selected_mean_theta,\n",
    "                  selected_mean_theta + selected_sigma_theta/2,\n",
    "                  selected_mean_theta + selected_sigma_theta]\n",
    "    phi_list   = [selected_mean_phi,\n",
    "                  selected_mean_phi - half_phi,\n",
    "                  selected_mean_phi - selected_sigma_phi,\n",
    "                  selected_mean_phi - half_phi,\n",
    "                  selected_mean_phi,\n",
    "                  selected_mean_phi + half_phi,\n",
    "                  selected_mean_phi + selected_sigma_phi,\n",
    "                  selected_mean_phi + half_phi,\n",
    "                  selected_mean_phi]\n",
    "\n",
    "    x = pd_label[index, 0]\n",
    "    y = pd_label[index, 1]\n",
    "    z = pd_label[index, 2]\n",
    "    \n",
    "    cam_x.append(x)\n",
    "    cam_y.append(y)\n",
    "    cam_z.append(z)\n",
    "\n",
    "    d = math.sqrt(x**2 + y**2 + z**2)\n",
    "    # print(\"d\", d)\n",
    "\n",
    "    x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "    ax.plot3D(x_list, y_list, z_list, 'red')\n",
    "    # ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "    # ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot3D(cam_x, cam_y, cam_z, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "selected_mean_theta = mu_theta[index, 1]\n",
    "selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "selected_mean_phi = mu_phi[index, 1]\n",
    "selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta - selected_sigma_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta + selected_sigma_theta]\n",
    "phi_list   = [selected_mean_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi - selected_sigma_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi + selected_sigma_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi]\n",
    "\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "selected_mean_theta = mu_theta[index, 1]\n",
    "selected_sigma_theta = sigma_theta[index, 1] * 3\n",
    "selected_mean_phi = mu_phi[index, 1]\n",
    "selected_sigma_phi = sigma_phi[index, 1] * 3\n",
    "\n",
    "half_phi = getPhi(selected_sigma_theta/2, selected_sigma_theta, selected_sigma_phi)\n",
    "\n",
    "theta_list = [selected_mean_theta + selected_sigma_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta - selected_sigma_theta,\n",
    "              selected_mean_theta - selected_sigma_theta/2,\n",
    "              selected_mean_theta,\n",
    "              selected_mean_theta + selected_sigma_theta/2,\n",
    "              selected_mean_theta + selected_sigma_theta]\n",
    "phi_list   = [selected_mean_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi - selected_sigma_phi,\n",
    "              selected_mean_phi - half_phi,\n",
    "              selected_mean_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi + selected_sigma_phi,\n",
    "              selected_mean_phi + half_phi,\n",
    "              selected_mean_phi]\n",
    "\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n",
    "\n",
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = math.sqrt(gt_label[0, 0]**2 + gt_label[0, 0]**2 + gt_label[0, 0]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "x = pd_label[index, 0]\n",
    "y = pd_label[index, 1]\n",
    "z = pd_label[index, 2]\n",
    "\n",
    "d = math.sqrt(x**2 + y**2 + z**2)\n",
    "print(\"d\", d)\n",
    "\n",
    "x_list, y_list, z_list = getCircle(theta_list, phi_list, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot3D(x_list, y_list, z_list, 'gray')\n",
    "# ax.scatter3D(x_list, y_list, z_list, 'gray')\n",
    "ax.scatter3D(x, y, z, cmap='Red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = -0.18\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "pdf = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            # pdf[i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            pdf[i, j, k] = likelihood[0, 0]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = 0.01\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "pdf_1 = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            # pdf_![i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            pdf_1[i, j, k] = likelihood[0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "radius = 0.2\n",
    "density = 40 # sample size alone radius\n",
    "# radius = 0.01\n",
    "# density = 20 # sample size alone radius\n",
    "x_center, y_center, z_center = pd_label[index]\n",
    "\n",
    "s_x = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_y = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "s_z = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "\n",
    "x_min = x_center - radius\n",
    "y_min = y_center - radius\n",
    "z_min = z_center - radius\n",
    "unit_size = radius/density\n",
    "\n",
    "for i in range(density*2 + 1):\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            s_x[i, j, k] = x_min + i*unit_size\n",
    "            s_y[i, j, k] = y_min + j*unit_size\n",
    "            s_z[i, j, k] = z_min + k*unit_size\n",
    "            \n",
    "\n",
    "\n",
    "pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z = model_mdn(pd_label_tensor[index])\n",
    "pi = pi.unsqueeze(0)\n",
    "mu_x = mu_x.unsqueeze(0)\n",
    "mu_y = mu_y.unsqueeze(0)\n",
    "mu_z = mu_z.unsqueeze(0)\n",
    "sigma_x = sigma_x.unsqueeze(0)\n",
    "sigma_y = sigma_y.unsqueeze(0)\n",
    "sigma_z = sigma_z.unsqueeze(0)\n",
    "# print(pi.shape)\n",
    "# print(mu_x.shape)\n",
    "# print(mu_y.shape)\n",
    "# print(mu_z.shape)\n",
    "# print(sigma_x.shape)\n",
    "# print(sigma_y.shape)dd\n",
    "# print(sigma_z.shape)\n",
    "\n",
    "size = pi.shape[0]\n",
    "n_gaussians = pi.shape[1]\n",
    "# build mean matrix\n",
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)\n",
    "# print(mean.shape)\n",
    "# build covariance matrix with standard trivariate normal distribution\n",
    "cov = torch.zeros([size, n_gaussians, 3, 3])\n",
    "cov[:, :, 0, 0] = sigma_x**2\n",
    "cov[:, :, 1, 1] = sigma_y**2\n",
    "cov[:, :, 2, 2] = sigma_z**2\n",
    "# print(cov.shape)\n",
    "\n",
    "# mean[0, 0] = -0.18\n",
    "# mean[0, 1] = 0\n",
    "# mean[0, 2] = -0.01\n",
    "\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "        \n",
    "pdf_all = np.zeros([density*2 + 1, density*2 + 1, density*2 + 1])\n",
    "for i in range(density*2 + 1):\n",
    "    print(i)\n",
    "    for j in range(density*2 + 1):\n",
    "        for k in range(density*2 + 1):\n",
    "            likelihood = torch.exp(normal_3d.log_prob(torch.tensor([[s_x[i, j, k], s_y[i, j, k], s_z[i, j, k]]])))\n",
    "            # get mixture sum\n",
    "            pdf_all[i, j, k] = torch.sum(likelihood * pi, dim=1)\n",
    "            # pdf_4[i, j, k] = likelihood[0, 4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd_label_tensor[0])\n",
    "print(pi[0])\n",
    "print(mu_x[0])\n",
    "print(mu_y[0])\n",
    "print(mu_z[0])\n",
    "print(sigma_x[0])\n",
    "print(sigma_y[0])\n",
    "print(sigma_z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "src = mlab.pipeline.scalar_field(pdf)\n",
    "mlab.pipeline.iso_surface(src, contours=[0.1, ], opacity=0.3, color = (0.8,0.0,0.0))\n",
    "# mlab.pipeline.iso_surface(src, contours=[pdf.max()-0.1*pdf.ptp(), ],)\n",
    "\n",
    "src_1 = mlab.pipeline.scalar_field(pdf_1)\n",
    "mlab.pipeline.iso_surface(src_1, contours=[0.1, ], opacity=0.3, color = (0.8,0.0,0.0))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)\n",
    "\n",
    "# src_half = mlab.pipeline.scalar_field(pdf_half)\n",
    "# mlab.pipeline.iso_surface(src_half, contours=[0.1, ], opacity=0.3, color = (0.0,0.8,0.0))\n",
    "# mlab.pipeline.iso_surface(src, contours=[pdf.max()-0.1*pdf.ptp(), ],)\n",
    "\n",
    "# src_1_half = mlab.pipeline.scalar_field(pdf_1_half)\n",
    "# mlab.pipeline.iso_surface(src_1_half, contours=[0.1, ], opacity=0.3, color = (0.0,0.8,0.0))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)\n",
    "\n",
    "src_all = mlab.pipeline.scalar_field(pdf_all)\n",
    "mlab.pipeline.iso_surface(src_all, contours=[0.1, ], opacity=0.3, color = (0.0,0.0,0.8))\n",
    "# mlab.pipeline.iso_surface(src_1, contours=[pdf_1.max()-0.1*pdf_1.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.1318e-02 + 3.6224e-01 + 9.2028e-07 + 1.2253e-02 + 1.7447e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(s_x.flatten(), s_y.flatten(), s_z.flatten(), c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.1585, 0.1787, 0.1571, 0.3795, 0.1261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.0165, 0.0245, 0.1137, 2.8152, 0.0082])\n",
    "b = torch.tensor([0.0339, 0.0643, 0.1532, 0.5262, 0.2225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a*b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:, [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.stack((a, b), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(7) / (math.exp(1) + math.exp(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mu_x, mu_y, mu_z), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:, :, 0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gaussians = 5\n",
    "size = 14000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.unsqueeze(0).repeat(n_gaussians, 1, 1).unsqueeze(0).repeat(size, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.repeat(5, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = e.repeat(10, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[0, :, 0], input_tensor[0, :, 1], input_tensor[0, :, 2], 'gray')\n",
    "ax.scatter(gt_label_tensor[0,0], gt_label_tensor[0,1], gt_label_tensor[0,2], 'green')\n",
    "ax.scatter(pd_label_tensor[0,0], pd_label_tensor[0,1], pd_label_tensor[0,2], 'red')\n",
    "# ax.plot3D(gt_label_tensor[0:2,0], gt_label_tensor[0:2,1], gt_label_tensor[0:2,2], 'green')\n",
    "# ax.plot3D(pd_label_tensor[0:2,0], pd_label_tensor[0:2,1], pd_label_tensor[0:2,2], 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "# ax.plot3D(test[:,0], test[:,1], test[:,2], 'gray')\n",
    "ax.plot3D(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], 'gray')\n",
    "ax.scatter(input_tensor[:3, 0, 0], input_tensor[:3, 0, 1], input_tensor[:3, 0, 2], c = 'gray')\n",
    "\n",
    "ax.plot3D(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], 'green')\n",
    "ax.scatter(input_tensor[2:6, 0, 0], input_tensor[2:6, 0, 1], input_tensor[2:6, 0, 2], c = 'green')\n",
    "\n",
    "ax.plot3D(gt_label_tensor[0:3,0], gt_label_tensor[0:3,1], gt_label_tensor[0:3,2], 'black')\n",
    "ax.plot3D(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], 'red')\n",
    "ax.scatter(pd_label_tensor[0:3,0], pd_label_tensor[0:3,1], pd_label_tensor[0:3,2], c = 'red')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pd = pd_label_tensor[:, 0].reshape(-1,1)\n",
    "y_pd = pd_label_tensor[:, 1].reshape(-1,1)\n",
    "z_pd = pd_label_tensor[:, 2].reshape(-1,1)\n",
    "\n",
    "x_gt = gt_label_tensor[:, 0].reshape(-1,1)\n",
    "y_gt = gt_label_tensor[:, 1].reshape(-1,1)\n",
    "z_gt = gt_label_tensor[:, 2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt = x_gt.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_gt)\n",
    "plt.plot(x_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply MDN on x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 14985\n",
    "\n",
    "x_data = torch.Tensor(x_pd)\n",
    "y_data = torch.Tensor(x_gt)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_data, y_data, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.z_h = nn.Sequential(\n",
    "            nn.Linear(1, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.z_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_mu = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.z_sigma = nn.Linear(n_hidden, n_gaussians)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_h = self.z_h(x)\n",
    "        pi = F.softmax(self.z_pi(z_h), -1)\n",
    "        mu = self.z_mu(z_h)\n",
    "        sigma = torch.exp(self.z_sigma(z_h))\n",
    "        return pi, mu, sigma\n",
    "\n",
    "model = MDN(n_hidden=20, n_gaussians=5)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, mu, sigma, pi):\n",
    "    m = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    loss = torch.exp(m.log_prob(y))\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "    pi, mu, sigma = model(x_data)\n",
    "    loss = mdn_loss_fn(y_data, mu, sigma, pi)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, sigma = model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the highest index of the 5 elements in each row\n",
    "k = torch.multinomial(pi, 1).view(-1)\n",
    "y_pred = torch.normal(mu, sigma)[np.arange(n_samples), k].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "# lt.scatter(x_data, y_data, alpha=0.4)\n",
    "# plt.scatter(x_data, y_pred, alpha=0.8, color='red')\n",
    "# plt.scatter(x_test[0], y_pred[0], alpha=0.4, color='yellow')\n",
    "plt.scatter(x_data, mu[:,0].detach(), label='mean 0')\n",
    "plt.scatter(x_data, mu[:,1].detach(), label='mean 1')\n",
    "plt.scatter(x_data, mu[:,2].detach(), label='mean 2')\n",
    "plt.scatter(x_data, mu[:,3].detach(), label='mean 3')\n",
    "plt.scatter(x_data, mu[:,4].detach(), label='mean 4')\n",
    "# plt.plot(x_test.numpy(), mu[:,0].detach().numpy())\n",
    "# plt.scatter(x_data[:20], y_data[:20], alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pi.detach().numpy()\n",
    "mu = mu.detach().numpy()\n",
    "sigma = sigma.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.zeros((n_samples, 100))\n",
    "x = np.linspace(-2, 2, 100)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    each = []\n",
    "    for j in range(5):\n",
    "        if (j == 0):\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = y\n",
    "        else:\n",
    "            y = [pi[i, j] * ele for ele in scipy.stats.norm.pdf(x, mu[i, j],sigma[i, j])]\n",
    "            each.append(y)\n",
    "            cumulated_y = [p + q for p, q in zip(cumulated_y, y)]\n",
    "    p[i, :] = cumulated_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "for i in range(0,n_samples):\n",
    "    ax.plot3D([x_data[i, 0]]*100, x, p[i, :])\n",
    "\n",
    "ax.set_xlabel('index')\n",
    "ax.set_ylabel('x')\n",
    "ax.set_zlabel('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply MDN on 3D projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1_1 = torch.Tensor([1, 1, 1])\n",
    "mean_1_2 = torch.Tensor([2, 2, 2])\n",
    "mean_1_3 = torch.Tensor([3, 3, 3])\n",
    "mean_1_4 = torch.Tensor([4, 4, 4])\n",
    "mean_1_5 = torch.Tensor([5, 5, 5])\n",
    "\n",
    "mean_2_1 = torch.Tensor([6, 6, 6])\n",
    "mean_2_2 = torch.Tensor([7, 7, 7])\n",
    "mean_2_3 = torch.Tensor([8, 8, 8])\n",
    "mean_2_4 = torch.Tensor([9, 9, 9])\n",
    "mean_2_5 = torch.Tensor([10, 10, 10])\n",
    "\n",
    "sigma_1 = 1\n",
    "sigma_2 = 1\n",
    "sigma_3 = 1\n",
    "cov_1_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 3\n",
    "sigma_2 = 3\n",
    "sigma_3 = 3\n",
    "cov_1_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov_1_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 5\n",
    "sigma_2 = 5\n",
    "sigma_3 = 5\n",
    "cov_1_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1_5[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 6\n",
    "sigma_2 = 6\n",
    "sigma_3 = 6\n",
    "cov_2_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "\n",
    "sigma_1 = 7\n",
    "sigma_2 = 7\n",
    "sigma_3 = 7\n",
    "cov_2_2 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_2[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_2[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_2[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_2[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 8\n",
    "sigma_2 = 8\n",
    "sigma_3 = 8\n",
    "cov_2_3 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_3[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_3[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_3[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_3[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 9\n",
    "sigma_2 = 9\n",
    "sigma_3 = 9\n",
    "cov_2_4 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_4[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_4[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_4[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_4[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "                         \n",
    "sigma_1 = 10\n",
    "sigma_2 = 10\n",
    "sigma_3 = 10\n",
    "cov_2_5 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_2_5[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_2_5[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_2_5[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_2_5[1, 2] = rho_2_3*sigma_2*sigma_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = torch.stack((mean_1_1, mean_1_2, mean_1_3, mean_1_4, mean_1_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_2 = torch.stack((mean_2_1, mean_2_2, mean_2_3, mean_2_4, mean_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.stack((mean_1, mean_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1 = torch.stack((cov_1_1, cov_1_2, cov_1_3, cov_1_4, cov_1_5), dim=0)\n",
    "cov_2 = torch.stack((cov_2_1, cov_2_2, cov_2_3, cov_2_4, cov_2_5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov_1, cov_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack((y, y), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = mean_1.unsqueeze(0)\n",
    "cov_1 = cov_1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean_1, covariance_matrix=cov_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1], [2,2,2], [3,3,3], [4,4,4], [5,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,2,3]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = torch.stack((cov, cov_1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1, 1, 1],[1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 4\n",
    "sigma_2 = 4\n",
    "sigma_3 = 4\n",
    "cov = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.zeros(3)\n",
    "sigma_1 = 2\n",
    "sigma_2 = 2\n",
    "sigma_3 = 2\n",
    "cov_1 = torch.Tensor([[sigma_1**2, 0, 0], [0, sigma_2**2, 0], [0, 0, sigma_3**2]])\n",
    "rho_1_2 = 0.7\n",
    "rho_2_3 = 0.7\n",
    "rho_1_3 = 0.7\n",
    "cov_1[1, 0] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[2, 0] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[2, 1] = rho_2_3*sigma_2*sigma_3\n",
    "cov_1[0, 1] = rho_1_2*sigma_1*sigma_2\n",
    "cov_1[0, 2] = rho_1_3*sigma_1*sigma_3\n",
    "cov_1[1, 2] = rho_2_3*sigma_2*sigma_3\n",
    "normal_3d = torch.distributions.multivariate_normal.MultivariateNormal(mean, covariance_matrix=cov_1)\n",
    "\n",
    "y = torch.Tensor([[1, 2, 3]])\n",
    "torch.exp(normal_3d.log_prob(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mayavi import mlab\n",
    "\n",
    "s = np.zeros([21, 21, 21])\n",
    "for i in range(21):\n",
    "    for j in range(21):\n",
    "        for k in range(21):\n",
    "            s[i, j, k] = torch.exp(normal_3d.log_prob(torch.FloatTensor([i-10, j-10, k-10])))\n",
    "            \n",
    "src = mlab.pipeline.scalar_field(s)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.min()+0.1*s.ptp(), ], opacity=0.3)\n",
    "mlab.pipeline.iso_surface(src, contours=[s.max()-0.1*s.ptp(), ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 1001)\n",
    "y = np.linspace(0, 10, 1001)\n",
    "z = np.linspace(0, 10, 1001)\n",
    "x_p = x + 0.5 * np.random.randn(1001)\n",
    "y_p = y + 0.5 * np.random.randn(1001)\n",
    "z_p = z + 0.5 * np.random.randn(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(x, y, z)\n",
    "ax.scatter(x_p, y_p, z_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.l_h = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_pi = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_x = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_mu_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        self.l_sigma_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        # self.l_correlation_x_y = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_x_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        # self.l_correlation_y_z = nn.Linear(n_hidden, n_gaussians)\n",
    "        \n",
    "        self.l_correlation_x_y = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_x_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.l_correlation_y_z = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_gaussians),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.l_h(x)\n",
    "        # print(\"h\", h.shape)\n",
    "        # print(\"h[0]\", h[0, :])\n",
    "        pi = F.softmax(self.l_pi(h), -1)\n",
    "        # print(\"pi\", pi.shape)\n",
    "        # print(\"pi[0]\", pi[0, :])\n",
    "        mu_x = self.l_mu_x(h)\n",
    "        # print(\"mu_x\", pi.shape)\n",
    "        mu_y = self.l_mu_y(h)\n",
    "        # print(\"mu_y\", pi.shape)\n",
    "        mu_z = self.l_mu_z(h)\n",
    "        # print(\"mu_z\", pi.shape)\n",
    "        \n",
    "        # use exp to ensure positive range\n",
    "        sigma_x = torch.exp(self.l_sigma_x(h))\n",
    "        # print(\"sigma_x\", pi.shape)\n",
    "        sigma_y = torch.exp(self.l_sigma_y(h))\n",
    "        # print(\"sigma_y\", pi.shape)\n",
    "        sigma_z = torch.exp(self.l_sigma_z(h))\n",
    "        # print(\"sigma_z\", pi.shape)\n",
    "        \n",
    "        # use tanh to ensoure range of (-1, 1)\n",
    "        correlation_x_y = self.l_correlation_x_y(h)\n",
    "        # print(\"correlation_x_y\", pi.shape)\n",
    "        # print(\"correlation_x_y[0]\", correlation_x_y[0, :])\n",
    "        correlation_x_z = self.l_correlation_x_z(h)\n",
    "        # print(\"correlation_x_z\", pi.shape)\n",
    "        # print(\"correlation_x_z[0]\", correlation_x_z[0, :])\n",
    "        correlation_y_z = self.l_correlation_y_z(h)\n",
    "        # print(\"correlation_y_z\", pi.shape)\n",
    "        # print(\"correlation_y_z[0]\", correlation_y_z[0, :])\n",
    "        \n",
    "        return pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z\n",
    "\n",
    "model = MDN(3, n_hidden=20, n_gaussians=5)\n",
    "#if torch.cuda.is_available():\n",
    "#    model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss_fn(y, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z):\n",
    "    # print(\"mu_x shape\", mu_x.shape)\n",
    "    # mu = torch.stack((mu_x, mu_y, mu_z), 2)\n",
    "    # print(\"mu shape \", mu.shape)\n",
    "    # print(\"mu[0]\", mu[0])\n",
    "    \n",
    "    ##############################################################################\n",
    "    \n",
    "    size = y.shape[0]\n",
    "    n_gaussians = pi.shape[1]\n",
    "    # print(\"sample size: \", size)\n",
    "    # print(\"num of gaus: \", n_gaussians)\n",
    "    # print(\"correlation size: \", correlation_x_y.shape)\n",
    "    # build mean matrix\n",
    "    mean = torch.zeros([size, n_gaussians, 3])\n",
    "    # build covariance matrix with standard trivariate normal distribution\n",
    "    cov = torch.ones([size, n_gaussians, 3, 3])\n",
    "    cov[:, :, 0, 1] = correlation_x_y\n",
    "    cov[:, :, 1, 0] = correlation_x_y\n",
    "    cov[:, :, 0, 2] = correlation_x_z\n",
    "    cov[:, :, 2, 0] = correlation_x_z\n",
    "    cov[:, :, 1, 2] = correlation_y_z\n",
    "    cov[:, :, 2, 1] = correlation_y_z\n",
    "    \n",
    "    print(\"mean: \", mean.shape)\n",
    "    print(\"cov: \", cov.shape)\n",
    "    \n",
    "    print(\"mean is cuda: \", mean.is_cuda)\n",
    "    print(\" cov is cuda: \", cov.is_cuda)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    w = y[0]**2*(correlation_y_z**2 - 1) +\\\n",
    "        y[1]**2*(correlation_x_z**2 - 1) +\\\n",
    "        y[2]**2*(correlation_x_y**2 - 1) +\\\n",
    "        2*(y[0]*y[:1]*(correlation_x_y - correlation_x_z*correlation_y_z) +\\\n",
    "           y[0]*y[:2]*(correlation_x_z - correlation_x_y*correlation_y_z) +\\\n",
    "           y[1]*y[:2]*(correlation_y_z - correlation_x_y*correlation_x_z))\n",
    "    \n",
    "    return w\n",
    "    '''\n",
    "    '''\n",
    "    cov = torch.Tensor([[sigma_x**2, 0, 0], [0, sigma_y**2, 0], [0, 0, sigma_z**2]])\n",
    "    cov[1, 0] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[2, 0] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[2, 1] = correlation_y_z*sigma_y*sigma_z\n",
    "    cov[0, 1] = correlation_x_y*sigma_x*sigma_y\n",
    "    cov[0, 2] = correlation_x_z*sigma_x*sigma_z\n",
    "    cov[1, 2] = correlation_y_z*sigma_y*sigma_z\n",
    "    '''\n",
    "    m = torch.distributions.multivariate_normal.MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "    # normalize input y\n",
    "    y_repeat = y.unsqueeze(1).repeat(1, n_gaussians, 1)\n",
    "    # print(y_repeat.shape)\n",
    "    # print(y_repeat[0])\n",
    "    mu_all = torch.stack((mu_x, mu_y, mu_z), dim = 2)\n",
    "    print(\"mu_all shape: \", mu_all.shape)\n",
    "    y_sub = y_repeat - mu_all\n",
    "    print(\"y_sub shape\", y_sub.shape)\n",
    "    sigma_all = torch.stack((sigma_x, sigma_y, sigma_z), dim = 2)\n",
    "    print(\"sigma\", sigma_x.shape)\n",
    "    print(\"sigma all\", sigma_all.shape)\n",
    "    \n",
    "    y_normal = torch.div(y_sub, sigma_all)\n",
    "    \n",
    "    print(\"y_normal size\", y_normal.shape)\n",
    "    \n",
    "    # y: [x, y, z]; row: x, y, z; column: N, number of samples\n",
    "    # y: N x 3\n",
    "    # loss: N x 1\n",
    "    print(y_normal.is_cuda)\n",
    "    loss = torch.exp(m.log_prob(y_normal))\n",
    "    print(\"loss shape: \", loss.shape)\n",
    "    print(\"pi shape: \", pi.shape)\n",
    "    loss = torch.sum(loss * pi, dim=1)\n",
    "    loss = -torch.log(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z = model(pd_label_tensor)\n",
    "    loss = mdn_loss_fn(gt_label_tensor, pi, mu_x, mu_y, mu_z, sigma_x, sigma_y, sigma_z, correlation_x_y, correlation_x_z, correlation_y_z)\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # if epoch % 1000 == 0:\n",
    "    #     print(loss.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2,3)\n",
    "print(t)\n",
    "tr = t.repeat(2, 3)\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2],[4,5],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.Tensor([[7,8],[6,4],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(a*b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt[0,0,0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0, 0] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_y_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2939**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.square(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(gt_label_tensor[:,0], gt_label_tensor[:,1], gt_label_tensor[:,2], 'gray')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(pd_label_tensor[:,0], pd_label_tensor[:,1], pd_label_tensor[:,2], 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt_label_tensor[:,2])\n",
    "plt.plot(pd_label_tensor[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model/with_early_stop_after_400/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "\n",
    "\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_1_loss.append(total_test_loss/te    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_1):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)st_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_1, test_set_y_1, test_set_z_1, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find window size 6 is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_1_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input window size\", fontsize=14)\n",
    "plt.ylabel(\"MSE loss\", fontsize=14)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "plt.ylim([0.017,0.0255])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot result without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss = []\n",
    "for i in range(1, 11):\n",
    "    window_size = i\n",
    "    PATH = \"model_v3/with_early_stop_after_400_without_normalization/model_w_\" + str(i) + \".pt\"\n",
    "\n",
    "    test_set_x_1, test_set_y_1, test_set_z_1 = load_test_data('../../../../performance_test/data/test/test_1.csv')\n",
    "    test_set_x_2, test_set_y_2, test_set_z_2 = load_test_data('../../../../performance_test/data/test/test_2.csv')\n",
    "    test_set_x_3, test_set_y_3, test_set_z_3 = load_test_data('../../../../performance_test/data/test/test_3.csv')\n",
    "    test_set_x_4, test_set_y_4, test_set_z_4 = load_test_data('../../../../performance_test/data/test/test_4.csv')\n",
    "    test_set_x_5, test_set_y_5, test_set_z_5 = load_test_data('../../../../performance_test/data/test/test_5.csv')\n",
    "\n",
    "    '''\n",
    "    # do min-max-scaling for each test data set\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_1)\n",
    "    show_statistic(test_set_x_1)\n",
    "    min_max_scaling(test_set_x_2)\n",
    "    min_max_scaling(test_set_x_3)\n",
    "    min_max_scaling(test_set_x_4)\n",
    "    min_max_scaling(test_set_x_5)\n",
    "\n",
    "    min_max_scaling(test_set_y_1)\n",
    "    min_max_scaling(test_set_y_2)\n",
    "    min_max_scaling(test_set_y_3)\n",
    "    min_max_scaling(test_set_y_4)\n",
    "    min_max_scaling(test_set_y_5)\n",
    "\n",
    "    min_max_scaling(test_set_z_1)\n",
    "    min_max_scaling(test_set_z_2)\n",
    "    min_max_scaling(test_set_z_3)\n",
    "    min_max_scaling(test_set_z_4)\n",
    "    min_max_scaling(test_set_z_5)\n",
    "    '''\n",
    "    '''\n",
    "    show_statistic(test_set_x_1)\n",
    "    # do normalization on x of validation test set\n",
    "    normalize_one(test_set_x_1, x_mean, x_std)\n",
    "    show_statistic(test_set_x_1)\n",
    "    normalize_one(test_set_x_2, x_mean, x_std)\n",
    "    normalize_one(test_set_x_3, x_mean, x_std)\n",
    "    normalize_one(test_set_x_4, x_mean, x_std)\n",
    "    normalize_one(test_set_x_5, x_mean, x_std)\n",
    "    # do normalization on y of validation test set\n",
    "    normalize_one(test_set_y_1, y_mean, y_std)\n",
    "    normalize_one(test_set_y_2, y_mean, y_std)\n",
    "    normalize_one(test_set_y_3, y_mean, y_std)\n",
    "    normalize_one(test_set_y_4, y_mean, y_std)\n",
    "    normalize_one(test_set_y_5, y_mean, y_std)\n",
    "    # do normalization on z of validation test set\n",
    "    normalize_one(test_set_z_1, z_mean, z_std)\n",
    "    normalize_one(test_set_z_2, z_mean, z_std)\n",
    "    normalize_one(test_set_z_3, z_mean, z_std)\n",
    "    normalize_one(test_set_z_4, z_mean, z_std)\n",
    "    normalize_one(test_set_z_5, z_mean, z_std)\n",
    "    # show_statistic(train_set_x_1)\n",
    "    '''\n",
    "\n",
    "    test_dataset_1, test_label_1 = construct_test_tensor(test_set_x_1,\n",
    "                                                         test_set_y_1,\n",
    "                                                         test_set_z_1,\n",
    "                                                         window_size)\n",
    "    test_dataset_2, test_label_2 = construct_test_tensor(test_set_x_2,\n",
    "                                                         test_set_y_2,\n",
    "                                                         test_set_z_2,\n",
    "                                                         window_size)\n",
    "    test_dataset_3, test_label_3 = construct_test_tensor(test_set_x_3,\n",
    "                                                         test_set_y_3,\n",
    "                                                         test_set_z_3,\n",
    "                                                         window_size)\n",
    "    test_dataset_4, test_label_4 = construct_test_tensor(test_set_x_4,\n",
    "                                                         test_set_y_4,\n",
    "                                                         test_set_z_4,\n",
    "                                                         window_size)\n",
    "    test_dataset_5, test_label_5 = construct_test_tensor(test_set_x_5,\n",
    "                                                         test_set_y_5,\n",
    "                                                         test_set_z_5,\n",
    "                                                         window_size)\n",
    "\n",
    "    test_set_1 = MyTestDataSet(test_dataset_1, test_label_1)\n",
    "    test_set_2 = MyTestDataSet(test_dataset_2, test_label_2)\n",
    "    test_set_3 = MyTestDataSet(test_dataset_3, test_label_3)\n",
    "    test_set_4 = MyTestDataSet(test_dataset_4, test_label_4)\n",
    "    test_set_5 = MyTestDataSet(test_dataset_5, test_label_5)\n",
    "    print(len(test_set_1))\n",
    "    print(len(test_set_2))\n",
    "    print(len(test_set_3))\n",
    "    print(len(test_set_4))\n",
    "    print(len(test_set_5))\n",
    "    show_statistic(test_set_x_1)\n",
    "    show_statistic(test_set_x_2)\n",
    "    show_statistic(test_set_x_3)\n",
    "    show_statistic(test_set_x_4)\n",
    "    show_statistic(test_set_x_5)\n",
    "\n",
    "\n",
    "    batch_size = 650\n",
    "    test_loader_1 = DataLoader(test_set_1, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_2 = DataLoader(test_set_2, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_3 = DataLoader(test_set_3, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_4 = DataLoader(test_set_4, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_loader_5 = DataLoader(test_set_5, batch_size=batch_size, num_workers=0) # dont shuffle test data for using continous trajectory later on\n",
    "    # test_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    input_dim = 3\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 1\n",
    "    output_dim = 3\n",
    "\n",
    "    load_model = my_model.LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    load_model.load_state_dict(torch.load(PATH))\n",
    "    load_model.eval()\n",
    "    # mmodel = torch.load(PATH)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        load_model.cuda()\n",
    "\n",
    "    # get test results\n",
    "    seq_dim = window_size\n",
    "    input_dim = 3\n",
    "    # test_seq = []\n",
    "    test_predd = []\n",
    "    # test_gt = []\n",
    "    total_test_loss = 0.0\n",
    "    test_batch = 0\n",
    "    for i, (seqs, labels) in enumerate(test_loader_3):\n",
    "        if torch.cuda.is_available():\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim).cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            seqs = Variable(seqs.view(-1, seq_dim, input_dim))\n",
    "\n",
    "        outputs = load_model(seqs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_test_loss += loss.data.item()\n",
    "        test_predd.append(outputs)\n",
    "        test_batch = i + 1\n",
    "\n",
    "    print(total_test_loss/test_batch)\n",
    "    test_3_loss.append(total_test_loss/test_batch)\n",
    "\n",
    "    for i in range(len(test_predd)):\n",
    "        if (i == 0):\n",
    "            pred = test_predd[i].cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = np.append(pred, test_predd[i].cpu().detach().numpy(), axis = 0)\n",
    "\n",
    "    from mpl_toolkits import mplot3d\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    # Data for a three-dimensional line\n",
    "    zline = np.linspace(0, 15, 1000)\n",
    "    xline = np.sin(zline)\n",
    "    yline = np.cos(zline)\n",
    "    ax.plot3D(test_set_x_3, test_set_y_3, test_set_z_3, 'gray')\n",
    "    ax.plot3D(pred[:,0], pred[:,1], pred[:,2], 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(test_3_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "\n",
    "# plt.bar(x_pos, test_1_loss)\n",
    "plt.bar(x_pos, test_3_loss, color=['tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:green',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue',\n",
    "                                   'tab:blue'], zorder = 3)\n",
    "\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "plt.xlabel(\"Input Window Size\", fontsize=12)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "plt.xticks(x_pos, x)\n",
    "\n",
    "# plt.ylim([0.017,0.0255])\n",
    "plt.ylim([0.025,0.039])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
